{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAwdPxXLYzRx"
      },
      "outputs": [],
      "source": [
        "# https://openrouter.ai/settings/keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~ensorflow*\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/tensorflow*\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/keras*\n",
        "!pip install --upgrade --force-reinstall protobuf==3.20.*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "W9x7bwJw2xjf",
        "outputId": "a68f4027-8201-4c25-d738-e94c4f85a0d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires tqdm, which is not installed.\n",
            "kaggle 1.7.4.5 requires tqdm, which is not installed.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow-probability>=0.13.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tqdm>=4.64.1, which is not installed.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "41a6a967f1b848c69c21914a7a41d7b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~qdm*\n"
      ],
      "metadata": {
        "id": "Gl8F0P4M3Her"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35cylDd3Mi9",
        "outputId": "2846cb52-cd3e-46c3-cb01-68bdccfee569"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "3WlsNSphZdsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7be6c9c4-7463-4802-f11d-7efb9bad885d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "yfkUbrGAbC50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8adc8901-d438-4324-e84c-460a9d9dbd85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "yH1N0y8RbSmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "fb853eac-7c5f-4a4b-ed85-1882a5120b06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "4v0uipM7b5lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8bd7be09-0cf8-45aa-ac11-dfa0aeaad1bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jaish19/GenAI---RAG-using-LangChain.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIe8qK4kFlmm",
        "outputId": "b50dd790-8cbd-4561-b126-16520b632749"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GenAI---RAG-using-LangChain' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\") # Replace with your actual OpenRouter API key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Step 2: Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"mistralai/mistral-7b-instruct\",  # or try \"meta-llama/llama-3-8b-instruct\", etc.\n",
        "    temperature=0.7,\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Step 3: Interact with the model (normal chat mode)\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = llm([HumanMessage(content=query)])\n",
        "    print(\"\\nAssistant:\", response.content)\n"
      ],
      "metadata": {
        "id": "ZHYUOORrfR4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a28d2364-b305-42a3-c712-5dc4465f4107"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3908245900.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3908245900.py:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm([HumanMessage(content=query)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:  Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help!\n",
            "\n",
            "You: bye\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA # RAG - To retrieve the contents from the pdfs or any docs\n",
        "from langchain.chains.llm import LLMChain  # It's LangChain function to wrap the tools/agents to create a seemless workflow\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate  # To provide the equipped prompt\n",
        "from langchain_community.document_loaders import PDFPlumberLoader # To read the pdf\n",
        "from langchain_experimental.text_splitter import SemanticChunker # Text chunks\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        " # Embed the texts or contents\n",
        "from langchain_community.vectorstores import FAISS  # Vector DB\n",
        "from google.colab import userdata\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n"
      ],
      "metadata": {
        "id": "LwAeB3X03ozM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\")\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"  # Endpoint of the openRouter"
      ],
      "metadata": {
        "id": "wldmxxtr4DJK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and process PDF\n",
        "loader = PDFPlumberLoader(\"/content/GenAI---RAG-using-LangChain/neural_network.pdf\")\n",
        "docs = loader.load()\n",
        "print(\"Pages loaded:\", len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIJxIGy4FHQ",
        "outputId": "2f493c0c-6df7-415a-b91c-8f1f7d0d3cac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pages loaded: 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-WFpT_B5V55",
        "outputId": "ea2b77ba-0923-4c6d-952c-8c09b2efecff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers protobuf\n",
        "!pip install transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MJTbbGq6Lib",
        "outputId": "bd540b98-429b-4fc1-e0ec-10e38b4fe1ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.55.2\n",
            "Uninstalling transformers-4.55.2:\n",
            "  Successfully uninstalled transformers-4.55.2\n",
            "\u001b[33mWARNING: Skipping protobuf as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers\n",
            "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Using cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.55.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PROCESS 1: CHUNK process\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Semantic splitter with custom\n",
        "# With SemanticChunker (in LangChain), you don't define chunk_size directly like in other splitters\n",
        "# (CharacterTextSplitter, RecursiveCharacterTextSplitter, etc.).\n",
        "# Instead, semantic chunking decides where to split based on semantic similarity between segments, not on fixed sizes.\n",
        "splitter = SemanticChunker(\n",
        "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    ),\n",
        "    breakpoint_threshold_type=\"percentile\", # or standard_deviation\n",
        "    breakpoint_threshold_amount=90, # Value for the threshold (e.g. 95 = 95th percentile distance between embeddings)\n",
        "    buffer_size=1 # (Optional) Adds context from neighboring chunks\n",
        ")\n",
        "\n",
        "# Split document\n",
        "chunks = splitter.split_documents(docs)\n",
        "for chunk in chunks:\n",
        "    print(chunk.page_content)\n",
        "\n",
        "\n",
        "# PROCESS 2: Vector DB\n",
        "embedder = HuggingFaceEmbeddings()\n",
        "vector = FAISS.from_documents(chunks, embedder)\n",
        "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "'''\n",
        "# Try with some queries for similarity search\n",
        "results = retriever.get_relevant_documents(\"What is the remedy for cold and flu?\")\n",
        "\n",
        "for doc in results:\n",
        "    print(doc.page_content)\n",
        "'''\n",
        "\n",
        "# PROCESS 3: LLM MODEL CHOOSING\n",
        "# Use ChatOpenAI with OpenRouter backend\n",
        "llm = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3-8b-instruct\",   #\"mistralai/mistral-7b-instruct\"\n",
        "    temperature=1.3, # It's a less temp -> so it'll give you some relative information about the content.\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        ")\n",
        "\n",
        "# PROCESS 4: PROMPT CREATION\n",
        "prompt = \"\"\"\n",
        "You are a helpful assistant.\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Answer only using the context and be concise (3â€“4 sentences).\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "\n",
        "# PROCESS 5: LANGCHAIN CREATION - LLM & PROMPT\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=True)\n",
        "\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "\n",
        "# DOCUMENT CHAIN PROCESS\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        "    callbacks=None,\n",
        ")\n",
        "\n",
        "# PROCESS 6: WRAPPING ALL TOGETHER ALONG WITH RETRIEVER\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Example query\n"
      ],
      "metadata": {
        "id": "6DWOwM-JY03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b5ba9063-2a9f-4a73-c278-d021a45c786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Networks and Deep Learning\n",
            "MichaelNielsen\n",
            "Theoriginalonlinebookcanbefoundat\n",
            "http://neuralnetworksanddeeplearning.com\n",
            "\n",
            "\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) i\n",
            "(cid:12)\n",
            "Contents\n",
            "Whatthisbookisabout iii\n",
            "Ontheexercisesandproblems v\n",
            "1 Usingneuralnetstorecognizehandwrittendigits 1\n",
            "1.1 Perceptrons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 2\n",
            "1.2 Sigmoidneurons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 7\n",
            "1.3 Thearchitectureofneuralnetworks.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 10\n",
            "1.4 Asimplenetworktoclassifyhandwrittendigits. .\n",
            ". . . . . . . . . . . . . . . .\n",
            ". 12\n",
            "1.5 Learningwithgradientdescent. .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 15\n",
            "1.6 Implementingournetworktoclassifydigits . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 24\n",
            "1.7 Towarddeeplearning .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 35\n",
            "2 Howthebackpropagationalgorithmworks 39\n",
            "2.1 Warmup:afastmatrix-basedapproachtocomputingtheoutputfromaneural\n",
            "network . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 40\n",
            "2.2 Thetwoassumptionsweneedaboutthecostfunction .\n",
            ".\n",
            ". . . . . . . . . . .\n",
            ". 42\n",
            "2.3 TheHadamardproduct,s t .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 43\n",
            "2.4 Thefourfundamentalequ(cid:12)ationsbehindbackpropagation .\n",
            ".\n",
            ". . . . . . . . .\n",
            ". 43\n",
            "2.5 Proofofthefourfundamentalequations(optional) .\n",
            ".\n",
            ". . . . . . . . . . . . .\n",
            ". 48\n",
            "2.6 Thebackpropagationalgorithm . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 49\n",
            "2.7 Thecodeforbackpropagation . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 50\n",
            "2.8 Inwhatsenseisbackpropagationafastalgorithm? .\n",
            ". . . . . . . . . . . . . .\n",
            ". 52\n",
            "2.9 Backpropagation: thebigpicture .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 53\n",
            "3 Improvingthewayneuralnetworkslearn 59\n",
            "3.1 Thecross-entropycostfunction . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 60\n",
            "3.1.1 Introducingthecross-entropycostfunction . .\n",
            ". . . . . . . . . . . . .\n",
            ". 62\n",
            "3.1.2 Usingthecross-entropytoclassifyMNISTdigits. .\n",
            ". . . . . . . . . . .\n",
            ".\n",
            "67\n",
            "3.1.3 Whatdoesthecross-entropymean?\n",
            "Wheredoesitcomefrom?\n",
            ".\n",
            ". .\n",
            ". 68\n",
            "3.1.4 Softmax . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 70\n",
            "3.2 Overfittingandregularization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 73\n",
            "3.2.1 Regularization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 78\n",
            "3.2.2 Whydoesregularizationhelpreduceoverfitting? .\n",
            ". . . . . . . . . . .\n",
            ". 83\n",
            "3.2.3 Othertechniquesforregularization . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 87\n",
            "3.3 Weightinitialization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 94\n",
            "3.4 Handwritingrecognitionrevisited: thecode. .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 98\n",
            "3.5 Howtochooseaneuralnetworkâ€™shyper-parameters?. .\n",
            ". . . . . . . . . . . .\n",
            ". 107\n",
            "3.6 Othertechniques .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "118\n",
            "\n",
            "(cid:12)\n",
            "ii (cid:12) Contents\n",
            "(cid:12)\n",
            "3.6.1 Variationsonstochasticgradientdescent . .\n",
            ". . . . . . . . . . . . . . .\n",
            ". 118\n",
            "4 Avisualproofthatneuralnetscancomputeanyfunction 127\n",
            "4.1 Twocaveats .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 129\n",
            "4.2 Universalitywithoneinputandoneoutput . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 130\n",
            "4.3 Manyinputvariables .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 139\n",
            "4.4 Extensionbeyondsigmoidneurons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 146\n",
            "4.5 Fixingupthestepfunctions.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "148\n",
            "5 Whyaredeepneuralnetworkshardtotrain?\n",
            "151\n",
            "5.1 Thevanishinggradientproblem .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "154\n",
            "5.2 Whatâ€™scausingthevanishinggradientproblem? Unstablegradientsindeep\n",
            "neuralnets .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 159\n",
            "5.3 Unstablegradientsinmorecomplexnetworks . .\n",
            ". . . . . . . . . . . . . . . .\n",
            ". 163\n",
            "5.4 Otherobstaclestodeeplearning .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 164\n",
            "6 Deeplearning 167\n",
            "6.1 Introducingconvolutionalnetworks. .\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 169\n",
            "6.2 Convolutionalneuralnetworksinpractice . .\n",
            ". . . . . . . . . . . . . . . . . . .\n",
            ". 176\n",
            "6.3 Thecodeforourconvolutionalnetworks. .\n",
            ". . . . . . . . . . . . . . . . . . . .\n",
            ". 185\n",
            "6.4 Recentprogressinimagerecognition . .\n",
            ". . . . . . . . . . . . . . . . . . . . . .\n",
            ". 196\n",
            "6.5 Otherapproachestodeepneuralnets.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . .\n",
            ". 202\n",
            "6.6 Onthefutureofneuralnetworks . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "205\n",
            "A Isthereasimplealgorithmforintelligence?\n",
            "211\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) iii\n",
            "(cid:12)\n",
            "What this book is about\n",
            "Neuralnetworksareoneofthemostbeautifulprogrammingparadigmseverinvented. In\n",
            "theconventionalapproachtoprogramming,wetellthecomputerwhattodo,breakingbig\n",
            "problemsupintomanysmall,preciselydefinedtasksthatthecomputercaneasilyperform.\n",
            "Bycontrast,inaneuralnetworkwedonâ€™ttellthecomputerhowtosolveourproblem. Instead,\n",
            "itlearnsfromobservationaldata,figuringoutitsownsolutiontotheproblemathand. Automaticallylearningfromdatasoundspromising. However, until2006wedidnâ€™t\n",
            "know how to train neural networks to surpass more traditional approaches, except for\n",
            "afewspecializedproblems. Whatchangedin2006wasthediscoveryoftechniquesfor\n",
            "learning in so-called deep neural networks. These techniques are now known as deep\n",
            "learning. Theyâ€™vebeendevelopedfurther,andtodaydeepneuralnetworksanddeeplearning\n",
            "achieveoutstandingperformanceonmanyimportantproblemsincomputervision,speech\n",
            "recognition,andnaturallanguageprocessing. Theyâ€™rebeingdeployedonalargescaleby\n",
            "companiessuchasGoogle,Microsoft,andFacebook. Thepurposeofthisbookistohelpyoumasterthecoreconceptsofneuralnetworks,\n",
            "includingmoderntechniquesfordeeplearning. Afterworkingthroughthebookyouwill\n",
            "havewrittencodethatusesneuralnetworksanddeeplearningtosolvecomplexpattern\n",
            "recognitionproblems. Andyouwillhaveafoundationtouseneuralnetworksanddeep\n",
            "learningtoattackproblemsofyourowndevising. A principle-oriented approach\n",
            "Oneconvictionunderlyingthebookisthatitâ€™sbettertoobtainasolidunderstandingofthe\n",
            "coreprinciplesofneuralnetworksanddeeplearning,ratherthanahazyunderstanding\n",
            "ofalonglaundrylistofideas.\n",
            "Ifyouâ€™veunderstoodthecoreideaswell,youcanrapidly\n",
            "understandothernewmaterial. Inprogramminglanguageterms,thinkofitasmastering\n",
            "thecoresyntax,librariesanddatastructuresofanewlanguage. Youmaystillonlyâ€œknowâ€a\n",
            "tinyfractionofthetotallanguageâ€“manylanguageshaveenormousstandardlibrariesâ€“but\n",
            "newlibrariesanddatastructurescanbeunderstoodquicklyandeasily. Thismeansthebookisemphaticallynotatutorialinhowtousesomeparticularneural\n",
            "networklibrary.\n",
            "Ifyoumostlywanttolearnyourwayaroundalibrary,donâ€™treadthisbook! Findthelibraryyouwishtolearn,andworkthroughthetutorialsanddocumentation. But\n",
            "bewarned. Whilethishasanimmediateproblem-solvingpayoff,ifyouwanttounderstand\n",
            "whatâ€™sreallygoingoninneuralnetworks,ifyouwantinsightsthatwillstillberelevant\n",
            "yearsfromnow,thenitâ€™snotenoughjusttolearnsomehotlibrary. Youneedtounderstand\n",
            "thedurable,lastinginsightsunderlyinghowneuralnetworkswork. Technologiescomeand\n",
            "technologiesgo,butinsightisforever. \n",
            "(cid:12)\n",
            "iv (cid:12) Whatthisbookisabout\n",
            "(cid:12)\n",
            "A hands-on approach\n",
            "Weâ€™lllearnthecoreprinciplesbehindneuralnetworksanddeeplearningbyattackinga\n",
            "concreteproblem: theproblemofteachingacomputertorecognizehandwrittendigits. This\n",
            "problemisextremelydifficulttosolveusingtheconventionalapproachtoprogramming. Andyet,asweâ€™llsee,itcanbesolvedprettywellusingasimpleneuralnetwork,withjusta\n",
            "fewtensoflinesofcode,andnospeciallibraries. Whatâ€™smore,weâ€™llimprovetheprogram\n",
            "throughmanyiterations,graduallyincorporatingmoreandmoreofthecoreideasabout\n",
            "neuralnetworksanddeeplearning. Thishands-onapproachmeansthatyouâ€™llneedsomeprogrammingexperiencetoread\n",
            "thebook.\n",
            "Butyoudonâ€™tneedtobeaprofessionalprogrammer. Iâ€™vewrittenthecodeinPython\n",
            "(version2.7),which,evenifyoudonâ€™tprograminPython,shouldbeeasytounderstandwith\n",
            "justalittleeffort. Throughthecourseofthebookwewilldevelopalittleneuralnetwork\n",
            "library,whichyoucanusetoexperimentandtobuildunderstanding.\n",
            "Allthecodeisavailable\n",
            "fordownloadhere. Onceyouâ€™vefinishedthebook,orasyoureadit,youcaneasilypickup\n",
            "oneofthemorefeature-completeneuralnetworklibrariesintendedforuseinproduction. Onarelatednote,themathematicalrequirementstoreadthebookaremodest. There\n",
            "issomemathematicsinmostchapters,butitâ€™susuallyjustelementaryalgebraandplotsof\n",
            "functions,whichIexpectmostreaderswillbeokaywith. Ioccasionallyusemoreadvanced\n",
            "mathematics,buthavestructuredthematerialsoyoucanfollowevenifsomemathematical\n",
            "detailseludeyou. TheonechapterwhichusesheaviermathematicsextensivelyisChapter2,\n",
            "whichrequiresalittlemultivariablecalculusandlinearalgebra. Ifthosearenâ€™tfamiliar,I\n",
            "beginChapter2withadiscussionofhowtonavigatethemathematics. Ifyouâ€™refindingit\n",
            "reallyheavygoing,youcansimplyskiptothesummaryofthechapterâ€™smainresults. Inany\n",
            "case,thereâ€™snoneedtoworryaboutthisattheoutset. Itâ€™srareforabooktoaimtobebothprinciple-orientedandhands-on. ButIbelieve\n",
            "youâ€™lllearnbestifwebuildoutthefundamentalideasofneuralnetworks. Weâ€™lldevelop\n",
            "livingcode,notjustabstracttheory,codewhichyoucanexploreandextend. Thiswayyouâ€™ll\n",
            "understandthefundamentals,bothintheoryandpractice,andbewellsettoaddfurtherto\n",
            "yourknowledge. \n",
            "(cid:12)\n",
            "(cid:12) v\n",
            "(cid:12)\n",
            "On the exercises and problems\n",
            "Itâ€™snotuncommonfortechnicalbookstoincludeanadmonitionfromtheauthorthatreaders\n",
            "mustdotheexercisesandproblems. IalwaysfeelalittlepeculiarwhenIreadsuchwarnings.\n",
            "WillsomethingbadhappentomeifIdonâ€™tdotheexercisesandproblems? Ofcoursenot. Iâ€™llgainsometime,butattheexpenseofdepthofunderstanding. Sometimesthatâ€™sworthit. Sometimesitâ€™snot. Sowhatâ€™sworthdoinginthisbook? Myadviceisthatyoureallyshouldattemptmostof\n",
            "theexercises,andyoushouldaimnottodomostoftheproblems. Youshoulddomostoftheexercisesbecausetheyâ€™rebasicchecksthatyouâ€™veunderstood\n",
            "thematerial. Ifyoucanâ€™tsolveanexerciserelativelyeasily,youâ€™veprobablymissedsomething\n",
            "fundamental. Ofcourse,ifyoudogetstuckonanoccasionalexercise,justmoveonâ€“chances\n",
            "areitâ€™sjustasmallmisunderstandingonyourpart,ormaybeIâ€™vewordedsomethingpoorly. Butifmostexercisesareastruggle,thenyouprobablyneedtorereadsomeearliermaterial. Theproblemsareanothermatter. Theyâ€™remoredifficultthantheexercises,andyouâ€™ll\n",
            "likelystruggletosolvesomeproblems. Thatâ€™sannoying,but,ofcourse,patienceintheface\n",
            "ofsuchfrustrationistheonlywaytotrulyunderstandandinternalizeasubject. With that said, I donâ€™t recommend working through all the problems.\n",
            "Whatâ€™s even\n",
            "betteristofindyourownproject. Maybeyouwanttouseneuralnetstoclassifyyourmusic\n",
            "collection. Ortopredictstockprices. Orwhatever.\n",
            "Butfindaprojectyoucareabout. Then\n",
            "youcanignoretheproblemsinthebook,orusethemsimplyasinspirationforworkonyour\n",
            "ownproject. Strugglingwithaprojectyoucareaboutwillteachyoufarmorethanworking\n",
            "throughanynumberofsetproblems. Emotionalcommitmentisakeytoachievingmastery. Ofcourse,youmaynothavesuchaprojectinmind,atleastupfront. Thatâ€™sfine. Work\n",
            "throughthoseproblemsyoufeelmotivatedtoworkon. Andusethematerialinthebookto\n",
            "helpyousearchforideasforcreativepersonalprojects. \n",
            "(cid:12)\n",
            "vi (cid:12) Ontheexercisesandproblems\n",
            "(cid:12)\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 1\n",
            "(cid:12)\n",
            "1111\n",
            "Using neural nets to recognize\n",
            "handwritten digits\n",
            "1\n",
            "Thehumanvisualsystemisoneofthewondersoftheworld. Considerthefollowingsequence\n",
            "ofhandwrittendigits:\n",
            "Mostpeopleeffortlesslyrecognizethosedigitsas504192. Thateaseisdeceptive. Ineach\n",
            "hemisphereofourbrain,humanshaveaprimaryvisualcortex,alsoknownasV ,containing\n",
            "1\n",
            "140millionneurons,withtensofbillionsofconnectionsbetweenthem. Andyethuman\n",
            "visioninvolvesnotjustV ,butanentireseriesofvisualcorticesâ€“V ,V ,V ,andV â€“doing\n",
            "1 2 3 4 5\n",
            "progressivelymorecompleximageprocessing. Wecarryinourheadsasupercomputer,tuned\n",
            "byevolutionoverhundredsofmillionsofyears,andsuperblyadaptedtounderstandthe\n",
            "visualworld. Recognizinghandwrittendigitsisnâ€™teasy. Rather,wehumansarestupendously,\n",
            "astoundinglygoodatmakingsenseofwhatoureyesshowus. Butnearlyallthatworkis\n",
            "doneunconsciously. Andsowedonâ€™tusuallyappreciatehowtoughaproblemourvisual\n",
            "systemssolve. Thedifficultyofvisualpatternrecognitionbecomesapparentifyouattempttowrite\n",
            "acomputerprogramtorecognizedigitslikethoseabove.\n",
            "Whatseemseasywhenwedoit\n",
            "ourselvessuddenlybecomesextremelydifficult. Simpleintuitionsabouthowwerecognize\n",
            "shapesâ€“â€œa9hasaloopatthetop,andaverticalstrokeinthebottomrightâ€â€“turnoutto\n",
            "benotsosimpletoexpressalgorithmically. Whenyoutrytomakesuchrulesprecise,you\n",
            "quicklygetlostinamorassofexceptionsandcaveatsandspecialcases.\n",
            "Itseemshopeless. Neuralnetworksapproachtheprobleminadifferentway. Theideaistotakealarge\n",
            "numberofhandwrittendigits,knownastrainingexamples,\n",
            "\n",
            "(cid:12)\n",
            "2 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "andthendevelopasystemwhichcanlearnfromthosetrainingexamples. Inotherwords,the\n",
            "neuralnetworkusestheexamplestoautomaticallyinferrulesforrecognizinghandwritten\n",
            "digits. Furthermore,byincreasingthenumberoftrainingexamples,thenetworkcanlearn\n",
            "moreabouthandwriting,andsoimproveitsaccuracy. SowhileIâ€™veshownjust100training\n",
            "digitsabove,perhapswecouldbuildabetterhandwritingrecognizerbyusingthousandsor\n",
            "evenmillionsorbillionsoftrainingexamples. Inthischapterweâ€™llwriteacomputerprogramimplementinganeuralnetworkthat\n",
            "learnstorecognizehandwrittendigits. Theprogramisjust74lineslong,andusesnospecial\n",
            "neuralnetworklibraries. Butthisshortprogramcanrecognizedigitswithanaccuracyover\n",
            "96percent,withouthumanintervention.\n",
            "Furthermore,inlaterchaptersweâ€™lldevelopideas\n",
            "whichcanimproveaccuracytoover99percent.\n",
            "Infact,thebestcommercialneuralnetworks\n",
            "arenowsogoodthattheyareusedbybankstoprocesscheques, andbypostofficesto\n",
            "recognizeaddresses. Weâ€™refocusingonhandwritingrecognitionbecauseitâ€™sanexcellentprototypeproblemfor\n",
            "learningaboutneuralnetworksingeneral. Asaprototypeithitsasweetspot:itâ€™schallenging\n",
            "â€“itâ€™snosmallfeattorecognizehandwrittendigitsâ€“butitâ€™snotsodifficultastorequirean\n",
            "extremelycomplicatedsolution,ortremendouscomputationalpower. Furthermore,itâ€™sa\n",
            "greatwaytodevelopmoreadvancedtechniques,suchasdeeplearning. Andsothroughout\n",
            "thebookweâ€™llreturnrepeatedlytotheproblemofhandwritingrecognition. Laterinthe\n",
            "book,weâ€™lldiscusshowtheseideasmaybeappliedtootherproblemsincomputervision,\n",
            "andalsoinspeech,naturallanguageprocessing,andotherdomains. Ofcourse,ifthepointofthechapterwasonlytowriteacomputerprogramtorecognize\n",
            "handwrittendigits,thenthechapterwouldbemuchshorter! Butalongthewayweâ€™lldevelop\n",
            "manykeyideasaboutneuralnetworks,includingtwoimportanttypesofartificialneuron\n",
            "(theperceptronandthesigmoidneuron),andthestandardlearningalgorithmforneural\n",
            "networks,knownasstochasticgradientdescent. Throughout,Ifocusonexplainingwhy\n",
            "thingsaredonethewaytheyare,andonbuildingyourneuralnetworksintuition. That\n",
            "requiresalengthierdiscussionthanifIjustpresentedthebasicmechanicsofwhatâ€™sgoingon,\n",
            "butitâ€™sworthitforthedeeperunderstandingyouâ€™llattain. Amongstthepayoffs,bytheend\n",
            "ofthechapterweâ€™llbeinpositiontounderstandwhatdeeplearningis,andwhyitmatters. 1.1 Perceptrons\n",
            "Whatisaneuralnetwork? Togetstarted,Iâ€™llexplainatypeofartificialneuroncalleda\n",
            "perceptron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank\n",
            "\n",
            "(cid:12)\n",
            "1.1. Perceptrons (cid:12) 3\n",
            "(cid:12)\n",
            "Rosenblatt,inspiredbyearlierworkbyWarrenMcCullochandWalterPitts. Today,itâ€™smore 1\n",
            "commontouseothermodelsofartificialneuronsâ€“inthisbook,andinmuchmodernwork\n",
            "onneuralnetworks,themainneuronmodelusedisonecalledthesigmoidneuron. Weâ€™llget\n",
            "tosigmoidneuronsshortly. Buttounderstandwhysigmoidneuronsaredefinedtheway\n",
            "theyare,itâ€™sworthtakingthetimetofirstunderstandperceptrons. Sohowdoperceptronswork? Aperceptrontakesseveralbinaryinputs, x ,x ,...,and\n",
            "1 2\n",
            "producesasinglebinaryoutput:\n",
            "In the example shown the perceptron has three inputs, x , x , x . In general it could\n",
            "1 2 3\n",
            "havemoreorfewerinputs. Rosenblattproposedasimpleruletocomputetheoutput. He\n",
            "introducedweights,w ,w ,...,realnumbersexpressingtheimportanceoftherespective\n",
            "1 2\n",
            "inputstotheoutput. Theneuronâ€™soutput,0or1,isdeterminedbywhethertheweighted\n",
            "(cid:80)\n",
            "sum w x islessthanorgreaterthansomethresholdvalue. Justliketheweights,the\n",
            "j j j\n",
            "thresholdisarealnumberwhichisaparameteroftheneuron. Toputitinmoreprecise\n",
            "algebraicterms:\n",
            "(cid:168) (cid:80)\n",
            "0 if w x threshold\n",
            "output=\n",
            "1 if\n",
            "(cid:80) j\n",
            "w\n",
            "j\n",
            "x\n",
            "j >â‰¤\n",
            "threshold\n",
            "(1.1)\n",
            "j j j\n",
            "Thatâ€™sallthereistohowaperceptronworks! Thatâ€™sthebasicmathematicalmodel.\n",
            "Awayyoucanthinkabouttheperceptronisthat\n",
            "itâ€™sadevicethatmakesdecisionsbyweighingupevidence. Letmegiveanexample. Itâ€™s\n",
            "notaveryrealisticexample,butitâ€™seasytounderstand,andweâ€™llsoongettomorerealistic\n",
            "examples. Supposetheweekendiscomingup,andyouâ€™veheardthatthereâ€™sgoingtobea\n",
            "cheesefestivalinyourcity. Youlikecheese,andaretryingtodecidewhetherornottogoto\n",
            "thefestival.\n",
            "Youmightmakeyourdecisionbyweighingupthreefactors:\n",
            "1. Istheweathergood? 2. Doesyourboyfriendorgirlfriendwanttoaccompanyyou? 3.\n",
            "Isthefestivalnearpublictransit?\n",
            "(Youdonâ€™townacar). Wecanrepresentthesethreefactorsbycorrespondingbinaryvariables x , x and x . For\n",
            "1 2 3\n",
            "instance,weâ€™dhavex 1=1iftheweatherisgood,andx 1=0iftheweatherisbad. Similarly,\n",
            "x 2=1ifyourboyfriendorgirlfriendwantstogo,and x 2=0ifnot. Andsimilarlyagainfor\n",
            "x andpublictransit. 3\n",
            "Now,supposeyouabsolutelyadorecheese,somuchsothatyouâ€™rehappytogotothe\n",
            "festivalevenifyourboyfriendorgirlfriendisuninterestedandthefestivalishardtogetto. Butperhapsyoureallyloathebadweather,andthereâ€™snowayyouâ€™dgotothefestivalif\n",
            "theweatherisbad. Youcanuseperceptronstomodelthiskindofdecision-making. One\n",
            "waytodothisistochooseaweightw\n",
            "1\n",
            "=6fortheweather,andw\n",
            "2\n",
            "=2andw\n",
            "3\n",
            "=2for\n",
            "theotherconditions. Thelargervalueofw indicatesthattheweathermattersalottoyou,\n",
            "1\n",
            "muchmorethanwhetheryourboyfriendorgirlfriendjoinsyou,orthenearnessofpublic\n",
            "transit. Finally,supposeyouchooseathresholdof5fortheperceptron. Withthesechoices,\n",
            "theperceptronimplementsthedesireddecision-makingmodel,outputting1wheneverthe\n",
            "\n",
            "(cid:12)\n",
            "4 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 weatherisgood,and0whenevertheweatherisbad. Itmakesnodifferencetotheoutput\n",
            "whetheryourboyfriendorgirlfriendwantstogo,orwhetherpublictransitisnearby.\n",
            "Byvaryingtheweightsandthethreshold,wecangetdifferentmodelsofdecision-making. Forexample,supposeweinsteadchoseathresholdof3. Thentheperceptronwoulddecide\n",
            "thatyoushouldgotothefestivalwhenevertheweatherwasgoodorwhenboththefestival\n",
            "wasnearpublictransitandyourboyfriendorgirlfriendwaswillingtojoinyou. Inother\n",
            "words,itâ€™dbeadifferentmodelofdecision-making. Droppingthethresholdmeansyouâ€™re\n",
            "morewillingtogotothefestival. Obviously,theperceptronisnâ€™tacompletemodelofhumandecision-making!\n",
            "Butwhat\n",
            "theexampleillustratesishowaperceptroncanweighupdifferentkindsofevidenceinorder\n",
            "tomakedecisions. Anditshouldseemplausiblethatacomplexnetworkofperceptrons\n",
            "couldmakequitesubtledecisions:\n",
            "Inthisnetwork,thefirstcolumnofperceptronsâ€“whatweâ€™llcallthefirstlayerofperceptrons\n",
            "â€“ismakingthreeverysimpledecisions,byweighingtheinputevidence. Whataboutthe\n",
            "perceptronsinthesecondlayer? Eachofthoseperceptronsismakingadecisionbyweighing\n",
            "uptheresultsfromthefirstlayerofdecision-making. Inthiswayaperceptroninthesecond\n",
            "layercanmakeadecisionatamorecomplexandmoreabstractlevelthanperceptronsin\n",
            "thefirstlayer. Andevenmorecomplexdecisionscanbemadebytheperceptroninthethird\n",
            "layer. Inthisway,amany-layernetworkofperceptronscanengageinsophisticateddecision\n",
            "making. Incidentally,whenIdefinedperceptronsIsaidthataperceptronhasjustasingleoutput. Inthenetworkabovetheperceptronslookliketheyhavemultipleoutputs. Infact,theyâ€™re\n",
            "stillsingleoutput. Themultipleoutputarrowsaremerelyausefulwayofindicatingthatthe\n",
            "outputfromaperceptronisbeingusedastheinputtoseveralotherperceptrons. Itâ€™sless\n",
            "unwieldythandrawingasingleoutputlinewhichthensplits.\n",
            "Letâ€™ssimplifythewaywedescribeperceptrons. Thecondition (cid:80) w x >thresholdis\n",
            "j j j\n",
            "cumbersome, and we can make two notational changes to simplify it. The first change\n",
            "(cid:80) (cid:80)\n",
            "istowrite j w j x j asadotproduct, w x = j w j x j ,wherewand x arevectorswhose\n",
            "componentsaretheweightsandinputsÂ·,respectively. Thesecondchangeistomovethe\n",
            "threshold to the other side of the inequality, and to replace it by whatâ€™s known as the\n",
            "perceptronâ€™sbias,b threshold. Usingthebiasinsteadofthethreshold,theperceptron\n",
            "rulecanberewrittenâ‰¡:âˆ’\n",
            "(cid:168)\n",
            "0 if w x+b 0\n",
            "output=\n",
            "1 if w\n",
            "Â·\n",
            "x+b\n",
            "â‰¤>0 (1.2)\n",
            "Â·\n",
            "Youcanthinkofthebiasasameasureofhoweasyitistogettheperceptrontooutput\n",
            "a1. Ortoputitinmorebiologicalterms, thebiasisameasureofhoweasyitistoget\n",
            "\n",
            "(cid:12)\n",
            "1.1. Perceptrons (cid:12) 5\n",
            "(cid:12)\n",
            "theperceptrontofire. Foraperceptronwithareallybigbias,itâ€™sextremelyeasyforthe 1\n",
            "perceptrontooutputa1. Butifthebiasisverynegative,thenitâ€™sdifficultfortheperceptron\n",
            "tooutputa1. Obviously,introducingthebiasisonlyasmallchangeinhowwedescribe\n",
            "perceptrons,butweâ€™llseelaterthatitleadstofurthernotationalsimplifications. Becauseof\n",
            "this,intheremainderofthebookwewonâ€™tusethethreshold,weâ€™llalwaysusethebias. Iâ€™vedescribedperceptronsasamethodforweighingevidencetomakedecisions. Another\n",
            "wayperceptronscanbeusedistocomputetheelementarylogicalfunctionsweusuallythink\n",
            "ofasunderlyingcomputation,functionssuchasAND,OR,andNAND.Forexample,suppose\n",
            "wehaveaperceptronwithtwoinputs,eachwithweightâ€“2,andanoverallbiasof3. Hereâ€™s\n",
            "ourperceptron:\n",
            "Thenweseethatinput00producesoutput1,since( 2) 0+( 2) 0+3=3ispositive. Here,Iâ€™veintroducedthe symboltomakethemultiplâˆ’icatiâˆ—onsexâˆ’pliciâˆ—t. Similarcalculations\n",
            "showthattheinputs01anâˆ—d10produceoutput1. Buttheinput11producesoutput0,since\n",
            "( 2) 1+( 2) 1+3= 1isnegative.\n",
            "AndsoourperceptronimplementsaNANDgate! âˆ’ Tâˆ—he NAâˆ’NDâˆ—exampleâˆ’shows that we can use perceptrons to compute simple logical\n",
            "functions. Infact,wecanusenetworksofperceptronstocomputeanylogicalfunctionat\n",
            "all. ThereasonisthattheNANDgateisuniversalforcomputation,thatis,wecanbuildany\n",
            "computationupoutofNANDgates. Forexample,wecanuseNANDgatestobuildacircuit\n",
            "(cid:76)\n",
            "whichaddstwobits, x and x . Thisrequirescomputingthebitwisesum, x x ,aswell\n",
            "1 2 1 2\n",
            "asacarrybitwhichissetto1whenbothx andx are1,i.e.,thecarrybitisjustthebitwise\n",
            "1 2\n",
            "product x x :\n",
            "1 2\n",
            "TogetanequivalentnetworkofperceptronswereplacealltheNANDgatesbyperceptrons\n",
            "withtwoinputs,eachwithweightâ€“2,andanoverallbiasof3. Hereâ€™stheresultingnetwork.\n",
            "NotethatIâ€™vemovedtheperceptroncorrespondingtothebottomrightNANDgatealittle,\n",
            "justtomakeiteasiertodrawthearrowsonthediagram:\n",
            "\n",
            "(cid:12)\n",
            "6 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 Onenotableaspectofthisnetworkofperceptronsisthattheoutputfromtheleftmostper-\n",
            "ceptronisusedtwiceasinputtothebottommostperceptron. WhenIdefinedtheperceptron\n",
            "modelIdidnâ€™tsaywhetherthiskindofdouble-output-to-the-same-placewasallowed.\n",
            "Actu-\n",
            "ally,itdoesnâ€™tmuchmatter.\n",
            "Ifwedonâ€™twanttoallowthiskindofthing,thenitâ€™spossible\n",
            "tosimplymergethetwolines,intoasingleconnectionwithaweightofâ€“4insteadoftwo\n",
            "connectionswithâ€“2weights. (Ifyoudonâ€™tfindthisobvious,youshouldstopandproveto\n",
            "yourselfthatthisisequivalent.) Withthatchange,thenetworklooksasfollows,withall\n",
            "unmarkedweightsequaltoâ€“2,allbiasesequalto3,andasingleweightofâ€“4,asmarked:\n",
            "UptonowIâ€™vebeendrawinginputslike x and x asvariablesfloatingtotheleftofthe\n",
            "1 2\n",
            "networkofperceptrons. Infact,itâ€™sconventionaltodrawanextralayerofperceptronsâ€“the\n",
            "inputlayerâ€“toencodetheinputs:\n",
            "Thisnotationforinputperceptrons,inwhichwehaveanoutput,butnoinputs,\n",
            "isashorthand. Itdoesnâ€™tactuallymeanaperceptronwithnoinputs. Toseethis,suppose\n",
            "(cid:80)\n",
            "wedidhaveaperceptronwithnoinputs. Thentheweightedsum w x wouldalwaysbe\n",
            "j j j\n",
            "zero,andsotheperceptronwouldoutput1if b>0,and0if b 0. Thatis,theperceptron\n",
            "wouldsimplyoutputafixedvalue,notthedesiredvalue(x ,â‰¤intheexampleabove). Itâ€™s\n",
            "1\n",
            "bettertothinkoftheinputperceptronsasnotreallybeingperceptronsatall,butrather\n",
            "specialunitswhicharesimplydefinedtooutputthedesiredvalues, x ,x ,.... 1 2\n",
            "Theadderexampledemonstrateshowanetworkofperceptronscanbeusedtosimulatea\n",
            "circuitcontainingmanyNANDgates. AndbecauseNANDgatesareuniversalforcomputation,\n",
            "itfollowsthatperceptronsarealsouniversalforcomputation. Thecomputationaluniversalityofperceptronsissimultaneouslyreassuringanddisap-\n",
            "pointing. Itâ€™sreassuringbecauseittellsusthatnetworksofperceptronscanbeaspowerfulas\n",
            "\n",
            "(cid:12)\n",
            "1.2. Sigmoidneurons (cid:12) 7\n",
            "(cid:12)\n",
            "anyothercomputingdevice. Butitâ€™salsodisappointing,becauseitmakesitseemasthough 1\n",
            "perceptronsaremerelyanewtypeofNANDgate.\n",
            "Thatâ€™shardlybignews!\n",
            "However,thesituationisbetterthanthisviewsuggests. Itturnsoutthatwecandevise\n",
            "learning algorithms which can automatically tune the weights and biases of a network\n",
            "ofartificialneurons. Thistuninghappensinresponsetoexternalstimuli,withoutdirect\n",
            "interventionbyaprogrammer. Theselearningalgorithmsenableustouseartificialneurons\n",
            "inawaywhichisradicallydifferenttoconventionallogicgates. Insteadofexplicitlylaying\n",
            "outacircuitofNANDandothergates,ourneuralnetworkscansimplylearntosolveproblems,\n",
            "sometimesproblemswhereitwouldbeextremelydifficulttodirectlydesignaconventional\n",
            "circuit. 1.2 Sigmoid neurons\n",
            "Learningalgorithmssoundterrific. Buthowcanwedevisesuchalgorithmsforaneural\n",
            "network? Supposewehaveanetworkofperceptronsthatweâ€™dliketousetolearntosolve\n",
            "someproblem. Forexample,theinputstothenetworkmightbetherawpixeldatafrom\n",
            "ascanned,handwrittenimageofadigit. Andweâ€™dlikethenetworktolearnweightsand\n",
            "biasessothattheoutputfromthenetworkcorrectlyclassifiesthedigit. Toseehowlearning\n",
            "mightwork,supposewemakeasmallchangeinsomeweight(orbias)inthenetwork. What\n",
            "weâ€™dlikeisforthissmallchangeinweighttocauseonlyasmallcorrespondingchangein\n",
            "theoutputfromthenetwork. Asweâ€™llseeinamoment,thispropertywillmakelearning\n",
            "possible. Schematically,hereâ€™swhatwewant(obviouslythisnetworkistoosimpletodo\n",
            "handwritingrecognition!):\n",
            "Ifitweretruethatasmallchangeinaweight(orbias)causesonlyasmallchangeinoutput,\n",
            "thenwecouldusethisfacttomodifytheweightsandbiasestogetournetworktobehave\n",
            "moreinthemannerwewant. Forexample,supposethenetworkwasmistakenlyclassifying\n",
            "animageasanâ€œ8â€whenitshouldbeaâ€œ9â€. Wecouldfigureouthowtomakeasmallchange\n",
            "intheweightsandbiasessothenetworkgetsalittleclosertoclassifyingtheimageasaâ€œ9â€. Andthenweâ€™drepeatthis,changingtheweightsandbiasesoverandovertoproducebetter\n",
            "andbetteroutput.\n",
            "Thenetworkwouldbelearning. Theproblemisthatthisisnâ€™twhathappenswhenournetworkcontainsperceptrons. Infact,asmallchangeintheweightsorbiasofanysingleperceptroninthenetworkcan\n",
            "sometimescausetheoutputofthatperceptrontocompletelyflip,sayfrom0to1. That\n",
            "flipmaythencausethebehaviouroftherestofthenetworktocompletelychangeinsome\n",
            "\n",
            "(cid:12)\n",
            "8 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 verycomplicatedway. Sowhileyourâ€œ9â€mightnowbeclassifiedcorrectly,thebehaviourof\n",
            "thenetworkonalltheotherimagesislikelytohavecompletelychangedinsomehard-to-\n",
            "controlway. Thatmakesitdifficulttoseehowtograduallymodifytheweightsandbiasesso\n",
            "thatthenetworkgetsclosertothedesiredbehaviour.\n",
            "Perhapsthereâ€™ssomecleverwayof\n",
            "gettingaroundthisproblem. Butitâ€™snotimmediatelyobvioushowwecangetanetworkof\n",
            "perceptronstolearn. Wecanovercomethisproblembyintroducinganewtypeofartificialneuroncalleda\n",
            "sigmoidneuron. Sigmoidneuronsaresimilartoperceptrons,butmodifiedsothatsmall\n",
            "changesintheirweightsandbiascauseonlyasmallchangeintheiroutput. Thatâ€™sthecrucial\n",
            "factwhichwillallowanetworkofsigmoidneuronstolearn. Okay,letmedescribethesigmoidneuron. Weâ€™lldepictsigmoidneuronsinthesameway\n",
            "wedepictedperceptrons:\n",
            "Justlikeaperceptron,thesigmoidneuronhasinputs, x ,x ,.... Butinsteadofbeingjust0\n",
            "1 2\n",
            "or1,theseinputscanalsotakeonanyvaluesbetween0and1. So,forinstance,0.638...isa\n",
            "validinputforasigmoidneuron. Alsojustlikeaperceptron,thesigmoidneuronhasweights\n",
            "foreachinput,w ,w ,...,andanoverallbias, b.\n",
            "Buttheoutputisnot0or1. Instead,itâ€™s\n",
            "1 2\n",
            "Ïƒ (wx+b),whereÏƒiscalledthesigmoidfunction1,andisdefinedby:\n",
            "1\n",
            "Ïƒ (z) . (1.3)\n",
            "â‰¡ 1+e z\n",
            "âˆ’\n",
            "Toputitallalittlemoreexplicitly,theoutputofasigmoidneuronwithinputs x ,x ,...,\n",
            "1 2\n",
            "weightsw ,w ,...,andbias bis\n",
            "1 2\n",
            "1\n",
            ". (1.4)\n",
            "(cid:128) (cid:80) (cid:138)\n",
            "1+exp\n",
            "j\n",
            "w\n",
            "j\n",
            "x\n",
            "j\n",
            "b\n",
            "âˆ’ âˆ’\n",
            "Atfirstsight,sigmoidneuronsappearverydifferenttoperceptrons. Thealgebraicformof\n",
            "thesigmoidfunctionmayseemopaqueandforbiddingifyouâ€™renotalreadyfamiliarwith\n",
            "it. Infact,therearemanysimilaritiesbetweenperceptronsandsigmoidneurons,andthe\n",
            "algebraicformofthesigmoidfunctionturnsouttobemoreofatechnicaldetailthanatrue\n",
            "barriertounderstanding. Tounderstandthesimilaritytotheperceptronmodel,supposez w x+bisalarge\n",
            "positivenumber. Thene\n",
            "âˆ’\n",
            "z 0andsoÏƒ (z) 1. Inotherwords,whenâ‰¡z= Â·w x+bislarge\n",
            "andpositive,theoutputfroâ‰ˆmthesigmoidnâ‰ˆeuronisapproximately1,justasÂ·itwouldhave\n",
            "beenforaperceptron. Supposeontheotherhandthatz=w x+bisverynegative.\n",
            "Then\n",
            "e\n",
            "âˆ’\n",
            "z ,andÏƒ (z) 0. Sowhenz=w x+bisverynegativÂ·e,thebehaviourofasigmoid\n",
            "â†’âˆž â‰ˆ Â·\n",
            "1Incidentally,Ïƒissometimescalledthelogisticfunction,andthisnewclassofneuronscalledlogistic\n",
            "neurons.Itâ€™susefultorememberthisterminology,sincethesetermsareusedbymanypeopleworking\n",
            "withneuralnets.However,weâ€™llstickwiththesigmoidterminology. \n",
            "(cid:12)\n",
            "1.2. Sigmoidneurons (cid:12) 9\n",
            "(cid:12)\n",
            "neuronalsocloselyapproximatesaperceptron. Itâ€™sonlywhenw x+bisofmodestsize 1\n",
            "thatthereâ€™smuchdeviationfromtheperceptronmodel. Â·\n",
            "WhataboutthealgebraicformofÏƒ?\n",
            "Howcanweunderstandthat? Infact,theexact\n",
            "formofÏƒisnâ€™tsoimportantâ€“whatreallymattersistheshapeofthefunctionwhenplotted. Hereâ€™stheshape:\n",
            "Sigmoidfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "âˆ’ âˆ’ âˆ’\n",
            "Thisshapeisasmoothedoutversionofastepfunction:\n",
            "Stepfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "âˆ’ âˆ’ âˆ’\n",
            "IfÏƒhadinfactbeenastepfunction,thenthesigmoidneuronwouldbeaperceptron,since\n",
            "theoutputwouldbe1or0dependingonwhetherw x+bwaspositiveornegative2. By\n",
            "usingtheactualÏƒfunctionweget,asalreadyimpliedÂ· above,asmoothedoutperceptron. Indeed,itâ€™sthesmoothnessoftheÏƒfunctionthatisthecrucialfact,notitsdetailedform. ThesmoothnessofÏƒmeansthatsmallchangesâˆ†w intheweightsandâˆ†binthebiaswill\n",
            "j\n",
            "produceasmallchangeâˆ†outputintheoutputfromtheneuron. Infact,calculustellsus\n",
            "thatâˆ†outputiswellapproximatedby\n",
            "(cid:88)âˆ‚output âˆ‚output\n",
            "âˆ†output\n",
            "â‰ˆ j\n",
            "âˆ‚w\n",
            "j\n",
            "âˆ†w j+ âˆ‚b âˆ†b (1.5)\n",
            "2Actually,whenw x+b=0theperceptronoutputs0,whilethestepfunctionoutputs1.So,strictly\n",
            "speaking,weâ€™dneedtÂ·omodifythestepfunctionatthatonepoint.Butyougettheidea. \n",
            "(cid:12)\n",
            "10 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 wherethesumisoveralltheweights,w ,andâˆ‚output/âˆ‚w andâˆ‚output/âˆ‚bdenotepartial\n",
            "j j\n",
            "derivativesoftheoutputwithrespecttow and b,respectively. Donâ€™tpanicifyouâ€™renot\n",
            "j\n",
            "comfortablewithpartialderivatives! Whiletheexpressionabovelookscomplicated,withall\n",
            "thepartialderivatives,itâ€™sactuallysayingsomethingverysimple(andwhichisverygood\n",
            "news): âˆ†outputisalinearfunctionofthechangesâˆ†w andâˆ†bintheweightsandbias. j\n",
            "Thislinearitymakesiteasytochoosesmallchangesintheweightsandbiasestoachieve\n",
            "anydesiredsmallchangeintheoutput. Sowhilesigmoidneuronshavemuchofthesame\n",
            "qualitativebehaviorasperceptrons,theymakeitmucheasiertofigureouthowchanging\n",
            "theweightsandbiaseswillchangetheoutput. Ifitâ€™stheshapeofÏƒ whichreallymatters, andnotitsexactform, thenwhyusethe\n",
            "particularformusedforÏƒinEquation1.3? Infact,laterinthebookwewilloccasionally\n",
            "considerneuronswheretheoutputis f(w x+b)forsomeotheractivationfunction f(). The\n",
            "mainthingthatchangeswhenweuseadÂ·ifferentactivationfunctionisthattheparÂ·ticular\n",
            "valuesforthepartialderivativesinEquation1.5change. Itturnsoutthatwhenwecompute\n",
            "thosepartialderivativeslater,usingÏƒwillsimplifythealgebra,simplybecauseexponentials\n",
            "havelovelypropertieswhendifferentiated. Inanycase,Ïƒiscommonly-usedinworkon\n",
            "neuralnets,andistheactivationfunctionweâ€™llusemostofteninthisbook. Howshouldweinterprettheoutputfromasigmoidneuron?Obviously,onebigdifference\n",
            "betweenperceptronsandsigmoidneuronsisthatsigmoidneuronsdonâ€™tjustoutput0or\n",
            "1. Theycanhaveasoutputanyrealnumberbetween0and1,sovaluessuchas0.173...\n",
            "and0.689...arelegitimateoutputs. Thiscanbeuseful,forexample,ifwewanttousethe\n",
            "outputvaluetorepresenttheaverageintensityofthepixelsinanimageinputtoaneural\n",
            "network. Butsometimesitcanbeanuisance. Supposewewanttheoutputfromthenetwork\n",
            "toindicateeitherâ€œtheinputimageisa9â€orâ€œtheinputimageisnota9â€. Obviously,itâ€™dbe\n",
            "easiesttodothisiftheoutputwasa0ora1,asinaperceptron. Butinpracticewecan\n",
            "setupaconventiontodealwiththis,forexample,bydecidingtointerpretanyoutputofat\n",
            "least0.5asindicatingaâ€œ9â€,andanyoutputlessthan0.5asindicatingâ€œnota9â€. Iâ€™llalways\n",
            "explicitlystatewhenweâ€™reusingsuchaconvention,soitshouldnâ€™tcauseanyconfusion. Exercises\n",
            "Sigmoidneuronssimulatingperceptrons,partISupposewetakealltheweights\n",
            "â€¢ andbiasesinanetworkofperceptrons,andmultiplythembyapositiveconstant,c>0. Showthatthebehaviorofthenetworkdoesnâ€™tchange. Sigmoidneuronssimulatingperceptrons,partIISupposewehavethesamesetup\n",
            "â€¢ asthelastproblemâ€“anetworkofperceptrons. Supposealsothattheoverallinputto\n",
            "thenetworkofperceptronshasbeenchosen.\n",
            "Wewonâ€™tneedtheactualinputvalue,we\n",
            "justneedtheinputtohavebeenfixed. Supposetheweightsandbiasesaresuchthat\n",
            "w x+b=0fortheinputxtoanyparticularperceptroninthenetwork. Nowreplace\n",
            "allÂ·thepe(cid:54)rceptronsinthenetworkbysigmoidneurons,andmultiplytheweightsand\n",
            "biasesbyapositiveconstantc>0. Showthatinthelimitasc thebehaviourof\n",
            "thisnetworkofsigmoidneuronsisexactlythesameasthenâ†’etwâˆžorkofperceptrons. Howcanthisfailwhenw x+b=0foroneoftheperceptrons? Â·\n",
            "1.3 The architecture of neural networks\n",
            "InthenextsectionIâ€™llintroduceaneuralnetworkthatcandoaprettygoodjobclassifying\n",
            "handwrittendigits. Inpreparationforthat,ithelpstoexplainsometerminologythatletsus\n",
            "namedifferentpartsofanetwork.\n",
            "Supposewehavethenetwork:\n",
            "\n",
            "(cid:12)\n",
            "1.3. Thearchitectureofneuralnetworks (cid:12) 11\n",
            "(cid:12)\n",
            "1\n",
            "Asmentionedearlier,theleftmostlayerinthisnetworkiscalledtheinputlayer,andthe\n",
            "neuronswithinthelayerarecalledinputneurons. Therightmostoroutputlayercontains\n",
            "theoutputneurons,or,asinthiscase,asingleoutputneuron. Themiddlelayeriscalleda\n",
            "hiddenlayer,sincetheneuronsinthislayerareneitherinputsnoroutputs. Thetermâ€œhiddenâ€\n",
            "perhapssoundsalittlemysteriousâ€“thefirsttimeIheardthetermIthoughtitmusthave\n",
            "somedeepphilosophicalormathematicalsignificanceâ€“butitreallymeansnothingmore\n",
            "thanâ€œnotaninputoranoutputâ€. Thenetworkabovehasjustasinglehiddenlayer,butsome\n",
            "networkshavemultiplehiddenlayers. Forexample,thefollowingfour-layernetworkhas\n",
            "twohiddenlayers:\n",
            "Somewhatconfusingly,andforhistoricalreasons,suchmultiplelayernetworksaresome-\n",
            "times called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons,\n",
            "notperceptrons. Iâ€™mnotgoingtousetheMLPterminologyinthisbook,sinceIthinkitâ€™s\n",
            "confusing,butwantedtowarnyouofitsexistence.\n",
            "Thedesignoftheinputandoutputlayersinanetworkisoftenstraightforward.\n",
            "For\n",
            "example,supposeweâ€™retryingtodeterminewhetherahandwrittenimagedepictsaâ€œ9â€ornot. Anaturalwaytodesignthenetworkistoencodetheintensitiesoftheimagepixelsintothe\n",
            "inputneurons. Iftheimageisa64by64greyscaleimage,thenweâ€™dhave4,096=64 64\n",
            "inputneurons,withtheintensitiesscaledappropriatelybetween0and1. TheoutputlÃ—ayer\n",
            "willcontainjustasingleneuron,withoutputvaluesoflessthan0.5indicatingâ€œinputimage\n",
            "isnota9â€,andvaluesgreaterthan0.5indicatingâ€œinputimageisa9â€. Whilethedesignoftheinputandoutputlayersofaneuralnetworkisoftenstraight-\n",
            "forward,therecanbequiteanarttothedesignofthehiddenlayers. Inparticular,itâ€™snot\n",
            "\n",
            "(cid:12)\n",
            "12 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 possibletosumupthedesignprocessforthehiddenlayerswithafewsimplerulesofthumb. Instead,neuralnetworksresearchershavedevelopedmanydesignheuristicsforthehidden\n",
            "layers,whichhelppeoplegetthebehaviourtheywantoutoftheirnets. Forexample,such\n",
            "heuristicscanbeusedtohelpdeterminehowtotradeoffthenumberofhiddenlayersagainst\n",
            "thetimerequiredtotrainthenetwork. Weâ€™llmeetseveralsuchdesignheuristicslaterinthis\n",
            "book. Uptonow,weâ€™vebeendiscussingneuralnetworkswheretheoutputfromonelayeris\n",
            "usedasinputtothenextlayer. Suchnetworksarecalledfeedforwardneuralnetworks. This\n",
            "meanstherearenoloopsinthenetworkâ€“informationisalwaysfedforward,neverfed\n",
            "back. Ifwedidhaveloops,weâ€™dendupwithsituationswheretheinputtotheÏƒfunction\n",
            "dependedontheoutput. Thatâ€™dbehardtomakesenseof,andsowedonâ€™tallowsuchloops. However,thereareothermodelsofartificialneuralnetworksinwhichfeedbackloops\n",
            "arepossible. Thesemodelsarecalledrecurrentneuralnetworks.\n",
            "Theideainthesemodelsis\n",
            "tohaveneuronswhichfireforsomelimiteddurationoftime,beforebecomingquiescent. Thatfiringcanstimulateotherneurons,whichmayfirealittlewhilelater,alsoforalimited\n",
            "duration. Thatcausesstillmoreneuronstofire,andsoovertimewegetacascadeofneurons\n",
            "firing.\n",
            "Loopsdonâ€™tcauseproblemsinsuchamodel,sinceaneuronâ€™soutputonlyaffectsits\n",
            "inputatsomelatertime,notinstantaneously. Recurrent neural nets have been less influential than feedforward networks, in part\n",
            "becausethelearningalgorithmsforrecurrentnetsare(atleasttodate)lesspowerful. But\n",
            "recurrentnetworksarestillextremelyinteresting. Theyâ€™remuchcloserinspirittohowour\n",
            "brainsworkthanfeedforwardnetworks. Anditâ€™spossiblethatrecurrentnetworkscansolve\n",
            "importantproblemswhichcanonlybesolvedwithgreatdifficultybyfeedforwardnetworks.\n",
            "However,tolimitourscope,inthisbookweâ€™regoingtoconcentrateonthemorewidely-used\n",
            "feedforwardnetworks. 1.4 A simple network to classify handwritten digits\n",
            "Havingdefinedneuralnetworks,letâ€™sreturntohandwritingrecognition. Wecansplitthe\n",
            "problemofrecognizinghandwrittendigitsintotwosub-problems. First,weâ€™dlikeaway\n",
            "of breaking an image containing many digits into a sequence of separate images, each\n",
            "containingasingledigit. Forexample,weâ€™dliketobreaktheimage\n",
            "intosixseparateimages,\n",
            "Wehumanssolvethissegmentationproblemwithease,butitâ€™schallengingforacomputer\n",
            "programtocorrectlybreakuptheimage. Oncetheimagehasbeensegmented,theprogram\n",
            "then needs to classify each individual digit. So, for instance, weâ€™d like our program to\n",
            "recognizethatthefirstdigitabove,\n",
            "\n",
            "(cid:12)\n",
            "1.4. Asimplenetworktoclassifyhandwrittendigits (cid:12) 13\n",
            "(cid:12)\n",
            "isa5. 1\n",
            "Weâ€™llfocusonwritingaprogramtosolvethesecondproblem,thatis,classifyingindividual\n",
            "digits. Wedothisbecauseitturnsoutthatthesegmentationproblemisnotsodifficultto\n",
            "solve,onceyouhaveagoodwayofclassifyingindividualdigits. Therearemanyapproaches\n",
            "to solving the segmentation problem. One approach is to trial many different ways of\n",
            "segmentingtheimage,usingtheindividualdigitclassifiertoscoreeachtrialsegmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its\n",
            "classificationinallsegments,andalowscoreiftheclassifierishavingalotoftroubleinone\n",
            "ormoresegments. Theideaisthatiftheclassifierishavingtroublesomewhere,thenitâ€™s\n",
            "probablyhavingtroublebecausethesegmentationhasbeenchosenincorrectly.\n",
            "Thisidea\n",
            "andothervariationscanbeusedtosolvethesegmentationproblemquitewell. Soinsteadof\n",
            "worryingaboutsegmentationweâ€™llconcentrateondevelopinganeuralnetworkwhichcan\n",
            "solvethemoreinterestinganddifficultproblem,namely,recognizingindividualhandwritten\n",
            "digits. Torecognizeindividualdigitswewilluseathree-layerneuralnetwork:\n",
            "Theinputlayerofthenetworkcontainsneuronsencodingthevaluesoftheinputpixels. As\n",
            "discussedinthenextsection,ourtrainingdataforthenetworkwillconsistofmany28by28\n",
            "pixelimagesofscannedhandwrittendigits,andsotheinputlayercontains784=28 28\n",
            "neurons. ForsimplicityIâ€™veomittedmostofthe784inputneuronsinthediagramabove.Ã—The\n",
            "inputpixelsaregreyscale,withavalueof0.0representingwhite,avalueof1.0representing\n",
            "black,andinbetweenvaluesrepresentinggraduallydarkeningshadesofgrey. Thesecondlayerofthenetworkisahiddenlayer.\n",
            "Wedenotethenumberofneuronsin\n",
            "\n",
            "(cid:12)\n",
            "14 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 thishiddenlayerbyn,andweâ€™llexperimentwithdifferentvaluesforn. Theexampleshown\n",
            "illustratesasmallhiddenlayer,containingjustn=15neurons. Theoutputlayerofthenetworkcontains10neurons. Ifthefirstneuronfires,i.e.,has\n",
            "anoutput 1,thenthatwillindicatethatthenetworkthinksthedigitisa0. Ifthesecond\n",
            "neuronfireâ‰ˆsthenthatwillindicatethatthenetworkthinksthedigitisa1. Andsoon. A\n",
            "littlemoreprecisely,wenumbertheoutputneuronsfrom0through9,andfigureoutwhich\n",
            "neuronhasthehighestactivationvalue. Ifthatneuronis,say,neuronnumber6,thenour\n",
            "networkwillguessthattheinputdigitwasa6. Andsoonfortheotheroutputneurons.\n",
            "Youmightwonderwhyweuse10outputneurons. Afterall,thegoalofthenetwork\n",
            "istotelluswhichdigit(0,1,2,...,9)correspondstotheinputimage. Aseeminglynatural\n",
            "wayofdoingthatistousejust4outputneurons,treatingeachneuronastakingonabinary\n",
            "value,dependingonwhethertheneuronâ€™soutputiscloserto0orto1. Fourneuronsare\n",
            "enoughtoencodetheanswer,since24 =16ismorethanthe10possiblevaluesfortheinput\n",
            "digit. Whyshouldournetworkuse10neuronsinstead?\n",
            "Isnâ€™tthatinefficient? Theultimate\n",
            "justificationisempirical: wecantryoutbothnetworkdesigns,anditturnsoutthat,forthis\n",
            "particularproblem,thenetworkwith10outputneuronslearnstorecognizedigitsbetter\n",
            "thanthenetworkwith4outputneurons. Butthatleavesuswonderingwhyusing10output\n",
            "neuronsworksbetter.\n",
            "Istheresomeheuristicthatwouldtellusinadvancethatweshould\n",
            "usethe10-outputencodinginsteadofthe4-outputencoding? To understand why we do this, it helps to think about what the neural network is\n",
            "doingfromfirstprinciples. Considerfirstthecasewhereweuse10outputneurons. Letâ€™s\n",
            "concentrateonthefirstoutputneuron,theonethatâ€™stryingtodecidewhetherornotthe\n",
            "digitisa0. Itdoesthisbyweighingupevidencefromthehiddenlayerofneurons. What\n",
            "arethosehiddenneuronsdoing? Well,justsupposeforthesakeofargumentthatthefirst\n",
            "neuroninthehiddenlayerdetectswhetherornotanimagelikethefollowingispresent:\n",
            "Itcandothisbyheavilyweightinginputpixelswhichoverlapwiththeimage, andonly\n",
            "lightlyweightingtheotherinputs. Inasimilarway,letâ€™ssupposeforthesakeofargument\n",
            "thatthesecond,third,andfourthneuronsinthehiddenlayerdetectwhetherornotthe\n",
            "followingimagesarepresent:\n",
            "Asyoumayhaveguessed,thesefourimagestogethermakeupthe0imagethatwesawin\n",
            "thelineofdigitsshownearlier:\n",
            "Soifallfourofthesehiddenneuronsarefiringthenwecanconcludethatthedigitisa0. Of\n",
            "course,thatâ€™snottheonlysortofevidencewecanusetoconcludethattheimagewasa0\n",
            "â€“wecouldlegitimatelygeta0inmanyotherways(say,throughtranslationsoftheabove\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 15\n",
            "(cid:12)\n",
            "images,orslightdistortions). Butitseemssafetosaythatatleastinthiscaseweâ€™dconclude 1\n",
            "thattheinputwasa0.\n",
            "Supposingtheneuralnetworkfunctionsinthisway,wecangiveaplausibleexplanation\n",
            "forwhyitâ€™sbettertohave10outputsfromthenetwork,ratherthan4. Ifwehad4outputs,\n",
            "then the first output neuron would be trying to decide what the most significant bit of\n",
            "thedigitwas. Andthereâ€™snoeasywaytorelatethatmostsignificantbittosimpleshapes\n",
            "likethoseshownabove. Itâ€™shardtoimaginethatthereâ€™sanygoodhistoricalreasonthe\n",
            "componentshapesofthedigitwillbecloselyrelatedto(say)themostsignificantbitinthe\n",
            "output. Now, with all that said, this is all just a heuristic. Nothing says that the three-layer\n",
            "neuralnetworkhastooperateinthewayIdescribed,withthehiddenneuronsdetecting\n",
            "simplecomponentshapes. Maybeacleverlearningalgorithmwillfindsomeassignmentof\n",
            "weightsthatletsususeonly4outputneurons. ButasaheuristicthewayofthinkingIâ€™ve\n",
            "describedworksprettywell,andcansaveyoualotoftimeindesigninggoodneuralnetwork\n",
            "architectures. Exercise\n",
            "Thereisawayofdeterminingthebitwiserepresentationofadigitbyaddinganextra\n",
            "â€¢ layertothethree-layernetworkabove. Theextralayerconvertstheoutputfromthe\n",
            "previouslayerintoabinaryrepresentation,asillustratedinthefigurebelow. Finda\n",
            "setofweightsandbiasesforthenewoutputlayer. Assumethatthefirst3layersof\n",
            "neuronsaresuchthatthecorrectoutputinthethirdlayer(i.e.,theoldoutputlayer)\n",
            "hasactivationatleast0.99,andincorrectoutputshaveactivationlessthan0.01. 1.5 Learning with gradient descent\n",
            "Nowthatwehaveadesignforourneuralnetwork,howcanitlearntorecognizedigits? The\n",
            "firstthingweâ€™llneedisadatasettolearnfromâ€“aso-calledtrainingdataset.\n",
            "Weâ€™llusethe\n",
            "MNISTdataset,whichcontainstensofthousandsofscannedimagesofhandwrittendigits,\n",
            "togetherwiththeircorrectclassifications. MNISTâ€™snamecomesfromthefactthatitisa\n",
            "modifiedsubsetoftwodatasetscollectedbyNIST,theUnitedStatesâ€™NationalInstituteof\n",
            "StandardsandTechnology. Hereâ€™safewimagesfromMNIST:\n",
            "\n",
            "(cid:12)\n",
            "16 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Asyoucansee,thesedigitsare,infact,thesameasthoseshownatthebeginningofthis\n",
            "chapterasachallengetorecognize. Ofcourse,whentestingournetworkweâ€™llaskitto\n",
            "recognizeimageswhicharenâ€™tinthetrainingset! TheMNISTdatacomesintwoparts.\n",
            "Thefirstpartcontains60,000imagestobeused\n",
            "astrainingdata. Theseimagesarescannedhandwritingsamplesfrom250people,halfof\n",
            "whomwereUSCensusBureauemployees,andhalfofwhomwerehighschoolstudents. Theimagesaregreyscaleand28by28pixelsinsize. ThesecondpartoftheMNISTdata\n",
            "setis10,000imagestobeusedastestdata. Again,theseare28by28greyscaleimages. Weâ€™llusethetestdatatoevaluatehowwellourneuralnetworkhaslearnedtorecognize\n",
            "digits. Tomakethisagoodtestofperformance,thetestdatawastakenfromadifferent\n",
            "setof250peoplethantheoriginaltrainingdata(albeitstillagroupsplitbetweenCensus\n",
            "Bureauemployeesandhighschoolstudents). Thishelpsgiveusconfidencethatoursystem\n",
            "canrecognizedigitsfrompeoplewhosewritingitdidnâ€™tseeduringtraining. Weâ€™llusethenotation x todenoteatraininginput. Itâ€™llbeconvenienttoregardeach\n",
            "traininginput x asa28 28=784-dimensionalvector. Eachentryinthevectorrepresents\n",
            "thegreyvalueforasinglÃ—epixelintheimage. Weâ€™lldenotethecorrespondingdesiredoutput\n",
            "by y= y(x),where yisa10-dimensionalvector. Forexample,ifaparticulartrainingimage,\n",
            "x,depictsa6,then y(x)=(0,0,0,0,0,0,1,0,0,0) T isthedesiredoutputfromthenetwork. NotethatT hereisthetransposeoperation,turningarowvectorintoanordinary(column)\n",
            "vector. Whatweâ€™dlikeisanalgorithmwhichletsusfindweightsandbiasessothattheoutput\n",
            "fromthenetworkapproximates y(x)foralltraininginputs x. Toquantifyhowwellweâ€™re\n",
            "achievingthisgoalwedefineacostfunction3:\n",
            "1 (cid:88)\n",
            "C(w,b) y(x) a 2 (1.6)\n",
            "â‰¡ 2n x (cid:107) âˆ’ (cid:107)\n",
            "Here,wdenotesthecollectionofallweightsinthenetwork, ballthebiases,nisthetotal\n",
            "numberoftraininginputs,aisthevectorofoutputsfromthenetworkwhen x isinput,and\n",
            "thesumisoveralltraininginputs, x. Ofcourse,theoutputadependson x,wand b,butto\n",
            "keepthenotationsimpleIhavenâ€™texplicitlyindicatedthisdependence.\n",
            "Thenotation v\n",
            "justdenotestheusuallengthfunctionforavectorv. Weâ€™llcallC thequadraticcostfunct(cid:107)ion(cid:107);\n",
            "itâ€™salsosometimesknownasthemeansquarederrororjustMSE.Inspectingtheformofthe\n",
            "quadraticcostfunction,weseethatC(w,b)isnon-negative,sinceeveryterminthesum\n",
            "isnon-negative. Furthermore,thecostC(w,b)becomessmall,i.e.,C(w,b) 0,precisely\n",
            "when y(x)isapproximatelyequaltotheoutput,a,foralltraininginputs,x. Sâ‰ˆoourtraining\n",
            "algorithmhasdoneagoodjobifitcanfindweightsandbiasessothat C(w,b) 0. By\n",
            "contrast,itâ€™snotdoingsowellwhenC(w,b)islargeâ€“thatwouldmeanthat y(xâ‰ˆ )isnot\n",
            "closetotheoutputaforalargenumberofinputs. Sotheaimofourtrainingalgorithmwill\n",
            "betominimizethecostC(w,b)asafunctionoftheweightsandbiases. Inotherwords,we\n",
            "wanttofindasetofweightsandbiaseswhichmakethecostassmallaspossible. Weâ€™lldo\n",
            "thatusinganalgorithmknownasgradientdescent. 3Sometimesreferredtoasalossorobjectivefunction. Weusethetermcostfunctionthroughout\n",
            "thisbook,butyoushouldnotetheotherterminology,sinceitâ€™softenusedinresearchpapersandother\n",
            "discussionsofneuralnetworks.\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 17\n",
            "(cid:12)\n",
            "Whyintroducethequadraticcost? Afterall,arenâ€™tweprimarilyinterestedinthenumber 1\n",
            "ofimagescorrectlyclassifiedbythenetwork?\n",
            "Whynottrytomaximizethatnumberdirectly,\n",
            "ratherthanminimizingaproxymeasurelikethequadraticcost? Theproblemwiththat\n",
            "isthatthenumberofimagescorrectlyclassifiedisnotasmoothfunctionoftheweights\n",
            "andbiasesinthenetwork. Forthemostpart,makingsmallchangestotheweightsand\n",
            "biaseswonâ€™tcauseanychangeatallinthenumberoftrainingimagesclassifiedcorrectly. Thatmakesitdifficulttofigureouthowtochangetheweightsandbiasestogetimproved\n",
            "performance. Ifweinsteaduseasmoothcostfunctionlikethequadraticcostitturnsoutto\n",
            "beeasytofigureouthowtomakesmallchangesintheweightsandbiasessoastogetan\n",
            "improvementinthecost. Thatâ€™swhywefocusfirstonminimizingthequadraticcost,and\n",
            "onlyafterthatwillweexaminetheclassificationaccuracy. Evengiventhatwewanttouseasmoothcostfunction,youmaystillwonderwhywe\n",
            "choosethequadraticfunctionusedinEquation1.6. Isnâ€™tthisaratheradhocchoice? Perhaps\n",
            "ifwechoseadifferentcostfunctionweâ€™dgetatotallydifferentsetofminimizingweights\n",
            "andbiases? Thisisavalidconcern,andlaterweâ€™llrevisitthecostfunction,andmakesome\n",
            "modifications. However,thequadraticcostfunctionofEquation1.6worksperfectlywellfor\n",
            "understandingthebasicsoflearninginneuralnetworks,soweâ€™llstickwithitfornow. Recapping,ourgoalintraininganeuralnetworkistofindweightsandbiaseswhich\n",
            "minimizethequadraticcostfunctionC(w,b). Thisisawell-posedproblem,butitâ€™sgotalot\n",
            "ofdistractingstructureascurrentlyposedâ€“theinterpretationofwand basweightsand\n",
            "biases,theÏƒfunctionlurkinginthebackground,thechoiceofnetworkarchitecture,MNIST,\n",
            "andsoon. Itturnsoutthatwecanunderstandatremendousamountbyignoringmostof\n",
            "thatstructure,andjustconcentratingontheminimizationaspect.\n",
            "Sofornowweâ€™regoingto\n",
            "forgetallaboutthespecificformofthecostfunction,theconnectiontoneuralnetworks,\n",
            "andsoon. Instead,weâ€™regoingtoimaginethatweâ€™vesimplybeengivenafunctionofmany\n",
            "variablesandwewanttominimizethatfunction. Weâ€™regoingtodevelopatechniquecalled\n",
            "gradientdescentwhichcanbeusedtosolvesuchminimizationproblems. Thenweâ€™llcome\n",
            "backtothespecificfunctionwewanttominimizeforneuralnetworks. Okay,letâ€™ssupposeweâ€™retryingtominimizesomefunction, C(v). Thiscouldbeany\n",
            "real-valuedfunctionofmanyvariables,v=v1,v2,.... NotethatIâ€™vereplacedthewand b\n",
            "notationbyvtoemphasizethatthiscouldbeanyfunctionâ€“weâ€™renotspecificallythinkingin\n",
            "theneuralnetworkscontextanymore. TominimizeC(v)ithelpstoimagineC asafunction\n",
            "ofjusttwovariables,whichweâ€™llcallv andv :\n",
            "1 2\n",
            "Whatweâ€™dlikeistofindwhere C achievesitsglobalminimum. Now, ofcourse, forthe\n",
            "functionplottedabove,wecaneyeballthegraphandfindtheminimum.\n",
            "Inthatsense,Iâ€™ve\n",
            "\n",
            "(cid:12)\n",
            "18 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 perhapsshownslightlytoosimpleafunction! Ageneralfunction,C,maybeacomplicated\n",
            "functionofmanyvariables,anditwonâ€™tusuallybepossibletojusteyeballthegraphtofind\n",
            "theminimum. Onewayofattackingtheproblemistousecalculustotrytofindtheminimumanalytically. WecouldcomputederivativesandthentryusingthemtofindplaceswhereCisanextremum. WithsomeluckthatmightworkwhenC isafunctionofjustoneorafewvariables. But\n",
            "itâ€™llturnintoanightmarewhenwehavemanymorevariables. Andforneuralnetworks\n",
            "weâ€™lloftenwantfarmorevariablesâ€“thebiggestneuralnetworkshavecostfunctionswhich\n",
            "dependonbillionsofweightsandbiasesinanextremelycomplicatedway. Usingcalculusto\n",
            "minimizethatjustwonâ€™twork! (Afterassertingthatweâ€™llgaininsightbyimaginingC asafunctionofjusttwovariables,\n",
            "Iâ€™veturnedaroundtwiceintwoparagraphsandsaid, â€œhey, butwhatifitâ€™safunctionof\n",
            "manymorethantwovariables?â€ Sorryaboutthat. PleasebelievemewhenIsaythatit\n",
            "reallydoeshelptoimagineC asafunctionoftwovariables. Itjusthappensthatsometimes\n",
            "thatpicturebreaksdown,andthelasttwoparagraphsweredealingwithsuchbreakdowns. Goodthinkingaboutmathematicsofteninvolvesjugglingmultipleintuitivepictures,learning\n",
            "whenitâ€™sappropriatetouseeachpicture,andwhenitâ€™snot.)\n",
            "Okay,socalculusdoesnâ€™twork. Fortunately,thereisabeautifulanalogywhichsuggests\n",
            "analgorithmwhichworksprettywell. Westartbythinkingofourfunctionasakindofa\n",
            "valley.\n",
            "Ifyousquintjustalittleattheplotabove,thatshouldnâ€™tbetoohard. Andweimagine\n",
            "aballrollingdowntheslopeofthevalley. Oureverydayexperiencetellsusthattheball\n",
            "willeventuallyrolltothebottomofthevalley.\n",
            "Perhapswecanusethisideaasawayto\n",
            "findaminimumforthefunction? Weâ€™drandomlychooseastartingpointforan(imaginary)\n",
            "ball,andthensimulatethemotionoftheballasitrolleddowntothebottomofthevalley. Wecoulddothissimulationsimplybycomputingderivatives(andperhapssomesecond\n",
            "derivatives)ofC â€“thosederivativeswouldtelluseverythingweneedtoknowaboutthe\n",
            "localâ€œshapeâ€ofthevalley,andthereforehowourballshouldroll. BasedonwhatIâ€™vejustwritten,youmightsupposethatweâ€™llbetryingtowritedown\n",
            "Newtonâ€™sequationsofmotionfortheball,consideringtheeffectsoffrictionandgravity,\n",
            "andsoon. Actually,weâ€™renotgoingtotaketheball-rollinganalogyquitethatseriouslyâ€“\n",
            "weâ€™redevisinganalgorithmtominimizeC,notdevelopinganaccuratesimulationofthe\n",
            "lawsofphysics! Theballâ€™s-eyeviewismeanttostimulateourimagination,notconstrainour\n",
            "thinking. Soratherthangetintoallthemessydetailsofphysics,letâ€™ssimplyaskourselves:\n",
            "ifweweredeclaredGodforaday,andcouldmakeupourownlawsofphysics,dictatingto\n",
            "theballhowitshouldroll,whatlaworlawsofmotioncouldwepickthatwouldmakeitso\n",
            "theballalwaysrolledtothebottomofthevalley?\n",
            "Tomakethisquestionmoreprecise,letâ€™sthinkaboutwhathappenswhenwemovethe\n",
            "ballasmallamountâˆ†v inthe v direction,andasmallamountâˆ†v inthe v direction. 1 1 2 2\n",
            "CalculustellsusthatC changesasfollows:\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ†C\n",
            "â‰ˆ\n",
            "âˆ‚v\n",
            "1\n",
            "âˆ†v 1+âˆ‚v\n",
            "2\n",
            "âˆ†v 2 . (1.7)\n",
            "Weâ€™regoingtofindawayofchoosingâˆ†v andâˆ†v soastomakeâˆ†C negative;i.e.,weâ€™ll\n",
            "1 2\n",
            "choosethemsotheballisrollingdownintothevalley. Tofigureouthowtomakesucha\n",
            "choiceithelpstodefineâˆ†vtobethevectorofchangesinv,âˆ†v ( âˆ†v 1 ,âˆ†v 2) T,whereT is\n",
            "againthetransposeoperation,turningrowvectorsintocolumnâ‰¡vectors. Weâ€™llalsodefine\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 19\n",
            "(cid:12)\n",
            "thegradientofC tobethevectorofpartialderivatives,\n",
            "(cid:128)âˆ‚C, âˆ‚C (cid:138)T\n",
            ". Wedenotethegradient\n",
            "1\n",
            "âˆ‚v1 âˆ‚v2\n",
            "vectorby C,i.e.:\n",
            "âˆ‡ (cid:129)âˆ‚C âˆ‚C(cid:139)T\n",
            "C , . (1.8)\n",
            "âˆ‡ â‰¡\n",
            "âˆ‚v\n",
            "1\n",
            "âˆ‚v\n",
            "2\n",
            "Inamomentweâ€™llrewritethechangeâˆ†C intermsofâˆ†v andthegradient, C. Before\n",
            "gettingtothat,though,Iwanttoclarifysomethingthatsometimesgetspeopleâˆ‡hungupon\n",
            "thegradient.\n",
            "Whenmeetingthe C notationforthefirsttime,peoplesometimeswonder\n",
            "howtheyshouldthinkabouttheâˆ‡ symbol. What, exactly, does C mean? Infact, itâ€™s\n",
            "perfectlyfinetothinkof C asasâˆ‡inglemathematicalobjectâ€“theâˆ‡vectordefinedaboveâ€“\n",
            "whichhappenstobewrittâˆ‡enusingtwosymbols. Inthispointofview, C isjustapieceof\n",
            "notationalflag-waving,tellingyouâ€œhey, C isagradientvectorâ€. Thereâˆ‡aremoreadvanced\n",
            "pointsofviewwhere C canbeviewedâˆ‡asanindependentmathematicalentityinitsown\n",
            "right(forexample,asâˆ‡adifferentialoperator),butwewonâ€™tneedsuchpointsofview. Withthesedefinitions,theexpression1.7forâˆ†C canberewrittenas\n",
            "âˆ†C C âˆ†v (1.9)\n",
            "â‰ˆâˆ‡ Â·\n",
            "Thisequationhelpsexplainwhy C iscalledthegradientvector: C relateschangesin\n",
            "vtochangesinC,justasweâ€™dexpâˆ‡ectsomethingcalledagradienttoâˆ‡do.\n",
            "Butwhatâ€™sreally\n",
            "excitingabouttheequationisthatitletsusseehowtochooseâˆ†vsoastomakeâˆ†Cnegative. Inparticular,supposewechoose\n",
            "âˆ†v= Î· C, (1.10)\n",
            "âˆ’ âˆ‡\n",
            "whereÎ·isasmall,positiveparameter(knownasthelearningrate). ThenEquation1.9tells\n",
            "usthatâˆ†C Î· C C= Î· C 2. Because C 2 0,thisguaranteesthatâˆ†C 0,\n",
            "i.e.,C willaâ‰ˆlwâˆ’aysâˆ‡decÂ·râˆ‡ease,nâˆ’eve(cid:107)râˆ‡inc(cid:107)rease,ifwe(cid:107)câˆ‡han(cid:107)geâ‰¥vaccordingtotheprescriptionâ‰¤in\n",
            "1.10. (Within,ofcourse,thelimitsoftheapproximationinEquation1.9).\n",
            "Thisisexactlythe\n",
            "propertywewanted! Andsoweâ€™lltakeEquation1.10todefinetheâ€œlawofmotionâ€forthe\n",
            "ballinourgradientdescentalgorithm. Thatis,weâ€™lluseEquation1.10tocomputeavalue\n",
            "forâˆ†v,thenmovetheballâ€™spositionvbythatamount:\n",
            "v v (cid:48)=v Î· C. (1.11)\n",
            "â†’ âˆ’ âˆ‡\n",
            "Thenweâ€™llusethisupdateruleagain,tomakeanothermove. Ifwekeepdoingthis,over\n",
            "andover,weâ€™llkeepdecreasingC untilâ€“wehopeâ€“wereachaglobalminimum. Summingup,thewaythegradientdescentalgorithmworksistorepeatedlycompute\n",
            "thegradient C,andthentomoveintheoppositedirection,â€œfallingdownâ€theslopeofthe\n",
            "valley. Wecaâˆ‡nvisualizeitlikethis:\n",
            "\n",
            "(cid:12)\n",
            "20 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Noticethatwiththisrulegradientdescentdoesnâ€™treproducerealphysicalmotion. Inreal\n",
            "lifeaballhasmomentum, andthatmomentummayallowittorollacrosstheslope, or\n",
            "even(momentarily)rolluphill.\n",
            "Itâ€™sonlyaftertheeffectsoffrictionsetinthattheballis\n",
            "guaranteedtorolldownintothevalley. Bycontrast,ourruleforchoosingâˆ†vjustsaysâ€œgo\n",
            "down,rightnowâ€. Thatâ€™sstillaprettygoodruleforfindingtheminimum! Tomakegradientdescentworkcorrectly,weneedtochoosethelearningrateÎ·tobe\n",
            "smallenoughthatEquation1.9isagoodapproximation. Ifwedonâ€™t,wemightendupwith\n",
            "âˆ†C>0,whichobviouslywouldnotbegood! Atthesametime,wedonâ€™twantÎ·tobetoo\n",
            "small,sincethatwillmakethechangesâˆ†vtiny,andthusthegradientdescentalgorithm\n",
            "willworkveryslowly. Inpracticalimplementations,Î·isoftenvariedsothatEquation1.9\n",
            "remainsagoodapproximation,butthealgorithmisnâ€™ttooslow.\n",
            "Weâ€™llseelaterhowthis\n",
            "works. Iâ€™veexplainedgradientdescentwhenC isafunctionofjusttwovariables. But,infact,\n",
            "everythingworksjustaswellevenwhenC isafunctionofmanymorevariables. Supposein\n",
            "particularthatC isafunctionofmvariables,v ,...,v . Thenthechangeâˆ†C inC produced\n",
            "1 m\n",
            "byasmallchangeâˆ†v=( âˆ†v 1 ,...,âˆ†v m) T is\n",
            "âˆ†C C âˆ†v, (1.12)\n",
            "â‰ˆâˆ‡ Â·\n",
            "wherethegradient C isthevector\n",
            "âˆ‡\n",
            "(cid:129)âˆ‚C âˆ‚C (cid:139)T\n",
            "C ,..., . (1.13)\n",
            "âˆ‡ â‰¡\n",
            "âˆ‚v\n",
            "1\n",
            "âˆ‚v\n",
            "m\n",
            "Justasforthetwovariablecase,wecanchoose\n",
            "âˆ†v= Î· C, (1.14)\n",
            "âˆ’ âˆ‡\n",
            "andweâ€™reguaranteedthatour(approximate)expression1.12forâˆ†C willbenegative. This\n",
            "givesusawayoffollowingthegradienttoaminimum,evenwhenC isafunctionofmany\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 21\n",
            "(cid:12)\n",
            "variables,byrepeatedlyapplyingtheupdaterule 1\n",
            "v v (cid:48)=v Î· C. (1.15)\n",
            "â†’ âˆ’ âˆ‡\n",
            "Youcanthinkofthisupdateruleasdefiningthegradientdescentalgorithm. Itgivesusa\n",
            "wayofrepeatedlychangingthepositionvinordertofindaminimumofthefunctionC. The\n",
            "ruledoesnâ€™talwaysworkâ€“severalthingscangowrongandpreventgradientdescentfrom\n",
            "findingtheglobalminimumofC,apointweâ€™llreturntoexploreinlaterchapters. But,in\n",
            "practicegradientdescentoftenworksextremelywell,andinneuralnetworksweâ€™llfindthat\n",
            "itâ€™sapowerfulwayofminimizingthecostfunction,andsohelpingthenetlearn. Indeed,thereâ€™sevenasenseinwhichgradientdescentistheoptimalstrategyforsearching\n",
            "foraminimum. Letâ€™ssupposethatweâ€™retryingtomakeamoveâˆ†v inpositionsoasto\n",
            "decrease C asmuchaspossible. Thisisequivalenttominimizingâˆ†C C âˆ†v. Weâ€™ll\n",
            "constrainthesizeofthemovesothat âˆ†v = ÎµforsomesmallfixedÎµ>â‰ˆ0.âˆ‡InoÂ·therwords,\n",
            "wewantamovethatisasmallstepof(cid:107)afi(cid:107)xedsize,andweâ€™retryingtofindthemovement\n",
            "directionwhichdecreasesC asmuchaspossible. Itcanbeprovedthatthechoiceofâˆ†v\n",
            "whichminimizes C âˆ†v isâˆ†v = Î· C,whereÎ· = Îµ/ C isdeterminedbythesize\n",
            "constraint âˆ†v = âˆ‡Îµ. Â·Sogradientdeâˆ’sceâˆ‡ntcanbevieweda(cid:107)sâˆ‡aw(cid:107)ayoftakingsmallstepsin\n",
            "thedirectio(cid:107)nwh(cid:107)ichdoesthemosttoimmediatelydecreaseC.\n",
            "Exercises\n",
            "Provetheassertionofthelastparagraph. Hint: Ifyouâ€™renotalreadyfamiliarwiththe\n",
            "â€¢ Cauchy-Schwarzinequality,youmayfindithelpfultofamiliarizeyourselfwithit. Iexplainedgradientdescentwhen C isafunctionoftwovariables,andwhenitâ€™s\n",
            "â€¢ afunctionofmorethantwovariables.\n",
            "WhathappenswhenC isafunctionofjust\n",
            "onevariable? Canyouprovideageometricinterpretationofwhatgradientdescentis\n",
            "doingintheone-dimensionalcase? People have investigated many variations of gradient descent, including variations\n",
            "thatmorecloselymimicarealphysicalball. Theseball-mimickingvariationshavesome\n",
            "advantages,butalsohaveamajordisadvantage: itturnsouttobenecessarytocompute\n",
            "secondpartialderivativesofC,andthiscanbequitecostly. Toseewhyitâ€™scostly,suppose\n",
            "wewanttocomputeallthesecondpartialderivativesâˆ‚2C/âˆ‚vâˆ‚v . Ifthereareamillion\n",
            "j k\n",
            "suchv variablesthenweâ€™dneedtocomputesomethinglikeatrillion(i.e.,amillionsquared)\n",
            "j\n",
            "secondpartialderivatives4! Thatâ€™sgoingtobecomputationallycostly.\n",
            "Withthatsaid,there\n",
            "aretricksforavoidingthiskindofproblem,andfindingalternativestogradientdescentis\n",
            "anactiveareaofinvestigation. Butinthisbookweâ€™llusegradientdescent(andvariations)\n",
            "asourmainapproachtolearninginneuralnetworks. Howcanweapplygradientdescenttolearninaneuralnetwork? Theideaistouse\n",
            "gradientdescenttofindtheweightsw andbiases b whichminimizethecostinEquation\n",
            "k l\n",
            "1.6. Toseehowthisworks,letâ€™srestatethegradientdescentupdaterule,withtheweights\n",
            "andbiasesreplacingthevariablesv. Inotherwords,ourâ€œpositionâ€nowhascomponentsw\n",
            "j k\n",
            "and b,andthegradientvector C hascorrespondingcomponentsâˆ‚C/âˆ‚w andâˆ‚C/âˆ‚b. l k l\n",
            "âˆ‡\n",
            "4Actually,morelikehalfatrillion,sinceâˆ‚2C/âˆ‚v j âˆ‚v k= âˆ‚2C/âˆ‚v k âˆ‚v j .Still,yougetthepoint. \n",
            "(cid:12)\n",
            "22 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 Writingoutthegradientdescentupdateruleintermsofcomponents,wehave\n",
            "âˆ‚C\n",
            "w\n",
            "k\n",
            "â†’\n",
            "w (cid:48)k=w\n",
            "k\n",
            "âˆ’\n",
            "Î·\n",
            "âˆ‚w\n",
            "k\n",
            "(1.16)\n",
            "âˆ‚C\n",
            "b\n",
            "l\n",
            "â†’\n",
            "b l(cid:48)=b\n",
            "l\n",
            "âˆ’\n",
            "Î·\n",
            "âˆ‚b\n",
            "l\n",
            ". (1.17)\n",
            "Byrepeatedlyapplyingthisupdaterulewecanâ€œrolldownthehillâ€,andhopefullyfinda\n",
            "minimumofthecostfunction. Inotherwords,thisisarulewhichcanbeusedtolearnina\n",
            "neuralnetwork. There are a number of challenges in applying the gradient descent rule. Weâ€™ll look\n",
            "into those in depth in later chapters.\n",
            "But for now I just want to mention one problem. Tounderstandwhattheproblemis,letâ€™slookbackatthequadraticcostinEquation1.6. NoticethatthiscostfunctionhastheformC= 1\n",
            "n\n",
            "(cid:80)\n",
            "x\n",
            "C\n",
            "x\n",
            ",thatis,itâ€™sanaverageovercosts\n",
            "C y(x) a 2 forindividualtrainingexamples. Inpractice,tocomputethegradient C we\n",
            "x (cid:107) 2âˆ’ (cid:107)\n",
            "neeâ‰¡dtocomputethegradients C separatelyforeachtraininginput,x,andthenaâˆ‡verage\n",
            "x\n",
            "them, C= 1\n",
            "n\n",
            "(cid:80)\n",
            "x\n",
            "C\n",
            "x\n",
            ". Unfortuâˆ‡nately,whenthenumberoftraininginputsisverylargethis\n",
            "cantakâˆ‡ealongtimâˆ‡e,andlearningthusoccursslowly. Anideacalledstochasticgradientdescentcanbeusedtospeeduplearning. Theidea\n",
            "istoestimatethegradient C bycomputing C forasmallsampleofrandomlychosen\n",
            "x\n",
            "traininginputs. Byaveraginâˆ‡goverthissmallsâˆ‡ampleitturnsoutthatwecanquicklygeta\n",
            "goodestimateofthetruegradient C,andthishelpsspeedupgradientdescent,andthus\n",
            "learning. âˆ‡\n",
            "Tomaketheseideasmoreprecise,stochasticgradientdescentworksbyrandomlypicking\n",
            "outasmallnumbermofrandomlychosentraininginputs.\n",
            "Weâ€™lllabelthoserandomtraining\n",
            "inputsX ,X ,...,X ,andrefertothemasamini-batch. Providedthesamplesizemislarge\n",
            "1 2 m\n",
            "enoughweexpectthattheaveragevalueofthe C willberoughlyequaltotheaverage\n",
            "Xj\n",
            "overall C ,thatis, âˆ‡\n",
            "âˆ‡ x (cid:80)m j=1 âˆ‡ C Xj (cid:80) x âˆ‡ C x = C, (1.18)\n",
            "m â‰ˆ n âˆ‡\n",
            "wherethesecondsumisovertheentiresetoftrainingdata. Swappingsidesweget\n",
            "1 (cid:88) m\n",
            "C C , (1.19)\n",
            "âˆ‡ â‰ˆ m j=1âˆ‡ Xj\n",
            "confirmingthatwecanestimatetheoverallgradientbycomputinggradientsjustforthe\n",
            "randomlychosenmini-batch. Toconnectthisexplicitlytolearninginneuralnetworks,supposewkandbldenotethe\n",
            "weightsandbiasesinourneuralnetwork. Thenstochasticgradientdescentworksbypicking\n",
            "outarandomlychosenmini-batchoftraininginputs,andtrainingwiththose,\n",
            "w\n",
            "k â†’\n",
            "w (cid:48)k=w\n",
            "k âˆ’ m\n",
            "Î· (cid:88)\n",
            "j\n",
            "âˆ‚\n",
            "âˆ‚\n",
            "C\n",
            "w\n",
            "X\n",
            "k\n",
            "j (1.20)\n",
            "b\n",
            "l â†’\n",
            "b l(cid:48)=b\n",
            "l âˆ’ m\n",
            "Î· (cid:88)\n",
            "j\n",
            "âˆ‚\n",
            "âˆ‚\n",
            "C\n",
            "b\n",
            "X\n",
            "l\n",
            "j, (1.21)\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 23\n",
            "(cid:12)\n",
            "wherethesumsareoverallthetrainingexamplesX inthecurrentmini-batch. Thenwe 1\n",
            "j\n",
            "pickoutanotherrandomlychosenmini-batchandtrainwiththose. Andsoon,untilweâ€™ve\n",
            "exhaustedthetraininginputs,whichissaidtocompleteanepochoftraining. Atthatpoint\n",
            "westartoverwithanewtrainingepoch. Incidentally,itâ€™sworthnotingthatconventionsvaryaboutscalingofthecostfunctionand\n",
            "ofmini-batchupdatestotheweightsandbiases. InEquation1.6wescaledtheoverallcost\n",
            "functionbyafactor 1. Peoplesometimesomitthe 1,summingoverthecostsofindividual\n",
            "n n\n",
            "trainingexamplesinsteadofaveraging. Thisisparticularlyusefulwhenthetotalnumber\n",
            "oftrainingexamplesisnâ€™tknowninadvance. Thiscanoccurifmoretrainingdataisbeing\n",
            "generatedinrealtime,forinstance. And,inasimilarway,themini-batchupdaterules1.20\n",
            "and1.21sometimesomitthe 1 termoutthefrontofthesums. Conceptuallythismakeslittle\n",
            "m\n",
            "difference,sinceitâ€™sequivalenttorescalingthelearningrateÎ·. Butwhendoingdetailed\n",
            "comparisonsofdifferentworkitâ€™sworthwatchingoutfor. Wecanthinkofstochasticgradientdescentasbeinglikepoliticalpolling: itâ€™smucheasier\n",
            "tosampleasmallmini-batchthanitistoapplygradientdescenttothefullbatch,justas\n",
            "carryingoutapolliseasierthanrunningafullelection. Forexample,ifwehaveatraining\n",
            "setofsizen=60,000,asinMNIST,andchooseamini-batchsizeof(say)m=10,thismeans\n",
            "weâ€™llgetafactorof6,000speedupinestimatingthegradient! Ofcourse,theestimatewonâ€™t\n",
            "beperfectâ€“therewillbestatisticalfluctuationsâ€“butitdoesnâ€™tneedtobeperfect: allwe\n",
            "reallycareaboutismovinginageneraldirectionthatwillhelpdecreaseC,andthatmeans\n",
            "wedonâ€™tneedanexactcomputationofthegradient. Inpractice,stochasticgradientdescent\n",
            "isacommonlyusedandpowerfultechniqueforlearninginneuralnetworks,anditâ€™sthe\n",
            "basisformostofthelearningtechniquesweâ€™lldevelopinthisbook. Exercize\n",
            "Anextremeversionofgradientdescentistouseamini-batchsizeofjust1. Thatis,\n",
            "â€¢ givenatraininginput, x,weupdateourweightsandbiasesaccordingtotherules\n",
            "w\n",
            "k\n",
            "w (cid:48)k=w\n",
            "k\n",
            "Î·âˆ‚C\n",
            "x\n",
            "/âˆ‚w\n",
            "k\n",
            "and b\n",
            "l\n",
            "b l(cid:48)=b\n",
            "l\n",
            "Î·âˆ‚C\n",
            "x\n",
            "/âˆ‚b\n",
            "l\n",
            ".\n",
            "Thenwechooseanother\n",
            "traiâ†’ninginput,âˆ’andupdatetheweighâ†’tsandbiaâˆ’sesagain.\n",
            "Andsoon,repeatedly. This\n",
            "procedureisknownasonline,on-line,orincrementallearning. Inonlinelearning,a\n",
            "neuralnetworklearnsfromjustonetraininginputatatime(justashumanbeings\n",
            "do). Nameoneadvantageandonedisadvantageofonlinelearning,comparedto\n",
            "stochasticgradientdescentwithamini-batchsizeof,say,20. Letmeconcludethissectionbydiscussingapointthatsometimesbugspeoplenewtogradient\n",
            "descent. InneuralnetworksthecostC is,ofcourse,afunctionofmanyvariablesâ€“allthe\n",
            "weights and biases â€“ andso in some sense defines a surface in avery high-dimensional\n",
            "space. Somepeoplegethungupthinking: â€œHey, Ihavetobeabletovisualizeallthese\n",
            "extra dimensionsâ€.\n",
            "And they may start to worry: â€œI canâ€™t think in four dimensions, let\n",
            "alonefive(orfivemillion)â€. Istheresomespecialabilitytheyâ€™remissing,someabilitythat\n",
            "â€œrealâ€ supermathematicians have? Of course, the answer is no.\n",
            "Even most professional\n",
            "mathematicianscanâ€™tvisualizefourdimensionsespeciallywell,ifatall. Thetricktheyuse,\n",
            "instead,istodevelopotherwaysofrepresentingwhatâ€™sgoingon. Thatâ€™sexactlywhatwe\n",
            "didabove: weusedanalgebraic(ratherthanvisual)representationofâˆ†C tofigureouthow\n",
            "tomovesoastodecreaseC. Peoplewhoaregoodatthinkinginhighdimensionshavea\n",
            "mentallibrarycontainingmanydifferenttechniquesalongtheselines;ouralgebraictrickis\n",
            "justoneexample. Thosetechniquesmaynothavethesimplicityweâ€™reaccustomedtowhen\n",
            "visualizingthreedimensions,butonceyoubuildupalibraryofsuchtechniques,youcanget\n",
            "prettygoodatthinkinginhighdimensions. Iwonâ€™tgointomoredetailhere,butifyouâ€™re\n",
            "\n",
            "(cid:12)\n",
            "24 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 interestedthenyoumayenjoyreadingthisdiscussionofsomeofthetechniquesprofessional\n",
            "mathematiciansusetothinkinhighdimensions. Whilesomeofthetechniquesdiscussedare\n",
            "quitecomplex,muchofthebestcontentisintuitiveandaccessible,andcouldbemastered\n",
            "byanyone. 1.6 Implementing our network to classify digits\n",
            "Alright,letâ€™swriteaprogramthatlearnshowtorecognizehandwrittendigits,usingstochastic\n",
            "gradientdescentandtheMNISTtrainingdata.Weâ€™lldothiswithashortPython(2.7)program,\n",
            "just74linesofcode! ThefirstthingweneedistogettheMNISTdata. Ifyouâ€™reagituser\n",
            "thenyoucanobtainthedatabycloningthecoderepositoryforthisbook,\n",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
            "Ifyoudonâ€™tusegitthenyoucandownloadthedataandcodehere. Incidentally,whenIdescribedtheMNISTdataearlier,Isaiditwassplitinto60,000\n",
            "trainingimages,and10,000testimages. Thatâ€™stheofficialMNISTdescription. Actually,\n",
            "weâ€™regoingtosplitthedataalittledifferently. Weâ€™llleavethetestimagesasis,butsplitthe\n",
            "60,000-imageMNISTtrainingsetintotwoparts: asetof50,000images,whichweâ€™lluse\n",
            "totrainourneuralnetwork,andaseparate10,000imagevalidationset. Wewonâ€™tusethe\n",
            "validationdatainthischapter,butlaterinthebookweâ€™llfinditusefulinfiguringouthowto\n",
            "setcertainhyper-parametersoftheneuralnetworkâ€“thingslikethelearningrate,andsoon,\n",
            "whicharenâ€™tdirectlyselectedbyourlearningalgorithm. Althoughthevalidationdataisnâ€™t\n",
            "partoftheoriginalMNISTspecification,manypeopleuseMNISTinthisfashion,andthe\n",
            "useofvalidationdataiscommoninneuralnetworks. WhenIrefertotheâ€œMNISTtraining\n",
            "dataâ€fromnowon,Iâ€™llbereferringtoour50,000imagedataset,nottheoriginal60,000\n",
            "imagedataset5. ApartfromtheMNISTdatawealsoneedaPythonlibrarycalledNumpy,fordoingfast\n",
            "linearalgebra. Ifyoudonâ€™talreadyhaveNumpyinstalled,youcangetithere.\n",
            "Letmeexplainthecorefeaturesoftheneuralnetworkscode,beforegivingafulllisting,\n",
            "below.\n",
            "ThecenterpieceisaNetworkclass, whichweusetorepresentaneuralnetwork. Hereâ€™sthecodeweusetoinitializeaNetworkobject:\n",
            "class Network(object):\n",
            "def __init__(self, sizes):\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
            "Inthiscode,thelistsizescontainsthenumberofneuronsintherespectivelayers. So,for\n",
            "example,ifwewanttocreateaNetworkobjectwith2neuronsinthefirstlayer,3neurons\n",
            "inthesecondlayer,and1neuroninthefinallayer,weâ€™ddothiswiththecode:\n",
            "5Asnotedearlier,theMNISTdatasetisbasedontwodatasetscollectedbyNIST,theUnitedStatesâ€™\n",
            "NationalInstituteofStandardsandTechnology.ToconstructMNISTtheNISTdatasetswerestripped\n",
            "downandputintoamoreconvenientformatbyYannLeCun,CorinnaCortes,andChristopherJ.C. Burges.Seethislinkformoredetails.Thedatasetinmyrepositoryisinaformthatmakesiteasyto\n",
            "loadandmanipulatetheMNISTdatainPython. Iobtainedthisparticularformofthedatafromthe\n",
            "LISAmachinelearninglaboratoryattheUniversityofMontreal(link). \n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 25\n",
            "(cid:12)\n",
            "1\n",
            "net = Network([2, 3, 1])\n",
            "ThebiasesandweightsintheNetworkobjectareallinitializedrandomly,usingtheNumpy\n",
            "np.random.randnfunctiontogenerateGaussiandistributionswithmean0andstandard\n",
            "deviation1. Thisrandominitializationgivesourstochasticgradientdescentalgorithma\n",
            "placetostartfrom.\n",
            "Inlaterchaptersweâ€™llfindbetterwaysofinitializingtheweightsand\n",
            "biases,butthiswilldofornow. NotethattheNetworkinitializationcodeassumesthatthe\n",
            "firstlayerofneuronsisaninputlayer,andomitstosetanybiasesforthoseneurons,since\n",
            "biasesareonlyeverusedincomputingtheoutputsfromlaterlayers. NotealsothatthebiasesandweightsarestoredaslistsofNumpymatrices. So, for\n",
            "examplenet.weights[1]isaNumpymatrixstoringtheweightsconnectingthesecondand\n",
            "thirdlayersofneurons. (Itâ€™snotthefirstandsecondlayers,sincePythonâ€™slistindexingstarts\n",
            "at0.) Sincenet.weights[1]isratherverbose,letâ€™sjustdenotethatmatrixw. Itâ€™samatrix\n",
            "suchthatw istheweightfortheconnectionbetweenthek-thneuroninthesecondlayer,\n",
            "jk\n",
            "andthe j-thneuroninthethirdlayer. Thisorderingofthe jandkindicesmayseemstrangeâ€“\n",
            "surelyitâ€™dmakemoresensetoswapthe jandkindicesaround? Thebigadvantageofusing\n",
            "thisorderingisthatitmeansthatthevectorofactivationsofthethirdlayerofneuronsis:\n",
            "a (cid:48)= Ïƒ (wa+b). (1.22)\n",
            "Thereâ€™squiteabitgoingoninthisequation,soletâ€™sunpackitpiecebypiece. aisthevector\n",
            "ofactivationsofthesecondlayerofneurons.\n",
            "Toobtaina wemultiplyabytheweightmatrix\n",
            "(cid:48)\n",
            "w,andaddthevector bofbiases. WethenapplythefunctionÏƒelementwisetoeveryentry\n",
            "inthevector wa+b6. Itâ€™seasytoverifythatEquation1.22givesthesameresultasour\n",
            "earlierrule,Equation1.4,forcomputingtheoutputofasigmoidneuron. Exercise\n",
            "WriteoutEquation1.22incomponentform,andverifythatitgivesthesameresult\n",
            "â€¢ astherule1.4forcomputingtheoutputofasigmoidneuron. Withallthisinmind,itâ€™seasytowritecodecomputingtheoutputfromaNetworkinstance. Webeginbydefiningthesigmoidfunction:\n",
            "def sigmoid(z):\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "NotethatwhentheinputzisavectororNumpyarray,Numpyautomaticallyappliesthe\n",
            "functionsigmoidelementwise,thatis,invectorizedform. WethenaddafeedforwardmethodtotheNetworkclass,which,givenaninputafor\n",
            "thenetwork,returnsthecorrespondingoutput7. AllthemethoddoesisappliesEquation\n",
            "1.22foreachlayer:\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "6ThisiscalledvectorizingthefunctionÏƒ. 7Itisassumedthattheinputaisan(n, 1)Numpyndarray,nota(n,)vector.Here,nisthenumber\n",
            "ofinputstothenetwork.Ifyoutrytousean(n,)vectorasinputyouâ€™llgetstrangeresults.Although\n",
            "usingan(n,)vectorappearsthemorenaturalchoice,usingan(n, 1)ndarraymakesitparticularly\n",
            "easytomodifythecodetofeedforwardmultipleinputsatonce,andthatissometimesconvenient.\n",
            "\n",
            "(cid:12)\n",
            "26 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 return a\n",
            "Ofcourse,themainthingwewantourNetworkobjectstodoistolearn. Tothatend\n",
            "weâ€™llgivethemanSGDmethodwhichimplementsstochasticgradientdescent.\n",
            "Hereâ€™sthe\n",
            "code.\n",
            "Itâ€™salittlemysteriousinafewplaces,butIâ€™llbreakitdownbelow,afterthelisting. def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
            "\"\"\"Train the neural network using mini-batch stochastic gradient descent. The\n",
            "\"training_data\" is a list of tuples \"(x, y)\" representing the training\n",
            "inputs and the desired outputs. The other non-optional parameters are self-\n",
            "explanatory. If \"test_data\" is provided then the network will be evaluated\n",
            "against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially. \"\"\"\n",
            "if test_data:\n",
            "n_test = len(test_data)\n",
            "n = len(training_data)\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n,\n",
            "mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(mini_batch, eta)\n",
            "if test_data:\n",
            "print \"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)\n",
            "else:\n",
            "print \"Epoch {0} complete\".format(j)\n",
            "Thetraining_dataisalistoftuples(x, y)representingthetraininginputsandcorre-\n",
            "sponding desiredoutputs. The variablesepochsandmini_batch_sizeare whatyouâ€™d\n",
            "expectâ€“thenumberofepochstotrainfor,andthesizeofthemini-batchestousewhen\n",
            "sampling. etaisthelearningrate,Î·. Iftheoptionalargumenttest_dataissupplied,then\n",
            "theprogramwillevaluatethenetworkaftereachepochoftraining,andprintoutpartial\n",
            "progress. Thisisusefulfortrackingprogress,butslowsthingsdownsubstantially. Thecodeworksasfollows. Ineachepoch,itstartsbyrandomlyshufflingthetraining\n",
            "data,andthenpartitionsitintomini-batchesoftheappropriatesize. Thisisaneasywayof\n",
            "samplingrandomlyfromthetrainingdata. Thenforeachmini_batchweapplyasingle\n",
            "stepofgradientdescent. Thisisdonebythecodeself.update_mini_batch(mini_batch\n",
            ", eta), which updates the network weights and biases according to a single iteration\n",
            "ofgradientdescent,usingjustthetrainingdatainmini_batch. Hereâ€™sthecodeforthe\n",
            "update_mini_batchmethod:\n",
            "def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the networkâ€™s weights and biases by applying gradient descent using\n",
            "backpropagation to a single mini batch. The \"mini_batch\" is a list of tuples\n",
            "\"(x, y)\", and \"eta\" is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights,\n",
            "nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b\n",
            ")]\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 27\n",
            "(cid:12)\n",
            "1\n",
            "Mostoftheworkisdonebytheline\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "Thisinvokessomethingcalledthebackpropagationalgorithm,whichisafastwayofcomput-\n",
            "ingthegradientofthecostfunction. Soupdate_mini_batchworkssimplybycomputing\n",
            "these gradients for every training example in the mini_batch, and then updating self\n",
            ".weightsandself.biasesappropriately. Iâ€™mnotgoingtoshowthecodeforself.backproprightnow. Weâ€™llstudyhowback-\n",
            "propagationworksinthenextchapter,includingthecodeforself.backprop. Fornow,just\n",
            "assumethatitbehavesasclaimed,returningtheappropriategradientforthecostassociated\n",
            "tothetrainingexamplex. Letâ€™slookatthefullprogram, includingthedocumentationstrings, whichIomitted\n",
            "above. Apart from self.backprop the program is self-explanatory â€“ all the heavy lift-\n",
            "ingisdoneinself.SGDandself.update_mini_batch,whichweâ€™vealreadydiscussed. Theself.backpropmethodmakesuseofafewextrafunctionstohelpincomputingthe\n",
            "gradient,namelysigmoid_prime,whichcomputesthederivativeoftheÏƒfunction,and\n",
            "self.cost_derivative,whichIwonâ€™tdescribehere. Youcangetthegistofthese(and\n",
            "perhapsthedetails)justbylookingatthecodeanddocumentationstrings.\n",
            "Weâ€™lllookat\n",
            "themindetailinthenextchapter. Notethatwhiletheprogramappearslengthy,muchofthe\n",
            "codeisdocumentationstringsintendedtomakethecodeeasytounderstand. Infact,the\n",
            "programcontainsjust74linesofnon-whitespace,non-commentcode. Allthecodemaybe\n",
            "foundonGitHubhere. \"\"\"\n",
            "network.py\n",
            "~~~~~~~~~~\n",
            "A module to implement the stochastic gradient descent learning\n",
            "algorithm for a feedforward neural network. Gradients are calculated\n",
            "using backpropagation. Note that I have focused on making the code\n",
            "simple, easily readable, and easily modifiable.\n",
            "It is not optimized,\n",
            "and omits many desirable features. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import random\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "class Network(object):\n",
            "def __init__(self, sizes):\n",
            "\"\"\"The list â€˜â€˜sizesâ€˜â€˜ contains the number of neurons in the\n",
            "respective layers of the network. For example, if the list\n",
            "was [2, 3, 1] then it would be a three-layer network, with the\n",
            "first layer containing 2 neurons, the second layer 3 neurons,\n",
            "and the third layer 1 neuron. The biases and weights for the\n",
            "network are initialized randomly, using a Gaussian\n",
            "distribution with mean 0, and variance 1. Note that the first\n",
            "layer is assumed to be an input layer, and by convention we\n",
            "wonâ€™t set any biases for those neurons, since biases are only\n",
            "ever used in computing the outputs from later layers.\"\"\"\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
            "\n",
            "(cid:12)\n",
            "28 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if â€˜â€˜aâ€˜â€˜ is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "return a\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
            "\"\"\"Train the neural network using mini-batch stochastic\n",
            "gradient descent. The â€˜â€˜training_dataâ€˜â€˜ is a list of tuples\n",
            "â€˜â€˜(x, y)â€˜â€˜ representing the training inputs and the desired\n",
            "outputs. The other non-optional parameters are\n",
            "self-explanatory. If â€˜â€˜test_dataâ€˜â€˜ is provided then the\n",
            "network will be evaluated against the test data after each\n",
            "epoch, and partial progress printed out. This is useful for\n",
            "tracking progress, but slows things down substantially.\"\"\"\n",
            "if test_data:\n",
            "n_test = len(test_data)\n",
            "n = len(training_data)\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [\n",
            "training_data[k:k+mini_batch_size]\n",
            "for k in xrange(0, n, mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(mini_batch, eta)\n",
            "if test_data:\n",
            "print \"Epoch {0}: {1} / {2}\".format(\n",
            "j, self.evaluate(test_data), n_test)\n",
            "else:\n",
            "print \"Epoch {0} complete\".format(j)\n",
            "def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the networkâ€™s weights and biases by applying\n",
            "gradient descent using backpropagation to a single mini batch. The â€˜â€˜mini_batchâ€˜â€˜ is a list of tuples â€˜â€˜(x, y)â€˜â€˜, and â€˜â€˜etaâ€˜â€˜\n",
            "is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "def backprop(self, x, y):\n",
            "\"\"\"Return a tuple â€˜â€˜(nabla_b, nabla_w)â€˜â€˜ representing the\n",
            "gradient for the cost function C_x.\n",
            "â€˜â€˜nabla_bâ€˜â€˜ and\n",
            "â€˜â€˜nabla_wâ€˜â€˜ are layer-by-layer lists of numpy arrays, similar\n",
            "to â€˜â€˜self.biasesâ€˜â€˜ and â€˜â€˜self.weightsâ€˜â€˜.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 29\n",
            "(cid:12)\n",
            "zs.append(z) 1\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. Itâ€™s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "def evaluate(self, test_data):\n",
            "\"\"\"Return the number of test inputs for which the neural\n",
            "network outputs the correct result. Note that the neural\n",
            "networkâ€™s output is assumed to be the index of whichever\n",
            "neuron in the final layer has the highest activation.\"\"\"\n",
            "test_results = [(np.argmax(self.feedforward(x)), y)\n",
            "for (x, y) in test_data]\n",
            "return sum(int(x == y) for (x, y) in test_results)\n",
            "def cost_derivative(self, output_activations, y):\n",
            "\"\"\"Return the vector of partial derivatives \\partial C_x /\n",
            "\\partial a for the output activations.\"\"\"\n",
            "return (output_activations-y)\n",
            "#### Miscellaneous functions\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "Howwelldoestheprogramrecognizehandwrittendigits? Well,letâ€™sstartbyloadinginthe\n",
            "MNISTdata.\n",
            "Iâ€™lldothisusingalittlehelperprogram,mnist_loader.py,tobedescribed\n",
            "below. WeexecutethefollowingcommandsinaPythonshell,\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            "Ofcourse,thiscouldalsobedoneinaseparatePythonprogram,butifyouâ€™refollowing\n",
            "alongitâ€™sprobablyeasiesttodoinaPythonshell. AfterloadingtheMNISTdata,weâ€™llsetupaNetworkwith30hiddenneurons. Wedo\n",
            "thisafterimportingthePythonprogramlistedabove,whichisnamednetwork,\n",
            ">>> import network\n",
            ">>> net = network.Network([784, 30, 10])\n",
            "\n",
            "(cid:12)\n",
            "30 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Finally,weâ€™llusestochasticgradientdescenttolearnfromtheMNISTtraining_dataover\n",
            "30epochs,withamini-batchsizeof10,andalearningrateofÎ· =3.0,\n",
            ">>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
            "Notethatifyouâ€™rerunningthecodeasyoureadalong,itwilltakesometimetoexecuteâ€“\n",
            "foratypicalmachine(asof2015)itwilllikelytakeafewminutestorun. Isuggestyouset\n",
            "thingsrunning,continuetoread,andperiodicallychecktheoutputfromthecode.\n",
            "Ifyouâ€™re\n",
            "inarushyoucanspeedthingsupbydecreasingthenumberofepochs,bydecreasingthe\n",
            "numberofhiddenneurons,orbyusingonlypartofthetrainingdata. Notethatproduction\n",
            "codewouldbemuch,muchfaster: thesePythonscriptsareintendedtohelpyouunderstand\n",
            "howneuralnetswork,nottobehigh-performancecode! And,ofcourse,onceweâ€™vetrained\n",
            "anetworkitcanberunveryquicklyindeed,onalmostanycomputingplatform. Forexample,\n",
            "onceweâ€™velearnedagoodsetofweightsandbiasesforanetwork,itcaneasilybeported\n",
            "toruninJavascriptinawebbrowser,orasanativeapponamobiledevice. Inanycase,\n",
            "hereisapartialtranscriptoftheoutputofonetrainingrunoftheneuralnetwork. The\n",
            "transcriptshowsthenumberoftestimagescorrectlyrecognizedbytheneuralnetworkafter\n",
            "eachepochoftraining. Asyoucansee,afterjustasingleepochthishasreached9,129out\n",
            "of10,000,andthenumbercontinuestogrow,\n",
            "Epoch 0: 9129 / 10000\n",
            "Epoch 1: 9295 / 10000\n",
            "Epoch 2: 9348 / 10000\n",
            "... Epoch 27: 9528 / 10000\n",
            "Epoch 28: 9542 / 10000\n",
            "Epoch 29: 9534 / 10000\n",
            "Thatis,thetrainednetworkgivesusaclassificationrateofabout95percentâ€“95.42percent\n",
            "atitspeak(â€œEpoch28â€)! Thatâ€™squiteencouragingasafirstattempt. Ishouldwarnyou,\n",
            "however,thatifyourunthecodethenyourresultsarenotnecessarilygoingtobequitethe\n",
            "sameasmine,sinceweâ€™llbeinitializingournetworkusing(different)randomweightsand\n",
            "biases. TogenerateresultsinthischapterIâ€™vetakenbest-of-threeruns. Letâ€™sreruntheaboveexperiment,changingthenumberofhiddenneuronsto100.\n",
            "As\n",
            "wasthecaseearlier,ifyouâ€™rerunningthecodeasyoureadalong,youshouldbewarnedthat\n",
            "ittakesquiteawhiletoexecute(onmymachinethisexperimenttakestensofsecondsfor\n",
            "eachtrainingepoch),soitâ€™swisetocontinuereadinginparallelwhilethecodeexecutes. >>> net = network.Network([784, 100, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
            "Sureenough,thisimprovestheresultsto96.59percent. Atleastinthiscase,usingmore\n",
            "hiddenneuronshelpsusgetbetterresults8\n",
            "Ofcourse,toobtaintheseaccuraciesIhadtomakespecificchoicesforthenumberof\n",
            "epochsoftraining,themini-batchsize,andthelearningrate,Î·. AsImentionedabove,these\n",
            "areknownashyper-parametersforourneuralnetwork,inordertodistinguishthemfrom\n",
            "theparameters(weightsandbiases)learntbyourlearningalgorithm. Ifwechooseour\n",
            "8Readerfeedbackindicatesquitesomevariationinresultsforthisexperiment,andsometraining\n",
            "runsgiveresultsquiteabitworse.Usingthetechniquesintroducedinchapter3willgreatlyreducethe\n",
            "variationinperformanceacrossdifferenttrainingrunsforournetworks. \n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 31\n",
            "(cid:12)\n",
            "hyper-parameterspoorly,wecangetbadresults. Suppose,forexample,thatweâ€™dchosen 1\n",
            "thelearningratetobeÎ· =0.001,\n",
            ">>> net = network.Network([784, 100, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)\n",
            "Theresultsaremuchlessencouraging,\n",
            "Epoch 0: 1139 / 10000\n",
            "Epoch 1: 1136 / 10000\n",
            "Epoch 2: 1135 / 10000\n",
            "... Epoch 27: 2101 / 10000\n",
            "Epoch 28: 2123 / 10000\n",
            "Epoch 29: 2142 / 10000\n",
            "However,youcanseethattheperformanceofthenetworkisgettingslowlybetterovertime. Thatsuggestsincreasingthelearningrate,saytoÎ· =0.01.\n",
            "Ifwedothat,wegetbetter\n",
            "results,whichsuggestsincreasingthelearningrateagain. (Ifmakingachangeimproves\n",
            "things, try doing more!) If we do that several times over, weâ€™ll end up with a learning\n",
            "rateofsomethinglikeÎ· =1.0(andperhapsfinetuneto3.0),whichisclosetoourearlier\n",
            "experiments. Soeventhoughweinitiallymadeapoorchoiceofhyper-parameters,weat\n",
            "leastgotenoughinformationtohelpusimproveourchoiceofhyper-parameters. Ingeneral,\n",
            "debugginganeuralnetworkcanbechallenging.\n",
            "Thisisespeciallytruewhentheinitial\n",
            "choiceofhyper-parametersproducesresultsnobetterthanrandomnoise. Supposewetry\n",
            "thesuccessful30hiddenneuronnetworkarchitecturefromearlier,butwiththelearning\n",
            "ratechangedtoÎ· =100.0:\n",
            ">>> net = network.Network([784, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)\n",
            "Atthispointweâ€™veactuallygonetoofar,andthelearningrateistoohigh:\n",
            "Epoch 0: 1009 / 10000\n",
            "Epoch 1: 1009 / 10000\n",
            "Epoch 2: 1009 / 10000\n",
            "Epoch 3: 1009 / 10000\n",
            "... Epoch 27: 982 / 10000\n",
            "Epoch 28: 982 / 10000\n",
            "Epoch 29: 982 / 10000\n",
            "Nowimaginethatwewerecomingtothisproblemforthefirsttime.\n",
            "Ofcourse,weknow\n",
            "fromourearlierexperimentsthattherightthingtodoistodecreasethelearningrate. But\n",
            "ifwewerecomingtothisproblemforthefirsttimethentherewouldnâ€™tbemuchinthe\n",
            "outputtoguideusonwhattodo. Wemightworrynotonlyaboutthelearningrate,but\n",
            "abouteveryotheraspectofourneuralnetwork. Wemightwonderifweâ€™veinitializedthe\n",
            "weightsandbiasesinawaythatmakesithardforthenetworktolearn? Ormaybewedonâ€™t\n",
            "haveenoughtrainingdatatogetmeaningfullearning? Perhapswehavenâ€™trunforenough\n",
            "epochs? Ormaybeitâ€™simpossibleforaneuralnetworkwiththisarchitecturetolearnto\n",
            "recognizehandwrittendigits? Maybethelearningrateistoolow? Or,maybe,thelearning\n",
            "rateistoohigh? Whenyouâ€™recomingtoaproblemforthefirsttime,youâ€™renotalwayssure. Thelessontotakeawayfromthisisthatdebugginganeuralnetworkisnottrivial,and,\n",
            "justasforordinaryprogramming,thereisanarttoit. Youneedtolearnthatartofdebugging\n",
            "\n",
            "(cid:12)\n",
            "32 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 inordertogetgood resultsfromneuralnetworks. More generally, weneedto develop\n",
            "heuristicsforchoosinggoodhyper-parametersandagoodarchitecture.\n",
            "Weâ€™lldiscussall\n",
            "theseatlengththroughthebook,includinghowIchosethehyper-parametersabove. Exercise\n",
            "Trycreatinganetworkwithjusttwolayersâ€“aninputandanoutputlayer,nohidden\n",
            "â€¢ layerâ€“with784and10neurons,respectively. Trainthenetworkusingstochastic\n",
            "gradientdescent. Whatclassificationaccuracycanyouachieve?\n",
            "Earlier,IskippedoverthedetailsofhowtheMNISTdataisloaded. Itâ€™sprettystraightforward.\n",
            "Forcompleteness,hereâ€™sthecode. ThedatastructuresusedtostoretheMNISTdataare\n",
            "describedinthedocumentationstringsâ€“itâ€™sstraightforwardstuff,tuplesandlistsofNumpy\n",
            "ndarrayobjects(thinkofthemasvectorsifyouâ€™renotfamiliarwithndarrays):\n",
            "\"\"\"\n",
            "mnist_loader\n",
            "~~~~~~~~~~~~\n",
            "A library to load the MNIST image data. For details of the data\n",
            "structures that are returned, see the doc strings for â€˜â€˜load_dataâ€˜â€˜\n",
            "and â€˜â€˜load_data_wrapperâ€˜â€˜. In practice, â€˜â€˜load_data_wrapperâ€˜â€˜ is the\n",
            "function usually called by our neural network code. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import cPickle\n",
            "import gzip\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "def load_data():\n",
            "\"\"\"Return the MNIST data as a tuple containing the training data, the\n",
            "validation data, and the test data. The â€˜â€˜training_dataâ€˜â€˜ is returned as a tuple with two entries. The first entry\n",
            "contains the actual training images. This is a\n",
            "numpy ndarray with 50,000 entries. Each entry is, in turn, a numpy ndarray\n",
            "with 784 values, representing the 28 * 28 = 784\n",
            "pixels in a single MNIST image. The second entry in the â€˜â€˜training_dataâ€˜â€˜ tuple is a numpy ndarray containing\n",
            "50,000 entries. Those entries are just the digit\n",
            "values (0...9) for the corresponding images contained in the first entry of\n",
            "the tuple. The â€˜â€˜validation_dataâ€˜â€˜ and â€˜â€˜test_dataâ€˜â€˜ are similar, except each contains\n",
            "only 10,000 images. This is a nice data format, but for use in neural networks itâ€™s helpful to\n",
            "modify the format of the â€˜â€˜training_dataâ€˜â€˜ a little. Thatâ€™s done in the wrapper function â€˜â€˜load_data_wrapper()â€˜â€˜, see below. \"\"\"\n",
            "f = gzip.open(â€™../data/mnist.pkl.gzâ€™, â€™rbâ€™)\n",
            "training_data, validation_data, test_data = cPickle.load(f)\n",
            "f.close()\n",
            "return (training_data, validation_data, test_data)\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 33\n",
            "(cid:12)\n",
            "def load_data_wrapper(): 1\n",
            "\"\"\"Return a tuple containing â€˜â€˜(training_data, validation_data,\n",
            "test_data)â€˜â€˜. Based on â€˜â€˜load_dataâ€˜â€˜, but the format is more\n",
            "convenient for use in our implementation of neural networks. In particular, â€˜â€˜training_dataâ€˜â€˜ is a list containing 50,000\n",
            "2-tuples â€˜â€˜(x, y)â€˜â€˜. â€˜â€˜xâ€˜â€˜ is a 784-dimensional numpy.ndarray\n",
            "containing the input image. â€˜â€˜yâ€˜â€˜ is a 10-dimensional\n",
            "numpy.ndarray representing the unit vector corresponding to the\n",
            "correct digit for â€˜â€˜xâ€˜â€˜. â€˜â€˜validation_dataâ€˜â€˜ and â€˜â€˜test_dataâ€˜â€˜ are lists containing 10,000\n",
            "2-tuples â€˜â€˜(x, y)â€˜â€˜. In each case, â€˜â€˜xâ€˜â€˜ is a 784-dimensional\n",
            "numpy.ndarry containing the input image, and â€˜â€˜yâ€˜â€˜ is the\n",
            "corresponding classification, i.e., the digit values (integers)\n",
            "corresponding to â€˜â€˜xâ€˜â€˜. Obviously, this means weâ€™re using slightly different formats for\n",
            "the training data and the validation / test data. These formats\n",
            "turn out to be the most convenient for use in our neural network\n",
            "code.\"\"\"\n",
            "tr_d, va_d, te_d = load_data()\n",
            "training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
            "training_results = [vectorized_result(y) for y in tr_d[1]]\n",
            "training_data = zip(training_inputs, training_results)\n",
            "validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
            "validation_data = zip(validation_inputs, va_d[1])\n",
            "test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
            "test_data = zip(test_inputs, te_d[1])\n",
            "return (training_data, validation_data, test_data)\n",
            "def vectorized_result(j):\n",
            "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
            "position and zeroes elsewhere. This is used to convert a digit\n",
            "(0...9) into a corresponding desired output from the neural\n",
            "network.\"\"\"\n",
            "e = np.zeros((10, 1))\n",
            "e[j] = 1.0\n",
            "return e\n",
            "Isaidabovethatourprogramgetsprettygoodresults.Whatdoesthatmean?Goodcompared\n",
            "towhat?\n",
            "Itâ€™sinformativetohavesomesimple(non-neural-network)baselineteststocompare\n",
            "against,tounderstandwhatitmeanstoperformwell. Thesimplestbaselineofall,ofcourse,\n",
            "istorandomlyguessthedigit.\n",
            "Thatâ€™llberightabouttenpercentofthetime. Weâ€™redoing\n",
            "muchbetterthanthat! Whataboutalesstrivialbaseline? Letâ€™stryanextremelysimpleidea: weâ€™lllookathow\n",
            "darkanimageis. Forinstance,animageofa2willtypicallybequiteabitdarkerthanan\n",
            "imageofa1,justbecausemorepixelsareblackenedout,asthefollowingexamplesillustrate:\n",
            "Thissuggestsusingthetrainingdatatocomputeaveragedarknessesforeachdigit,0,1,2,...,9. Whenpresentedwithanewimage,wecomputehowdarktheimageis,andthenguessthat\n",
            "itâ€™swhicheverdigithastheclosestaveragedarkness. Thisisasimpleprocedure,andiseasy\n",
            "tocodeup,soIwonâ€™texplicitlywriteoutthecodeâ€“ifyouâ€™reinteresteditâ€™sintheGitHub\n",
            "\n",
            "(cid:12)\n",
            "34 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 repository. Butitâ€™sabigimprovementoverrandomguessing,getting2,225ofthe10,000\n",
            "testimagescorrect,i.e.,22.25percentaccuracy. Itâ€™snotdifficulttofindotherideaswhichachieveaccuraciesinthe20to50percent\n",
            "range. Ifyouworkabitharderyoucangetupover50percent. Buttogetmuchhigher\n",
            "accuraciesithelpstouseestablishedmachinelearningalgorithms. Letâ€™stryusingoneofthe\n",
            "bestknownalgorithms,thesupportvectormachineorSVM.Ifyouâ€™renotfamiliarwithSVMs,\n",
            "nottoworry,weâ€™renotgoingtoneedtounderstandthedetailsofhowSVMswork. Instead,\n",
            "weâ€™lluseaPythonlibrarycalledscikit-learn,whichprovidesasimplePythoninterfacetoa\n",
            "fastC-basedlibraryforSVMsknownasLIBSVM. Ifwerunscikit-learnâ€™sSVMclassifierusingthedefaultsettings,thenitgets9,435of\n",
            "10,000testimagescorrect. (Thecodeisavailablehere.) Thatâ€™sabigimprovementover\n",
            "ournaiveapproachofclassifyinganimagebasedonhowdarkitis. Indeed,itmeansthat\n",
            "theSVMisperformingroughlyaswellasourneuralnetworks,justalittleworse. Inlater\n",
            "chaptersweâ€™llintroducenewtechniquesthatenableustoimproveourneuralnetworksso\n",
            "thattheyperformmuchbetterthantheSVM. Thatâ€™snottheendofthestory,however. The9,435of10,000resultisforscikit-learnâ€™s\n",
            "defaultsettingsforSVMs. SVMshaveanumberoftunableparameters,anditâ€™spossibleto\n",
            "searchforparameterswhichimprovethisout-of-the-boxperformance. Iwonâ€™texplicitlydo\n",
            "thissearch,butinsteadreferyoutothisblogpostbyAndreasMÃ¼llerifyouâ€™dliketoknow\n",
            "more. MuellershowsthatwithsomeworkoptimizingtheSVMâ€™sparametersitâ€™spossibleto\n",
            "gettheperformanceupabove98.5percentaccuracy. Inotherwords,awell-tunedSVMonly\n",
            "makesanerroronaboutonedigitin70. Thatâ€™sprettygood! Canneuralnetworksdobetter?\n",
            "Infact,theycan. Atpresent,well-designedneuralnetworksoutperformeveryother\n",
            "techniqueforsolvingMNIST,includingSVMs. Thecurrent(2013)recordisclassifying9,979\n",
            "of10,000imagescorrectly.\n",
            "ThiswasdonebyLiWan,MatthewZeiler,SixinZhang,Yann\n",
            "LeCun,andRobFergus.\n",
            "Weâ€™llseemostofthetechniquestheyusedlaterinthebook. At\n",
            "thatleveltheperformanceisclosetohuman-equivalent,andisarguablybetter,sincequitea\n",
            "fewoftheMNISTimagesaredifficultevenforhumanstorecognizewithconfidence,for\n",
            "example:\n",
            "Itrustyouâ€™llagreethatthosearetoughtoclassify! WithimagesliketheseintheMNIST\n",
            "datasetitâ€™sremarkablethatneuralnetworkscanaccuratelyclassifyallbut21ofthe10,000\n",
            "testimages. Usually,whenprogrammingwebelievethatsolvingacomplicatedproblem\n",
            "likerecognizingtheMNISTdigitsrequiresasophisticatedalgorithm. Buteventheneural\n",
            "networksintheWanetalpaperjustmentionedinvolvequitesimplealgorithms,variations\n",
            "onthealgorithmweâ€™veseeninthischapter. Allthecomplexityislearned,automatically,\n",
            "fromthetrainingdata. Insomesense, themoralofbothourresultsandthoseinmore\n",
            "sophisticatedpapers,isthatforsomeproblems:\n",
            "sophisticatedalgorithm simplelearningalgorithm+goodtrainingdata. â‰¤\n",
            "\n",
            "(cid:12)\n",
            "1.7.\n",
            "Towarddeeplearning (cid:12) 35\n",
            "(cid:12)\n",
            "1\n",
            "Figure1.1:Credits:1.EsterInbar.2.Unknown.3.NASA,ESA,G.Illingworth,D.Magee,andP.Oesch(University\n",
            "ofCalifornia,SantaCruz),R.Bouwens(LeidenUniversity),andtheHUDF09Team. 1.7 Toward deep learning\n",
            "Whileourneuralnetworkgivesimpressiveperformance, thatperformanceissomewhat\n",
            "mysterious. Theweightsandbiasesinthenetworkwerediscoveredautomatically. Andthat\n",
            "meanswedonâ€™timmediatelyhaveanexplanationofhowthenetworkdoeswhatitdoes. Canwefindsomewaytounderstandtheprinciplesbywhichournetworkisclassifying\n",
            "handwrittendigits? And,givensuchprinciples,canwedobetter? Toputthesequestionsmorestarkly,supposethatafewdecadeshenceneuralnetworks\n",
            "leadtoartificialintelligence(AI).Willweunderstandhowsuchintelligentnetworkswork? Perhapsthenetworkswillbeopaquetous,withweightsandbiaseswedonâ€™tunderstand,\n",
            "becausetheyâ€™vebeenlearnedautomatically. IntheearlydaysofAIresearchpeoplehoped\n",
            "thattheefforttobuildanAIwouldalsohelpusunderstandtheprinciplesbehindintelligence\n",
            "and,maybe,thefunctioningofthehumanbrain. Butperhapstheoutcomewillbethatwe\n",
            "endupunderstandingneitherthebrainnorhowartificialintelligenceworks! Toaddressthesequestions,letâ€™sthinkbacktotheinterpretationofartificialneuronsthat\n",
            "Igaveatthestartofthechapter,asameansofweighingevidence. Supposewewantto\n",
            "determinewhetheranimageshowsahumanfaceornot:\n",
            "Wecouldattackthisproblemthesamewayweattackedhandwritingrecognitionâ€“by\n",
            "usingthepixelsintheimageasinputtoaneuralnetwork,withtheoutputfromthenetwork\n",
            "asingleneuronindicatingeitherâ€œYes,itâ€™safaceâ€orâ€œNo,itâ€™snotafaceâ€. Letâ€™ssupposewedothis,butthatweâ€™renotusingalearningalgorithm. Instead,weâ€™re\n",
            "goingtotrytodesignanetworkbyhand,choosingappropriateweightsandbiases. How\n",
            "mightwegoaboutit? Forgettingneuralnetworksentirelyforthemoment,aheuristicwe\n",
            "coulduseistodecomposetheproblemintosub-problems: doestheimagehaveaneyein\n",
            "thetopleft? Doesithaveaneyeinthetopright?\n",
            "Doesithaveanoseinthemiddle? Doesit\n",
            "haveamouthinthebottommiddle? Istherehairontop?\n",
            "Andsoon. Iftheanswerstoseveralofthesequestionsareâ€œyesâ€,orevenjustâ€œprobablyyesâ€,then\n",
            "weâ€™dconcludethattheimageislikelytobeaface. Conversely,iftheanswerstomostofthe\n",
            "questionsareâ€œnoâ€,thentheimageprobablyisnâ€™taface. Ofcourse,thisisjustaroughheuristic,anditsuffersfrommanydeficiencies. Maybe\n",
            "thepersonisbald,sotheyhavenohair. Maybewecanonlyseepartoftheface,orthe\n",
            "faceisatanangle,sosomeofthefacialfeaturesareobscured. Still,theheuristicsuggests\n",
            "thatifwecansolvethesub-problemsusingneuralnetworks,thenperhapswecanbuilda\n",
            "neuralnetworkforface-detection,bycombiningthenetworksforthesub-problems. Hereâ€™s\n",
            "\n",
            "(cid:12)\n",
            "36 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 a possible architecture, with rectangles denoting the sub-networks. Note that this isnâ€™t\n",
            "intendedasarealisticapproachtosolvingtheface-detectionproblem;rather,itâ€™stohelpus\n",
            "buildintuitionabouthownetworksfunction.\n",
            "Hereâ€™sthearchitecture:\n",
            "Itâ€™salsoplausiblethatthesub-networkscanbedecomposed. Supposeweâ€™reconsideringthe\n",
            "question: â€œIsthereaneyeinthetopleft?â€ Thiscanbedecomposedintoquestionssuchas:\n",
            "â€œIsthereaneyebrow?â€;â€œArethereeyelashes?â€;â€œIsthereaniris?â€;andsoon. Ofcourse,these\n",
            "questionsshouldreallyincludepositionalinformation,aswellâ€“â€œIstheeyebrowinthetop\n",
            "left,andabovetheiris?â€,thatkindofthingâ€“butletâ€™skeepitsimple. Thenetworktoanswer\n",
            "thequestionâ€œIsthereaneyeinthetopleft?â€ cannowbedecomposed:\n",
            "Those questions too can be broken down, further and further through multiple layers. Ultimately,weâ€™llbeworkingwithsub-networksthatanswerquestionssosimpletheycan\n",
            "easilybeansweredatthelevelofsinglepixels. Thosequestionsmight, forexample, be\n",
            "aboutthepresenceorabsenceofverysimpleshapesatparticularpointsintheimage. Such\n",
            "questionscanbeansweredbysingleneuronsconnectedtotherawpixelsintheimage. \n",
            "(cid:12)\n",
            "1.7. Towarddeeplearning (cid:12) 37\n",
            "(cid:12)\n",
            "Theendresultisanetworkwhichbreaksdownaverycomplicatedquestionâ€“doesthis 1\n",
            "imageshowafaceornotâ€“intoverysimplequestionsanswerableatthelevelofsinglepixels. Itdoesthisthroughaseriesofmanylayers,withearlylayersansweringverysimpleand\n",
            "specificquestionsabouttheinputimage,andlaterlayersbuildingupahierarchyofever\n",
            "morecomplexandabstractconcepts. Networkswiththiskindofmany-layerstructureâ€“two\n",
            "ormorehiddenlayersâ€“arecalleddeepneuralnetworks. Ofcourse,Ihavenâ€™tsaidhowtodothisrecursivedecompositionintosub-networks. It\n",
            "certainlyisnâ€™tpracticaltohand-designtheweightsandbiasesinthenetwork. Instead,weâ€™d\n",
            "like to use learning algorithms so that the network can automatically learn the weights\n",
            "andbiasesâ€“andthus,thehierarchyofconceptsâ€“fromtrainingdata. Researchersinthe\n",
            "1980sand1990striedusingstochasticgradientdescentandbackpropagationtotraindeep\n",
            "networks. Unfortunately,exceptforafewspecialarchitectures,theydidnâ€™thavemuchluck.\n",
            "Thenetworkswouldlearn,butveryslowly,andinpracticeoftentooslowlytobeuseful. Since2006,asetoftechniqueshasbeendevelopedthatenablelearningindeepneural\n",
            "nets. Thesedeeplearningtechniquesarebasedonstochasticgradientdescentandback-\n",
            "propagation,butalsointroducenewideas. Thesetechniqueshaveenabledmuchdeeper\n",
            "(andlarger)networkstobetrainedâ€“peoplenowroutinelytrainnetworkswith5to10\n",
            "hiddenlayers. And,itturnsoutthattheseperformfarbetteronmanyproblemsthanshallow\n",
            "neuralnetworks,i.e.,networkswithjustasinglehiddenlayer. Thereason,ofcourse,is\n",
            "theabilityofdeepnetstobuildupacomplexhierarchyofconcepts. Itâ€™sabitliketheway\n",
            "conventionalprogramminglanguagesusemodulardesignandideasaboutabstractionto\n",
            "enablethecreationofcomplexcomputerprograms. Comparingadeepnetworktoashallow\n",
            "networkisabitlikecomparingaprogramminglanguagewiththeabilitytomakefunction\n",
            "callstoastrippeddownlanguagewithnoabilitytomakesuchcalls. Abstractiontakesa\n",
            "differentforminneuralnetworksthanitdoesinconventionalprogramming,butitâ€™sjustas\n",
            "important.\n",
            "\n",
            "(cid:12)\n",
            "38 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 39\n",
            "(cid:12)\n",
            "2222\n",
            "How the backpropagation\n",
            "algorithm works\n",
            "2\n",
            "Inthelastchapterwesawhowneuralnetworkscanlearntheirweightsandbiasesusing\n",
            "thegradientdescentalgorithm. Therewas,however,agapinourexplanation: wedidnâ€™t\n",
            "discuss how to compute the gradient of the cost function. Thatâ€™s quite a gap!\n",
            "In this\n",
            "chapterIâ€™llexplainafastalgorithmforcomputingsuchgradients,analgorithmknownas\n",
            "backpropagation. Thebackpropagationalgorithmwasoriginallyintroducedinthe1970s,butitsimportance\n",
            "wasnâ€™tfullyappreciateduntilafamous1986paperbyDavidRumelhart,GeoffreyHinton,\n",
            "andRonaldWilliams. Thatpaperdescribesseveralneuralnetworkswherebackpropagation\n",
            "worksfarfasterthanearlierapproachestolearning,makingitpossibletouseneuralnetsto\n",
            "solveproblemswhichhadpreviouslybeeninsoluble. Today,thebackpropagationalgorithm\n",
            "istheworkhorseoflearninginneuralnetworks. Thischapterismoremathematicallyinvolvedthantherestofthebook. Ifyouâ€™renotcrazy\n",
            "aboutmathematicsyoumaybetemptedtoskipthechapter,andtotreatbackpropagationas\n",
            "ablackboxwhosedetailsyouâ€™rewillingtoignore. Whytakethetimetostudythosedetails?\n",
            "Thereason,ofcourse,isunderstanding. Attheheartofbackpropagationisanexpression\n",
            "forthepartialderivativeâˆ‚C/âˆ‚wofthecostfunctionC withrespecttoanyweightw(orbias\n",
            "b)inthenetwork. Theexpressiontellsushowquicklythecostchangeswhenwechangethe\n",
            "weightsandbiases. Andwhiletheexpressionissomewhatcomplex,italsohasabeautytoit,\n",
            "witheachelementhavinganatural,intuitiveinterpretation. Andsobackpropagationisnâ€™t\n",
            "justafastalgorithmforlearning. Itactuallygivesusdetailedinsightsintohowchangingthe\n",
            "weightsandbiaseschangestheoverallbehaviourofthenetwork. Thatâ€™swellworthstudying\n",
            "indetail. Withthatsaid,ifyouwanttoskimthechapter,orjumpstraighttothenextchapter,thatâ€™s\n",
            "fine. Iâ€™vewrittentherestofthebooktobeaccessibleevenifyoutreatbackpropagationasa\n",
            "blackbox. Thereare,ofcourse,pointslaterinthebookwhereIreferbacktoresultsfrom\n",
            "thischapter. Butatthosepointsyoushouldstillbeabletounderstandthemainconclusions,\n",
            "\n",
            "(cid:12)\n",
            "40 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "evenifyoudonâ€™tfollowallthereasoning. 2.1 Warm up: a fast matrix-based approach to computing the\n",
            "2\n",
            "output from a neural network\n",
            "Beforediscussingbackpropagation, letâ€™swarm upwithafastmatrix-basedalgorithm to\n",
            "computetheoutputfromaneuralnetwork. Weactuallyalreadybrieflysawthisalgorithm\n",
            "neartheendofthelastchapter(section1.6),butIdescribeditquickly,soitâ€™sworthrevisiting\n",
            "indetail.\n",
            "Inparticular,thisisagoodwayofgettingcomfortablewiththenotationusedin\n",
            "backpropagation,inafamiliarcontext.\n",
            "Letâ€™sbeginwithanotationwhichletsusrefertoweightsinthenetworkinanunam-\n",
            "biguousway. Weâ€™llusewl todenotetheweightfortheconnectionfromthek-thneuronin\n",
            "jk\n",
            "the(l 1)-thlayertothe j-thneuroninthel-thlayer. So,forexample,thediagrambelow\n",
            "showsâˆ’theweightonaconnectionfromthefourthneuroninthesecondlayertothesecond\n",
            "neuroninthethirdlayerofanetwork:\n",
            "Thisnotationiscumbersomeatfirst,anditdoestakesomeworktomaster. Butwithalittle\n",
            "effortyouâ€™llfindthenotationbecomeseasyandnatural. Onequirkofthenotationisthe\n",
            "orderingofthejandkindices. Youmightthinkthatitmakesmoresensetousejtorefer\n",
            "totheinputneuron, andktotheoutputneuron, notviceversa, asisactuallydone. Iâ€™ll\n",
            "explainthereasonforthisquirkbelow. Weuseasimilarnotationforthenetworkâ€™sbiases\n",
            "andactivations. Explicitly,weuse bl forthebiasofthe j-thneuroninthel-thlayer. Andwe\n",
            "j\n",
            "useal fortheactivationofthe j-thneuroninthel-thlayer. Thefollowingdiagramshows\n",
            "j\n",
            "examplesofthesenotationsinuse:\n",
            "Withthesenotations,theactivational ofthe j-thneuroninthel-thlayerisrelatedtothe\n",
            "j\n",
            "activationsinthe(l 1)-thlayerbytheequation(compareEquation1.4andsurrounding\n",
            "discussioninthelastâˆ’chapter)\n",
            "\n",
            "(cid:12)\n",
            "2.1. Warmup: afastmatrix-basedapproachtocomputingtheoutputfromaneuralnetwork (cid:12) 41\n",
            "(cid:12)\n",
            "(cid:130) (cid:140)\n",
            "(cid:88)\n",
            "al j= Ïƒ wl jk a k l âˆ’ 1 +bl j , (2.1)\n",
            "k\n",
            "2\n",
            "wherethesumisoverallneuronskinthe(l 1)-thlayer. Torewritethisexpressionina\n",
            "matrixformwedefineaweightmatrixwl foreâˆ’achlayer,l. Theentriesoftheweightmatrix\n",
            "wl arejusttheweightsconnectingtothel-thlayerofneurons,thatis,theentryinthe j-th\n",
            "rowandk-thcolumniswl . Similarly,foreachlayerl wedefineabiasvector, bl. Youcan\n",
            "jk\n",
            "probablyguesshowthisworksâ€“thecomponentsofthebiasvectorarejustthevalues bl,\n",
            "j\n",
            "onecomponentforeachneuroninthel-thlayer. Andfinally,wedefineanactivationvector\n",
            "alwhosecomponentsaretheactivationsal. Thelastingredientweneedtorewrite2.1ina\n",
            "j\n",
            "matrixformistheideaofvectorizingafunctionsuchasÏƒ. Wemetvectorizationbrieflyin\n",
            "thelastchapter,buttorecap,theideaisthatwewanttoapplyafunctionsuchasÏƒtoevery\n",
            "elementinavectorv. WeusetheobviousnotationÏƒ (v)todenotethiskindofelementwise\n",
            "applicationofafunction.\n",
            "Thatis,thecomponentsofÏƒ (v)arejustÏƒ (v)j = Ïƒ (v j). Asan\n",
            "example,ifwehavethefunction f(x)=x2thenthevectorizedformof f hastheeffect\n",
            "(cid:130)(cid:150) (cid:153)(cid:140) (cid:150) (cid:153) (cid:150) (cid:153)\n",
            "2 f(2) 4\n",
            "f = = , (2.2)\n",
            "3 f(3) 9\n",
            "thatis,thevectorized f justsquareseveryelementofthevector. Withthesenotationsinmind,Equation2.1canberewritteninthebeautifulandcompact\n",
            "vectorizedform\n",
            "al = Ïƒ (wlal âˆ’ 1 +bl ).\n",
            "(2.3)\n",
            "Thisexpressiongivesusamuchmoreglobalwayofthinkingabouthowtheactivationsin\n",
            "onelayerrelatetoactivationsinthepreviouslayer: wejustapplytheweightmatrixtothe\n",
            "activations,thenaddthebiasvector,andfinallyapplytheÏƒfunction1. Thatglobalviewis\n",
            "ofteneasierandmoresuccinct(andinvolvesfewerindices!) thantheneuron-by-neuron\n",
            "viewweâ€™vetakentonow. Thinkofitasawayofescapingindexhell,whileremainingprecise\n",
            "aboutwhatâ€™sgoingon. Theexpressionisalsousefulinpractice,becausemostmatrixlibraries\n",
            "providefastwaysofimplementingmatrixmultiplication,vectoraddition,andvectorization. Indeed,thecode(see1.6)inthelastchaptermadeimplicituseofthisexpressiontocompute\n",
            "thebehaviourofthenetwork.\n",
            "WhenusingEquation2.3tocomputeal,wecomputetheintermediatequantityzl\n",
            "wlal\n",
            "âˆ’\n",
            "1 +bl alongtheway. Thisquantityturnsouttobeusefulenoughtobeworthnaminâ‰¡g:\n",
            "we call zl the weighted input to the neurons in layer l. Weâ€™ll make considerable use of\n",
            "the weighted input zl later in the chapter. Equation 2.3 is sometimes written in terms\n",
            "oftheweightedinput,asal = Ïƒ (zl ). Itâ€™salsoworthnotingthatzl hascomponentszl j =\n",
            "(cid:80)\n",
            "k\n",
            "wl\n",
            "jk\n",
            "a\n",
            "k\n",
            "l\n",
            "âˆ’\n",
            "1 +bl\n",
            "j\n",
            ",thatis,zl\n",
            "j\n",
            "isjusttheweightedinputtotheactivationfunctionforneuron j\n",
            "inlayerl. 1Bytheway,itâ€™sthisexpressionthatmotivatesthequirkinthewl notationmentionedearlier.Ifwe\n",
            "jk\n",
            "usedjtoindextheinputneuron,andktoindextheoutputneuron,thenweâ€™dneedtoreplacetheweight\n",
            "matrixinEquation2.3bythetransposeoftheweightmatrix.Thatâ€™sasmallchange,butannoying,and\n",
            "weâ€™dlosetheeasysimplicityofsaying(andthinking)â€œapplytheweightmatrixtotheactivationsâ€. \n",
            "(cid:12)\n",
            "42 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2.2 The two assumptions we need about the cost function\n",
            "Thegoalofbackpropagationistocomputethepartialderivativesâˆ‚C/âˆ‚wandâˆ‚C/âˆ‚bofthe\n",
            "2 costfunctionCwithrespecttoanyweightworbiasbinthenetwork. Forbackpropagation\n",
            "toworkweneedtomaketwomainassumptionsabouttheformofthecostfunction.\n",
            "Before\n",
            "statingthoseassumptions,though,itâ€™susefultohaveanexamplecostfunctioninmind. Weâ€™ll\n",
            "usethequadraticcostfunctionfromlastchapter(c.f.\n",
            "Equation1.6). Inthenotationofthe\n",
            "lastsection,thequadraticcosthastheform\n",
            "C= 1 (cid:88)(cid:13) (cid:13)y(x) aL (x) (cid:13) (cid:13) 2 , (2.4)\n",
            "2n x âˆ’\n",
            "where: n is the total number of training examples; the sum is over individual training\n",
            "examples, x; y= y(x)isthecorrespondingdesiredoutput; Ldenotesthenumberoflayers\n",
            "inthenetwork;andaL =aL (x)isthevectorofactivationsoutputfromthenetworkwhen x\n",
            "isinput. Okay,sowhatassumptionsdoweneedtomakeaboutourcostfunction, C,inorder\n",
            "thatbackpropagationcanbeapplied? Thefirstassumptionweneedisthatthecostfunction\n",
            "canbewrittenasanaverage C = 1 n (cid:80) x C x overcostfunctions C x forindividualtraining\n",
            "examples, x. Thisisthecaseforthequadraticcostfunction,wherethecostforasingle\n",
            "trainingexampleisC x = 1 2 y aL 2. Thisassumptionwillalsoholdtrueforalltheother\n",
            "costfunctionsweâ€™llmeetin(cid:107)thiâˆ’sboo(cid:107)k. Thereasonweneedthisassumptionisbecausewhatbackpropagationactuallyletsus\n",
            "doiscomputethepartialderivativesâˆ‚C /âˆ‚wandâˆ‚C /âˆ‚bforasingletrainingexample. x x\n",
            "Wethenrecoverâˆ‚C/âˆ‚wandâˆ‚C/âˆ‚bbyaveragingovertrainingexamples. Infact,withthis\n",
            "assumptioninmind,weâ€™llsupposethetrainingexample x hasbeenfixed,anddropthe x\n",
            "subscript,writingthecostC asC. Weâ€™lleventuallyputthe x backin,butfornowitâ€™sa\n",
            "x\n",
            "notationalnuisancethatisbetterleftimplicit. Thesecondassumptionwemakeaboutthecostisthatitcanbewrittenasafunctionof\n",
            "theoutputsfromtheneuralnetwork:\n",
            "Forexample,thequadraticcostfunctionsatisfiesthisrequirement,sincethequadraticcost\n",
            "forasingletrainingexample x maybewrittenas\n",
            "1 1(cid:88)\n",
            "C= 2(cid:107) y âˆ’ aL (cid:107) 2 = 2 j (y j âˆ’ aL j) 2, (2.5)\n",
            "andthusisafunctionoftheoutputactivations. Ofcourse,thiscostfunctionalsodepends\n",
            "\n",
            "(cid:12)\n",
            "2.3. TheHadamardproduct,s t (cid:12) 43\n",
            "(cid:12)\n",
            "(cid:12)\n",
            "onthedesiredoutput y,andyoumaywonderwhyweâ€™renotregardingthecostalsoas\n",
            "a function of y. Remember, though, that the input training example x is fixed, and so\n",
            "theoutput y isalsoafixedparameter. Inparticular,itâ€™snotsomethingwecanmodifyby\n",
            "changingtheweightsandbiasesinanyway,i.e.,itâ€™snotsomethingwhichtheneuralnetwork 2\n",
            "learns. AndsoitmakessensetoregardC asafunctionoftheoutputactivationsaL alone,\n",
            "with y merelyaparameterthathelpsdefinethatfunction. 2.3 The Hadamard product, s t\n",
            "(cid:12)\n",
            "Thebackpropagationalgorithmisbasedoncommonlinearalgebraicoperationsâ€“things\n",
            "likevectoraddition,multiplyingavectorbyamatrix,andsoon.\n",
            "Butoneoftheoperations\n",
            "isalittlelesscommonlyused.\n",
            "Inparticular,supposesand t aretwovectorsofthesame\n",
            "dimension. Thenweuses t todenotetheelementwiseproductofthetwovectors. Thus\n",
            "thecomponentsofs t are(cid:12)just(s t)j=s\n",
            "j\n",
            "t\n",
            "j\n",
            ". Asanexample,\n",
            "(cid:12) (cid:12)\n",
            "(cid:150) (cid:153) (cid:150) (cid:153) (cid:150) (cid:153) (cid:150) (cid:153)\n",
            "1 3 1 3 3\n",
            "= âˆ— = . (2.6)\n",
            "2 (cid:12) 4 2 4 8\n",
            "âˆ—\n",
            "ThiskindofelementwisemultiplicationissometimescalledtheHadamardproductorSchur\n",
            "product.\n",
            "Weâ€™llrefertoitastheHadamardproduct. Goodmatrixlibrariesusuallyprovidefast\n",
            "implementationsoftheHadamardproduct,andthatcomesinhandywhenimplementing\n",
            "backpropagation. 2.4 The four fundamental equations behind backpropagation\n",
            "Backpropagationisaboutunderstandinghowchangingtheweightsandbiasesinanetwork\n",
            "changesthecostfunction. Ultimately,thismeanscomputingthepartialderivativesâˆ‚C/âˆ‚wl\n",
            "jk\n",
            "andâˆ‚C/âˆ‚bl. Buttocomputethose,wefirstintroduceanintermediatequantity,Î´l,which\n",
            "j j\n",
            "wecalltheerrorinthe j-thneuroninthel-thlayer. Backpropagationwillgiveusaprocedure\n",
            "tocomputetheerrorÎ´l,andthenwillrelateÎ´l toâˆ‚C/âˆ‚wl andâˆ‚C/âˆ‚bl. j j jk j\n",
            "Tounderstandhowtheerrorisdefined,imaginethereisademoninourneuralnetwork:\n",
            "Thedemonsitsatthe j-thneuroninlayerl. Astheinputtotheneuroncomesin,thedemon\n",
            "messeswiththeneuronâ€™soperation. Itaddsalittlechangeâˆ†zl totheneuronâ€™sweighted\n",
            "j\n",
            "input,sothatinsteadofoutputtingÏƒ (zl j),theneuroninsteadoutputsÏƒ (zl j+ âˆ†zl j). This\n",
            "changepropagatesthroughlaterlayersinthenetwork,finallycausingtheoverallcostto\n",
            "changebyanamount\n",
            "âˆ‚Câˆ†zl. âˆ‚zl j\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "44 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "Now,thisdemonisagooddemon,andistryingtohelpyouimprovethecost,i.e.,theyâ€™re\n",
            "tryingtofindaÎ´zl whichmakesthecostsmaller. Supposeâˆ‚C/âˆ‚zl hasalargevalue(either\n",
            "j j\n",
            "positiveornegative).\n",
            "Thenthedemoncanlowerthecostquiteabitbychoosingâˆ†zl tohave\n",
            "j\n",
            "2 theoppositesigntoâˆ‚C/âˆ‚zl. Bycontrast,ifâˆ‚C/âˆ‚zl isclosetozero,thenthedemoncanâ€™t\n",
            "j j\n",
            "improvethecostmuchatallbyperturbingtheweightedinputzl. Sofarasthedemoncan\n",
            "j\n",
            "tell,theneuronisalreadyprettynearoptimal2. Andsothereâ€™saheuristicsenseinwhich\n",
            "âˆ‚C/âˆ‚zl isameasureoftheerrorintheneuron. j\n",
            "Motivatedbythisstory,wedefinetheerrorÎ´l ofneuron jinlayerl by\n",
            "j\n",
            "âˆ‚C\n",
            "Î´l . (2.7)\n",
            "j\n",
            "â‰¡\n",
            "âˆ‚zl\n",
            "j\n",
            "Asperourusualconventions,weuseÎ´l todenotethevectoroferrorsassociatedwithlayerl. BackpropagationwillgiveusawayofcomputingÎ´l foreverylayer,andthenrelatingthose\n",
            "errorstothequantitiesofrealinterest,âˆ‚C/âˆ‚wl andâˆ‚C/âˆ‚bl. jk j\n",
            "Youmightwonderwhythedemonischangingtheweightedinputzl. Surelyitâ€™dbemore\n",
            "j\n",
            "naturaltoimaginethedemonchangingtheoutputactivational,withtheresultthatweâ€™dbe\n",
            "j\n",
            "using\n",
            "âˆ‚C\n",
            "asourmeasureoferror. Infact,ifyoudothisthingsworkoutquitesimilarlyto\n",
            "âˆ‚al\n",
            "j\n",
            "thediscussionbelow. Butitturnsouttomakethepresentationofbackpropagationalittle\n",
            "morealgebraicallycomplicated. Soweâ€™llstickwithÎ´l j= âˆ‚ âˆ‚ z C l asourmeasureoferror3. j\n",
            "Planofattack: Backpropagationisbasedaroundfourfundamentalequations. Together,\n",
            "thoseequationsgiveusawayofcomputingboththeerrorÎ´l andthegradientofthecost\n",
            "function.\n",
            "Istatethefourequationsbelow. Bewarned, though: youshouldnâ€™texpectto\n",
            "instantaneouslyassimilatetheequations. Suchanexpectationwillleadtodisappointment. Infact,thebackpropagationequationsaresorichthatunderstandingthemwellrequires\n",
            "considerabletimeandpatienceasyougraduallydelvedeeperintotheequations. Thegood\n",
            "newsisthatsuchpatienceisrepaidmanytimesover. Andsothediscussioninthissectionis\n",
            "merelyabeginning,helpingyouonthewaytoathoroughunderstandingoftheequations. Hereâ€™sapreviewofthewaysweâ€™lldelvemoredeeplyintotheequationslaterinthe\n",
            "chapter: Iâ€™ll give a short proof of the equations, which helps explain why they are true;\n",
            "weâ€™llrestatetheequationsinalgorithmicformaspseudocode,andseehowthepseudocode\n",
            "canbeimplementedasreal,runningPythoncode;and,inthefinalsectionofthechapter,\n",
            "weâ€™lldevelopanintuitivepictureofwhatthebackpropagationequationsmean,andhow\n",
            "someonemightdiscoverthemfromscratch. Alongthewayweâ€™llreturnrepeatedlytothe\n",
            "fourfundamentalequations,andasyoudeepenyourunderstandingthoseequationswill\n",
            "cometoseemcomfortableand,perhaps,evenbeautifulandnatural. Anequationfortheerrorintheoutputlayer,Î´L: ThecomponentsofÎ´L aregivenby\n",
            "âˆ‚C\n",
            "Î´L j = âˆ‚aL Ïƒ (cid:48)(z j L ). (BP1)\n",
            "j\n",
            "2Thisisonlythecaseforsmallchangesâˆ†zl,ofcourse.Weâ€™llassumethatthedemonisconstrained\n",
            "j\n",
            "tomakesuchsmallchanges. 3InclassificationproblemslikeMNISTthetermâ€œerrorâ€issometimesusedtomeantheclassification\n",
            "failurerate. E.g.,iftheneuralnetcorrectlyclassifies96.0percentofthedigits,thentheerroris4.0\n",
            "percent. Obviously,thishasquiteadifferentmeaningfromourÎ´vectors.\n",
            "Inpractice,youshouldnâ€™t\n",
            "havetroubletellingwhichmeaningisintendedinanygivenusage. \n",
            "(cid:12)\n",
            "2.4. Thefourfundamentalequationsbehindbackpropagation (cid:12) 45\n",
            "(cid:12)\n",
            "Thisisaverynaturalexpression. Thefirsttermontheright,âˆ‚C/âˆ‚aL,justmeasureshow\n",
            "j\n",
            "fastthecostischangingasafunctionofthe j-thoutputactivation. If,forexample,C doesnâ€™t\n",
            "dependmuchonaparticularoutputneuron, j,thenÎ´L willbesmall,whichiswhatweâ€™d\n",
            "j\n",
            "expect. Thesecondtermontheright,Ïƒ (cid:48)(z\n",
            "j\n",
            "L ),measureshowfasttheactivationfunctionÏƒis 2\n",
            "changingatzL. j\n",
            "NoticethateverythinginEq.BP1iseasilycomputed. Inparticular,wecomputezL while\n",
            "j\n",
            "computingthebehaviourofthenetwork,anditâ€™sonlyasmalladditionaloverheadtocompute\n",
            "Ïƒ (cid:48)(z\n",
            "j\n",
            "L ). Theexactformofâˆ‚C/âˆ‚aL\n",
            "j\n",
            "will,ofcourse,dependontheformofthecostfunction. However, provided the cost function is known there should be little trouble computing\n",
            "âˆ‚C/âˆ‚aL j . Forexample,ifweâ€™reusingthequadraticcostfunctionthenC= 1 2 (cid:80) j(y j aL j) 2,\n",
            "andsoâˆ‚C/âˆ‚aL\n",
            "j\n",
            "=(aL\n",
            "j\n",
            "y j),whichobviouslyiseasilycomputable. âˆ’\n",
            "EquationBP1isac âˆ’ omponentwiseexpressionforÎ´L. Itâ€™saperfectlygoodexpression,but\n",
            "notthematrix-basedformwewantforbackpropagation. However,itâ€™seasytorewritethe\n",
            "equationinamatrix-basedform,as\n",
            "Î´L = a C Ïƒ (cid:48)(zL ). (BP1a)\n",
            "âˆ‡ (cid:12)\n",
            "Here, C isdefinedtobeavectorwhosecomponentsarethepartialderivativesâˆ‚C/âˆ‚aL. a j\n",
            "You caâˆ‡n think of C as expressing the rate of change of C with respect to the output\n",
            "a\n",
            "activations. Itâ€™seasâˆ‡ytoseethatEquationsBP1aandBP1areequivalent,andforthatreason\n",
            "fromnowonweâ€™lluseBP1interchangeablytorefertobothequations.\n",
            "Asanexample,inthe\n",
            "caseofthequadraticcostwehave\n",
            "a\n",
            "C=(aL y),andsothefullymatrix-basedformof\n",
            "BP1becomes âˆ‡ âˆ’\n",
            "Î´L =(aL y) Ïƒ (cid:48)(zL ). (2.8)\n",
            "âˆ’ (cid:12)\n",
            "Asyoucansee,everythinginthisexpressionhasanicevectorform,andiseasilycomputed\n",
            "usingalibrarysuchasNumpy. AnequationfortheerrorÎ´lintermsoftheerrorinthenextlayer,Î´l+1:Inparticular\n",
            "Î´l =((wl+1 ) TÎ´l+1 ) Ïƒ (cid:48)(zl ), (BP2)\n",
            "(cid:12)\n",
            "where(wl+1 ) T isthetransposeoftheweightmatrixwl+1forthe(l+1)-thlayer. Thisequation\n",
            "appearscomplicated,buteachelementhasaniceinterpretation. Supposeweknowthe\n",
            "errorÎ´l+1atthe(l+1)-thlayer. Whenweapplythetransposeweightmatrix,(wl+1\n",
            ")\n",
            "T,we\n",
            "canthinkintuitivelyofthisasmovingtheerrorbackwardthroughthenetwork,givingus\n",
            "somesortofmeasureoftheerrorattheoutputofthel-thlayer. WethentaketheHadamard\n",
            "product Ïƒ (cid:48)(zl ).\n",
            "Thismovestheerrorbackwardthroughtheactivationfunctioninlayerl,\n",
            "givingus(cid:12)theerrorÎ´l intheweightedinputtolayerl.\n",
            "Bycombining(BP2)with(BP1)wecancomputetheerrorÎ´lforanylayerinthenetwork. Westartbyusing(BP1)tocomputeÎ´L,thenapplyEquation(BP2)tocomputeÎ´L 1,then\n",
            "âˆ’\n",
            "Equation(BP2)againtocomputeÎ´L 2,andsoon,allthewaybackthroughthenetwork. âˆ’\n",
            "Anequationfortherateofchangeofthecostwithrespecttoanybiasinthenet-\n",
            "work: Inparticular:\n",
            "âˆ‚C\n",
            "âˆ‚bl = Î´l j . (BP3)\n",
            "j\n",
            "Thatis,theerrorÎ´l isexactlyequaltotherateofchangeâˆ‚C/âˆ‚bl. Thisisgreatnews,since\n",
            "j j\n",
            "\n",
            "(cid:12)\n",
            "46 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "(BP1)and(BP2)havealreadytoldushowtocomputeÎ´l. Wecanrewrite(BP3)inshorthand\n",
            "j\n",
            "as\n",
            "âˆ‚C\n",
            "âˆ‚b = Î´, (2.9)\n",
            "2\n",
            "whereitisunderstoodthatÎ´isbeingevaluatedatthesameneuronasthebias b. An equation for the rate of change of the cost with respect to any weight in the\n",
            "network: Inparticular:\n",
            "âˆ‚C\n",
            "âˆ‚wl\n",
            "=a\n",
            "k\n",
            "l\n",
            "âˆ’\n",
            "1Î´l\n",
            "j\n",
            ". (BP4)\n",
            "jk\n",
            "Thistellsushowtocomputethepartialderivativesâˆ‚C/âˆ‚wl intermsofthequantitiesÎ´l\n",
            "jk\n",
            "andal 1,whichwealreadyknowhowtocompute. Theequationcanberewritteninaless\n",
            "âˆ’\n",
            "index-heavynotationas\n",
            "âˆ‚C\n",
            "âˆ‚w\n",
            "=a\n",
            "in\n",
            "Î´\n",
            "out\n",
            ", (2.10)\n",
            "whereitâ€™sunderstoodthata istheactivationoftheneuroninputtotheweightw,andÎ´\n",
            "in out\n",
            "istheerroroftheneuronoutputfromtheweightw. Zoomingintolookatjusttheweightw,\n",
            "andthetwoneuronsconnectedbythatweight,wecandepictthisas:\n",
            "AniceconsequenceofEquation2.10isthatwhentheactivationa issmall,a 0,the\n",
            "in in\n",
            "gradienttermâˆ‚C/âˆ‚wwillalsotendtobesmall. Inthiscase,weâ€™llsaytheweighâ‰ˆtlearns\n",
            "slowly,meaningthatitâ€™snotchangingmuchduringgradientdescent. Inotherwords,one\n",
            "consequenceof(BP4)isthatweightsoutputfromlow-activationneuronslearnslowly. Thereareotherinsightsalongtheselineswhichcanbeobtainedfrom(BP1)â€“(BP4). Letâ€™s\n",
            "startbylookingattheoutputlayer.\n",
            "ConsiderthetermÏƒ (cid:48)(z\n",
            "j\n",
            "L )in(BP1). Recallfromthegraph\n",
            "ofthesigmoidfunctioninthelastchapterthattheÏƒfunctionbecomesveryflatwhenÏƒ (z j L )\n",
            "isapproximately0or1. WhenthisoccurswewillhaveÏƒ (cid:48)(z j L ) 0. Andsothelessonisthat\n",
            "aweightinthefinallayerwilllearnslowlyiftheoutputneuronâ‰ˆiseitherlowactivation( 0)\n",
            "orhighactivation( 1). Inthiscaseitâ€™scommontosaytheoutputneuronhassaturâ‰ˆated\n",
            "and,asaresult,theâ‰ˆweighthasstoppedlearning(orislearningslowly). Similarremarks\n",
            "holdalsoforthebiasesofoutputneuron.\n",
            "Wecanobtainsimilarinsightsforearlierlayers. Inparticular,notetheÏƒ (cid:48)(zl )termin\n",
            "(BP2). ThismeansthatÎ´l islikelytogetsmalliftheneuronisnearsaturation.\n",
            "Andthis,in\n",
            "j\n",
            "turn,meansthatanyweightsinputtoasaturatedneuronwilllearnslowly4. Summingup,weâ€™velearntthataweightwilllearnslowlyifeithertheinputneuronis\n",
            "low-activation,oriftheoutputneuronhassaturated,i.e.,iseitherhigh-orlow-activation. Noneoftheseobservationsistoogreatlysurprising. Still,theyhelpimproveourmental\n",
            "modelofwhatâ€™sgoingonasaneuralnetworklearns. Furthermore,wecanturnthistype\n",
            "ofreasoningaround. Thefourfundamentalequationsturnouttoholdforanyactivation\n",
            "function,notjustthestandardsigmoidfunction(thatâ€™sbecause,asweâ€™llseeinamoment,\n",
            "4Thisreasoningwonâ€™tholdif(wl+1\n",
            ")\n",
            "TÎ´l+1haslargeenoughentriestocompensateforthesmallness\n",
            "ofÏƒ (cid:48)(zl j).ButIâ€™mspeakingofthegeneraltendency. \n",
            "(cid:12)\n",
            "2.4. Thefourfundamentalequationsbehindbackpropagation (cid:12) 47\n",
            "(cid:12)\n",
            "theproofsdonâ€™tuseanyspecialpropertiesofÏƒ). Andsowecanusetheseequationsto\n",
            "designactivationfunctionswhichhaveparticulardesiredlearningproperties. Asanexample\n",
            "togiveyoutheidea,supposeweweretochoosea(non-sigmoid)activationfunctionÏƒso\n",
            "thatÏƒ isalwayspositive,andnevergetsclosetozero. Thatwouldpreventtheslow-down 2\n",
            "(cid:48)\n",
            "oflearningthatoccurswhenordinarysigmoidneuronssaturate. Laterinthebookweâ€™llsee\n",
            "exampleswherethiskindofmodificationismadetotheactivationfunction. Keepingthe\n",
            "fourequationsBP1â€“BP4inmindcanhelpexplainwhysuchmodificationsaretried,and\n",
            "whatimpacttheycanhave. Problem\n",
            "Alternatepresentationoftheequationsofbackpropagation: Iâ€™vestatedtheequa-\n",
            "â€¢ tionsofbackpropagation(notablyBP1andBP2)usingtheHadamardproduct. This\n",
            "presentationmaybedisconcertingifyouâ€™reunusedtotheHadamardproduct.\n",
            "Thereâ€™s\n",
            "analternativeapproach,basedonconventionalmatrixmultiplication,whichsome\n",
            "readersmayfindenlightening. (1) Showthat(BP1)mayberewrittenas\n",
            "Î´L = Î£ (cid:48)(zL ) a C, (2.11)\n",
            "âˆ‡\n",
            "whereÎ£ (cid:48)(zL )isasquarematrixwhosediagonalentriesarethevaluesÏƒ (cid:48)(z\n",
            "j\n",
            "L ),\n",
            "andwhoseoff-diagonalentriesarezero. Notethatthismatrixactson C by\n",
            "a\n",
            "conventionalmatrixmultiplication. âˆ‡\n",
            "(2) Showthat(BP2)mayberewrittenas\n",
            "Î´l = Î£ (cid:48)(zl )(wl+1 ) TÎ´l+1. (2.12)\n",
            "(3) Bycombiningobservations(1)and(2)showthat\n",
            "Î´l = Î£ (cid:48)(zl )(wl+1 ) T...Î£ (cid:48)(zL âˆ’ 1 )(wL ) TÎ£ (cid:48)(zL ) a C (2.13)\n",
            "âˆ‡\n",
            "Forreaderscomfortablewithmatrixmultiplicationthisequationmaybeeasier\n",
            "tounderstandthan(BP1)and(BP2). ThereasonIâ€™vefocusedon(BP1)and\n",
            "(BP2)isbecausethatapproachturnsouttobefastertoimplementnumerically.\n",
            "\n",
            "(cid:12)\n",
            "48 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2.5 Proof of the four fundamental equations (optional)\n",
            "2 Weâ€™llnowprovethefourfundamentalequations(BP1)â€“(BP4). Allfourareconsequencesof\n",
            "thechainrulefrommultivariablecalculus.\n",
            "Ifyouâ€™recomfortablewiththechainrule,thenI\n",
            "stronglyencourageyoutoattemptthederivationyourselfbeforereadingon. Letâ€™sbeginwithEquation(BP1),whichgivesanexpressionfortheoutputerror,Î´l. To\n",
            "provethisequation,recallthatbydefinition\n",
            "âˆ‚C\n",
            "Î´L j = âˆ‚zL . (2.14)\n",
            "j\n",
            "Applyingthechainrule,wecanre-expressthepartialderivativeaboveintermsofpartial\n",
            "derivativeswithrespecttotheoutputactivations,\n",
            "(cid:88) âˆ‚C âˆ‚aL\n",
            "Î´L j = âˆ‚aL âˆ‚z k L , (2.15)\n",
            "k k j\n",
            "wherethesumisoverallneuronskintheoutputlayer. Ofcourse,theoutputactivationaL\n",
            "k\n",
            "ofthek-thneurondependsonlyontheweightedinputz\n",
            "j\n",
            "L forthe j-thneuronwhenk= j. Andsoâˆ‚a\n",
            "k\n",
            "L/âˆ‚z\n",
            "j\n",
            "L vanisheswhenk= j.\n",
            "Asaresultwecansimplifythepreviousequationto\n",
            "(cid:54)\n",
            "Î´L j = âˆ‚ âˆ‚ a C L âˆ‚ âˆ‚ a zL L j . (2.16)\n",
            "j j\n",
            "RecallingthataL j = Ïƒ (z j L )thesecondtermontherightcanbewrittenasÏƒ (cid:48)(z j L ),andthe\n",
            "equationbecomes\n",
            "âˆ‚C\n",
            "Î´L j = âˆ‚aL Ïƒ (cid:48)(z j L ), (2.17)\n",
            "j\n",
            "whichisjust(BP1),incomponentform. Next,weâ€™llprove(BP2),whichgivesanequation\n",
            "fortheerrorÎ´l intermsoftheerrorinthenextlayer,Î´l+1. Todothis,wewanttorewrite\n",
            "Î´l j= âˆ‚C/âˆ‚zl j intermsofÎ´ k l+1 = âˆ‚C/âˆ‚z k l+1. Wecandothisusingthechainrule,\n",
            "âˆ‚C (cid:88) âˆ‚C âˆ‚zl+1 (cid:88) âˆ‚zl+1\n",
            "Î´l j= âˆ‚zl = âˆ‚zl+1 âˆ‚z k l = âˆ‚z k l Î´ k l+1, (2.18)\n",
            "j k k j k j\n",
            "where in the last line we have interchanged the two terms on the right-hand side, and\n",
            "substitutedthedefinitionofÎ´l+1. Toevaluatethefirsttermonthelastline,notethat\n",
            "k\n",
            "(cid:88) (cid:88)\n",
            "z k l+1 = wl k + j 1al j+b k l+1 = wl k + j 1Ïƒ (zl j)+b k l+1. (2.19)\n",
            "j j\n",
            "Differentiating,weobtain\n",
            "âˆ‚zl+1\n",
            "âˆ‚z\n",
            "k\n",
            "l\n",
            "=wl\n",
            "k\n",
            "+\n",
            "j\n",
            "1Ïƒ (cid:48)(zl j). (2.20)\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "2.6. Thebackpropagationalgorithm (cid:12) 49\n",
            "(cid:12)\n",
            "Substitutingbackinto(2.18)weobtain\n",
            "(cid:88)\n",
            "Î´l j= wl k + j 1Î´ k l+1Ïƒ (cid:48)(zl j). (2.21)\n",
            "k 2\n",
            "Thisisjust(BP2)writtenincomponentform.\n",
            "Thefinaltwoequationswewanttoproveare(BP3)and(BP4). Thesealsofollowfrom\n",
            "thechainrule,inamannersimilartotheproofsofthetwoequationsabove. Ileavethemto\n",
            "youasanexercise. Exercise\n",
            "ProveEquations(BP3)and(BP4). â€¢\n",
            "Thatcompletestheproofofthefourfundamentalequationsofbackpropagation. Theproof\n",
            "mayseemcomplicated. Butitâ€™sreallyjusttheoutcomeofcarefullyapplyingthechainrule. Alittlelesssuccinctly,wecanthinkofbackpropagationasawayofcomputingthegradient\n",
            "ofthecostfunctionbysystematicallyapplyingthechainrulefrommulti-variablecalculus. Thatâ€™salltherereallyistobackpropagationâ€“therestisdetails. 2.6 The backpropagation algorithm\n",
            "Thebackpropagationequationsprovideuswithawayofcomputingthegradientofthecost\n",
            "function. Letâ€™sexplicitlywritethisoutintheformofanalgorithm:\n",
            "1.\n",
            "Input x: Setthecorrespondingactivationa1fortheinputlayer.\n",
            "2.\n",
            "Feedforward: Foreachl=2,3,...,Lcomputezl =wlal âˆ’ 1 +bl andal = Ïƒ (z l). 3. OutputerrorÎ´L: ComputethevectorÎ´L = a C Ïƒ (cid:48)(zL ). 4. Backpropagatetheerror:Foreachl=L 1,âˆ‡L 2,(cid:12)...,2computeÎ´l =((wl+1 ) TÎ´l+1 )\n",
            "Ïƒ (cid:48)(zl ). âˆ’ âˆ’ (cid:12)\n",
            "5. Output: Thegradientofthecostfunctionisgivenby âˆ‚ âˆ‚ w C l =a k l âˆ’ 1Î´l j and âˆ‚ âˆ‚ b C l = Î´l j . jk j\n",
            "Examiningthealgorithmyoucanseewhyitâ€™scalledbackpropagation. Wecomputetheerror\n",
            "vectorsÎ´l backward,startingfromthefinallayer. Itmayseempeculiarthatweâ€™regoing\n",
            "throughthenetworkbackward. Butifyouthinkabouttheproofofbackpropagation,the\n",
            "backwardmovementisaconsequenceofthefactthatthecostisafunctionofoutputsfrom\n",
            "thenetwork. Tounderstandhowthecostvarieswithearlierweightsandbiasesweneed\n",
            "torepeatedlyapplythechainrule,workingbackwardthroughthelayerstoobtainusable\n",
            "expressions. Exercises\n",
            "BackpropagationwithasinglemodifiedneuronSupposewemodifyasingleneuron\n",
            "â€¢ inafeedforwardnetworksothattheoutputfromtheneuronisgivenby f( (cid:80) j w j x j+\n",
            "b),where f issomefunctionotherthanthesigmoid. Howshouldwemodifythe\n",
            "backpropagationalgorithminthiscase? Backpropagation with linear neurons Suppose we replace the usual non-linear\n",
            "â€¢ Ïƒ function with Ïƒ (z) = z throughout the network. Rewrite the backpropagation\n",
            "algorithmforthiscase. As Iâ€™ve described it above, the backpropagation algorithm computes the gradient of the\n",
            "costfunctionforasingletrainingexample,C =C\n",
            "x\n",
            ". Inpractice,itâ€™scommontocombine\n",
            "backpropagationwithalearningalgorithmsuchasstochasticgradientdescent,inwhichwe\n",
            "computethegradientformanytrainingexamples. Inparticular,givenamini-batchofm\n",
            "\n",
            "(cid:12)\n",
            "50 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "trainingexamples,thefollowingalgorithmappliesagradientdescentlearningstepbasedon\n",
            "thatmini-batch:\n",
            "1. Inputasetoftrainingexamples\n",
            "2 2. Foreachtrainingexamplex: Setthecorrespondinginputactivationax,1,andperform\n",
            "thefollowingsteps:\n",
            "Feedforward: Foreachl=2,3,...,L computezx,l = wlax,l âˆ’ 1 +bl and ax,l =\n",
            "â€¢ Ïƒ (zx,l ). OutputerrorÎ´x,L: ComputethevectorÎ´x,L = a C x Ïƒ (cid:48)(zx,L ). â€¢ Backpropagate the error: For each l = L âˆ‡1,L (cid:12)2,...,2 compute Î´x,l =\n",
            "â€¢ ((wl+1 ) TÎ´x,l+1 ) Ïƒ (cid:48)(zx,l ). âˆ’ âˆ’\n",
            "3.\n",
            "Gradientdescent: For(cid:12)eachl=L,L 1,...,2updatetheweightsaccordingtotherule\n",
            "wl wl\n",
            "m\n",
            "Î·(cid:80)\n",
            "x\n",
            "Î´x,l (ax,l\n",
            "âˆ’\n",
            "1\n",
            ")\n",
            "T,andthâˆ’ebiasesaccordingtotherulebl bl\n",
            "m\n",
            "Î·(cid:80)\n",
            "x\n",
            "Î´x,l. â†’ âˆ’ â†’ âˆ’\n",
            "Ofcourse,toimplementstochasticgradientdescentinpracticeyoualsoneedanouterloop\n",
            "generatingmini-batchesoftrainingexamples,andanouterloopsteppingthroughmultiple\n",
            "epochsoftraining. Iâ€™veomittedthoseforsimplicity. 2.7 The code for backpropagation\n",
            "Havingunderstoodbackpropagationintheabstract,wecannowunderstandthecodeused\n",
            "inthelastchaptertoimplementbackpropagation. Recallfromthatchapterthatthecodewas\n",
            "containedintheupdate_mini_batchandbackpropmethodsoftheNetworkclass. The\n",
            "codeforthesemethodsisadirecttranslationofthealgorithmdescribedabove. Inparticular,\n",
            "theupdate_mini_batchmethodupdatestheNetworkâ€™sweightsandbiasesbycomputing\n",
            "thegradientforthecurrentmini_batchoftrainingexamples:\n",
            "class Network(object):\n",
            "... def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the networkâ€™s weights and biases by applying\n",
            "gradient descent using backpropagation to a single mini batch. The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
            "is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "Mostoftheworkisdonebythelinedelta_nabla_b,delta_nabla_w = self.backprop\n",
            "(x, y)whichusesthebackpropmethodtofigureoutthepartialderivativesâˆ‚C /âˆ‚bl and\n",
            "x j\n",
            "âˆ‚C /âˆ‚wl . Thebackpropmethodfollowsthealgorithminthelastsectionclosely.\n",
            "Thereis\n",
            "x jk\n",
            "onesmallchangeâ€“weuseaslightlydifferentapproachtoindexingthelayers. Thischange\n",
            "ismadetotakeadvantageofafeatureofPython,namelytheuseofnegativelistindices\n",
            "tocountbackwardfromtheendofalist,so,e.g.,l[-3]isthethirdlastentryinalistl. Thecodeforbackpropisbelow,togetherwithafewhelperfunctions,whichareusedto\n",
            "computetheÏƒfunction,thederivativeÏƒ,andthederivativeofthecostfunction. Withthese\n",
            "(cid:48)\n",
            "\n",
            "(cid:12)\n",
            "2.7. Thecodeforbackpropagation (cid:12) 51\n",
            "(cid:12)\n",
            "inclusionsyoushouldbeabletounderstandthecodeinaself-containedway. Ifsomethingâ€™s\n",
            "trippingyouup,youmayfindithelpfultoconsulttheoriginaldescription(andcomplete\n",
            "listing)ofthecode. 2\n",
            "class Network(object):\n",
            "... def backprop(self, x, y):\n",
            "\"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
            "gradient for the cost function C_x. \"nabla_b\" and\n",
            "\"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
            "to \"self.biases\" and \"self.weights\".\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "zs.append(z)\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. Itâ€™s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "... def cost_derivative(self, output_activations, y):\n",
            "\"\"\"Return the vector of partial derivatives \\partial{} C_x /\n",
            "\\partial{} a for the output activations.\"\"\"\n",
            "return (output_activations-y)\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "Problem\n",
            "Fully matrix-based approach to backpropagation over a mini-batch Our imple-\n",
            "â€¢ mentationofstochasticgradientdescentloopsovertrainingexamplesinamini-batch. Itâ€™spossibletomodifythebackpropagationalgorithmsothatitcomputesthegradients\n",
            "foralltrainingexamplesinamini-batchsimultaneously.\n",
            "Theideaisthatinsteadof\n",
            "beginningwithasingleinputvector, x,wecanbeginwithamatrixX =[x 1 x 2 ...x m]\n",
            "\n",
            "(cid:12)\n",
            "52 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "whosecolumnsarethevectorsinthemini-batch. Weforward-propagatebymultiply-\n",
            "ingbytheweightmatrices,addingasuitablematrixforthebiasterms,andapplying\n",
            "thesigmoidfunctioneverywhere. Webackpropagatealongsimilarlines. Explicitly\n",
            "2 writeoutpseudocodeforthisapproachtothebackpropagationalgorithm. Modify\n",
            "network.pysothatitusesthisfullymatrix-basedapproach. Theadvantageofthis\n",
            "approachisthatittakesfulladvantageofmodernlibrariesforlinearalgebra.\n",
            "Asa\n",
            "resultitcanbequiteabitfasterthanloopingoverthemini-batch. (Onmylaptop,\n",
            "forexample,thespeedupisaboutafactoroftwowhenrunonMNISTclassification\n",
            "problemslikethoseweconsideredinthelastchapter.) Inpractice,allseriouslibraries\n",
            "forbackpropagationusethisfullymatrix-basedapproachorsomevariant. 2.8 In what sense is backpropagation a fast algorithm? Inwhatsenseisbackpropagationafastalgorithm?\n",
            "Toanswerthisquestion,letâ€™sconsider\n",
            "anotherapproachtocomputingthegradient. Imagineitâ€™stheearlydaysofneuralnetworks\n",
            "research. Maybeitâ€™sthe1950sor1960s,andyouâ€™rethefirstpersonintheworldtothinkof\n",
            "usinggradientdescenttolearn! Buttomaketheideaworkyouneedawayofcomputing\n",
            "thegradientofthecostfunction. Youthinkbacktoyourknowledgeofcalculus,anddecide\n",
            "toseeifyoucanusethechainruletocomputethegradient. Butafterplayingaroundabit,\n",
            "thealgebralookscomplicated,andyougetdiscouraged.\n",
            "Soyoutrytofindanotherapproach. YoudecidetoregardthecostasafunctionoftheweightsC=C(w)alone(weâ€™llgetbackto\n",
            "thebiasesinamoment). Younumbertheweightsw ,w ,...,andwanttocomputeâˆ‚C/âˆ‚w\n",
            "1 2 j\n",
            "forsomeparticularweightw . Anobviouswayofdoingthatistousetheapproximation\n",
            "j\n",
            "âˆ‚C C(w+ Îµe j) C(w)\n",
            "âˆ‚w\n",
            "j â‰ˆ\n",
            "Îµ âˆ’ , (2.22)\n",
            "whereÎµ>0isasmallpositivenumber,ande istheunitvectorinthe j-thdirection. In\n",
            "j\n",
            "otherwords,wecanestimateâˆ‚C/âˆ‚w bycomputingthecostC fortwoslightlydifferent\n",
            "j\n",
            "valuesofw ,andthenapplyingEquation2.22. Thesameideawillletuscomputethepartial\n",
            "j\n",
            "derivativesâˆ‚C/âˆ‚bwithrespecttothebiases. Thisapproachlooksverypromising. Itâ€™ssimpleconceptually, andextremelyeasyto\n",
            "implement,usingjustafewlinesofcode. Certainly,itlooksmuchmorepromisingthanthe\n",
            "ideaofusingthechainruletocomputethegradient! Unfortunately,whilethisapproachappearspromising,whenyouimplementthecodeit\n",
            "turnsouttobeextremelyslow. Tounderstandwhy,imaginewehaveamillionweightsin\n",
            "ournetwork. Thenforeachdistinctweightw\n",
            "j\n",
            "weneedtocomputeC(w+ Îµe j)inorderto\n",
            "computeâˆ‚C/âˆ‚w . Thatmeansthattocomputethegradientweneedtocomputethecost\n",
            "j\n",
            "functionamilliondifferenttimes,requiringamillionforwardpassesthroughthenetwork\n",
            "(pertrainingexample). WeneedtocomputeC(w)aswell,sothatâ€™satotalofamillionand\n",
            "onepassesthroughthenetwork. Whatâ€™scleveraboutbackpropagationisthatitenablesustosimultaneouslycomputeall\n",
            "thepartialderivativesâˆ‚C/âˆ‚w usingjustoneforwardpassthroughthenetwork,followed\n",
            "j\n",
            "by one backward pass through the network. Roughly speaking, the computational cost\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 53\n",
            "(cid:12)\n",
            "of the backward pass is about the same as the forward pass5. And so the total cost of\n",
            "backpropagationisroughlythesameasmakingjusttwoforwardpassesthroughthenetwork. Comparethattothemillionandoneforwardpassesweneededfortheapproachbasedon\n",
            "(2.22)! Andsoeventhoughbackpropagationappearssuperficiallymorecomplexthanthe 2\n",
            "approachbasedon(2.22),itâ€™sactuallymuch,muchfaster. Thisspeedupwasfirstfullyappreciatedin1986,anditgreatlyexpandedtherangeof\n",
            "problemsthatneuralnetworkscouldsolve. That,inturn,causedarushofpeopleusing\n",
            "neuralnetworks. Ofcourse,backpropagationisnotapanacea. Eveninthelate1980speople\n",
            "ranupagainstlimits,especiallywhenattemptingtousebackpropagationtotraindeepneural\n",
            "networks,i.e.,networkswithmanyhiddenlayers. Laterinthebookweâ€™llseehowmodern\n",
            "computersandsomeclevernewideasnowmakeitpossibletousebackpropagationtotrain\n",
            "suchdeepneuralnetworks. 2.9 Backpropagation: the big picture\n",
            "AsIâ€™veexplainedit,backpropagationpresentstwomysteries. First,whatâ€™sthealgorithm\n",
            "reallydoing?\n",
            "Weâ€™vedevelopedapictureoftheerrorbeingbackpropagatedfromtheoutput. Butcanwegoanydeeper,andbuildupmoreintuitionaboutwhatisgoingonwhenwe\n",
            "doallthesematrixandvectormultiplications? Thesecondmysteryishowsomeonecould\n",
            "everhavediscoveredbackpropagationinthefirstplace? Itâ€™sonethingtofollowthestepsin\n",
            "analgorithm,oreventofollowtheproofthatthealgorithmworks. Butthatdoesnâ€™tmean\n",
            "youunderstandtheproblemsowellthatyoucouldhavediscoveredthealgorithminthe\n",
            "firstplace. Isthereaplausiblelineofreasoningthatcouldhaveledyoutodiscoverthe\n",
            "backpropagationalgorithm?\n",
            "InthissectionIâ€™lladdressboththesemysteries. Toimproveourintuitionaboutwhatthealgorithmisdoing,letâ€™simaginethatweâ€™ve\n",
            "madeasmallchangeâˆ†wl tosomeweightinthenetwork,wl :\n",
            "jk jk\n",
            "Thatchangeinweightwillcauseachangeintheoutputactivationfromthecorresponding\n",
            "neuron:\n",
            "5Thisshouldbeplausible,butitrequiressomeanalysistomakeacarefulstatement.Itâ€™splausible\n",
            "becausethedominantcomputationalcostintheforwardpassismultiplyingbytheweightmatrices,\n",
            "whileinthebackwardpassitâ€™smultiplyingbythetransposesoftheweightmatrices.Theseoperations\n",
            "obviouslyhavesimilarcomputationalcost. \n",
            "(cid:12)\n",
            "54 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2\n",
            "That,inturn,willcauseachangeinalltheactivationsinthenextlayer:\n",
            "Thosechangeswillinturncausechangesinthenextlayer,andthenthenext,andsoonall\n",
            "thewaythroughtocausingachangeinthefinallayer,andtheninthecostfunction:\n",
            "Thechangeâˆ†C inthecostisrelatedtothechangeâˆ†wl intheweightbytheequation\n",
            "jk\n",
            "âˆ‚C\n",
            "âˆ†C âˆ†wl . (2.23)\n",
            "â‰ˆ\n",
            "âˆ‚wl\n",
            "jk\n",
            "jk\n",
            "Thissuggeststhatapossibleapproachtocomputingâˆ‚C/âˆ‚wl istocarefullytrackhowa\n",
            "jk\n",
            "smallchangeinwl propagatestocauseasmallchangeinC.\n",
            "Ifwecandothat,beingcareful\n",
            "jk\n",
            "toexpresseverythingalongthewayintermsofeasilycomputablequantities,thenweshould\n",
            "beabletocomputeâˆ‚C/âˆ‚wl . jk\n",
            "Letâ€™strytocarrythisout. Thechangeâˆ†wl causesasmallchangeâˆ†al intheactivation\n",
            "jk j\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 55\n",
            "(cid:12)\n",
            "ofthe j-thneuroninthel-thlayer. Thischangeisgivenby\n",
            "âˆ‚al\n",
            "âˆ†al j\n",
            "â‰ˆ\n",
            "âˆ‚wl\n",
            "j\n",
            "j\n",
            "k\n",
            "âˆ†wl jk .\n",
            "(2.24) 2\n",
            "Thechangeinactivationâˆ†al willcausechangesinalltheactivationsinthenextlayer,i.e.,\n",
            "j\n",
            "the(l+1)-thlayer. Weâ€™llconcentrateonthewayjustasingleoneofthoseactivationsis\n",
            "affected,sayal+1,\n",
            "q\n",
            "Infact,itâ€™llcausethefollowingchange:\n",
            "âˆ‚al+1\n",
            "âˆ†al+1 q âˆ†al. (2.25)\n",
            "q\n",
            "â‰ˆ\n",
            "âˆ‚al\n",
            "j\n",
            "j\n",
            "SubstitutingintheexpressionfromEquation2.24,weget:\n",
            "âˆ‚al+1 âˆ‚al\n",
            "âˆ†al+1 q j âˆ†wl .\n",
            "(2.26)\n",
            "q\n",
            "â‰ˆ\n",
            "âˆ‚al\n",
            "j\n",
            "âˆ‚wl\n",
            "jk\n",
            "jk\n",
            "Ofcourse,thechangeâˆ†al+1will,inturn,causechangesintheactivationsinthenextlayer. q\n",
            "Infact,wecanimagineapathallthewaythroughthenetworkfromwl toC,witheach\n",
            "jk\n",
            "changeinactivationcausingachangeinthenextactivation,and,finally,achangeinthecost\n",
            "attheoutput. Ifthepathgoesthroughactivationsal,al+1, ,aL 1,aL thentheresulting\n",
            "j q nâˆ’ m\n",
            "expressionis Â·Â·Â·\n",
            "âˆ†C\n",
            "âˆ‚C âˆ‚a\n",
            "m\n",
            "L âˆ‚a\n",
            "n\n",
            "L\n",
            "âˆ’\n",
            "1\n",
            "... âˆ‚a\n",
            "q\n",
            "l+1 âˆ‚al\n",
            "j âˆ†wl , (2.27)\n",
            "â‰ˆ\n",
            "âˆ‚a\n",
            "m\n",
            "L âˆ‚a\n",
            "n\n",
            "L\n",
            "âˆ’\n",
            "1âˆ‚a\n",
            "p\n",
            "L\n",
            "âˆ’\n",
            "2 âˆ‚al\n",
            "j\n",
            "âˆ‚wl\n",
            "jk\n",
            "jk\n",
            "thatis,weâ€™vepickedupaâˆ‚a/âˆ‚atypetermforeachadditionalneuronweâ€™vepassedthrough,\n",
            "aswellastheâˆ‚C/âˆ‚aL termattheend. ThisrepresentsthechangeinC duetochangesin\n",
            "m\n",
            "theactivationsalongthisparticularpaththroughthenetwork. Ofcourse,thereâ€™smanypaths\n",
            "bywhichachangeinwl canpropagatetoaffectthecost,andweâ€™vebeenconsideringjusta\n",
            "jk\n",
            "singlepath. TocomputethetotalchangeinC itisplausiblethatweshouldsumoverallthe\n",
            "possiblepathsbetweentheweightandthefinalcost,i.e.,\n",
            "âˆ†C (cid:88) âˆ‚C âˆ‚a m L âˆ‚a n L âˆ’ 1 ... âˆ‚a q l+1 âˆ‚al j âˆ†wl , (2.28)\n",
            "â‰ˆmnp...q\n",
            "âˆ‚a\n",
            "m\n",
            "L âˆ‚a\n",
            "n\n",
            "L\n",
            "âˆ’\n",
            "1âˆ‚a\n",
            "p\n",
            "L\n",
            "âˆ’\n",
            "2 âˆ‚al\n",
            "j\n",
            "âˆ‚wl\n",
            "jk\n",
            "jk\n",
            "\n",
            "(cid:12)\n",
            "56 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "whereweâ€™vesummedoverallpossiblechoicesfortheintermediateneuronsalongthepath. Comparingwith(2.23)weseethat\n",
            "2 âˆ‚ âˆ‚ w C l = (cid:88) âˆ‚ âˆ‚ a C L âˆ‚ âˆ‚ a a L m L 1 âˆ‚ âˆ‚ a a n L L âˆ’ 1 2 ...\n",
            "âˆ‚ âˆ‚ a a q l+ l 1 âˆ‚ âˆ‚ w a l l j . (2.29)\n",
            "jk mnp...q m nâˆ’ pâˆ’ j jk\n",
            "Now,Equation2.29lookscomplicated.\n",
            "However,ithasaniceintuitiveinterpretation.\n",
            "Weâ€™re\n",
            "computingtherateofchangeofCwithrespecttoaweightinthenetwork. Whattheequation\n",
            "tellsusisthateveryedgebetweentwoneuronsinthenetworkisassociatedwitharate\n",
            "factorwhichisjustthepartialderivativeofoneneuronâ€™sactivationwithrespecttotheother\n",
            "neuronâ€™sactivation. Theedge fromthefirst weightto thefirstneuron hasa ratefactor\n",
            "âˆ‚al/âˆ‚wl . Theratefactorforapathisjusttheproductoftheratefactorsalongthepath. j jk\n",
            "Andthetotalrateofchangeâˆ‚C/âˆ‚wl isjustthesumoftheratefactorsofallpathsfromthe\n",
            "jk\n",
            "initialweighttothefinalcost. Thisprocedureisillustratedhere,forasinglepath:\n",
            "WhatIâ€™vebeenprovidinguptonowisaheuristicargument,awayofthinkingaboutwhatâ€™s\n",
            "goingonwhenyouperturbaweightinanetwork. Letmesketchoutalineofthinkingyou\n",
            "couldusetofurtherdevelopthisargument. First,youcouldderiveexplicitexpressionsfor\n",
            "alltheindividualpartialderivativesinEquation2.29. Thatâ€™seasytodowithabitofcalculus. Havingdonethat,youcouldthentrytofigureouthowtowriteallthesumsoverindicesas\n",
            "matrixmultiplications. Thisturnsouttobetedious,andrequiressomepersistence,butnot\n",
            "extraordinaryinsight. Afterdoingallthis,andthensimplifyingasmuchaspossible,what\n",
            "youdiscoveristhatyouendupwithexactlythebackpropagationalgorithm! Andsoyoucan\n",
            "thinkofthebackpropagationalgorithmasprovidingawayofcomputingthesumoverthe\n",
            "ratefactorforallthesepaths. Or,toputitslightlydifferently,thebackpropagationalgorithm\n",
            "isacleverwayofkeepingtrackofsmallperturbationstotheweights(andbiases)asthey\n",
            "propagatethroughthenetwork,reachtheoutput,andthenaffectthecost. Now,Iâ€™mnotgoingtoworkthroughallthishere. Itâ€™smessyandrequiresconsiderable\n",
            "caretoworkthroughallthedetails. Ifyouâ€™reupforachallenge,youmayenjoyattemptingit. Andevenifnot,Ihopethislineofthinkinggivesyousomeinsightintowhatbackpropagation\n",
            "isaccomplishing. Whatabouttheothermysteryâ€“howbackpropagationcouldhavebeendiscoveredin\n",
            "thefirstplace? Infact,ifyoufollowtheapproachIjustsketchedyouwilldiscoveraproof\n",
            "ofbackpropagation. Unfortunately,theproofisquiteabitlongerandmorecomplicated\n",
            "thantheoneIdescribedearlierinthischapter. Sohowwasthatshort(butmoremysterious)\n",
            "proofdiscovered? Whatyoufindwhenyouwriteoutallthedetailsofthelongproofis\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 57\n",
            "(cid:12)\n",
            "that,afterthefact,thereareseveralobvioussimplificationsstaringyouintheface. You\n",
            "makethosesimplifications,getashorterproof,andwritethatout.\n",
            "Andthenseveralmore\n",
            "obvioussimplificationsjumpoutatyou. Soyourepeatagain. Theresultafterafewiterations\n",
            "istheproofwesawearlier6 â€“short,butsomewhatobscure,becauseallthesignpoststo 2\n",
            "itsconstructionhavebeenremoved! Iam,ofcourse,askingyoutotrustmeonthis,but\n",
            "therereallyisnogreatmysterytotheoriginoftheearlierproof. Itâ€™sjustalotofhardwork\n",
            "simplifyingtheproofIâ€™vesketchedinthissection. 6Thereisonecleversteprequired.InEquation2.29theintermediatevariablesareactivationslike\n",
            "al+1.Thecleverideaistoswitchtousingweightedinputs,likezl+1,astheintermediatevariables.If\n",
            "q q\n",
            "youdonâ€™thavethisidea,andinsteadcontinueusingtheactivationsal+1,theproofyouobtainturnsout\n",
            "q\n",
            "tobeslightlymorecomplexthantheproofgivenearlierinthechapter. \n",
            "(cid:12)\n",
            "58 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 59\n",
            "(cid:12)\n",
            "3333\n",
            "Improving the way neural\n",
            "networks learn\n",
            "3\n",
            "When a golf player is first learning to play golf, they usually spend most of their time\n",
            "developingabasicswing. Onlygraduallydotheydevelopothershots,learningtochip,draw\n",
            "andfadetheball,buildingonandmodifyingtheirbasicswing.\n",
            "Inasimilarway,uptonow\n",
            "weâ€™vefocusedonunderstandingthebackpropagationalgorithm. Itâ€™sourâ€œbasicswingâ€,the\n",
            "foundationforlearninginmostworkonneuralnetworks. InthischapterIexplainasuiteof\n",
            "techniqueswhichcanbeusedtoimproveonourvanillaimplementationofbackpropagation,\n",
            "andsoimprovethewayournetworkslearn. Thetechniquesweâ€™lldevelopinthischapterinclude: abetterchoiceofcostfunction,\n",
            "knownasthecross-entropycostfunction;fourso-calledâ€œregularizationâ€methods(L1and\n",
            "L2regularization,dropout,andartificialexpansionofthetrainingdata),whichmakeour\n",
            "networksbetteratgeneralizingbeyondthetrainingdata;abettermethodforinitializing\n",
            "theweightsinthenetwork;andasetofheuristicstohelpchoosegoodhyper-parameters\n",
            "forthenetwork. Iâ€™llalsooverviewseveralothertechniquesinlessdepth. Thediscussions\n",
            "arelargelyindependentofoneanother,andsoyoumayjumpaheadifyouwish. Weâ€™llalso\n",
            "implementmanyofthetechniquesinrunningcode,andusethemtoimprovetheresults\n",
            "obtainedonthehandwritingclassificationproblemstudiedinChapter1. Ofcourse,weâ€™reonlycoveringafewofthemany,manytechniqueswhichhavebeen\n",
            "developedforuseinneuralnets. Thephilosophyisthatthebestentreetotheplethoraof\n",
            "availabletechniquesisin-depthstudyofafewofthemostimportant. Masteringthoseimpor-\n",
            "tanttechniquesisnotjustusefulinitsownright,butwillalsodeepenyourunderstandingof\n",
            "whatproblemscanarisewhenyouuseneuralnetworks. Thatwillleaveyouwellprepared\n",
            "toquicklypickupothertechniques,asyouneedthem.\n",
            "\n",
            "(cid:12)\n",
            "60 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "3.1 The cross-entropy cost function\n",
            "Mostofusfinditunpleasanttobewrong. SoonafterbeginningtolearnthepianoIgavemy\n",
            "firstperformancebeforeanaudience.\n",
            "Iwasnervous,andbeganplayingthepieceanoctave\n",
            "toolow. Igotconfused,andcouldnâ€™tcontinueuntilsomeonepointedoutmyerror. Iwas\n",
            "veryembarrassed. Yetwhileunpleasant,wealsolearnquicklywhenweâ€™redecisivelywrong. 3 YoucanbetthatthenexttimeIplayedbeforeanaudienceIplayedinthecorrectoctave! By\n",
            "contrast,welearnmoreslowlywhenourerrorsarelesswell-defined. Ideally,wehopeandexpectthatourneuralnetworkswilllearnfastfromtheirerrors.\n",
            "Isthiswhathappensinpractice? Toanswerthisquestion,letâ€™slookatatoyexample. The\n",
            "exampleinvolvesaneuronwithjustoneinput:\n",
            "Weâ€™lltrainthisneurontodosomethingridiculouslyeasy: taketheinput1totheoutput0. Ofcourse,thisissuchatrivialtaskthatwecouldeasilyfigureoutanappropriateweightand\n",
            "biasbyhand,withoutusingalearningalgorithm. However,itturnsouttobeilluminatingto\n",
            "usegradientdescenttoattempttolearnaweightandbias. Soletâ€™stakealookathowthe\n",
            "neuronlearns. Tomakethingsdefinite,Iâ€™llpicktheinitialweighttobe0.6andtheinitialbiastobe\n",
            "0.9. Thesearegenericchoicesusedasaplacetobeginlearning,Iwasnâ€™tpickingthemto\n",
            "bespecialinanyway. Theinitialoutputfromtheneuronis0.82,soquiteabitoflearning\n",
            "willbeneededbeforeourneurongetsnearthedesiredoutput,0.0. Thelearningrateis\n",
            "Î·=0.15,whichturnsouttobeslowenoughthatwecanfollowwhatâ€™shappening,butfast\n",
            "enoughthatwecangetsubstantiallearninginjustafewseconds. Thecostisthequadratic\n",
            "costfunction,C,introducedbackinChapter1. Iâ€™llremindyouoftheexactformofthecost\n",
            "functionshortly,sothereâ€™snoneedtogoanddigupthedefinition.\n",
            "Asyoucansee,theneuronrapidlylearnsaweightandbiasthatdrivesdownthecost,and\n",
            "givesanoutputfromtheneuronofabout0.09. Thatâ€™snotquitethedesiredoutput,0.0,but\n",
            "itisprettygood. Suppose,however,thatweinsteadchooseboththestartingweightandthe\n",
            "startingbiastobe2.0. Inthiscasetheinitialoutputis0.98,whichisverybadlywrong. Letâ€™s\n",
            "lookathowtheneuronlearnstooutput0inthiscase. \n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 61\n",
            "(cid:12)\n",
            "3\n",
            "Althoughthisexampleusesthesamelearningrate(Î· =0.15),wecanseethatlearning\n",
            "startsoutmuchmoreslowly. Indeed,forthefirst150orsolearningepochs,theweights\n",
            "andbiasesdonâ€™tchangemuchatall. Thenthelearningkicksinand,muchasinourfirst\n",
            "example,theneuronâ€™soutputrapidlymovescloserto0.0. Thisbehaviorisstrangewhencontrastedtohumanlearning. AsIsaidatthebeginning\n",
            "ofthissection,weoftenlearnfastestwhenweâ€™rebadlywrongaboutsomething. Butweâ€™ve\n",
            "justseenthatourartificialneuronhasalotofdifficultylearningwhenitâ€™sbadlywrongâ€“far\n",
            "moredifficultythanwhenitâ€™sjustalittlewrong. Whatâ€™smore,itturnsoutthatthisbehavior\n",
            "occursnotjustinthistoymodel,butinmoregeneralnetworks. Whyislearningsoslow?\n",
            "Andcanwefindawayofavoidingthisslowdown? Tounderstandtheoriginoftheproblem,considerthatourneuronlearnsbychangingthe\n",
            "weightandbiasataratedeterminedbythepartialderivativesofthecostfunction,âˆ‚C/âˆ‚w\n",
            "and âˆ‚C/âˆ‚b. So saying â€œlearning is slowâ€ is really the same as saying that those partial\n",
            "derivativesaresmall. Thechallengeistounderstandwhytheyaresmall.\n",
            "Tounderstand\n",
            "that,letâ€™scomputethepartialderivatives. Recallthatweâ€™reusingthequadraticcostfunction,\n",
            "which,fromEquation1.6,isgivenby\n",
            "(y a) 2\n",
            "C=\n",
            "âˆ’\n",
            ", (3.1)\n",
            "2\n",
            "where a istheneuronâ€™soutputwhenthetraininginput x =1isused, and y =0isthe\n",
            "correspondingdesiredoutput. Towritethismoreexplicitlyintermsoftheweightandbias,\n",
            "recallthata= Ïƒ (z),wherez=wx+b. Usingthechainruletodifferentiatewithrespectto\n",
            "theweightandbiasweget\n",
            "âˆ‚C\n",
            "âˆ‚w = (a\n",
            "âˆ’\n",
            "y) Ïƒ (cid:48)(z)x=aÏƒ (cid:48)(z) (3.2)\n",
            "âˆ‚C\n",
            "âˆ‚b = (a\n",
            "âˆ’\n",
            "y) Ïƒ (cid:48)(z)=aÏƒ (cid:48)(z), (3.3)\n",
            "whereIhavesubstituted x=1and y=0. Tounderstandthebehavioroftheseexpressions,\n",
            "letâ€™slookmorecloselyattheÏƒ (cid:48)(z)termontheright-handside. RecalltheshapeoftheÏƒ\n",
            "function:\n",
            "\n",
            "(cid:12)\n",
            "62 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Sigmoidfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "3\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "âˆ’ âˆ’ âˆ’\n",
            "Wecanseefromthisgraphthatwhentheneuronâ€™soutputiscloseto1,thecurvegetsvery\n",
            "flat,andsoÏƒ (cid:48)(z)getsverysmall. Equations3.2and3.3thentellusthatâˆ‚C/âˆ‚wandâˆ‚C/âˆ‚b\n",
            "getverysmall.\n",
            "Thisistheoriginofthelearningslowdown. Whatâ€™smore,asweshallseea\n",
            "littlelater,thelearningslowdownoccursforessentiallythesamereasoninmoregeneral\n",
            "neuralnetworks,notjustthetoyexampleweâ€™vebeenplayingwith. 3.1.1 Introducingthecross-entropycostfunction\n",
            "Howcanweaddressthelearningslowdown? Itturnsoutthatwecansolvetheproblemby\n",
            "replacingthequadraticcostwithadifferentcostfunction,knownasthecross-entropy. To\n",
            "understandthecross-entropy,letâ€™smovealittleawayfromoursuper-simpletoymodel. Weâ€™ll\n",
            "supposeinsteadthatweâ€™retryingtotrainaneuronwithseveralinputvariables, x ,x ,...,\n",
            "1 2\n",
            "correspondingweightsw ,w ,...,andabias, b:\n",
            "1 2\n",
            "Theoutputfromtheneuronis,ofcourse,a= Ïƒ (z),wherez= (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "b j+bistheweighted\n",
            "sumoftheinputs. Wedefinethecross-entropycostfunctionforthisneuronby\n",
            "1(cid:88)\n",
            "C= [ylna+(1 y)ln(1 a)], (3.4)\n",
            "âˆ’n x âˆ’ âˆ’\n",
            "wherenisthetotalnumberofitemsoftrainingdata,thesumisoveralltraininginputs, x,\n",
            "and y isthecorrespondingdesiredoutput. Itâ€™snotobviousthattheexpression(3.4)fixesthelearningslowdownproblem. Infact,\n",
            "frankly,itâ€™snotevenobviousthatitmakessensetocallthisacostfunction! Beforeaddressing\n",
            "thelearningslowdown,letâ€™sseeinwhatsensethecross-entropycanbeinterpretedasacost\n",
            "function. Twopropertiesinparticularmakeitreasonabletointerpretthecross-entropyasacost\n",
            "function. First,itâ€™snon-negative,thatis,C>0.\n",
            "Toseethis,noticethat: (a)alltheindividual\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 63\n",
            "(cid:12)\n",
            "termsinthesumin(3.4)arenegative,sincebothlogarithmsareofnumbersintherange0\n",
            "to1;and(b)thereisaminussignoutthefrontofthesum. Second,iftheneuronâ€™sactualoutputisclosetothedesiredoutputforalltraininginputs,\n",
            "x,thenthecross-entropywillbeclosetozero1. Toseethis,supposeforexamplethat y=0\n",
            "anda 0forsomeinputx.\n",
            "Thisisacasewhentheneuronisdoingagoodjobonthatinput. Weseeâ‰ˆthatthefirsttermintheexpression(57)forthecostvanishes,since y=0,whilethe\n",
            "secondtermisjust ln(1 a) 0. Asimilaranalysisholdswhen y=1anda 1. Andso 3\n",
            "thecontributiontoâˆ’thecoâˆ’stwiâ‰ˆllbelowprovidedtheactualoutputisclosetoâ‰ˆthedesired\n",
            "output. Summingup,thecross-entropyispositive,andtendstowardzeroastheneurongets\n",
            "betteratcomputingthedesiredoutput, y,foralltraininginputs,x. Thesearebothproperties\n",
            "weâ€™dintuitivelyexpectforacostfunction. Indeed,bothpropertiesarealsosatisfiedbythe\n",
            "quadraticcost. Sothatâ€™sgoodnewsforthecross-entropy. Butthecross-entropycostfunction\n",
            "hasthebenefitthat,unlikethequadraticcost,itavoidstheproblemoflearningslowing\n",
            "down.\n",
            "Toseethis,letâ€™scomputethepartialderivativeofthecross-entropycostwithrespect\n",
            "totheweights. Wesubstitutea= Ïƒ (z)into(3.4),andapplythechainruletwice,obtaining:\n",
            "âˆ‚C 1(cid:88)(cid:129) y 1 y (cid:139) âˆ‚Ïƒ 1(cid:88)(cid:129) y 1 y (cid:139)\n",
            "âˆ‚w\n",
            "j\n",
            "= âˆ’n\n",
            "x\n",
            "Ïƒ (z)âˆ’1 âˆ’Ïƒ (z) âˆ‚w\n",
            "j\n",
            "= âˆ’n\n",
            "x\n",
            "Ïƒ (z)âˆ’1 âˆ’Ïƒ (z) Ïƒ (cid:48)(z)x j . (3.5)\n",
            "âˆ’ âˆ’\n",
            "Puttingeverythingoveracommondenominatorandsimplifyingthisbecomes:\n",
            "âˆ‚ âˆ‚ w C j = 1 n (cid:88) x Ïƒ (z Ïƒ )( (cid:48) 1 (z)x Ïƒ j (z)) ( Ïƒ (z) âˆ’ y). (3.6)\n",
            "âˆ’\n",
            "Usingthedefinitionofthesigmoidfunction,Ïƒ (z)=1/ (1+e\n",
            "âˆ’\n",
            "z ),andalittlealgebrawecan\n",
            "showthatÏƒ (cid:48)(z)= Ïƒ (z)(1 Ïƒ (z)). Iâ€™llaskyoutoverifythisinanexercisebelow,butfor\n",
            "nowletâ€™sacceptitasgivenâˆ’. WeseethattheÏƒ (cid:48)(z)andÏƒ (z)(1 Ïƒ (z))termscancelinthe\n",
            "equationjustabove,anditsimplifiestobecome: âˆ’\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚w j = n x x j( Ïƒ (z) âˆ’ y). (3.7)\n",
            "Thisisabeautifulexpression.\n",
            "Ittellsusthattherateatwhichtheweightlearnsiscontrolled\n",
            "byÏƒ (z) y,i.e.,bytheerrorintheoutput. Thelargertheerror,thefastertheneuronwill\n",
            "learn. Thâˆ’isisjustwhatweâ€™dintuitivelyexpect. Inparticular,itavoidsthelearningslowdown\n",
            "causedbytheÏƒ (cid:48)(z)termintheanalogousequationforthequadraticcost,Equation(3.2). Whenweusethecross-entropy,theÏƒ (cid:48)(z)termgetscanceledout,andwenolongerneed\n",
            "worryaboutitbeingsmall. Thiscancellationisthespecialmiracleensuredbythecross-\n",
            "entropycostfunction. Actually,itâ€™snotreallyamiracle. Asweâ€™llseelater,thecross-entropy\n",
            "wasspeciallychosentohavejustthisproperty. Inasimilarway,wecancomputethepartialderivativeforthebias. Iwonâ€™tgothrough\n",
            "1ToprovethisIwillneedtoassumethatthedesiredoutputs yarealleither0or1.Thisisusually\n",
            "thecasewhensolvingclassificationproblems,forexample,orwhencomputingBooleanfunctions.To\n",
            "understandwhathappenswhenwedonâ€™tmakethisassumption,seetheexercisesattheendofthis\n",
            "section. \n",
            "(cid:12)\n",
            "64 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "allthedetailsagain,butyoucaneasilyverifythat\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚b = n x ( Ïƒ (z) âˆ’ y). (3.8)\n",
            "Again,thisavoidsthelearningslowdowncausedbytheÏƒ (cid:48)(z)termintheanalogousequation\n",
            "3 forthequadraticcost,Equation(3.3). Exercise\n",
            "VerifythatÏƒ (cid:48)(z)= Ïƒ (z)(1 Ïƒ (z))\n",
            "â€¢ âˆ’\n",
            "Letâ€™sreturntothetoyexampleweplayedwithearlier,andexplorewhathappenswhenwe\n",
            "usethecross-entropyinsteadofthequadraticcost. Tore-orientourselves,weâ€™llbeginwith\n",
            "thecasewherethequadraticcostdidjustfine,withstartingweight0.6andstartingbias0.9:\n",
            "Unsurprisingly,theneuronlearnsperfectlywellinthisinstance,justasitdidearlier.\n",
            "And\n",
            "nowletâ€™slookatthecasewhereourneurongotstuckbefore,withtheweightandbiasboth\n",
            "startingat2.0:\n",
            "Success! Thistimetheneuronlearnedquickly,justaswehoped. Ifyouobservecloselyyou\n",
            "canseethattheslopeofthecostcurvewasmuchsteeperinitiallythantheinitialflatregion\n",
            "onthecorrespondingcurveforthequadraticcost. Itâ€™sthatsteepnesswhichthecross-entropy\n",
            "buysus,preventingusfromgettingstuckjustwhenweâ€™dexpectourneurontolearnfastest,\n",
            "i.e.,whentheneuronstartsoutbadlywrong. \n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 65\n",
            "(cid:12)\n",
            "Ididnâ€™tsaywhatlearningratewasusedintheexamplesjustillustrated. Earlier,withthe\n",
            "quadraticcost,weusedÎ· =0.15.\n",
            "Shouldwehaveusedthesamelearningrateinthenew\n",
            "examples? Infact,withthechangeincostfunctionitâ€™snotpossibletosaypreciselywhatit\n",
            "meanstousetheâ€œsameâ€learningrate;itâ€™sanapplesandorangescomparison. Forbothcost\n",
            "functionsIsimplyexperimentedtofindalearningratethatmadeitpossibletoseewhatis\n",
            "goingon. Ifyouâ€™restillcurious,despitemydisavowal,hereâ€™sthelowdown: IusedÎ· =0.005\n",
            "intheexamplesjustgiven. 3\n",
            "Youmightobjectthatthechangeinlearningratemakesthegraphsabovemeaningless.\n",
            "Whocareshowfasttheneuronlearns,whenourchoiceoflearningratewasarbitraryto\n",
            "beginwith?! Thatobjectionmissesthepoint. Thepointofthegraphsisnâ€™tabouttheabsolute\n",
            "speedoflearning. Itâ€™sabouthowthespeedoflearningchanges. Inparticular,whenwe\n",
            "usethequadraticcostlearningisslowerwhentheneuronisunambiguouslywrongthan\n",
            "itislateron,astheneurongetsclosertothecorrectoutput;whilewiththecross-entropy\n",
            "learningisfasterwhentheneuronisunambiguouslywrong. Thosestatementsdonâ€™tdepend\n",
            "onhowthelearningrateisset. Weâ€™vebeenstudyingthecross-entropyforasingleneuron.However,itâ€™seasytogeneralize\n",
            "thecross-entropytomany-neuronmulti-layernetworks. Inparticular,suppose y= y\n",
            "1\n",
            ",y\n",
            "2\n",
            ",... are the desired values at the output neurons, i.e., the neurons in the final layer, while\n",
            "aL,aL,...aretheactualoutputvalues. Thenwedefinethecross-entropyby\n",
            "1 2\n",
            "(cid:88)(cid:148) (cid:151)\n",
            "y j lnaL j +(1 y j)ln(1 aL j) . (3.9)\n",
            "j âˆ’ âˆ’\n",
            "(cid:80)\n",
            "Thisisthesameasourearlierexpression,Equation(3.4),exceptnowweâ€™vegotthe\n",
            "j\n",
            "summingoveralltheoutputneurons. Iwonâ€™texplicitlyworkthroughaderivation,butit\n",
            "shouldbeplausiblethatusingtheexpression(3.9)avoidsalearningslowdowninmany-\n",
            "neuronnetworks.\n",
            "Ifyouâ€™reinterested,youcanworkthroughthederivationintheproblem\n",
            "below.\n",
            "Incidentally,Iâ€™musingthetermâ€œcross-entropyâ€inawaythathasconfusedsomeearly\n",
            "readers,sinceitsuperficiallyappearstoconflictwithothersources. Inparticular,itâ€™scommon\n",
            "(cid:80)\n",
            "todefinethecross-entropyfortwoprobabilitydistributions,p andq ,as p lnq . This\n",
            "j j j j j\n",
            "definitionmaybeconnectedto(3.4),ifwetreatasinglesigmoidneuronasoutputtinga\n",
            "probabilitydistributionconsistingoftheneuronâ€™sactivationaanditscomplement1 a. However,whenwehavemanysigmoidneuronsinthefinallayer,thevectoraL ofâˆ’activa-\n",
            "(cid:80)j\n",
            "tionsdonâ€™tusuallyformaprobabilitydistribution. Asaresult,adefinitionlike p lnq\n",
            "j j j\n",
            "doesnâ€™tevenmakesense,sinceweâ€™renotworkingwithprobabilitydistributions. Instead,\n",
            "youcanthinkof(3.9)asasummedsetofper-neuroncross-entropies,withtheactivation\n",
            "ofeachneuronbeinginterpretedaspartofatwo-elementprobabilitydistribution2. Inthis\n",
            "sense,(3.9)isageneralizationofthecross-entropyforprobabilitydistributions. Whenshouldweusethecross-entropyinsteadofthequadraticcost? Infact,thecross-\n",
            "entropyisnearlyalwaysthebetterchoice,providedtheoutputneuronsaresigmoidneurons. Toseewhy,considerthatwhenweâ€™resettingupthenetworkweusuallyinitializetheweights\n",
            "andbiasesusingsomesortofrandomization. Itmayhappenthatthoseinitialchoicesresult\n",
            "inthenetworkbeingdecisivelywrongforsometraininginputâ€“thatis,anoutputneuron\n",
            "willhavesaturatednear1,whenitshouldbe0,orviceversa. Ifweâ€™reusingthequadratic\n",
            "costthatwillslowdownlearning. Itwonâ€™tstoplearningcompletely,sincetheweightswill\n",
            "2Ofcourse,inournetworkstherearenoprobabilisticelements,sotheyâ€™renotreallyprobabilities. \n",
            "(cid:12)\n",
            "66 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "continuelearningfromothertraininginputs,butitâ€™sobviouslyundesirable. Exercises\n",
            "Onegotchawiththecross-entropyisthatitcanbedifficultatfirsttorememberthe\n",
            "â€¢ respectiverolesoftheysandtheas.\n",
            "Itâ€™seasytogetconfusedaboutwhethertheright\n",
            "formis\n",
            "[ylna+(1 y)ln(1 a)]. 3 âˆ’ âˆ’ âˆ’\n",
            "Whathappenstothesecondoftheseexpressionswhen y=0or1? Doesthisproblem\n",
            "afflictthefirstexpression?\n",
            "Whyorwhynot? Inthesingle-neurondiscussionatthestartofthissection,Iarguedthatthecross-\n",
            "â€¢ entropy is small if Ïƒ (z) y for all training inputs. The argument relied on y\n",
            "beingequaltoeither0orâ‰ˆ1. Thisisusuallytrueinclassificationproblems,butfor\n",
            "otherproblems(e.g.,regressionproblems) y cansometimestakevaluesintermediate\n",
            "between0and1. Showthatthecross-entropyisstillminimizedwhenÏƒ (z)= y for\n",
            "alltraininginputs. Whenthisisthecasethecross-entropyhasthevalue:\n",
            "1(cid:88)\n",
            "C= [ylny+(1 y)ln(1 y)]. (3.10)\n",
            "âˆ’n x âˆ’ âˆ’\n",
            "Thequantity [ylny+(1 y)ln(1 y)]issometimesknownasthebinaryentropy. âˆ’ âˆ’ âˆ’\n",
            "Problems\n",
            "Many-layermulti-neuronnetworksInthenotationintroducedinthelastchapter,\n",
            "â€¢ showthatforthequadraticcostthepartialderivativewithrespecttoweightsinthe\n",
            "outputlayeris\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚wL jk = n x a k L âˆ’ 1 (aL j âˆ’ y j) Ïƒ (cid:48)(z j L ). (3.11)\n",
            "ThetermÏƒ (cid:48)(z\n",
            "j\n",
            "L )causesalearningslowdownwheneveranoutputneuronsaturates\n",
            "onthewrongvalue. Showthatforthecross-entropycosttheoutputerrorÎ´L fora\n",
            "singletrainingexample x isgivenby\n",
            "Î´L =aL y. (3.12)\n",
            "âˆ’\n",
            "Usethisexpressiontoshowthatthepartialderivativewithrespecttotheweightsin\n",
            "theoutputlayerisgivenby\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚wL jk = n x a k L âˆ’ 1 (aL j âˆ’ y j). (3.13)\n",
            "TheÏƒ (cid:48)(z\n",
            "j\n",
            "L )termhasvanished,andsothecross-entropyavoidstheproblemoflearning\n",
            "slowdown,notjustwhenusedwithasingleneuron,aswesawearlier,butalsoin\n",
            "many-layermulti-neuronnetworks. Asimplevariationonthisanalysisholdsalsofor\n",
            "thebiases. Ifthisisnotobvioustoyou,thenyoushouldworkthroughthatanalysis\n",
            "aswell. UsingthequadraticcostwhenwehavelinearneuronsintheoutputlayerSup-\n",
            "â€¢ posethatwehaveamany-layermulti-neuronnetwork. Supposealltheneuronsin\n",
            "thefinallayerarelinearneurons,meaningthatthesigmoidactivationfunctionisnot\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 67\n",
            "(cid:12)\n",
            "applied,andtheoutputsaresimplyaL\n",
            "j\n",
            "=z\n",
            "j\n",
            "L. Showthatifweusethequadraticcost\n",
            "functionthentheoutputerrorÎ´L forasingletrainingexample x isgivenby\n",
            "Î´L =aL y. (3.14)\n",
            "âˆ’\n",
            "Similarlytothepreviousproblem,usethisexpressiontoshowthatthepartialderiva-\n",
            "tiveswithrespecttotheweightsandbiasesintheoutputlayeraregivenby 3\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚wL jk = n x a k L âˆ’ 1 (aL j âˆ’ y j) (3.15)\n",
            "âˆ‚C 1(cid:88)\n",
            "âˆ‚bL j = n x (aL j âˆ’ y j). (3.16)\n",
            "Thisshowsthatiftheoutputneuronsarelinearneuronsthenthequadraticcostwill\n",
            "notgiverisetoanyproblemswithalearningslowdown. Inthiscasethequadratic\n",
            "costis,infact,anappropriatecostfunctiontouse. 3.1.2 Usingthecross-entropytoclassifyMNISTdigits\n",
            "Thecross-entropyiseasytoimplementaspartofaprogramwhichlearnsusinggradient\n",
            "descentandbackpropagation. Weâ€™lldothatlaterinthechapter,developinganimproved\n",
            "versionofourearlierprogramforclassifyingtheMNISThandwrittendigits,network.py. Thenewprogramiscallednetwork2.py,andincorporatesnotjustthecross-entropy,but\n",
            "alsoseveralothertechniquesdevelopedinthischapter3. Fornow,letâ€™slookathowwellour\n",
            "newprogramclassifiesMNISTdigits. AswasthecaseinChapter1,weâ€™lluseanetworkwith\n",
            "30hiddenneurons,andweâ€™lluseamini-batchsizeof10. WesetthelearningratetoÎ· =0.54\n",
            "andwetrainfor30epochs. Theinterfacetonetwork2.pyisslightlydifferentthannetwork.\n",
            "py,butitshouldstillbeclearwhatisgoingon.\n",
            "Youcan,bytheway,getdocumentationabout\n",
            "network2.pyâ€™sinterfacebyusingcommandssuchashelp(network2.Network.SGD)ina\n",
            "Pythonshell. >>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data,\n",
            "monitor_evaluation_accuracy=True)\n",
            "3ThecodeisavailableonGitHub. 4InChapter1weusedthequadraticcostandalearningrateofÎ· =3.0.Asdiscussedabove,itâ€™snot\n",
            "possibletosaypreciselywhatitmeanstousetheâ€œsameâ€learningratewhenthecostfunctionischanged.\n",
            "ForbothcostfunctionsIexperimentedtofindalearningratethatprovidesnear-optimalperformance,\n",
            "giventheotherhyper-parameterchoices. Thereis,incidentally,averyroughgeneralheuristicforrelatingthelearningrateforthecross-entropy\n",
            "andthequadraticcost. Aswesawearlier,thegradienttermsforthequadraticcosthaveanextra\n",
            "Ïƒ (cid:48)= Ïƒ (1 Ïƒ )terminthem. SupposeweaveragethisovervaluesforÏƒ, (cid:82) 0 1 dÏƒÏƒ (1 Ïƒ )=1/6. We\n",
            "seethat(vâˆ’eryroughly)thequadraticcostlearnsanaverageof6timesslower,forthâˆ’esamelearning\n",
            "rate.Thissuggeststhatareasonablestartingpointistodividethelearningrateforthequadraticcost\n",
            "by6.Ofcourse,thisargumentisfarfromrigorous,andshouldnâ€™tbetakentooseriously.Still,itcan\n",
            "sometimesbeausefulstartingpoint. \n",
            "(cid:12)\n",
            "68 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Note,bytheway,thatthenet.large_weight_initializer()commandisusedtoinitial-\n",
            "izetheweightsandbiasesinthesamewayasdescribedinChapter1. Weneedtorunthis\n",
            "commandbecauselaterinthischapterweâ€™llchangethedefaultweightinitializationinour\n",
            "networks. Theresultfromrunningtheabovesequenceofcommandsisanetworkwith95.49\n",
            "percentaccuracy.\n",
            "ThisisprettyclosetotheresultweobtainedinChapter1,95.42percent,\n",
            "usingthequadraticcost. 3 Letâ€™slookalsoatthecasewhereweuse100hiddenneurons,thecross-entropy,and\n",
            "otherwisekeeptheparametersthesame. Inthiscaseweobtainanaccuracyof96.82percent. Thatâ€™sasubstantialimprovementovertheresultsfromChapter1, whereweobtaineda\n",
            "classificationaccuracyof96.59percent,usingthequadraticcost. Thatmaylooklikeasmall\n",
            "change, butconsiderthattheerrorratehasdroppedfrom3.41percentto3.18percent. Thatis,weâ€™veeliminatedaboutoneinfourteenoftheoriginalerrors. Thatâ€™squiteahandy\n",
            "improvement. Itâ€™sencouragingthatthecross-entropycostgivesussimilarorbetterresultsthanthe\n",
            "quadraticcost. However,theseresultsdonâ€™tconclusivelyprovethatthecross-entropyisa\n",
            "betterchoice. ThereasonisthatIâ€™veputonlyalittleeffortintochoosinghyper-parameters\n",
            "suchaslearningrate,mini-batchsize,andsoon. Fortheimprovementtobereallyconvincing\n",
            "weâ€™dneedtodoathoroughjoboptimizingsuchhyper-parameters. Still, theresultsare\n",
            "encouraging,andreinforceourearliertheoreticalargumentthatthecross-entropyisabetter\n",
            "choicethanthequadraticcost. This,bytheway,ispartofageneralpatternthatweâ€™llseethroughthischapterand,\n",
            "indeed,throughmuchoftherestofthebook.\n",
            "Weâ€™lldevelopanewtechnique,weâ€™lltryitout,\n",
            "andweâ€™llgetâ€œimprovedâ€results. Itis,ofcourse,nicethatweseesuchimprovements.\n",
            "But\n",
            "theinterpretationofsuchimprovementsisalwaysproblematic. Theyâ€™reonlytrulyconvincing\n",
            "if we see an improvement after putting tremendous effort into optimizing all the other\n",
            "hyper-parameters. Thatâ€™sagreatdealofwork,requiringlotsofcomputingpower,andweâ€™re\n",
            "notusuallygoingtodosuchanexhaustiveinvestigation.\n",
            "Instead,weâ€™llproceedonthebasis\n",
            "ofinformaltestslikethosedoneabove. Still,youshouldkeepinmindthatsuchtestsfall\n",
            "shortofdefinitiveproof,andremainalerttosignsthattheargumentsarebreakingdown. Bynow,weâ€™vediscussedthecross-entropyatgreatlength. Whygotosomucheffort\n",
            "whenitgivesonlyasmallimprovementtoourMNISTresults? Laterinthechapterweâ€™ll\n",
            "see other techniques â€“ notably, regularization â€“ which give much bigger improvements. Sowhysomuchfocusoncross-entropy? Partofthereasonisthatthecross-entropyisa\n",
            "widely-usedcostfunction, andsoisworthunderstandingwell. Butthemoreimportant\n",
            "reasonisthatneuronsaturationisanimportantprobleminneuralnets,aproblemweâ€™ll\n",
            "returntorepeatedlythroughoutthebook. AndsoIâ€™vediscussedthecross-entropyatlength\n",
            "becauseitâ€™sagoodlaboratorytobeginunderstandingneuronsaturationandhowitmaybe\n",
            "addressed. 3.1.3 Whatdoesthecross-entropymean? Wheredoesitcomefrom? Ourdiscussionofthecross-entropyhasfocusedonalgebraicanalysisandpracticalimplemen-\n",
            "tation. Thatâ€™suseful,butitleavesunansweredbroaderconceptualquestions,like: whatdoes\n",
            "thecross-entropymean? Istheresomeintuitivewayofthinkingaboutthecross-entropy? Andhowcouldwehavedreamedupthecross-entropyinthefirstplace? Letâ€™sbeginwiththelastofthesequestions: whatcouldhavemotivatedustothinkupthe\n",
            "cross-entropyinthefirstplace? Supposeweâ€™ddiscoveredthelearningslowdowndescribed\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 69\n",
            "(cid:12)\n",
            "earlier,andunderstoodthattheoriginwastheÏƒ (cid:48)(z)termsinEquations(3.2)and(3.3). Afterstaringatthoseequationsforabit,wemightwonderifitâ€™spossibletochooseacost\n",
            "functionsothattheÏƒ (cid:48)(z)termdisappeared. Inthatcase,thecostC=C\n",
            "x\n",
            "forasingletraining\n",
            "example x wouldsatisfy\n",
            "âˆ‚C\n",
            "âˆ‚w\n",
            "j\n",
            "= x j(a\n",
            "âˆ’\n",
            "y) (3.17)\n",
            "3\n",
            "âˆ‚C\n",
            "âˆ‚b = (a\n",
            "âˆ’\n",
            "y). (3.18)\n",
            "Ifwecouldchoosethecostfunctiontomaketheseequationstrue,thentheywouldcapture\n",
            "inasimplewaytheintuitionthatthegreatertheinitialerror,thefastertheneuronlearns.\n",
            "Theyâ€™d also eliminate the problem of a learning slowdown. In fact, starting from these\n",
            "equationsweâ€™llnowshowthatitâ€™spossibletoderivetheformofthecross-entropy,simplyby\n",
            "followingourmathematicalnoses. Toseethis,notethatfromthechainrulewehave\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ‚b = âˆ‚a Ïƒ (cid:48)(z).\n",
            "(3.19)\n",
            "UsingÏƒ (cid:48)(z)= Ïƒ (z)(1 Ïƒ (z))=a(1 a)thelastequationbecomes\n",
            "âˆ’ âˆ’\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ‚b = âˆ‚a a(1\n",
            "âˆ’\n",
            "a). (3.20)\n",
            "ComparingtoEquation3.18weobtain\n",
            "âˆ‚C a y\n",
            "âˆ‚a = a(1 âˆ’ a) . (3.21)\n",
            "âˆ’\n",
            "Integratingthisexpressionwithrespecttoagives\n",
            "C= [ylna+(1 y)ln(1 a)]+constant, (3.22)\n",
            "âˆ’ âˆ’ âˆ’\n",
            "forsomeconstantofintegration. Thisisthecontributiontothecostfromasingletraining\n",
            "example, x. Togetthefullcostfunctionwemustaverageovertrainingexamples,obtaining\n",
            "1(cid:88)\n",
            "C= [ylna+(1 y)ln(1 a)]+constant, (3.23)\n",
            "âˆ’n x âˆ’ âˆ’\n",
            "wheretheconstanthereistheaverageoftheindividualconstantsforeachtrainingexam-\n",
            "ple. AndsoweseethatEquations(3.17)and(3.18)uniquelydeterminetheformofthe\n",
            "cross-entropy,uptoanoverallconstantterm. Thecross-entropyisnâ€™tsomethingthatwas\n",
            "miraculouslypulledoutofthinair. Rather,itâ€™ssomethingthatwecouldhavediscoveredina\n",
            "simpleandnaturalway. Whatabouttheintuitivemeaningofthecross-entropy? Howshouldwethinkaboutit? ExplainingthisindepthwouldtakeusfurtherafieldthanIwanttogo. However,itisworth\n",
            "mentioningthatthereisastandardwayofinterpretingthecross-entropythatcomesfromthe\n",
            "fieldofinformationtheory. Roughlyspeaking,theideaisthatthecross-entropyisameasure\n",
            "\n",
            "(cid:12)\n",
            "70 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "ofsurprise. Inparticular,ourneuronistryingtocomputethefunction x y= y(x). But\n",
            "insteaditcomputesthefunction x a = a(x). Supposewethinkofaâ†’asourneuronâ€™s\n",
            "estimatedprobabilitythat yis1,andâ†’1 aistheestimatedprobabilitythattherightvaluefor\n",
            "y is0. Thenthecross-entropymeasureâˆ’showâ€œsurprisedâ€weare,onaverage,whenwelearn\n",
            "thetruevaluefor y. Wegetlowsurpriseiftheoutputiswhatweexpect,andhighsurprise\n",
            "iftheoutputisunexpected. Ofcourse,Ihavenâ€™tsaidexactlywhatâ€œsurpriseâ€means,andso\n",
            "3 thisperhapsseemslikeemptyverbiage. Butinfactthereisapreciseinformation-theoretic\n",
            "way of saying what is meant by surprise. Unfortunately, I donâ€™t know of a good, short,\n",
            "self-containeddiscussionofthissubjectthatâ€™savailableonline.\n",
            "Butifyouwanttodigdeeper,\n",
            "thenWikipediacontainsabriefsummarythatwillgetyoustarteddowntherighttrack.\n",
            "And\n",
            "thedetailscanbefilledinbyworkingthroughthematerialsabouttheKraftinequalityin\n",
            "chapter5ofthebookaboutinformationtheorybyCoverandThomas. Problem\n",
            "Weâ€™vediscussedatlengththelearningslowdownthatcanoccurwhenoutputneurons\n",
            "â€¢ saturate,innetworksusingthequadraticcosttotrain. Anotherfactorthatmayinhibit\n",
            "learningisthepresenceofthe x terminEquation(3.7). Becauseofthisterm,when\n",
            "j\n",
            "aninput x isneartozero,thecorrespondingweightw willlearnslowly. Explain\n",
            "j j\n",
            "whyitisnotpossibletoeliminatethex termthroughacleverchoiceofcostfunction. j\n",
            "3.1.4 Softmax\n",
            "Inthischapterweâ€™llmostlyusethecross-entropycosttoaddresstheproblemoflearning\n",
            "slowdown. However,Iwanttobrieflydescribeanotherapproachtotheproblem,basedon\n",
            "whatarecalledsoftmaxlayersofneurons. Weâ€™renotactuallygoingtousesoftmaxlayersin\n",
            "theremainderofthechapter,soifyouâ€™reinagreathurry,youcanskiptothenextsection. However,softmaxisstillworthunderstanding,inpartbecauseitâ€™sintrinsicallyinteresting,\n",
            "andinpartbecauseweâ€™llusesoftmaxlayersinChapter6,inourdiscussionofdeepneural\n",
            "networks. Theideaofsoftmaxistodefineanewtypeofoutputlayerforourneuralnetworks. I (cid:80) tb k e w g L j i k n a s k L âˆ’ in 1 + th b e L j .\n",
            "sa H m o e we w v a e y r, a w s e w d i o th nâ€™t a a s p i p g l m y o th id es la ig y m er o , i b d y fu fo n r c m tio in n g to th g e et w th ei e g o h u te tp d u i t n . p In u s t t s e 5 a z d j L ,i = n\n",
            "asoftmaxlayerweapplytheso-calledsoftmaxfunctiontothezL. Accordingtothisfunction,\n",
            "j\n",
            "theactivationaL ofthe j-thoutputneuronis\n",
            "j\n",
            "ex\n",
            "j\n",
            "L\n",
            "aL j = (cid:80) ez\n",
            "k\n",
            "L (3.24)\n",
            "k\n",
            "whereinthedenominatorwesumoveralltheoutputneurons. Ifyouâ€™renotfamiliarwiththesoftmaxfunction,Equation(3.24)maylookprettyopaque. Itâ€™scertainlynotobviouswhyweâ€™dwanttousethisfunction.Anditâ€™salsonotobviousthatthis\n",
            "willhelpusaddressthelearningslowdownproblem. TobetterunderstandEquation(3.24),\n",
            "supposewehaveanetworkwithfouroutputneurons,andfourcorrespondingweighted\n",
            "inputs,whichweâ€™lldenotezL,zL,zL,andzL. Figure3.1showsagraphofthecorresponding\n",
            "1 2 3 4\n",
            "outputactivationsfordifferentinputs6. AsyouincreasezL,youâ€™llseeanincreaseinthe\n",
            "4\n",
            "5Indescribingthesoftmaxweâ€™llmakefrequentuseofnotationintroducedinthelastchapter.You\n",
            "maywishtorevisitthatchapterifyouneedtorefreshyourmemoryaboutthemeaningofthenotation. 6Thisparagraphisanadaptationofananimationfromonlineversionofthebook. \n",
            "(cid:12)\n",
            "3.1.\n",
            "Thecross-entropycostfunction (cid:12) 71\n",
            "(cid:12)\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "4 2 0 2 4\n",
            "âˆ’ âˆ’ z\n",
            "4\n",
            ")4...1(\n",
            "j,\n",
            "j a\n",
            "âˆˆ\n",
            "a 1(z 1= 1) a 2(z 2=0)\n",
            "a 3(z 3= âˆ’1) a 4\n",
            "3\n",
            "Figure3.1:Equation3.24fordifferentfixedvaluesofzL andvariablezL.Lindexavoidedforclarity. 1,2,3 4\n",
            "correspondingoutputactivation,aL,andadecreaseintheotheroutputactivations.\n",
            "Similarly,\n",
            "4\n",
            "ifyoudecreasezL thenaL willdecrease,andalltheotheroutputactivationswillincrease. In\n",
            "4 4\n",
            "fact,ifyoulookclosely,youâ€™llseethatinbothcasesthetotalchangeintheotheractivations\n",
            "exactlycompensatesforthechangein aL. Thereasonisthattheoutputactivationsare\n",
            "4\n",
            "guaranteedtoalwayssumupto1,aswecanproveusingEquation(3.24)andalittlealgebra:\n",
            "(cid:88) j aL j = (cid:80) (cid:80) k j e e z z k j L L =1. (3.25)\n",
            "Asaresult,ifaL increases,thentheotheroutputactivationsmustdecreasebythesametotal\n",
            "4\n",
            "amount,toensurethesumoverallactivationsremains1. And,ofcourse,similarstatements\n",
            "holdforalltheotheractivations. Equation(3.24)alsoimpliesthattheoutputactivationsareallpositive,sincetheex-\n",
            "ponentialfunctionispositive. Combiningthiswiththeobservationinthelastparagraph,\n",
            "weseethattheoutputfromthesoftmaxlayerisasetofpositivenumberswhichsumup\n",
            "to1. Inotherwords,theoutputfromthesoftmaxlayercanbethoughtofasaprobability\n",
            "distribution. Thefactthatasoftmaxlayeroutputsaprobabilitydistributionisratherpleasing. Inmany\n",
            "problemsitâ€™sconvenienttobeabletointerprettheoutputactivationaL asthenetworkâ€™s\n",
            "j\n",
            "estimate of the probability that the correct output is j. So, for instance, in the MNIST\n",
            "classificationproblem,wecaninterpretaL asthenetworkâ€™sestimatedprobabilitythatthe\n",
            "j\n",
            "correctdigitclassificationis j. Bycontrast,iftheoutputlayerwasasigmoidlayer,thenwecertainlycouldnâ€™tassume\n",
            "thattheactivationsformedaprobabilitydistribution. Iwonâ€™texplicitlyproveit,butitshould\n",
            "beplausiblethattheactivationsfromasigmoidlayerwonâ€™tingeneralformaprobability\n",
            "distribution. Andsowithasigmoidoutputlayerwedonâ€™thavesuchasimpleinterpretation\n",
            "oftheoutputactivations. Exercise\n",
            "Constructanexampleshowingexplicitlythatinanetworkwithasigmoidoutputlayer,\n",
            "â€¢ theoutputactivationsaL wonâ€™talwayssumto1. j\n",
            "Weâ€™restartingtobuildupsomefeelforthesoftmaxfunctionandthewaysoftmaxlayers\n",
            "\n",
            "(cid:12)\n",
            "72 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "behave. Justtoreviewwhereweâ€™reat: theexponentialsinEquation(3.24)ensurethat\n",
            "alltheoutputactivationsarepositive.\n",
            "AndthesuminthedenominatorofEquation(3.24)\n",
            "ensuresthatthesoftmaxoutputssumto1. Sothatparticularformnolongerappearsso\n",
            "mysterious: rather,itisanaturalwaytoensurethattheoutputactivationsformaprobability\n",
            "distribution. YoucanthinkofsoftmaxasawayofrescalingthezL,andthensquishingthem\n",
            "j\n",
            "togethertoformaprobabilitydistribution. 3 Exercises\n",
            "MonotonicityofsoftmaxShowthatâˆ‚aL\n",
            "j\n",
            "/âˆ‚z\n",
            "k\n",
            "Lispositiveif j=kandnegativeif j=k. â€¢ Asaconsequence,increasingzL isguaranteedtoincreasethecorrespondingout(cid:54)put\n",
            "j\n",
            "activation,aL,andwilldecreasealltheotheroutputactivations. Wealreadysawthis\n",
            "j\n",
            "empiricallywiththesliders,butthisisarigorousproof. Non-localityofsoftmaxAnicethingaboutsigmoidlayersisthattheoutputaL isa\n",
            "j\n",
            "â€¢ functionofthecorrespondingweightedinput,aL j = Ïƒ (z j L ). Explainwhythisisnot\n",
            "thecaseforasoftmaxlayer: anyparticularoutputactivationaL dependsonallthe\n",
            "j\n",
            "weightedinputs. Problem\n",
            "InvertingthesoftmaxlayerSupposewehaveaneuralnetworkwithasoftmaxoutput\n",
            "â€¢ layer,andtheactivationsaLareknown. Showthatthecorrespondingweightedinputs\n",
            "j\n",
            "havetheformz\n",
            "j\n",
            "L =lnaL\n",
            "j\n",
            "+C,forsomeconstantC thatisindependentof j. Thelearningslowdownproblem:Weâ€™venowbuiltupconsiderablefamiliaritywithsoftmax\n",
            "layersofneurons. Butwehavenâ€™tyetseenhowasoftmaxlayerletsusaddressthelearning\n",
            "slowdownproblem. Tounderstandthat,letâ€™sdefinethelog-likelihoodcostfunction.\n",
            "Weâ€™ll\n",
            "use x todenoteatraininginputtothenetwork,and y todenotethecorrespondingdesired\n",
            "output. Thenthelog-likelihoodcostassociatedtothistraininginputis\n",
            "C lnaL (3.26)\n",
            "j\n",
            "â‰¡âˆ’\n",
            "So, for instance, if weâ€™re training with MNIST images, and input an image of a 7, then\n",
            "thelog-likelihoodcostis lnaL. Toseethatthismakesintuitivesense,considerthecase\n",
            "7\n",
            "whenthenetworkisdoinâˆ’gagoodjob,thatis,itisconfidenttheinputisa7. Inthatcase\n",
            "itwillestimateavalueforthecorrespondingprobabilityaL whichiscloseto1,andsothe\n",
            "7\n",
            "cost lnaL willbesmall. Bycontrast,whenthenetworkisnâ€™tdoingsuchagoodjob,the\n",
            "7\n",
            "probaâˆ’bilityaL willbesmaller,andthecost lnaL willbelarger. Sothelog-likelihoodcost\n",
            "7 7\n",
            "behavesasweâ€™dexpectacostfunctiontobeâˆ’have. Whataboutthelearningslowdownproblem?\n",
            "Toanalyzethat,recallthatthekeytothe\n",
            "learningslowdownisthebehaviourofthequantitiesâˆ‚C/âˆ‚wL andâˆ‚C/âˆ‚bL. Iwonâ€™tgo\n",
            "jk j\n",
            "throughthederivationexplicitlyâ€“Iâ€™llaskyoutodointheproblems,belowâ€“butwithalittle\n",
            "algebrayoucanshowthat7\n",
            "âˆ‚C\n",
            "âˆ‚bL\n",
            "j\n",
            "= aL j\n",
            "âˆ’\n",
            "y j (3.27)\n",
            "âˆ‚C\n",
            "âˆ‚wL\n",
            "jk\n",
            "= a k L âˆ’ 1 (aL j\n",
            "âˆ’\n",
            "y j) (3.28)\n",
            "7NotethatIâ€™mabusingnotationhere,using yinaslightlydifferentwaytolastparagraph.Inthelast\n",
            "paragraphweusedytodenotethedesiredoutputfromthenetworkâ€“e.g.,outputaâ€œ7â€ifanimageofa\n",
            "7wasinput.ButintheequationswhichfollowIâ€™musing ytodenotethevectorofoutputactivations\n",
            "whichcorrespondsto7,thatis,avectorwhichisall0s,exceptfora1inthe7thlocation. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 73\n",
            "(cid:12)\n",
            "Theseequationsarethesameastheanalogousexpressionsobtainedinourearlieranalysis\n",
            "ofthecross-entropy. Compare,forexample,Equation(3.28)toEquation(3.13).\n",
            "Itâ€™sthe\n",
            "sameequation,albeitinthelatterIâ€™veaveragedovertraininginstances.\n",
            "And,justasinthe\n",
            "earlieranalysis,theseexpressionsensurethatwewillnotencounteralearningslowdown. Infact,itâ€™susefultothinkofasoftmaxoutputlayerwithlog-likelihoodcostasbeingquite\n",
            "similartoasigmoidoutputlayerwithcross-entropycost. Given this similarity, should you use a sigmoid output layer and cross-entropy, or a 3\n",
            "softmaxoutputlayerandlog-likelihood? Infact,inmanysituationsbothapproacheswork\n",
            "well. Through the remainder of this chapter weâ€™ll use a sigmoid output layer, with the\n",
            "cross-entropycost. Later,inChapter6,weâ€™llsometimesuseasoftmaxoutputlayer,with\n",
            "log-likelihoodcost. Thereasonfortheswitchistomakesomeofourlaternetworksmore\n",
            "similartonetworksfoundincertaininfluentialacademicpapers. Asamoregeneralpoint\n",
            "ofprinciple,softmaxpluslog-likelihoodisworthusingwheneveryouwanttointerpretthe\n",
            "output activations as probabilities. Thatâ€™s not always a concern, but can be useful with\n",
            "classificationproblems(likeMNIST)involvingdisjointclasses. Problems\n",
            "DeriveEquations(3.27)and(3.28). â€¢ Where does the â€œsoftmaxâ€ name come from? Suppose we change the softmax\n",
            "â€¢ functionsotheoutputactivationsaregivenby\n",
            "ecz\n",
            "j\n",
            "L\n",
            "aL j = (cid:80) ecz\n",
            "k\n",
            "L , (3.29)\n",
            "k\n",
            "wherecisapositiveconstant. Notethatc=1correspondstothestandardsoftmax\n",
            "function. But if we use a different value of c we get a different function, which\n",
            "isnonethelessqualitativelyrathersimilartothesoftmax. Inparticular,showthat\n",
            "theoutputactivationsformaprobabilitydistribution,justasfortheusualsoftmax. Supposeweallowctobecomelarge,i.e.,c . Whatisthelimitingvalueforthe\n",
            "outputactivationsaL? Aftersolvingthisprâ†’oblâˆžemitshouldbecleartoyouwhywe\n",
            "j\n",
            "thinkofthec=1functionasaâ€œsoftenedâ€versionofthemaximumfunction. Thisis\n",
            "theoriginofthetermâ€œsoftmaxâ€. Backpropagationwithsoftmaxandthelog-likelihoodcostInthelastchapterwe\n",
            "â€¢ derivedthebackpropagationalgorithmforanetworkcontainingsigmoidlayers. To\n",
            "apply the algorithm to a network with a softmax layer we need to figure out an\n",
            "expressionfortheerrorÎ´L âˆ‚C/âˆ‚zLinthefinallayer.Showthatasuitableexpression\n",
            "j j\n",
            "is: â‰¡\n",
            "Î´L\n",
            "j\n",
            "=aL\n",
            "j\n",
            "y\n",
            "j\n",
            ". (3.30)\n",
            "âˆ’\n",
            "Usingthisexpressionwecanapplythebackpropagationalgorithmtoanetworkusing\n",
            "asoftmaxoutputlayerandthelog-likelihoodcost. 3.2 Overfitting and regularization\n",
            "TheNobelprizewinningphysicistEnricoFermiwasonceaskedhisopinionofamathematical\n",
            "model some colleagues had proposed as the solution to an important unsolved physics\n",
            "problem. Themodelgaveexcellentagreementwithexperiment,butFermiwasskeptical.\n",
            "He\n",
            "askedhowmanyfreeparameterscouldbesetinthemodel. â€œFourâ€wastheanswer. Fermi\n",
            "\n",
            "(cid:12)\n",
            "74 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "replied8: â€œIremembermyfriendJohnnyvonNeumannusedtosay,withfourparametersI\n",
            "canfitanelephant,andwithfiveIcanmakehimwigglehistrunk.â€. Thepoint,ofcourse,isthatmodelswithalargenumberoffreeparameterscandescribe\n",
            "anamazinglywiderangeofphenomena. Evenifsuchamodelagreeswellwiththeavailable\n",
            "data,thatdoesnâ€™tmakeitagoodmodel. Itmayjustmeanthereâ€™senoughfreedominthe\n",
            "3 model that it can describe almost any data set of the given size, without capturing any\n",
            "genuineinsightsintotheunderlyingphenomenon. Whenthathappensthemodelwillwork\n",
            "wellfortheexistingdata,butwillfailtogeneralizetonewsituations.\n",
            "Thetruetestofa\n",
            "modelisitsabilitytomakepredictionsinsituationsithasnâ€™tbeenexposedtobefore. Fermi and von Neumann were suspicious of models with four parameters. Our 30\n",
            "hiddenneuronnetworkforclassifyingMNISTdigitshasnearly24,000parameters! Thatâ€™s\n",
            "alotofparameters. Our100hiddenneuronnetworkhasnearly80,000parameters,and\n",
            "state-of-the-artdeepneuralnetssometimescontainmillionsorevenbillionsofparameters. Shouldwetrusttheresults? Letâ€™ssharpenthisproblemupbyconstructingasituationwhereournetworkdoesa\n",
            "badjobgeneralizingtonewsituations. Weâ€™lluseour30hiddenneuronnetwork,withits\n",
            "23,860parameters. Butwewonâ€™ttrainthenetworkusingall50,000MNISTtrainingimages. Instead,weâ€™llusejustthefirst1,000trainingimages. Usingthatrestrictedsetwillmake\n",
            "theproblemwithgeneralizationmuchmoreevident. Weâ€™lltraininasimilarwaytobefore,\n",
            "usingthecross-entropycostfunction,withalearningrateofÎ· =0.5andamini-batchsize\n",
            "of10. However,weâ€™lltrainfor400epochs,asomewhatlargernumberthanbefore,because\n",
            "weâ€™renotusingasmanytrainingexamples. Letâ€™susenetwork2tolookatthewaythecost\n",
            "functionchanges:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,\n",
            "monitor_evaluation_accuracy=True, monitor_training_cost=True)\n",
            "Usingtheresultswecanplotthewaythecostchangesasthenetworklearns9:\n",
            "8ThequotecomesfromacharmingarticlebyFreemanDyson,whoisoneofthepeoplewhoproposed\n",
            "theflawedmodel.Afour-parameterelephantmaybefoundhere. 9Thisandthenextfourgraphsweregeneratedbytheprogramoverfitting.py.\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 75\n",
            "(cid:12)\n",
            "3\n",
            "Thislooksencouraging,showingasmoothdecreaseinthecost,justasweexpect. Notethat\n",
            "Iâ€™veonlyshowntrainingepochs200through399.\n",
            "Thisgivesusaniceup-closeviewofthe\n",
            "laterstagesoflearning,which,asweâ€™llsee,turnsouttobewheretheinterestingactionis. Letâ€™snowlookathowtheclassificationaccuracyonthetestdatachangesovertime:\n",
            "Again,Iâ€™vezoomedinquiteabit. Inthefirst200epochs(notshown)theaccuracyrisesto\n",
            "justunder82percent. Thelearningthengraduallyslowsdown. Finally,ataroundepoch\n",
            "280theclassificationaccuracyprettymuchstopsimproving. Laterepochsmerelyseesmall\n",
            "stochasticfluctuationsnearthevalueoftheaccuracyatepoch280. Contrastthiswiththe\n",
            "earliergraph,wherethecostassociatedtothetrainingdatacontinuestosmoothlydrop. Ifwejustlookatthatcost,itappearsthatourmodelisstillgettingâ€œbetterâ€.\n",
            "Butthetest\n",
            "accuracyresultsshowtheimprovementisanillusion. JustlikethemodelthatFermidisliked,\n",
            "whatournetworklearnsafterepoch280nolongergeneralizestothetestdata. Andsoitâ€™s\n",
            "notusefullearning. Wesaythenetworkisoverfittingorovertrainingbeyondepoch280. YoumightwonderiftheproblemhereisthatIâ€™mlookingatthecostonthetraining\n",
            "data,asopposedtotheclassificationaccuracyonthetestdata. Inotherwords,maybethe\n",
            "problemisthatweâ€™remakinganapplesandorangescomparison. Whatwouldhappenifwe\n",
            "comparedthecostonthetrainingdatawiththecostonthetestdata,soweâ€™recomparing\n",
            "similarmeasures? Orperhapswecouldcomparetheclassificationaccuracyonboththe\n",
            "trainingdataandthetestdata? Infact, essentiallythesamephenomenonshowsupno\n",
            "\n",
            "(cid:12)\n",
            "76 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "matterhowwedothecomparison. Thedetailsdochange,however.\n",
            "Forinstance,letâ€™slook\n",
            "atthecostonthetestdata:\n",
            "3\n",
            "Wecanseethatthecostonthetestdataimprovesuntilaroundepoch15,butafterthatit\n",
            "actuallystartstogetworse,eventhoughthecostonthetrainingdataiscontinuingtoget\n",
            "better. Thisisanothersignthatourmodelisoverfitting. Itposesapuzzle,though,whichis\n",
            "whetherweshouldregardepoch15orepoch280asthepointatwhichoverfittingiscoming\n",
            "todominatelearning? Fromapracticalpointofview,whatwereallycareaboutisimproving\n",
            "classificationaccuracyonthetestdata,whilethecostonthetestdataisnomorethana\n",
            "proxyforclassificationaccuracy. Andsoitmakesmostsensetoregardepoch280asthe\n",
            "pointbeyondwhichoverfittingisdominatinglearninginourneuralnetwork. Anothersignofoverfittingmaybeseenintheclassificationaccuracyonthetraining\n",
            "data:\n",
            "Theaccuracyrisesallthewayupto100percent. Thatis,ournetworkcorrectlyclassifiesall\n",
            "1,000trainingimages! Meanwhile,ourtestaccuracytopsoutatjust82.27percent. Soour\n",
            "networkreallyislearningaboutpeculiaritiesofthetrainingset,notjustrecognizingdigits\n",
            "ingeneral. Itâ€™salmostasthoughournetworkismerelymemorizingthetrainingset,without\n",
            "understandingdigitswellenoughtogeneralizetothetestset.\n",
            "Overfittingisamajorprobleminneuralnetworks. Thisisespeciallytrueinmodern\n",
            "networks,whichoftenhaveverylargenumbersofweightsandbiases. Totraineffectively,\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 77\n",
            "(cid:12)\n",
            "weneedawayofdetectingwhenoverfittingisgoingon,sowedonâ€™tovertrain. Andweâ€™d\n",
            "liketohavetechniquesforreducingtheeffectsofoverfitting. Theobviouswaytodetectoverfittingistousetheapproachabove,keepingtrackof\n",
            "accuracyonthetestdataasournetworktrains. Ifweseethattheaccuracyonthetest\n",
            "dataisnolongerimproving,thenweshouldstoptraining. Ofcourse,strictlyspeaking,this\n",
            "isnotnecessarilyasignofoverfitting. Itmightbethataccuracyonthetestdataandthe\n",
            "trainingdatabothstopimprovingatthesametime. Still,adoptingthisstrategywillprevent 3\n",
            "overfitting. Infact,weâ€™lluseavariationonthisstrategy. RecallthatwhenweloadintheMNIST\n",
            "dataweloadinthreedatasets:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            "Uptonowweâ€™vebeenusingthetraining_dataandtest_data, andignoringtheval-\n",
            "idation_data. The validation_data contains 10,000 images of digits, images which\n",
            "aredifferentfromthe50,000imagesintheMNISTtrainingset,andthe10,000imagesin\n",
            "theMNISTtestset. Insteadofusingthetest_datatopreventoverfitting,wewillusethe\n",
            "validation_data.\n",
            "Todothis,weâ€™llusemuchthesamestrategyaswasdescribedabovefor\n",
            "thetest_data. Thatis,weâ€™llcomputetheclassificationaccuracyonthevalidation_data\n",
            "attheendofeachepoch. Oncetheclassificationaccuracyonthevalidation_datahas\n",
            "saturated,westoptraining. Thisstrategyiscalledearlystopping.\n",
            "Ofcourse,inpracticewe\n",
            "wonâ€™timmediatelyknowwhentheaccuracyhassaturated. Instead,wecontinuetraining\n",
            "untilweâ€™reconfidentthattheaccuracyhassaturated10. Whyusethevalidation_datatopreventoverfitting,ratherthanthetest_data? In\n",
            "fact,thisispartofamoregeneralstrategy,whichistousethevalidation_datatoevaluate\n",
            "differenttrialchoicesofhyper-parameterssuchasthenumberofepochstotrainfor,the\n",
            "learningrate,thebestnetworkarchitecture,andsoon. Weusesuchevaluationstofind\n",
            "andsetgoodvaluesforthehyper-parameters. Indeed,althoughIhavenâ€™tmentionedituntil\n",
            "now,thatis,inpart,howIarrivedatthehyper-parameterchoicesmadeearlierinthisbook. (Moreonthislater.)\n",
            "Ofcourse,thatdoesnâ€™tinanywayanswerthequestionofwhyweâ€™reusingthevalida-\n",
            "tion_datatopreventoverfitting,ratherthanthetest_data. Instead,itreplacesitwith\n",
            "amoregeneralquestion,whichiswhyweâ€™reusingthevalidation_dataratherthanthe\n",
            "test_datatosetgoodhyper-parameters? Tounderstandwhy,considerthatwhensetting\n",
            "hyper-parametersweâ€™relikelytotrymanydifferentchoicesforthehyper-parameters. Ifwe\n",
            "setthehyper-parametersbasedonevaluationsofthetest_dataitâ€™spossibleweâ€™llendup\n",
            "overfittingourhyper-parameterstothetest_data. Thatis,wemayendupfindinghyper-\n",
            "parameterswhichfitparticularpeculiaritiesofthetest_data,butwheretheperformance\n",
            "ofthenetworkwonâ€™tgeneralizetootherdatasets. Weguardagainstthatbyfiguringoutthe\n",
            "hyper-parametersusingthevalidation_data. Then,onceweâ€™vegotthehyper-parameters\n",
            "wewant,wedoafinalevaluationofaccuracyusingthetest_data.\n",
            "Thatgivesusconfidence\n",
            "thatourresultsonthetest_dataareatruemeasureofhowwellourneuralnetworkgener-\n",
            "alizes. Toputitanotherway,youcanthinkofthevalidationdataasatypeoftrainingdata\n",
            "10Itrequiressomejudgmenttodeterminewhentostop.InmyearliergraphsIidentifiedepoch280as\n",
            "theplaceatwhichaccuracysaturated.Itâ€™spossiblethatwastoopessimistic.Neuralnetworkssometimes\n",
            "plateauforawhileintraining,beforecontinuingtoimprove.Iwouldnâ€™tbesurprisedifmorelearning\n",
            "couldhaveoccurredevenafterepoch400,althoughthemagnitudeofanyfurtherimprovementwould\n",
            "likelybesmall.Soitâ€™spossibletoadoptmoreorlessaggressivestrategiesforearlystopping. \n",
            "(cid:12)\n",
            "78 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thathelpsuslearngoodhyper-parameters. Thisapproachtofindinggoodhyper-parameters\n",
            "issometimesknownastheholdoutmethod,sincethevalidation_dataiskeptapartor\n",
            "â€œheldoutâ€fromthetraining_data. Now,inpractice,evenafterevaluatingperformanceonthetest_datawemaychange\n",
            "ourmindsandwanttotryanotherapproachâ€“perhapsadifferentnetworkarchitectureâ€“\n",
            "whichwillinvolvefindinganewsetofhyper-parameters. Ifwedothis,isnâ€™tthereadanger\n",
            "3 weâ€™llendupoverfittingtothetest_dataaswell? Doweneedapotentiallyinfiniteregress\n",
            "ofdatasets,sowecanbeconfidentourresultswillgeneralize? Addressingthisconcernfully\n",
            "isadeepanddifficultproblem.\n",
            "Butforourpracticalpurposes,weâ€™renotgoingtoworrytoo\n",
            "muchaboutthisquestion. Instead,weâ€™llplungeahead,usingthebasicholdoutmethod,\n",
            "basedonthetraining_data,validation_data,andtest_data,asdescribedabove. Weâ€™vebeenlookingsofaratoverfittingwhenweâ€™rejustusing1,000trainingimages. Whathappenswhenweusethefulltrainingsetof50,000images? Weâ€™llkeepalltheother\n",
            "parametersthesame(30hiddenneurons, learningrate0.5, mini-batchsizeof10), but\n",
            "trainusingall50,000imagesfor30epochs. Hereâ€™sagraphshowingtheresultsforthe\n",
            "classificationaccuracyonboththetrainingdataandthetestdata. NotethatIâ€™veusedthe\n",
            "testdatahere,ratherthanthevalidationdata,inordertomaketheresultsmoredirectly\n",
            "comparablewiththeearliergraphs. Asyoucansee,theaccuracyonthetestandtrainingdataremainmuchclosertogetherthan\n",
            "whenwewereusing1,000trainingexamples. Inparticular,thebestclassificationaccuracy\n",
            "of97.86percentonthetrainingdataisonly2.53percenthigherthanthe95.33percenton\n",
            "thetestdata. Thatâ€™scomparedtothe17.73percentgapwehadearlier!\n",
            "Overfittingisstill\n",
            "goingon,butitâ€™sbeengreatlyreduced. Ournetworkisgeneralizingmuchbetterfromthe\n",
            "trainingdatatothetestdata. Ingeneral,oneofthebestwaysofreducingoverfittingisto\n",
            "increasethesizeofthetrainingdata. Withenoughtrainingdataitisdifficultforevenavery\n",
            "largenetworktooverfit. Unfortunately,trainingdatacanbeexpensiveordifficulttoacquire,\n",
            "sothisisnotalwaysapracticaloption. 3.2.1 Regularization\n",
            "Increasingtheamountoftrainingdataisonewayofreducingoverfitting. Arethereother\n",
            "wayswecanreducetheextenttowhichoverfittingoccurs?\n",
            "Onepossibleapproachisto\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 79\n",
            "(cid:12)\n",
            "reducethesizeofournetwork. However,largenetworkshavethepotentialtobemore\n",
            "powerfulthansmallnetworks,andsothisisanoptionweâ€™donlyadoptreluctantly. Fortunately,thereareothertechniqueswhichcanreduceoverfitting,evenwhenwehave\n",
            "afixednetworkandfixedtrainingdata. Theseareknownasregularizationtechniques. In\n",
            "thissectionIdescribeoneofthemostcommonlyusedregularizationtechniques,atechnique\n",
            "sometimesknownasweightdecayorL2regularization.\n",
            "TheideaofL2regularizationisto\n",
            "3\n",
            "addanextratermtothecostfunction,atermcalledtheregularizationterm. Hereâ€™sthe\n",
            "regularizedcross-entropy:\n",
            "1(cid:88)(cid:148) (cid:151) Î» (cid:88)\n",
            "C= âˆ’n xj y j lnaL j +(1 âˆ’ y j)ln(1 âˆ’ aL j) + 2n w w2. (3.31)\n",
            "Thefirsttermisjusttheusualexpressionforthecross-entropy. Butweâ€™veaddedasecond\n",
            "term,namelythesumofthesquaresofalltheweightsinthenetwork. Thisisscaledbya\n",
            "factorÎ»/2n,whereÎ»>0isknownastheregularizationparameter,andnis,asusual,the\n",
            "sizeofourtrainingset. Iâ€™lldiscusslaterhowÎ»ischosen. Itâ€™salsoworthnotingthatthe\n",
            "regularizationtermdoesnâ€™tincludethebiases. Iâ€™llalsocomebacktothatbelow.\n",
            "Ofcourse,itâ€™spossibletoregularizeothercostfunctions,suchasthequadraticcost. This\n",
            "canbedoneinasimilarway:\n",
            "1 (cid:88) Î» (cid:88)\n",
            "C= y aL 2 + w2. (3.32)\n",
            "2n x (cid:107) âˆ’ (cid:107) 2n w\n",
            "Inbothcaseswecanwritetheregularizedcostfunctionas\n",
            "Î» (cid:88)\n",
            "C=C 0+\n",
            "2n\n",
            "w2, (3.33)\n",
            "w\n",
            "whereC istheoriginal,unregularizedcostfunction. 0\n",
            "Intuitively,theeffectofregularizationistomakeitsothenetworkpreferstolearnsmall\n",
            "weights,allotherthingsbeingequal. Largeweightswillonlybeallowediftheyconsiderably\n",
            "improvethefirstpartofthecostfunction. Putanotherway,regularizationcanbeviewed\n",
            "asawayofcompromisingbetweenfindingsmallweightsandminimizingtheoriginalcost\n",
            "function. Therelativeimportanceofthetwoelementsofthecompromisedependsonthe\n",
            "valueofÎ»: whenÎ»issmallweprefertominimizetheoriginalcostfunction,butwhenÎ»is\n",
            "largeweprefersmallweights. Now,itâ€™sreallynotatallobviouswhymakingthiskindofcompromiseshouldhelpreduce\n",
            "overfitting!\n",
            "Butitturnsoutthatitdoes. Weâ€™lladdressthequestionofwhyithelpsinthe\n",
            "nextsection. Butfirst,letâ€™sworkthroughanexampleshowingthatregularizationreallydoes\n",
            "reduceoverfitting. Toconstructsuchanexample,wefirstneedtofigureouthowtoapplyourstochastic\n",
            "gradientdescentlearningalgorithminaregularizedneuralnetwork. Inparticular,weneed\n",
            "toknowhowtocomputethepartialderivativesâˆ‚C/âˆ‚wandâˆ‚C/âˆ‚bforalltheweightsand\n",
            "\n",
            "(cid:12)\n",
            "80 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "biasesinthenetwork. TakingthepartialderivativesofEquation(3.33)gives\n",
            "âˆ‚C âˆ‚C Î»\n",
            "âˆ‚w = âˆ‚w 0 + n w (3.34)\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ‚b = âˆ‚b 0. (3.35)\n",
            "3\n",
            "Theâˆ‚C /âˆ‚wandâˆ‚C /âˆ‚btermscanbecomputedusingbackpropagation,asdescribedin\n",
            "0 0\n",
            "thelastchapter. Andsoweseethatitâ€™seasytocomputethegradientoftheregularizedcost\n",
            "Î»\n",
            "function: justusebackpropagation,asusual,andthenadd wtothepartialderivativeofall\n",
            "n\n",
            "theweightterms. Thepartialderivativeswithrespecttothebiasesareunchanged,andso\n",
            "thegradientdescentlearningruleforthebiasesdoesnâ€™tchangefromtheusualrule:\n",
            "âˆ‚C\n",
            "b b Î· 0. (3.36)\n",
            "â†’ âˆ’\n",
            "âˆ‚b\n",
            "Thelearningrulefortheweightsbecomes:\n",
            "âˆ‚C Î·Î» (cid:129) Î·Î»(cid:139) âˆ‚C\n",
            "w\n",
            "â†’\n",
            "w\n",
            "âˆ’\n",
            "Î·\n",
            "âˆ‚w\n",
            "0\n",
            "âˆ’ n\n",
            "w= 1\n",
            "âˆ’ n\n",
            "w\n",
            "âˆ’\n",
            "Î·\n",
            "âˆ‚w\n",
            "0. (3.37)\n",
            "Thisisexactlythesameastheusualgradientdescentlearningrule,exceptwefirstrescale\n",
            "theweightwbyafactor1\n",
            "Î·Î»\n",
            ". Thisrescalingissometimesreferredtoasweightdecay,\n",
            "n\n",
            "sinceitmakestheweightssâˆ’maller. Atfirstglanceitlooksasthoughthismeanstheweights\n",
            "arebeingdrivenunstoppablytowardzero. Butthatâ€™snotright,sincetheothertermmay\n",
            "leadtheweightstoincrease,ifsodoingcausesadecreaseintheunregularizedcostfunction. Okay,thatâ€™showgradientdescentworks.\n",
            "Whataboutstochasticgradientdescent? Well,\n",
            "justasinunregularizedstochasticgradientdescent,wecanestimateâˆ‚C /âˆ‚wbyaveraging\n",
            "0\n",
            "overamini-batchofmtrainingexamples. Thustheregularizedlearningruleforstochastic\n",
            "gradientdescentbecomes(c.f. Equation(1.20))\n",
            "(cid:129) Î·Î»(cid:139) Î· (cid:88)âˆ‚C\n",
            "w 1 w x, (3.38)\n",
            "â†’ âˆ’ n âˆ’ m x âˆ‚w\n",
            "wherethesumisovertrainingexamples x inthemini-batch,andC isthe(unregularized)\n",
            "x\n",
            "costforeachtrainingexample. Thisisexactlythesameastheusualruleforstochastic\n",
            "gradientdescent,exceptforthe1 Î·Î»/nweightdecayfactor. Finally,andforcompleteness,\n",
            "letmestatetheregularizedlearniâˆ’ngruleforthebiases. Thisis,ofcourse,exactlythesame\n",
            "asintheunregularizedcase(c.f. Equation1.21),\n",
            "Î· (cid:88)âˆ‚C\n",
            "b b x, (3.39)\n",
            "â†’ âˆ’ m x âˆ‚b\n",
            "wherethesumisovertrainingexamples x inthemini-batch.\n",
            "Letâ€™sseehowregularizationchangestheperformanceofourneuralnetwork. Weâ€™lluse\n",
            "a network with 30 hidden neurons, a mini-batch size of 10, a learning rate of 0.5, and\n",
            "thecross-entropycostfunction. However,thistimeweâ€™llusearegularizationparameter\n",
            "ofÎ» =0.1.\n",
            "Notethatinthecode,weusethevariablenamelmbda,becauselambdaisa\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 81\n",
            "(cid:12)\n",
            "reservedwordinPython,withanunrelatedmeaning. Iâ€™vealsousedthetest_dataagain,\n",
            "notthevalidation_data. Strictlyspeaking,weshouldusethevalidation_data,forall\n",
            "the reasons we discussed earlier. But I decided to use the test_data because it makes\n",
            "theresultsmoredirectlycomparablewithourearlier,unregularizedresults.\n",
            "Youcaneasily\n",
            "changethecodetousethevalidation_datainstead,andyouâ€™llfindthatitgivessimilar\n",
            "results. 3\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, lmbda\n",
            "= 0.1, monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
            "monitor_training_cost=True, monitor_training_accuracy=True)\n",
            "Thecostonthetrainingdatadecreasesoverthewholetime,muchasitdidintheearlier,\n",
            "unregularizedcase11:\n",
            "Butthistimetheaccuracyonthetest_datacontinuestoincreasefortheentire400epochs:\n",
            "11Thisandthenexttwographswereproducedwiththeprogramoverfitting.py. \n",
            "(cid:12)\n",
            "82 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Clearly,theuseofregularizationhassuppressedoverfitting. Whatâ€™smore,theaccuracyis\n",
            "considerablyhigher,withapeakclassificationaccuracyof87.1percent,comparedtothepeak\n",
            "of82.27percentobtainedintheunregularizedcase. Indeed,wecouldalmostcertainlyget\n",
            "considerablybetterresultsbycontinuingtotrainpast400epochs. Itseemsthat,empirically,\n",
            "regularizationiscausingournetworktogeneralizebetter,andconsiderablyreducingthe\n",
            "effectsofoverfitting. 3 Whathappensifwemoveoutoftheartificialenvironmentofjusthaving1,000training\n",
            "images,andreturntothefull50,000imagetrainingset? Ofcourse,weâ€™veseenalreadythat\n",
            "overfittingismuchlessofaproblemwiththefull50,000images. Doesregularizationhelp\n",
            "anyfurther? Letâ€™skeepthehyper-parametersthesameasbeforeâ€“30epochs,learningrate\n",
            "0.5,mini-batchsizeof10. However,weneedtomodifytheregularizationparameter. The\n",
            "reasonisbecausethesizenofthetrainingsethaschangedfromn=1,000ton=50,000,and\n",
            "thischangestheweightdecayfactor1 Î·Î»/n. IfwecontinuedtouseÎ» =0.1thatwould\n",
            "meanmuchlessweightdecay,andthusâˆ’muchlessofaregularizationeffect. Wecompensate\n",
            "bychangingtoÎ» =5.0. Okay,letâ€™strainournetwork,stoppingfirsttore-initializetheweights:\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, lmbda = 5.0,\n",
            "... monitor_evaluation_accuracy=True, monitor_training_accuracy=True)\n",
            "Weobtaintheresults:\n",
            "Thereâ€™slotsofgoodnewshere.\n",
            "First,ourclassificationaccuracyonthetestdataisup,from\n",
            "95.49percentwhenrunningunregularized,to96.49percent.\n",
            "Thatâ€™sabigimprovement. Second, we can see that the gap between results on the training and test data is much\n",
            "narrowerthanbefore,runningatunderapercent. Thatâ€™sstillasignificantgap,butweâ€™ve\n",
            "obviouslymadesubstantialprogressreducingoverfitting. Finally,letâ€™sseewhattestclassificationaccuracywegetwhenweuse100hiddenneurons\n",
            "andaregularizationparameterofÎ» =5.0.Iwonâ€™tgothroughadetailedanalysisofoverfitting\n",
            "here,thisispurelyforfun,justtoseehowhighanaccuracywecangetwhenweuseour\n",
            "newtricks: thecross-entropycostfunctionandL2regularization. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 83\n",
            "(cid:12)\n",
            ">>> net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, lmbda=5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Thefinalresultisaclassificationaccuracyof97.92percentonthevalidationdata. Thatâ€™sa\n",
            "bigjumpfromthe30hiddenneuroncase. Infact,tuningjustalittlemore,torunfor60\n",
            "epochsatÎ· =0.1andÎ» =5.0webreakthe98percentbarrier,achieving98.04percent\n",
            "classificationaccuracyonthevalidationdata. Notbadforwhatturnsouttobe152linesof\n",
            "code! Iâ€™vedescribedregularizationasawaytoreduceoverfittingandtoincreaseclassification\n",
            "accuracies.\n",
            "Infact,thatâ€™snottheonlybenefit. Empirically,whendoingmultiplerunsof\n",
            "ourMNISTnetworks,butwithdifferent(random)weightinitializations,Iâ€™vefoundthatthe\n",
            "unregularizedrunswilloccasionallygetâ€œstuckâ€,apparentlycaughtinlocalminimaofthe\n",
            "costfunction. Theresultisthatdifferentrunssometimesprovidequitedifferentresults.\n",
            "By\n",
            "contrast,theregularizedrunshaveprovidedmuchmoreeasilyreplicableresults. Whyisthisgoingon? Heuristically,ifthecostfunctionisunregularized,thenthelength\n",
            "oftheweightvectorislikelytogrow,allotherthingsbeingequal. Overtimethiscanlead\n",
            "totheweightvectorbeingverylargeindeed. Thiscancausetheweightvectortogetstuck\n",
            "pointinginmoreorlessthesamedirection,sincechangesduetogradientdescentonlymake\n",
            "tinychangestothedirection,whenthelengthislong. Ibelievethisphenomenonismaking\n",
            "ithardforourlearningalgorithmtoproperlyexploretheweightspace,andconsequently\n",
            "hardertofindgoodminimaofthecostfunction. 3.2.2 Whydoesregularizationhelpreduceoverfitting? Weâ€™veseenempiricallythatregularizationhelpsreduceoverfitting. Thatâ€™sencouragingbut,\n",
            "unfortunately, itâ€™snotobviouswhyregularizationhelps! Astandardstorypeopletellto\n",
            "explainwhatâ€™sgoingonisalongthefollowinglines: smallerweightsare,insomesense,\n",
            "lowercomplexity,andsoprovideasimplerandmorepowerfulexplanationforthedata,and\n",
            "shouldthusbepreferred. Thatâ€™saprettytersestory,though,andcontainsseveralelements\n",
            "thatperhapsseemdubiousormystifying.\n",
            "Letâ€™sunpackthestoryandexamineitcritically. To\n",
            "dothat,letâ€™ssupposewehaveasimpledatasetforwhichwewishtobuildamodel:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "3\n",
            "\n",
            "(cid:12)\n",
            "84 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Implicitly,weâ€™restudyingsomereal-worldphenomenonhere,with x and y representing\n",
            "real-worlddata. Ourgoalistobuildamodelwhichletsuspredict y asafunctionof x. We\n",
            "couldtryusingneuralnetworkstobuildsuchamodel,butIâ€™mgoingtodosomethingeven\n",
            "simpler: Iâ€™lltrytomodel y asapolynomialin x. Iâ€™mdoingthisinsteadofusingneuralnets\n",
            "becauseusingpolynomialswillmakethingsparticularlytransparent.\n",
            "Onceweâ€™veunderstood\n",
            "thepolynomialcase,weâ€™lltranslatetoneuralnetworks. Now,therearetenpointsinthegraph\n",
            "above,whichmeanswecanfindaunique9-th-orderpolynomial y=a 0 x9 +a 1 x8 +...+a 9\n",
            "whichfitsthedataexactly. Hereâ€™sthegraphofthatpolynomial12:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "Thatprovidesanexactfit. Butwecanalsogetagoodfitusingthelinearmodel y=2x:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "3\n",
            "Whichoftheseisthebettermodel? Whichismorelikelytobetrue? Andwhichmodelismore\n",
            "likelytogeneralizewelltootherexamplesofthesameunderlyingreal-worldphenomenon? Thesearedifficultquestions. Infact,wecanâ€™tdeterminewithcertaintytheanswerto\n",
            "anyoftheabovequestions,withoutmuchmoreinformationabouttheunderlyingreal-world\n",
            "phenomenon. Butletâ€™sconsidertwopossibilities: (1)the9thorderpolynomialis,infact,\n",
            "themodelwhichtrulydescribesthereal-worldphenomenon,andthemodelwilltherefore\n",
            "generalizeperfectly;(2)thecorrectmodelis y=2x,butthereâ€™salittleadditionalnoisedue\n",
            "to,say,measurementerror,andthatâ€™swhythemodelisnâ€™tanexactfit. 12Iwonâ€™tshowthecoefficientsexplicitly,althoughtheyareeasytofindusingaroutinesuchasNumpyâ€™s\n",
            "polyfit.Youcanviewtheexactformofthepolynomialinthesourcecodeforthegraphifyouâ€™recurious.\n",
            "Itâ€™sthefunctionp(x)definedstartingonline14oftheprogramwhichproducesthegraph. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 85\n",
            "(cid:12)\n",
            "Itâ€™snotaprioripossibletosaywhichofthesetwopossibilitiesiscorrect. (Or,indeed,if\n",
            "somethirdpossibilityholds).\n",
            "Logically,eithercouldbetrue. Anditâ€™snotatrivialdifference. Itâ€™struethatonthedataprovidedthereâ€™sonlyasmalldifferencebetweenthetwomodels. Butsupposewewanttopredictthevalueof ycorrespondingtosomelargevalueofx,much\n",
            "largerthananyshownonthegraphabove. Ifwetrytodothattherewillbeadramatic\n",
            "differencebetweenthepredictionsofthetwomodels,asthe9thorderpolynomialmodel\n",
            "comestobedominatedbythe x9term,whilethelinearmodelremains,well,linear. 3\n",
            "Onepointofviewistosaythatinscienceweshouldgowiththesimplerexplanation,\n",
            "unlesscompellednotto. Whenwefindasimplemodelthatseemstoexplainmanydata\n",
            "pointswearetemptedtoshoutâ€œEureka!â€ Afterall,itseemsunlikelythatasimpleexplanation\n",
            "shouldoccurmerelybycoincidence. Rather,wesuspectthatthemodelmustbeexpressing\n",
            "someunderlyingtruthaboutthephenomenon. Inthecaseathand,themodel y=2x+noise\n",
            "seemsmuchsimplerthan y=a\n",
            "0\n",
            "x9 +a\n",
            "1\n",
            "x8 +.... Itwouldbesurprisingifthatsimplicityhad\n",
            "occurredbychance,andsowesuspectthat y=2x+noiseexpressessomeunderlyingtruth. Inthispointofview,the9thordermodelisreallyjustlearningtheeffectsoflocalnoise. And\n",
            "sowhilethe9thordermodelworksperfectlyfortheseparticulardatapoints,themodelwill\n",
            "failtogeneralizetootherdatapoints,andthenoisylinearmodelwillhavegreaterpredictive\n",
            "power. Letâ€™sseewhatthispointofviewmeansforneuralnetworks. Supposeournetworkmostly\n",
            "hassmallweights,aswilltendtohappeninaregularizednetwork. Thesmallnessofthe\n",
            "weightsmeansthatthebehaviourofthenetworkwonâ€™tchangetoomuchifwechangeafew\n",
            "randominputshereandthere. Thatmakesitdifficultforaregularizednetworktolearnthe\n",
            "effectsoflocalnoiseinthedata. Thinkofitasawayofmakingitsosinglepiecesofevidence\n",
            "donâ€™tmattertoomuchtotheoutputofthenetwork. Instead,aregularizednetworklearns\n",
            "torespondtotypesofevidencewhichareseenoftenacrossthetrainingset. Bycontrast,a\n",
            "networkwithlargeweightsmaychangeitsbehaviourquiteabitinresponsetosmallchanges\n",
            "intheinput. Andsoanunregularizednetworkcanuselargeweightstolearnacomplex\n",
            "modelthatcarriesalotofinformationaboutthenoiseinthetrainingdata. Inanutshell,\n",
            "regularizednetworksareconstrainedtobuildrelativelysimplemodelsbasedonpatterns\n",
            "seenofteninthetrainingdata,andareresistanttolearningpeculiaritiesofthenoiseinthe\n",
            "trainingdata. Thehopeisthatthiswillforceournetworkstodoreallearningaboutthe\n",
            "phenomenonathand,andtogeneralizebetterfromwhattheylearn.\n",
            "Withthatsaid,thisideaofpreferringsimplerexplanationshouldmakeyounervous. Peoplesometimesrefertothisideaasâ€œOccamâ€™sRazorâ€,andwillzealouslyapplyitasthough\n",
            "ithasthestatusofsomegeneralscientificprinciple. But,ofcourse,itâ€™snotageneralscientific\n",
            "principle. Thereisnoapriorilogicalreasontoprefersimpleexplanationsovermorecomplex\n",
            "explanations. Indeed,sometimesthemorecomplexexplanationturnsouttobecorrect. Letmedescribetwoexampleswheremorecomplexexplanationshaveturnedouttobe\n",
            "correct.\n",
            "Inthe1940sthephysicistMarcelScheinannouncedthediscoveryofanewparticle\n",
            "ofnature. Thecompanyheworkedfor,GeneralElectric,wasecstatic,andpublicizedthe\n",
            "discoverywidely. ButthephysicistHansBethewasskeptical.\n",
            "BethevisitedSchein, and\n",
            "lookedattheplatesshowingthetracksofScheinâ€™snewparticle. ScheinshowedBetheplate\n",
            "afterplate,butoneachplateBetheidentifiedsomeproblemthatsuggestedthedatashould\n",
            "bediscarded. Finally,ScheinshowedBetheaplatethatlookedgood. Bethesaiditmight\n",
            "justbeastatisticalfluke. Schein: â€œYes,butthechancethatthiswouldbestatistics,even\n",
            "accordingtoyourownformula,isoneinfive.â€ Bethe: â€œButwehavealreadylookedatfive\n",
            "plates.â€ Finally,Scheinsaid: â€œButonmyplates,eachoneofthegoodplates,eachoneof\n",
            "\n",
            "(cid:12)\n",
            "86 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thegoodpictures,youexplainbyadifferenttheory,whereasIhaveonehypothesisthat\n",
            "explainsalltheplates,thattheyare[thenewparticle].â€ Bethereplied: â€œThesoledifference\n",
            "betweenyourandmyexplanationsisthatyoursiswrongandallofmineareright. Your\n",
            "singleexplanationiswrong,andallofmymultipleexplanationsareright.â€ Subsequentwork\n",
            "confirmedthatNatureagreedwithBethe,andScheinâ€™sparticleisnomore13. Asasecondexample,in1859theastronomerUrbainLeVerrierobservedthattheorbit\n",
            "3 oftheplanetMercurydoesnâ€™thavequitetheshapethatNewtonâ€™stheoryofgravitationsaysit\n",
            "shouldhave.Itwasatiny,tinydeviationfromNewtonâ€™stheory,andseveraloftheexplanations\n",
            "proferredatthetimeboileddowntosayingthatNewtonâ€™stheorywasmoreorlessright,but\n",
            "neededatinyalteration. In1916,Einsteinshowedthatthedeviationcouldbeexplainedvery\n",
            "wellusinghisgeneraltheoryofrelativity,atheoryradicallydifferenttoNewtoniangravitation,\n",
            "andbasedonmuchmorecomplexmathematics. Despitethatadditionalcomplexity,todayitâ€™s\n",
            "acceptedthatEinsteinâ€™sexplanationiscorrect,andNewtoniangravity,eveninitsmodified\n",
            "forms,iswrong. ThisisinpartbecausewenowknowthatEinsteinâ€™stheoryexplainsmany\n",
            "otherphenomenawhichNewtonâ€™stheoryhasdifficultywith. Furthermore,andevenmore\n",
            "impressively,Einsteinâ€™stheoryaccuratelypredictsseveralphenomenawhicharenâ€™tpredicted\n",
            "byNewtoniangravityatall. Buttheseimpressivequalitieswerenâ€™tentirelyobviousinthe\n",
            "earlydays. Ifonehadjudgedmerelyonthegroundsofsimplicity,thensomemodifiedform\n",
            "ofNewtonâ€™stheorywouldarguablyhavebeenmoreattractive. Therearethreemoralstodrawfromthesestories.\n",
            "First,itcanbequiteasubtlebusiness\n",
            "decidingwhichoftwoexplanationsistrulyâ€œsimplerâ€. Second,evenifwecanmakesucha\n",
            "judgment,simplicityisaguidethatmustbeusedwithgreatcaution! Third,thetruetestof\n",
            "amodelisnotsimplicity,butratherhowwellitdoesinpredictingnewphenomena,innew\n",
            "regimesofbehaviour. Withthatsaid, andkeepingtheneedforcautioninmind, itâ€™sanempiricalfactthat\n",
            "regularizedneuralnetworksusuallygeneralizebetterthanunregularizednetworks. Andso\n",
            "throughtheremainderofthebookwewillmakefrequentuseofregularization. Iâ€™veincluded\n",
            "thestoriesabovemerelytohelpconveywhyno-onehasyetdevelopedanentirelyconvincing\n",
            "theoreticalexplanationforwhyregularizationhelpsnetworksgeneralize. Indeed,researchers\n",
            "continuetowritepaperswheretheytrydifferentapproachestoregularization,compare\n",
            "themtoseewhichworksbetter,andattempttounderstandwhydifferentapproacheswork\n",
            "betterorworse. Andsoyoucanviewregularizationassomethingofakludge. Whileitoften\n",
            "helps,wedonâ€™thaveanentirelysatisfactorysystematicunderstandingofwhatâ€™sgoingon,\n",
            "merelyincompleteheuristicsandrulesofthumb. Thereâ€™s a deeper set of issues here, issues which go to the heart of science.\n",
            "Itâ€™s the\n",
            "questionofhowwegeneralize. Regularizationmaygiveusacomputationalmagicwand\n",
            "thathelpsournetworksgeneralizebetter,butitdoesnâ€™tgiveusaprincipledunderstanding\n",
            "ofhowgeneralizationworks,norofwhatthebestapproachis14. Thisisparticularlygallingbecauseineverydaylife,wehumansgeneralizephenomenally\n",
            "well. Shownjustafewimagesofanelephantachildwillquicklylearntorecognizeother\n",
            "elephants. Ofcourse,theymayoccasionallymakemistakes,perhapsconfusingarhinoceros\n",
            "foranelephant, butingeneralthisprocessworksremarkablyaccurately.\n",
            "Sowehavea\n",
            "13ThestoryisrelatedbythephysicistRichardFeynmaninaninterviewwiththehistorianCharles\n",
            "Weiner. 14Theseissuesgobacktotheproblemofinduction,famouslydiscussedbytheScottishphilosopher\n",
            "DavidHumeinâ€œAnEnquiryConcerningHumanUnderstandingâ€(1748).Theproblemofinductionhas\n",
            "beengivenamodernmachinelearningformintheno-freelunchtheoremofDavidWolpertandWilliam\n",
            "Macready(1997). \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 87\n",
            "(cid:12)\n",
            "systemâ€“thehumanbrainâ€“withahugenumberoffreeparameters. Andafterbeingshown\n",
            "justoneorafewtrainingimagesthatsystemlearnstogeneralizetootherimages. Our\n",
            "brainsare,insomesense,regularizingamazinglywell! Howdowedoit?\n",
            "Atthispointwe\n",
            "donâ€™tknow. Iexpectthatinyearstocomewewilldevelopmorepowerfultechniquesfor\n",
            "regularizationinartificialneuralnetworks,techniquesthatwillultimatelyenableneural\n",
            "netstogeneralizewellevenfromsmalldatasets. Infact,ournetworksalreadygeneralizebetterthanonemightaprioriexpect. Anetwork 3\n",
            "with100hiddenneuronshasnearly80,000parameters. Wehaveonly50,000imagesinour\n",
            "trainingdata. Itâ€™sliketryingtofitan80,000thdegreepolynomialto50,000datapoints. By\n",
            "allrights,ournetworkshouldoverfitterribly.\n",
            "Andyet,aswesawearlier,suchanetwork\n",
            "actuallydoesaprettygoodjobgeneralizing. Whyisthatthecase?\n",
            "Itâ€™snotwellunderstood. Ithasbeenconjectured15thatâ€œthedynamicsofgradientdescentlearninginmultilayernets\n",
            "hasaâ€˜self-regularizationâ€™effectâ€. Thisisexceptionallyfortunate, butitâ€™salsosomewhat\n",
            "disquietingthatwedonâ€™tunderstandwhyitâ€™sthecase. Inthemeantime,wewilladoptthe\n",
            "pragmaticapproachanduseregularizationwheneverwecan. Ourneuralnetworkswillbe\n",
            "thebetterforit. LetmeconcludethissectionbyreturningtoadetailwhichIleftunexplainedearlier:\n",
            "thefactthatL2regularizationdoesnâ€™tconstrainthebiases. Ofcourse,itwouldbeeasyto\n",
            "modifytheregularizationproceduretoregularizethebiases. Empirically,doingthisoften\n",
            "doesnâ€™tchangetheresultsverymuch,sotosomeextentitâ€™smerelyaconventionwhetherto\n",
            "regularizethebiasesornot. However,itâ€™sworthnotingthathavingalargebiasdoesnâ€™tmake\n",
            "aneuronsensitivetoitsinputsinthesamewayashavinglargeweights. Andsowedonâ€™t\n",
            "needtoworryaboutlargebiasesenablingournetworktolearnthenoiseinourtrainingdata. Atthesametime,allowinglargebiasesgivesournetworksmoreflexibilityinbehaviourâ€“in\n",
            "particular,largebiasesmakeiteasierforneuronstosaturate,whichissometimesdesirable. Forthesereasonswedonâ€™tusuallyincludebiastermswhenregularizing. 3.2.3 Othertechniquesforregularization\n",
            "TherearemanyregularizationtechniquesotherthanL2regularization. Infact,somany\n",
            "techniqueshavebeendevelopedthatIcanâ€™tpossiblysummarizethemall. InthissectionI\n",
            "brieflydescribethreeotherapproachestoreducingoverfitting: L1regularization,dropout,\n",
            "and artificially increasing the training set size. We wonâ€™t go into nearly as much depth\n",
            "studyingthesetechniquesaswedidearlier.\n",
            "Instead, thepurposeistogetfamiliarwith\n",
            "themainideas,andtoappreciatesomethingofthediversityofregularizationtechniques\n",
            "available. L1regularization:Inthisapproachwemodifytheunregularizedcostfunctionbyadding\n",
            "thesumoftheabsolutevaluesoftheweights:\n",
            "Î»(cid:88)\n",
            "C=C 0+\n",
            "n w |\n",
            "w\n",
            "|\n",
            ". (3.40)\n",
            "Intuitively,thisissimilartoL2regularization,penalizinglargeweights,andtendingtomake\n",
            "thenetworkprefersmallweights. Ofcourse,theL1regularizationtermisnâ€™tthesameasthe\n",
            "L2regularizationterm,andsoweshouldnâ€™texpecttogetexactlythesamebehaviour. Letâ€™s\n",
            "15InGradient-BasedLearningAppliedtoDocumentRecognition,byYannLeCun,LÃ©onBottou,Yoshua\n",
            "Bengio,andPatrickHaffner(1998). \n",
            "(cid:12)\n",
            "88 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "trytounderstandhowthebehaviourofanetworktrainedusingL1regularizationdiffers\n",
            "fromanetworktrainedusingL2regularization. Todothat,weâ€™lllookatthepartialderivativesofthecostfunction. Differentiating(95)\n",
            "weobtain:\n",
            "3 âˆ‚C âˆ‚C Î»\n",
            "âˆ‚w = âˆ‚w 0 + n sgn(w), (3.41)\n",
            "wheresgn(w)isthesignofw,thatis,+1ifwispositive,and 1ifwisnegative. Usingthis\n",
            "expression,wecaneasilymodifybackpropagationtodostochaâˆ’sticgradientdescentusingL1\n",
            "regularization. TheresultingupdateruleforanL1regularizednetworkis\n",
            "Î·Î» âˆ‚C\n",
            "w\n",
            "â†’\n",
            "w (cid:48)=w\n",
            "âˆ’ n\n",
            "sgn(w)\n",
            "âˆ’\n",
            "Î·\n",
            "âˆ‚w\n",
            "0, (3.42)\n",
            "where, as per usual, we can estimate âˆ‚C /âˆ‚w using a mini-batch average, if we wish. 0\n",
            "ComparethattotheupdateruleforL2regularization(c.f. Equation(3.38)),\n",
            "(cid:129) Î·Î»(cid:139) âˆ‚C\n",
            "w\n",
            "â†’\n",
            "w (cid:48)=w 1\n",
            "âˆ’ n âˆ’\n",
            "Î·\n",
            "âˆ‚w\n",
            "0. (3.43)\n",
            "Inbothexpressionstheeffectofregularizationistoshrinktheweights.\n",
            "Thisaccordswithour\n",
            "intuitionthatbothkindsofregularizationpenalizelargeweights. Butthewaytheweights\n",
            "shrinkisdifferent. InL1regularization,theweightsshrinkbyaconstantamounttoward0. In\n",
            "L2regularization,theweightsshrinkbyanamountwhichisproportionaltow. Andsowhen\n",
            "aparticularweighthasalargemagnitude, w,L1regularizationshrinkstheweightmuch\n",
            "lessthanL2regularizationdoes. Bycontras|t,|when w issmall,L1regularizationshrinks\n",
            "theweightmuchmorethanL2regularization. Thene|tr|esultisthatL1regularizationtends\n",
            "toconcentratetheweightofthenetworkinarelativelysmallnumberofhigh-importance\n",
            "connections,whiletheotherweightsaredriventowardzero. Iâ€™veglossedoveranissueintheabovediscussion,whichisthatthepartialderivative\n",
            "âˆ‚C/âˆ‚wisnâ€™tdefinedwhenw=0. Thereasonisthatthefunction w hasasharpâ€œcornerâ€\n",
            "at w=0,andsoisnâ€™tdifferentiableatthatpoint. Thatâ€™sokay,th|ou|gh.\n",
            "Whatweâ€™lldois\n",
            "justapplytheusual(unregularized)ruleforstochasticgradientdescentwhenw=0. That\n",
            "shouldbeokayâ€“intuitively,theeffectofregularizationistoshrinkweights,andobviouslyit\n",
            "canâ€™tshrinkaweightwhichisalready0. Toputitmoreprecisely,weâ€™lluseEquations(3.41)\n",
            "and(3.42)withtheconventionthatsgn(0)=0. Thatgivesanice,compactrulefordoing\n",
            "stochasticgradientdescentwithL1regularization. Dropout: Dropoutisaradicallydifferenttechniqueforregularization. UnlikeL1and\n",
            "L2regularization,dropoutdoesnâ€™trelyonmodifyingthecostfunction. Instead,indropout\n",
            "wemodifythenetworkitself.\n",
            "Letmedescribethebasicmechanicsofhowdropoutworks,\n",
            "beforegettingintowhyitworks,andwhattheresultsare. Supposeweâ€™retryingtotrainanetwork:\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 89\n",
            "(cid:12)\n",
            "3\n",
            "Inparticular,supposewehaveatraininginput x andcorrespondingdesiredoutput y. Ordi-\n",
            "narily,weâ€™dtrainbyforward-propagating x throughthenetwork,andthenbackpropagating\n",
            "todeterminethecontributiontothegradient. Withdropout,thisprocessismodified. We\n",
            "startbyrandomly(andtemporarily)deletinghalfthehiddenneuronsinthenetwork,while\n",
            "leaving the input and output neurons untouched. After doing this, weâ€™ll end up with a\n",
            "networkalongthefollowinglines. Notethatthedropoutneurons,i.e.,theneuronswhich\n",
            "havebeentemporarilydeleted,arestillghostedin:\n",
            "Weforward-propagatetheinput x throughthemodifiednetwork,andthenbackpropagate\n",
            "theresult,alsothroughthemodifiednetwork.\n",
            "Afterdoingthisoveramini-batchofexamples,\n",
            "weupdatetheappropriateweightsandbiases. Wethenrepeattheprocess,firstrestoringthe\n",
            "dropoutneurons,thenchoosinganewrandomsubsetofhiddenneuronstodelete,estimating\n",
            "thegradientforadifferentmini-batch,andupdatingtheweightsandbiasesinthenetwork. Byrepeatingthisprocessoverandover,ournetworkwilllearnasetofweightsand\n",
            "biases.\n",
            "Ofcourse,thoseweightsandbiaseswillhavebeenlearntunderconditionsinwhich\n",
            "halfthehiddenneuronsweredroppedout. Whenweactuallyrunthefullnetworkthat\n",
            "meansthattwiceasmanyhiddenneuronswillbeactive. Tocompensateforthat,wehalve\n",
            "theweightsoutgoingfromthehiddenneurons. \n",
            "(cid:12)\n",
            "90 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Thisdropoutproceduremayseemstrangeandadhoc. Whywouldweexpectittohelp\n",
            "withregularization? Toexplainwhatâ€™sgoingon,Iâ€™dlikeyoutobrieflystopthinkingabout\n",
            "dropout,andinsteadimaginetrainingneuralnetworksinthestandardway(nodropout). In\n",
            "particular,imaginewetrainseveraldifferentneuralnetworks,allusingthesametraining\n",
            "data. Ofcourse, thenetworksmaynotstartoutidentical, andasaresultaftertraining\n",
            "theymaysometimesgivedifferentresults. Whenthathappenswecouldusesomekind\n",
            "3 ofaveragingorvotingschemetodecidewhichoutputtoaccept. Forinstance,ifwehave\n",
            "trainedfivenetworks,andthreeofthemareclassifyingadigitasaâ€œ3â€,thenitprobablyreally\n",
            "isaâ€œ3â€. Theothertwonetworksareprobablyjustmakingamistake. Thiskindofaveraging\n",
            "schemeisoftenfoundtobeapowerful(thoughexpensive)wayofreducingoverfitting.\n",
            "The\n",
            "reasonisthatthedifferentnetworksmayoverfitindifferentways,andaveragingmayhelp\n",
            "eliminatethatkindofoverfitting. Whatâ€™s this got to do with dropout? Heuristically, when we dropout different sets\n",
            "ofneurons, itâ€™sratherlikeweâ€™retrainingdifferentneuralnetworks. Andsothedropout\n",
            "procedureislikeaveragingtheeffectsofaverylargenumberofdifferentnetworks. The\n",
            "differentnetworkswilloverfitindifferentways,andso,hopefully,theneteffectofdropout\n",
            "willbetoreduceoverfitting. Arelatedheuristicexplanationfordropoutisgiveninoneoftheearliestpaperstouse\n",
            "thetechnique16: â€œThistechniquereducescomplexco-adaptationsofneurons,sinceaneuron\n",
            "cannotrelyonthepresenceofparticularotherneurons. Itis,therefore,forcedtolearnmore\n",
            "robustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsetsofthe\n",
            "otherneurons.â€ Inotherwords,ifwethinkofournetworkasamodelwhichismaking\n",
            "predictions,thenwecanthinkofdropoutasawayofmakingsurethatthemodelisrobust\n",
            "tothelossofanyindividualpieceofevidence. Inthis,itâ€™ssomewhatsimilartoL1andL2\n",
            "regularization,whichtendtoreduceweights,andthusmakethenetworkmorerobustto\n",
            "losinganyindividualconnectioninthenetwork. Ofcourse,thetruemeasureofdropoutisthatithasbeenverysuccessfulinimproving\n",
            "theperformanceofneuralnetworks. Theoriginalpaper17introducingthetechniqueapplied\n",
            "ittomanydifferenttasks. Forus, itâ€™sofparticularinterestthattheyapplieddropoutto\n",
            "MNISTdigitclassification,usingavanillafeedforwardneuralnetworkalonglinessimilarto\n",
            "thoseweâ€™vebeenconsidering. Thepapernotedthatthebestresultanyonehadachieved\n",
            "uptothatpointusingsuchanarchitecturewas98.4percentclassificationaccuracyonthe\n",
            "testset. Theyimprovedthatto98.7percentaccuracyusingacombinationofdropoutand\n",
            "amodifiedformofL2regularization. Similarlyimpressiveresultshavebeenobtainedfor\n",
            "manyothertasks,includingproblemsinimageandspeechrecognition,andnaturallanguage\n",
            "processing. Dropouthasbeenespeciallyusefulintraininglarge,deepnetworks,wherethe\n",
            "problemofoverfittingisoftenacute. Artificiallyexpandingthetrainingdata: WesawearlierthatourMNISTclassification\n",
            "accuracydroppeddowntopercentagesinthemid-80swhenweusedonly1,000training\n",
            "images.\n",
            "Itâ€™snotsurprisingthatthisisthecase,sincelesstrainingdatameansournetwork\n",
            "willbeexposedtofewervariationsinthewayhumanbeingswritedigits. Letâ€™strytraining\n",
            "our 30 hidden neuron network with a variety of different training data set sizes, to see\n",
            "howperformancevaries. Wetrainusingamini-batchsizeof10,alearningrateÎ· =0.5,\n",
            "16ImageNetClassificationwithDeepConvolutionalNeuralNetworks,byAlexKrizhevsky,IlyaSutskever,\n",
            "andGeoffreyHinton(2012).\n",
            "17Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectorsbyGeoffreyHinton,\n",
            "NitishSrivastava,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov(2012).Notethatthepaper\n",
            "discussesanumberofsubtletiesthatIhaveglossedoverinthisbriefintroduction. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 91\n",
            "(cid:12)\n",
            "a regularization parameter Î» = 5.0, and the cross-entropy cost function. We will train\n",
            "for30epochswhenthefulltrainingdatasetisused,andscaleupthenumberofepochs\n",
            "proportionallywhensmallertrainingsetsareused. Toensuretheweightdecayfactorremains\n",
            "thesameacrosstrainingsets,wewillusearegularizationparameterofÎ» =5.0whenthe\n",
            "fulltrainingdatasetisused,andscaledownÎ»proportionallywhensmallertrainingsetsare\n",
            "used18. 3\n",
            "Asyoucansee,theclassificationaccuraciesimproveconsiderablyasweusemoretraining\n",
            "data.\n",
            "Presumablythisimprovementwouldcontinuestillfurtherifmoredatawasavailable. Of course, looking at the graph above it does appear that weâ€™re getting near saturation. Suppose,however,thatweredothegraphwiththetrainingsetsizeplottedlogarithmically:\n",
            "Itseemsclearthatthegraphisstillgoinguptowardtheend. Thissuggeststhatifweused\n",
            "vastlymoretrainingdataâ€“say,millionsorevenbillionsofhandwritingsamples,insteadof\n",
            "just50,000â€“thenweâ€™dlikelygetconsiderablybetterperformance,evenfromthisverysmall\n",
            "network. Obtainingmoretrainingdataisagreatidea.\n",
            "Unfortunately,itcanbeexpensive,andso\n",
            "isnotalwayspossibleinpractice. However,thereâ€™sanotherideawhichcanworknearlyas\n",
            "well,andthatâ€™stoartificiallyexpandthetrainingdata. Suppose,forexample,thatwetake\n",
            "anMNISTtrainingimageofafive,\n",
            "18Thisandthenexttwographareproducedwiththeprogrammore_data.py. \n",
            "(cid:12)\n",
            "92 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "androtateitbyasmallamount,letâ€™ssay15degrees:\n",
            "3\n",
            "Itâ€™sstillrecognizablythesamedigit. Andyetatthepixellevelitâ€™squitedifferenttoanyimage\n",
            "currentlyintheMNISTtrainingdata. Itâ€™sconceivablethataddingthisimagetothetraining\n",
            "datamighthelpournetworklearnmoreabouthowtoclassifydigits. Whatâ€™smore,obviously\n",
            "weâ€™renotlimitedtoaddingjustthisoneimage. Wecanexpandourtrainingdatabymaking\n",
            "manysmallrotationsofalltheMNISTtrainingimages,andthenusingtheexpandedtraining\n",
            "datatoimproveournetworkâ€™sperformance.\n",
            "Thisideaisverypowerfulandhasbeenwidelyused. Letâ€™slookatsomeoftheresults\n",
            "fromapaper19 whichappliedseveralvariationsoftheideatoMNIST.Oneoftheneural\n",
            "networkarchitecturestheyconsideredwasalongsimilarlinestowhatweâ€™vebeenusing,a\n",
            "feedforwardnetworkwith800hiddenneuronsandusingthecross-entropycostfunction. RunningthenetworkwiththestandardMNISTtrainingdatatheyachievedaclassification\n",
            "accuracyof98.4percentontheirtestset. Butthentheyexpandedthetrainingdata,using\n",
            "notjustrotations,asIdescribedabove,butalsotranslatingandskewingtheimages.\n",
            "By\n",
            "trainingontheexpandeddatasettheyincreasedtheirnetworkâ€™saccuracyto98.9percent. Theyalsoexperimentedwithwhattheycalledâ€œelasticdistortionsâ€,aspecialtypeofimage\n",
            "distortionintendedtoemulatetherandomoscillationsfoundinhandmuscles. Byusingthe\n",
            "elasticdistortionstoexpandthedatatheyachievedanevenhigheraccuracy,99.3percent. Effectively,theywerebroadeningtheexperienceoftheirnetworkbyexposingittothesort\n",
            "ofvariationsthatarefoundinrealhandwriting. Variationsonthisideacanbeusedtoimproveperformanceonmanylearningtasks,not\n",
            "justhandwritingrecognition. Thegeneralprincipleistoexpandthetrainingdatabyapplying\n",
            "operationsthatreflectreal-worldvariation. Itâ€™snotdifficulttothinkofwaysofdoingthis. Suppose,forexample,thatyouâ€™rebuildinganeuralnetworktodospeechrecognition. We\n",
            "humanscanrecognizespeecheveninthepresenceofdistortionssuchasbackgroundnoise. Andsoyoucanexpandyourdatabyaddingbackgroundnoise.\n",
            "Wecanalsorecognizespeech\n",
            "ifitâ€™sspeduporsloweddown. Sothatâ€™sanotherwaywecanexpandthetrainingdata. These\n",
            "techniquesarenotalwaysusedâ€“forinstance,insteadofexpandingthetrainingdataby\n",
            "addingnoise,itmaywellbemoreefficienttocleanuptheinputtothenetworkbyfirst\n",
            "applyinganoisereductionfilter. Still,itâ€™sworthkeepingtheideaofexpandingthetraining\n",
            "datainmind,andlookingforopportunitiestoapplyit. Exercise\n",
            "Asdiscussedabove,onewayofexpandingtheMNISTtrainingdataistousesmall\n",
            "â€¢ rotationsoftrainingimages. Whatâ€™saproblemthatmightoccurifweallowarbitrarily\n",
            "largerotationsoftrainingimages? Anasideonbigdataandwhatitmeanstocompareclassificationaccuracies: Letâ€™slook\n",
            "againathowourneuralnetworkâ€™saccuracyvarieswithtrainingsetsize:\n",
            "19BestPracticesforConvolutionalNeuralNetworksAppliedtoVisualDocumentAnalysisbyPatrice\n",
            "Simard,DaveSteinkraus,andJohnPlatt(2003). \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 93\n",
            "(cid:12)\n",
            "3\n",
            "Supposethatinsteadofusinganeuralnetworkweusesomeothermachinelearningtechnique\n",
            "toclassifydigits. Forinstance,letâ€™stryusingthesupportvectormachines(SVM)whichwe\n",
            "metbrieflybackinChapter1. AswasthecaseinChapter1,donâ€™tworryifyouâ€™renotfamiliar\n",
            "withSVMs,wedonâ€™tneedtounderstandtheirdetails.\n",
            "Instead,weâ€™llusetheSVMsupplied\n",
            "bythescikit-learnlibrary. Hereâ€™showSVMperformancevariesasafunctionoftrainingset\n",
            "size. Iâ€™veplottedtheneuralnetresultsaswell,tomakecomparisoneasy20:\n",
            "Probablythefirstthingthatstrikesyouaboutthisgraphisthatourneuralnetworkoutper-\n",
            "formstheSVMforeverytrainingsetsize. Thatâ€™snice,althoughyoushouldnâ€™treadtoomuch\n",
            "intoit,sinceIjustusedtheout-of-the-boxsettingsfromscikit-learnâ€™sSVM,whileweâ€™vedone\n",
            "afairbitofworkimprovingourneuralnetwork. Amoresubtlebutmoreinterestingfact\n",
            "aboutthegraphisthatifwetrainourSVMusing50,000imagesthenitactuallyhasbetter\n",
            "performance(94.48percentaccuracy)thanourneuralnetworkdoeswhentrainedusing\n",
            "5,000images(93.24percentaccuracy). Inotherwords,moretrainingdatacansometimes\n",
            "compensatefordifferencesinthemachinelearningalgorithmused.\n",
            "Somethingevenmoreinterestingcanoccur. Supposeweâ€™retryingtosolveaproblem\n",
            "usingtwomachinelearningalgorithms,algorithmAandalgorithmB.Itsometimeshappens\n",
            "thatalgorithmAwilloutperformalgorithmBwithonesetoftrainingdata,whilealgorithm\n",
            "20Thisgraphwasproducedwiththeprogrammore_data.py(aswerethelastfewgraphs). \n",
            "(cid:12)\n",
            "94 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "BwilloutperformalgorithmAwithadifferentsetoftrainingdata. Wedonâ€™tseethataboveâ€“\n",
            "itwouldrequirethetwographstocrossâ€“butitdoeshappen21. Thecorrectresponsetothe\n",
            "questionâ€œIsalgorithmAbetterthanalgorithmB?â€isreally: â€œWhattrainingdatasetareyou\n",
            "using?â€\n",
            "Allthisisacautiontokeepinmind,bothwhendoingdevelopment,andwhenreading\n",
            "researchpapers. Manypapersfocusonfindingnewtrickstowringoutimprovedperformance\n",
            "3 onstandardbenchmarkdatasets. â€œOurwhiz-bangtechniquegaveusanimprovementofX\n",
            "percentonstandardbenchmarkYâ€isacanonicalformofresearchclaim. Suchclaimsare\n",
            "oftengenuinelyinteresting,buttheymustbeunderstoodasapplyingonlyinthecontextof\n",
            "thespecifictrainingdatasetused. Imagineanalternatehistoryinwhichthepeoplewho\n",
            "originallycreatedthebenchmarkdatasethadalargerresearchgrant. Theymighthaveused\n",
            "theextramoneytocollectmoretrainingdata. Itâ€™sentirelypossiblethattheâ€œimprovementâ€\n",
            "duetothewhiz-bangtechniquewoulddisappearonalargerdataset. Inotherwords,the\n",
            "purportedimprovementmightbejustanaccidentofhistory. Themessagetotakeaway,\n",
            "especiallyinpracticalapplications,isthatwhatwewantisbothbetteralgorithmsandbetter\n",
            "trainingdata. Itâ€™sfinetolookforbetteralgorithms,butmakesureyouâ€™renotfocusingon\n",
            "betteralgorithmstotheexclusionofeasywinsgettingmoreorbettertrainingdata. Problem\n",
            "(Researchproblem)Howdoourmachinelearningalgorithmsperforminthelimitof\n",
            "â€¢ verylargedatasets? Foranygivenalgorithmitâ€™snaturaltoattempttodefineanotion\n",
            "ofasymptoticperformanceinthelimitoftrulybigdata. Aquick-and-dirtyapproach\n",
            "tothisproblemistosimplytryfittingcurvestographslikethoseshownabove,and\n",
            "thentoextrapolatethefittedcurvesouttoinfinity. Anobjectiontothisapproach\n",
            "isthatdifferentapproachestocurvefittingwillgivedifferentnotionsofasymptotic\n",
            "performance. Canyoufindaprincipledjustificationforfittingtosomeparticularclass\n",
            "ofcurves? Ifso,comparetheasymptoticperformanceofseveraldifferentmachine\n",
            "learningalgorithms. Summingup: Weâ€™venowcompletedourdiveintooverfittingandregularization.\n",
            "Ofcourse,\n",
            "weâ€™llreturnagaintotheissue. AsIâ€™vementionedseveraltimes,overfittingisamajorproblem\n",
            "inneuralnetworks,especiallyascomputersgetmorepowerful,andwehavetheabilityto\n",
            "trainlargernetworks. Asaresultthereâ€™sapressingneedtodeveloppowerfulregularization\n",
            "techniquestoreduceoverfitting,andthisisanextremelyactiveareaofcurrentwork. 3.3 Weight initialization\n",
            "Whenwecreateourneuralnetworks,wehavetomakechoicesfortheinitialweightsand\n",
            "biases. Uptonow,weâ€™vebeenchoosingthemaccordingtoaprescriptionwhichIdiscussed\n",
            "onlybrieflybackinChapter1. Justtoremindyou,thatprescriptionwastochooseboththe\n",
            "weightsandbiasesusingindependentGaussianrandomvariables,normalizedtohavemean\n",
            "0andstandarddeviation1. Whilethisapproachhasworkedwell,itwasquiteadhoc,and\n",
            "itâ€™sworthrevisitingtoseeifwecanfindabetterwayofsettingourinitialweightsandbiases,\n",
            "andperhapshelpourneuralnetworkslearnfaster.\n",
            "ItturnsoutthatwecandoquiteabitbetterthaninitializingwithnormalizedGaussians. Toseewhy,supposeweâ€™reworkingwithanetworkwithalargenumberâ€“say1,000â€“of\n",
            "inputneurons. Andletâ€™ssupposeweâ€™veusednormalizedGaussianstoinitializetheweights\n",
            "21StrikingexamplesmaybefoundinScalingtoveryverylargecorporafornaturallanguagedisam-\n",
            "biguation,byMicheleBankoandEricBrill(2001).\n",
            "\n",
            "(cid:12)\n",
            "3.3.\n",
            "Weightinitialization (cid:12) 95\n",
            "(cid:12)\n",
            "connectingtothefirsthiddenlayer. FornowIâ€™mgoingtoconcentratespecificallyonthe\n",
            "weightsconnectingtheinputneuronstothefirstneuroninthehiddenlayer,andignorethe\n",
            "restofthenetwork:\n",
            "3\n",
            "Weâ€™llsupposeforsimplicitythatweâ€™retryingtotrainusingatraininginput x inwhichhalf\n",
            "theinputneuronsareon,i.e.,setto1,andhalftheinputneuronsareoff,i.e.,setto0. The\n",
            "argumentwhichfollowsappliesmoregenerally,butyouâ€™llgetthegistfromthisspecialcase. (cid:80)\n",
            "Letâ€™sconsidertheweightedsumz=\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bofinputstoourhiddenneuron. 500terms\n",
            "inthissumvanish,becausethecorrespondinginputx iszero.\n",
            "Andsozisasumoveratotal\n",
            "j\n",
            "of501normalizedGaussianrandomvariables,accountingforthe500weighttermsandthe\n",
            "1extrabiasterm. Thusz isitselfdistributedasaGaussianwithmeanzeroandstandard\n",
            "deviation(cid:112)501 22.4. Thatis,zhasaverybroadGaussiandistribution,notsharplypeaked\n",
            "atall: â‰ˆ\n",
            "0.02\n",
            "0.01\n",
            "30 20 10 10 20 30\n",
            "âˆ’ âˆ’ âˆ’\n",
            "Inparticular,wecanseefromthisgraphthatitâ€™squitelikelythat z willbeprettylarge,\n",
            "i.e.,eitherz 1orz 1. Ifthatâ€™sthecasethentheoutputÏƒ (z)fr|o|mthehiddenneuron\n",
            "will be very(cid:29)close to(cid:28)eitâˆ’her 1 or 0. That means our hidden neuron will have saturated. Andwhenthathappens,asweknow,makingsmallchangesintheweightswillmakeonly\n",
            "absolutelyminisculechangesintheactivationofourhiddenneuron. Thatminisculechange\n",
            "intheactivationofthehiddenneuronwill,inturn,barelyaffecttherestoftheneuronsin\n",
            "thenetworkatall,andweâ€™llseeacorrespondinglyminisculechangeinthecostfunction. Asaresult, thoseweightswillonlylearnveryslowlywhenweusethegradientdescent\n",
            "algorithm22. Itâ€™ssimilartotheproblemwediscussedearlierinthischapter,inwhichoutput\n",
            "neuronswhichsaturatedonthewrongvaluecausedlearningtoslowdown. Weaddressed\n",
            "22WediscussedthisinmoredetailinChapter2,whereweusedtheequationsofbackpropagationto\n",
            "showthatweightsinputtosaturatedneuronslearnedslowly. \n",
            "(cid:12)\n",
            "96 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thatearlierproblemwithacleverchoiceofcostfunction. Unfortunately,whilethathelped\n",
            "withsaturatedoutputneurons,itdoesnothingatallfortheproblemwithsaturatedhidden\n",
            "neurons. Iâ€™vebeentalkingabouttheweightsinputtothefirsthiddenlayer. Ofcourse,similar\n",
            "argumentsapplyalsotolaterhiddenlayers:iftheweightsinlaterhiddenlayersareinitialized\n",
            "usingnormalizedGaussians,thenactivationswilloftenbeverycloseto0or1,andlearning\n",
            "3 willproceedveryslowly. Istheresomewaywecanchoosebetterinitializationsfortheweightsandbiases,so\n",
            "thatwedonâ€™tgetthiskindofsaturation,andsoavoidalearningslowdown? Supposewe\n",
            "haveaneuronwithn inputweights. ThenweshallinitializethoseweightsasGaussian\n",
            "in\n",
            "randomvariableswithmean0andstandarddeviation1/ n . Thatis,weâ€™llsquashthe\n",
            "(cid:112) in\n",
            "Gaussiansdown,makingitlesslikelythatourneuronwillsaturate. Weâ€™llcontinuetochoose\n",
            "thebiasasaGaussianwithmean0andstandarddeviation1,forreasonsIâ€™llreturntoina\n",
            "(cid:80)\n",
            "moment. Withthesechoices,theweightedsumz=\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bwillagainbeaGaussian\n",
            "randomvariablewithmean0,butitâ€™llbemuchmoresharplypeakedthanitwasbefore. Suppose,aswedidearlier,that500oftheinputsarezeroand500are1.\n",
            "Thenitâ€™seasyto\n",
            "show(seetheexercisebelow)thatzhasaGaussiandistributionwithmean0andstandard\n",
            "deviation (cid:112) 3/2=1.22.... Thisismuchmoresharplypeakedthanbefore,somuchsothat\n",
            "eventhegraphbelowunderstatesthesituation,sinceIâ€™vehadtorescaletheverticalaxis,\n",
            "whencomparedtotheearliergraph:\n",
            "0.4\n",
            "30 20 10 10 20 30\n",
            "âˆ’ âˆ’ âˆ’\n",
            "Suchaneuronismuchlesslikelytosaturate,andcorrespondinglymuchlesslikelytohave\n",
            "problemswithalearningslowdown. Exercise\n",
            "Verifythatthestandarddeviationofz= (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bintheparagraphaboveis (cid:112) 3/2.\n",
            "â€¢ Itmayhelptoknowthat: (a)thevarianceofasumofindependentrandomvariables\n",
            "isthesumofthevariancesoftheindividualrandomvariables;and(b)thevarianceis\n",
            "thesquareofthestandarddeviation. I stated above that weâ€™ll continue to initialize the biases as before, as Gaussian random\n",
            "variableswithameanof0andastandarddeviationof1. Thisisokay,becauseitdoesnâ€™t\n",
            "makeittoomuchmorelikelythatourneuronswillsaturate.\n",
            "Infact,itdoesnâ€™tmuchmatter\n",
            "howweinitializethebiases,providedweavoidtheproblemwithsaturation. Somepeoplego\n",
            "sofarastoinitializeallthebiasesto0,andrelyongradientdescenttolearnappropriatebiases. Butsinceitâ€™sunlikelytomakemuchdifference,weâ€™llcontinuewiththesameinitialization\n",
            "procedureasbefore. Letâ€™scomparetheresultsforbothouroldandnewapproachestoweightinitialization,\n",
            "usingtheMNISTdigitclassificationtask. Asbefore,weâ€™lluse30hiddenneurons,amini-batch\n",
            "sizeof10,aregularizationparameterÎ» =5.0,andthecross-entropycostfunction. Wewill\n",
            "decreasethelearningrateslightlyfromÎ· =0.5to0.1,sincethatmakestheresultsalittle\n",
            "moreeasilyvisibleinthegraphs. Wecantrainusingtheoldmethodofweightinitialization:\n",
            "\n",
            "(cid:12)\n",
            "3.3. Weightinitialization (cid:12) 97\n",
            "(cid:12)\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True) 3\n",
            "Wecanalsotrainusingthenewapproachtoweightinitialization. Thisisactuallyeveneasier,\n",
            "sincenetwork2â€™sdefaultwayofinitializingtheweightsisusingthisnewapproach. That\n",
            "meanswecanomitthenet.large_weight_initializer()callabove:\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Plottingtheresults23,weobtain:\n",
            "Inbothcases,weendupwithaclassificationaccuracysomewhatover96percent.\n",
            "Thefinal\n",
            "classificationaccuracyisalmostexactlythesameinthetwocases. Butthenewinitialization\n",
            "techniquebringsustheremuch,muchfaster. Attheendofthefirstepochoftrainingthe\n",
            "oldapproachtoweightinitializationhasaclassificationaccuracyunder87percent,while\n",
            "thenewapproachisalreadyalmost93percent. Whatappearstobegoingonisthatour\n",
            "newapproachtoweightinitializationstartsusoffinamuchbetterregime,whichletsusget\n",
            "goodresultsmuchmorequickly. Thesamephenomenonisalsoseenifweplotresultswith\n",
            "100hiddenneurons:\n",
            "23Theprogramusedtogeneratethisandthenextgraphisweight_initialization.py. \n",
            "(cid:12)\n",
            "98 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "3\n",
            "Inthiscase,thetwocurvesdonâ€™tquitemeet. However,myexperimentssuggestthatwithjust\n",
            "afewmoreepochsoftraining(notshown)theaccuraciesbecomealmostexactlythesame. Soonthebasisoftheseexperimentsitlooksasthoughtheimprovedweightinitialization\n",
            "onlyspeedsuplearning,itdoesnâ€™tchangethefinalperformanceofournetworks. However,in\n",
            "Chapter4weâ€™llseeexamplesofneuralnetworkswherethelong-runbehaviourissignificantly\n",
            "betterwiththe1/ n weightinitialization. Thusitâ€™snotonlythespeedoflearningwhichis\n",
            "(cid:112) in\n",
            "improved,itâ€™ssometimesalsothefinalperformance. The1/ n approachtoweightinitializationhelpsimprovethewayourneuralnets\n",
            "(cid:112) in\n",
            "learn. Othertechniquesforweightinitializationhavealsobeenproposed,manybuildingon\n",
            "thisbasicidea. Iwonâ€™treviewtheotherapproacheshere,since1/ n workswellenoughfor\n",
            "(cid:112) in\n",
            "ourpurposes.\n",
            "Ifyouâ€™reinterestedinlookingfurther,Irecommendlookingatthediscussion\n",
            "onpages14and15ofa2012paperbyYoshuaBengio24,aswellasthereferencestherein. Problem\n",
            "ConnectingregularizationandtheimprovedmethodofweightinitializationL2\n",
            "â€¢ regularizationsometimesautomaticallygivesussomethingsimilartothenewap-\n",
            "proachtoweightinitialization. Supposeweareusingtheoldapproachtoweight\n",
            "initialization. Sketchaheuristicargumentthat: (1)supposingÎ»isnottoosmall,\n",
            "thefirstepochsoftrainingwillbedominatedalmostentirelybyweightdecay;(2)\n",
            "providedÎ·Î» ntheweightswilldecaybyafactorofexp( Î·Î»/m)perepoch;and\n",
            "(3)supposing(cid:28)Î»isnottoolarge,theweightdecaywilltailoâˆ’ffwhentheweightsare\n",
            "downtoasizearound1/ n ,wherenisthetotalnumberofweightsinthenetwork. (cid:112) in\n",
            "Arguethattheseconditionsareallsatisfiedintheexamplesgraphedinthissection. 3.4 Handwriting recognition revisited: the code\n",
            "Letâ€™simplementtheideasweâ€™vediscussedinthischapter. Weâ€™lldevelopanewprogram,\n",
            "network2.py,whichisanimprovedversionoftheprogramnetwork.pywedevelopedin\n",
            "Chapter1.\n",
            "Ifyouhavenâ€™tlookedatnetwork.pyinawhilethenyoumayfindithelpfulto\n",
            "spendafewminutesquicklyreadingovertheearlierdiscussion. Itâ€™sonly74linesofcode,\n",
            "andiseasilyunderstood. 24PracticalRecommendationsforGradient-BasedTrainingofDeepArchitectures,byYoshuaBengio\n",
            "(2012). \n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 99\n",
            "(cid:12)\n",
            "Aswasthecaseinnetwork.py,thestarofnetwork2.pyistheNetworkclass,which\n",
            "weusetorepresentourneuralnetworks. WeinitializeaninstanceofNetworkwithalistof\n",
            "sizesfortherespectivelayersinthenetwork,andachoiceforthecosttouse,defaultingto\n",
            "thecross-entropy:\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "self.num_layers = len(sizes) 3\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "Thefirstcoupleoflinesofthe__init__methodarethesameasinnetwork.py,andare\n",
            "prettyself-explanatory. Butthenexttwolinesarenew,andweneedtounderstandwhat\n",
            "theyâ€™redoingindetail.\n",
            "Letâ€™sstartbyexaminingthedefault_weight_initializermethod. Thismakesuse\n",
            "ofournewandimprovedapproachtoweightinitialization. Asweâ€™veseen,inthatapproach\n",
            "theweightsinputtoaneuronareinitializedasGaussianrandomvariableswithmean0and\n",
            "standarddeviation1dividedbythesquarerootofthenumberofconnectionsinputtothe\n",
            "neuron. Alsointhismethodweâ€™llinitializethebiases,usingGaussianrandomvariableswith\n",
            "mean0andstandarddeviation1. Hereâ€™sthecode:\n",
            "def default_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "Tounderstandthecode,itmayhelptorecallthatnpistheNumpylibraryfordoinglinear\n",
            "algebra. Weâ€™llimportNumpyatthebeginningofourprogram. Also,noticethatwedonâ€™t\n",
            "initialize any biases for the first layer of neurons. We avoid doing this because the first\n",
            "layerisaninputlayer,andsoanybiaseswouldnotbeused. Wedidexactlythesamething\n",
            "innetwork.py. Complementingthedefault_weight_initializerweâ€™llalsoincludea\n",
            "large_weight_initializermethod. Thismethodinitializestheweightsandbiasesusing\n",
            "the old approach from Chapter 1, with both weights and biases initialized as Gaussian\n",
            "randomvariableswithmean0andstandarddeviation1. Thecodeis,ofcourse,onlyatiny\n",
            "bitdifferentfromthedefault_weight_initializer:\n",
            "def large_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self. sizes[1:])]\n",
            "Iâ€™veincludedthelarge_weight_initializermethodmostlyasaconveniencetomake\n",
            "iteasiertocomparetheresultsinthischaptertothoseinChapter1. Icanâ€™tthinkofmany\n",
            "practicalsituationswhereIwouldrecommendusingit!\n",
            "ThesecondnewthinginNetworkâ€™s__init__methodisthatwenowinitializeacost\n",
            "attribute. Tounderstandhowthatworks, letâ€™slookattheclassweusetorepresentthe\n",
            "cross-entropycost25:\n",
            "class CrossEntropyCost(object):\n",
            "25Ifyouâ€™renotfamiliarwithPythonâ€™sstaticmethodsyoucanignorethe@staticmethoddecorators,\n",
            "andjusttreatfnanddeltaasordinarymethods.Ifyouâ€™recuriousaboutdetails,all@staticmethod\n",
            "doesistellthePythoninterpreterthatthemethodwhichfollowsdoesnâ€™tdependontheobjectinany\n",
            "way.Thatâ€™swhyselfisnâ€™tpassedasaparametertothefnanddeltamethods. \n",
            "(cid:12)\n",
            "100 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y)\n",
            "Letâ€™sbreakthisdown. Thefirstthingtoobserveisthateventhoughthecross-entropyis,\n",
            "3\n",
            "mathematicallyspeaking,afunction,weâ€™veimplementeditasaPythonclass,notaPython\n",
            "function. WhyhaveImadethatchoice? Thereasonisthatthecostplaystwodifferentroles\n",
            "inournetwork. Theobviousroleisthatitâ€™sameasureofhowwellanoutputactivation,a,\n",
            "matchesthedesiredoutput,y. ThisroleiscapturedbytheCrossEntropyCost.fnmethod. (Note,bytheway,thatthenp.nan_to_numcallinsideCrossEntropyCost.fnensuresthat\n",
            "Numpydealscorrectlywiththelogofnumbersveryclosetozero.) Butthereâ€™salsoasecond\n",
            "waythecostfunctionentersournetwork. RecallfromChapter2thatwhenrunningthe\n",
            "backpropagationalgorithmweneedtocomputethenetworkâ€™soutputerror,Î´L. Theformof\n",
            "theoutputerrordependsonthechoiceofcostfunction: differentcostfunction,different\n",
            "formfortheoutputerror. Forthecross-entropytheoutputerroris,aswesawinEquation\n",
            "(3.12),\n",
            "Î´L =aL y. (3.44)\n",
            "âˆ’\n",
            "Forthisreasonwedefineasecondmethod,CrossEntropyCost.delta,whosepurposeisto\n",
            "tellournetworkhowtocomputetheoutputerror. Andthenwebundlethesetwomethodsup\n",
            "intoasingleclasscontainingeverythingournetworksneedtoknowaboutthecostfunction. In a similar way, network2.py also contains a class to represent the quadratic cost\n",
            "function. ThisisincludedforcomparisonwiththeresultsofChapter1,sincegoingforward\n",
            "weâ€™llmostlyusethecrossentropy.\n",
            "Thecodeisjustbelow. TheQuadraticCost.fnmethod\n",
            "isastraightforwardcomputationofthequadraticcostassociatedtotheactualoutput,a,\n",
            "andthedesiredoutput,y. ThevaluereturnedbyQuadraticCost.deltaisbasedonthe\n",
            "expression(2.8)fortheoutputerrorforthequadraticcost,whichwederivedbackinChapter\n",
            "2. class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "Weâ€™venowunderstoodthemaindifferencesbetweennetwork2.pyandnetwork.py. Itâ€™s\n",
            "all pretty simple stuff.\n",
            "There are a number of smaller changes, which Iâ€™ll discuss below,\n",
            "includingtheimplementationofL2regularization. Beforegettingtothat,letâ€™slookatthe\n",
            "completecodefornetwork2.py. Youdonâ€™tneedtoreadallthecodeindetail,butitisworth\n",
            "understandingthebroadstructure,andinparticularreadingthedocumentationstrings,so\n",
            "youunderstandwhateachpieceoftheprogramisdoing. Ofcourse,youâ€™realsowelcometo\n",
            "delveasdeeplyasyouwish!\n",
            "Ifyougetlost,youmaywishtocontinuereadingtheprose\n",
            "below,andreturntothecodelater. Anyway,hereâ€™sthecode:\n",
            "\"\"\"network2.py\n",
            "~~~~~~~~~~~~~~\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 101\n",
            "(cid:12)\n",
            "An improved version of network.py, implementing the stochastic\n",
            "gradient descent learning algorithm for a feedforward neural network. Improvements include the addition of the cross-entropy cost function,\n",
            "regularization, and better initialization of network weights.\n",
            "Note\n",
            "that I have focused on making the code simple, easily readable, and\n",
            "easily modifiable.\n",
            "It is not optimized, and omits many desirable\n",
            "features. \"\"\"\n",
            "#### Libraries 3\n",
            "# Standard library\n",
            "import json\n",
            "import random\n",
            "import sys\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "#### Define the quadratic and cross-entropy cost functions\n",
            "class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output â€˜â€˜aâ€˜â€˜ and desired output â€˜â€˜yâ€˜â€˜. \"\"\"\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer.\"\"\"\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "class CrossEntropyCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output â€˜â€˜aâ€˜â€˜ and desired output\n",
            "â€˜â€˜yâ€˜â€˜. Note that np.nan_to_num is used to ensure numerical\n",
            "stability. In particular, if both â€˜â€˜aâ€˜â€˜ and â€˜â€˜yâ€˜â€˜ have a 1.0\n",
            "in the same slot, then the expression (1-y)*np.log(1-a)\n",
            "returns nan. The np.nan_to_num ensures that that is converted\n",
            "to the correct value (0.0). \"\"\"\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer. Note that the\n",
            "parameter â€˜â€˜zâ€˜â€˜ is not used by the method. It is included in\n",
            "the methodâ€™s parameters in order to make the interface\n",
            "consistent with the delta method for other cost classes. \"\"\"\n",
            "return (a-y)\n",
            "#### Main Network class\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "\"\"\"The list â€˜â€˜sizesâ€˜â€˜ contains the number of neurons in the respective\n",
            "\n",
            "(cid:12)\n",
            "102 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "layers of the network. For example, if the list was [2, 3, 1]\n",
            "then it would be a three-layer network, with the first layer\n",
            "containing 2 neurons, the second layer 3 neurons, and the\n",
            "third layer 1 neuron. The biases and weights for the network\n",
            "are initialized randomly, using\n",
            "â€˜â€˜self.default_weight_initializerâ€˜â€˜ (see docstring for that\n",
            "method). 3 \"\"\"\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "def default_weight_initializer(self):\n",
            "\"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
            "and standard deviation 1 over the square root of the number of\n",
            "weights connecting to the same neuron. Initialize the biases\n",
            "using a Gaussian distribution with mean 0 and standard\n",
            "deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we wonâ€™t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "def large_weight_initializer(self):\n",
            "\"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
            "and standard deviation 1. Initialize the biases using a\n",
            "Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we wonâ€™t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. This weight and bias initializer uses the same approach as in\n",
            "Chapter 1, and is included for purposes of comparison.\n",
            "It\n",
            "will usually be better to use the default weight initializer\n",
            "instead. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)\n",
            "for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if â€˜â€˜aâ€˜â€˜ is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "return a\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
            "evaluation_data=None, monitor_evaluation_cost=False,\n",
            "monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
            "monitor_training_accuracy=False):\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 103\n",
            "(cid:12)\n",
            "\"\"\"Train the neural network using mini-batch stochastic gradient\n",
            "descent. The â€˜â€˜training_dataâ€˜â€˜ is a list of tuples â€˜â€˜(x, y)â€˜â€˜\n",
            "representing the training inputs and the desired outputs.\n",
            "The\n",
            "other non-optional parameters are self-explanatory, as is the\n",
            "regularization parameter â€˜â€˜lmbdaâ€˜â€˜. The method also accepts\n",
            "â€˜â€˜evaluation_dataâ€˜â€˜, usually either the validation or test\n",
            "data. We can monitor the cost and accuracy on either the\n",
            "evaluation data or the training data, by setting the\n",
            "appropriate flags. The method returns a tuple containing four 3\n",
            "lists: the (per-epoch) costs on the evaluation data, the\n",
            "accuracies on the evaluation data, the costs on the training\n",
            "data, and the accuracies on the training data. All values are\n",
            "evaluated at the end of each training epoch. So, for example,\n",
            "if we train for 30 epochs, then the first element of the tuple\n",
            "will be a 30-element list containing the cost on the\n",
            "evaluation data at the end of each epoch. Note that the lists\n",
            "are empty if the corresponding flag is not set. \"\"\"\n",
            "if evaluation_data:\n",
            "n_data = len(evaluation_data)\n",
            "n = len(training_data)\n",
            "evaluation_cost, evaluation_accuracy = [], []\n",
            "training_cost, training_accuracy = [], []\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [\n",
            "training_data[k:k+mini_batch_size]\n",
            "for k in xrange(0, n, mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(\n",
            "mini_batch, eta, lmbda, len(training_data))\n",
            "print \"Epoch %s training complete\" % j\n",
            "if monitor_training_cost:\n",
            "cost = self.total_cost(training_data, lmbda)\n",
            "training_cost.append(cost)\n",
            "print \"Cost on training data: {}\".format(cost)\n",
            "if monitor_training_accuracy:\n",
            "accuracy = self.accuracy(training_data, convert=True)\n",
            "training_accuracy.append(accuracy)\n",
            "print \"Accuracy on training data: {} / {}\".format(accuracy, n)\n",
            "if monitor_evaluation_cost:\n",
            "cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
            "evaluation_cost.append(cost)\n",
            "print \"Cost on evaluation data: {}\".format(cost)\n",
            "if monitor_evaluation_accuracy:\n",
            "accuracy = self.accuracy(evaluation_data)\n",
            "evaluation_accuracy.append(accuracy)\n",
            "print \"Accuracy on evaluation data: {} / {}\".format(self.accuracy(\n",
            "evaluation_data), n_data)\n",
            "print\n",
            "return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
            "def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
            "\"\"\"Update the networkâ€™s weights and biases by applying gradient\n",
            "descent using backpropagation to a single mini batch. The\n",
            "â€˜â€˜mini_batchâ€˜â€˜ is a list of tuples â€˜â€˜(x, y)â€˜â€˜, â€˜â€˜etaâ€˜â€˜ is the\n",
            "learning rate, â€˜â€˜lmbdaâ€˜â€˜ is the regularization parameter, and\n",
            "â€˜â€˜nâ€˜â€˜ is the total size of the training data set.\n",
            "\"\"\"\n",
            "\n",
            "(cid:12)\n",
            "104 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "3 self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "def backprop(self, x, y):\n",
            "\"\"\"Return a tuple â€˜â€˜(nabla_b, nabla_w)â€˜â€˜ representing the\n",
            "gradient for the cost function C_x. â€˜â€˜nabla_bâ€˜â€˜ and\n",
            "â€˜â€˜nabla_wâ€˜â€˜ are layer-by-layer lists of numpy arrays, similar\n",
            "to â€˜â€˜self.biasesâ€˜â€˜ and â€˜â€˜self.weightsâ€˜â€˜.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "zs.append(z)\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. Itâ€™s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "def accuracy(self, data, convert=False):\n",
            "\"\"\"Return the number of inputs in â€˜â€˜dataâ€˜â€˜ for which the neural\n",
            "network outputs the correct result. The neural networkâ€™s\n",
            "output is assumed to be the index of whichever neuron in the\n",
            "final layer has the highest activation.\n",
            "The flag â€˜â€˜convertâ€˜â€˜ should be set to False if the data set is\n",
            "validation or test data (the usual case), and to True if the\n",
            "data set is the training data. The need for this flag arises\n",
            "due to differences in the way the results â€˜â€˜yâ€˜â€˜ are\n",
            "represented in the different data sets. In particular, it\n",
            "flags whether we need to convert between the different\n",
            "representations. It may seem strange to use different\n",
            "representations for the different data sets. Why not use the\n",
            "same representation for all three data sets? Itâ€™s done for\n",
            "efficiency reasons -- the program usually evaluates the cost\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 105\n",
            "(cid:12)\n",
            "on the training data and the accuracy on other data sets. These are different types of computations, and using different\n",
            "representations speeds things up. More details on the\n",
            "representations can be found in\n",
            "mnist_loader.load_data_wrapper. \"\"\"\n",
            "if convert:\n",
            "results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in 3\n",
            "data]\n",
            "else:\n",
            "results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
            "return sum(int(x == y) for (x, y) in results)\n",
            "def total_cost(self, data, lmbda, convert=False):\n",
            "\"\"\"Return the total cost for the data set â€˜â€˜dataâ€˜â€˜. The flag\n",
            "â€˜â€˜convertâ€˜â€˜ should be set to False if the data set is the\n",
            "training data (the usual case), and to True if the data set is\n",
            "the validation or test data. See comments on the similar (but\n",
            "reversed) convention for the â€˜â€˜accuracyâ€˜â€˜ method, above. \"\"\"\n",
            "cost = 0.0\n",
            "for x, y in data:\n",
            "a = self.feedforward(x)\n",
            "if convert:\n",
            "y = vectorized_result(y)\n",
            "cost += self.cost.fn(a, y)/len(data)\n",
            "cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
            "return cost\n",
            "def save(self, filename):\n",
            "\"\"\"Save the neural network to the file â€˜â€˜filenameâ€˜â€˜.\"\"\"\n",
            "data = {\"sizes\": self.sizes,\n",
            "\"weights\": [w.tolist() for w in self.weights],\n",
            "\"biases\": [b.tolist() for b in self.biases],\n",
            "\"cost\": str(self.cost.__name__)}\n",
            "f = open(filename, \"w\")\n",
            "json.dump(data, f)\n",
            "f.close()\n",
            "#### Loading a Network\n",
            "def load(filename):\n",
            "\"\"\"Load a neural network from the file â€˜â€˜filenameâ€˜â€˜. Returns an\n",
            "instance of Network. \"\"\"\n",
            "f = open(filename, \"r\")\n",
            "data = json.load(f)\n",
            "f.close()\n",
            "cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
            "net = Network(data[\"sizes\"], cost=cost)\n",
            "net.weights = [np.array(w) for w in data[\"weights\"]]\n",
            "net.biases = [np.array(b) for b in data[\"biases\"]]\n",
            "return net\n",
            "#### Miscellaneous functions\n",
            "def vectorized_result(j):\n",
            "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jâ€™th position\n",
            "and zeroes elsewhere. This is used to convert a digit (0...9)\n",
            "into a corresponding desired output from the neural network.\n",
            "\n",
            "(cid:12)\n",
            "106 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "\"\"\"\n",
            "e = np.zeros((10, 1))\n",
            "e[j] = 1.0\n",
            "return e\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "3\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "OneofthemoreinterestingchangesinthecodeistoincludeL2regularization. Although\n",
            "thisisamajorconceptualchange,itâ€™ssotrivialtoimplementthatitâ€™seasytomissinthe\n",
            "code.\n",
            "Forthemostpartitjustinvolvespassingtheparameterlmbdatovariousmethods,\n",
            "notablytheNetwork.SGDmethod. Therealworkisdoneinasinglelineoftheprogram,the\n",
            "fourth-lastlineoftheNetwork.update_mini_batchmethod. Thatâ€™swherewemodifythe\n",
            "gradientdescentupdateruletoincludeweightdecay. Butalthoughthemodificationistiny,\n",
            "ithasabigimpactonresults!\n",
            "Thisis,bytheway,commonwhenimplementingnewtechniquesinneuralnetworks. Weâ€™vespentthousandsofwordsdiscussingregularization. Itâ€™sconceptuallyquitesubtleand\n",
            "difficulttounderstand. Andyetitwastrivialtoaddtoourprogram! Itoccurssurprisingly\n",
            "oftenthatsophisticatedtechniquescanbeimplementedwithsmallchangestocode. Anothersmallbutimportantchangetoourcodeistheadditionofseveraloptionalflags\n",
            "tothestochasticgradientdescentmethod,Network.SGD.Theseflagsmakeitpossibleto\n",
            "monitorthecostandaccuracyeitheronthetraining_dataoronasetofevaluation_data\n",
            "whichcanbepassedtoNetwork.SGD. Weâ€™veusedtheseflagsoftenearlierinthechapter,\n",
            "butletmegiveanexampleofhowitworks,justtoremindyou:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True, monitor_evaluation_cost=True,\n",
            "monitor_training_accuracy=True,\n",
            "... monitor_training_cost=True)\n",
            "Here,weâ€™resettingtheevaluation_datatobethevalidation_data. Butwecouldalso\n",
            "havemonitoredperformanceonthetest_dataoranyotherdataset. Wealsohavefour\n",
            "flagstellingustomonitorthecostandaccuracyonboththeevaluation_dataandthe\n",
            "training_data. ThoseflagsareFalsebydefault,buttheyâ€™vebeenturnedonhereinorder\n",
            "tomonitorourNetworkâ€™sperformance. Furthermore,network2.pyâ€™sNetwork.SGDmethod\n",
            "returnsafour-elementtuplerepresentingtheresultsofthemonitoring. Wecanusethisas\n",
            "follows:\n",
            ">>> evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net\n",
            ".SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=\n",
            "True, monitor_training_accuracy=True, monitor_training_cost=True)\n",
            "So,forexample,evaluation_costwillbea30-elementlistcontainingthecostonthe\n",
            "evaluationdataattheendofeachepoch. Thissortofinformationisextremelyusefulin\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 107\n",
            "(cid:12)\n",
            "understandinganetworkâ€™sbehaviour.\n",
            "Itcan,forexample,beusedtodrawgraphsshowing\n",
            "howthenetworklearnsovertime. Indeed,thatâ€™sexactlyhowIconstructedallthegraphs\n",
            "earlierinthechapter. Note,however,thatifanyofthemonitoringflagsarenotset,thenthe\n",
            "correspondingelementinthetuplewillbetheemptylist. OtheradditionstothecodeincludeaNetwork.savemethod,tosaveNetworkobjects\n",
            "todisk,andafunctiontoloadthembackinagainlater. Notethatthesavingandloading\n",
            "isdoneusingJSON,notPythonâ€™spickleorcPicklemodules,whicharetheusualwaywe 3\n",
            "saveandloadobjectstoandfromdiskinPython. UsingJSONrequiresmorecodethan\n",
            "pickleorcPicklewould. TounderstandwhyIâ€™veusedJSON,imaginethatatsometimein\n",
            "thefuturewedecidedtochangeourNetworkclasstoallowneuronsotherthansigmoid\n",
            "neurons. Toimplementthatchangeweâ€™dmostlikelychangetheattributesdefinedinthe\n",
            "Network.__init__method. Ifweâ€™vesimplypickledtheobjectsthatwouldcauseourload\n",
            "functiontofail. UsingJSONtodotheserializationexplicitlymakesiteasytoensurethatold\n",
            "Networkswillstillload. Therearemanyotherminorchangesinthecodefornetwork2.py,buttheyâ€™reallsimple\n",
            "variationsonnetwork.py. Thenetresultistoexpandour74-lineprogramtoafarmore\n",
            "capable152lines. Problems\n",
            "ModifythecodeabovetoimplementL1regularization,anduseL1regularizationto\n",
            "â€¢ classifyMNISTdigitsusinga30hiddenneuronnetwork. Canyoufindaregularization\n",
            "parameterthatenablesyoutodobetterthanrunningunregularized? TakealookattheNetwork.cost_derivativemethodinnetwork.py.\n",
            "Thatmethod\n",
            "â€¢ waswrittenforthequadraticcost. Howwouldyourewritethemethodforthecross-\n",
            "entropycost? Canyouthinkofaproblemthatmightariseinthecross-entropyversion? Innetwork2.pyweâ€™veeliminatedtheNetwork.cost_derivativemethodentirely,\n",
            "insteadincorporatingitsfunctionalityintotheCrossEntropyCost.deltamethod.\n",
            "Howdoesthissolvetheproblemyouâ€™vejustidentified? 3.5 How to choose a neural networkâ€™s hyper-parameters? UpuntilnowIhavenâ€™texplainedhowIâ€™vebeenchoosingvaluesforhyper-parameterssuch\n",
            "asthelearningrate,Î·,theregularizationparameter,Î»,andsoon. Iâ€™vejustbeensupplying\n",
            "valueswhichworkprettywell. Inpractice,whenyouâ€™reusingneuralnetstoattackaproblem,\n",
            "it can be difficult to find good hyper-parameters. Imagine, for example, that weâ€™ve just\n",
            "beenintroducedtotheMNISTproblem,andhavebegunworkingonit,knowingnothing\n",
            "atallaboutwhathyper-parameterstouse. Letâ€™ssupposethatbygoodfortuneinourfirst\n",
            "experimentswechoosemanyofthehyper-parametersinthesamewayaswasdoneearlier\n",
            "this chapter: 30 hidden neurons, a mini-batch size of 10, training for 30 epochs using\n",
            "thecross-entropy. ButwechoosealearningrateÎ· =10.0andregularizationparameter\n",
            "Î» =1000.0. Hereâ€™swhatIsawononesuchrun:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 10.0, lmbda = 1000.0,\n",
            "\n",
            "(cid:12)\n",
            "108 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 1030 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 990 / 10000\n",
            "3\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "... Epoch 27 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 983 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 967 / 10000\n",
            "Ourclassificationaccuraciesarenobetterthanchance! Ournetworkisactingasarandom\n",
            "noisegenerator!\n",
            "â€œWell,thatâ€™seasytofix,â€youmightsay,â€œjustdecreasethelearningrateandregularization\n",
            "hyper-parametersâ€. Unfortunately,youdonâ€™taprioriknowthosearethehyper-parameters\n",
            "youneedtoadjust. Maybetherealproblemisthatour30hiddenneuronnetworkwillnever\n",
            "workwell,nomatterhowtheotherhyper-parametersarechosen? Maybewereallyneedat\n",
            "least100hiddenneurons? Or300hiddenneurons? Ormultiplehiddenlayers? Oradifferent\n",
            "approachtoencodingtheoutput? Maybeournetworkislearning,butweneedtotrain\n",
            "formoreepochs? Maybethemini-batchesaretoosmall? Maybeweâ€™ddobetterswitching\n",
            "backtothequadraticcostfunction? Maybeweneedtotryadifferentapproachtoweight\n",
            "initialization? Andsoon,onandonandon. Itâ€™seasytofeellostinhyper-parameterspace. Thiscanbeparticularlyfrustratingifyournetworkisverylarge,orusesalotoftraining\n",
            "data,sinceyoumaytrainforhoursordaysorweeks,onlytogetnoresult. Ifthesituation\n",
            "persists,itdamagesyourconfidence. Maybeneuralnetworksarethewrongapproachto\n",
            "yourproblem?\n",
            "Maybeyoushouldquityourjobandtakeupbeekeeping? InthissectionIexplainsomeheuristicswhichcanbeusedtosetthehyper-parameters\n",
            "inaneuralnetwork. Thegoalistohelpyoudevelopaworkflowthatenablesyoutodoa\n",
            "prettygoodjobsettinghyper-parameters. Ofcourse,Iwonâ€™tcovereverythingabouthyper-\n",
            "parameteroptimization. Thatâ€™sahugesubject, anditâ€™snot, inanycase, aproblemthat\n",
            "isevercompletelysolved,noristhereuniversalagreementamongstpractitionersonthe\n",
            "rightstrategiestouse. Thereâ€™salwaysonemoretrickyoucantrytoekeoutabitmore\n",
            "performancefromyournetwork. Buttheheuristicsinthissectionshouldgetyoustarted. Broadstrategy:Whenusingneuralnetworkstoattackanewproblemthefirstchallenge\n",
            "istogetanynon-triviallearning,i.e.,forthenetworktoachieveresultsbetterthanchance. Thiscanbesurprisinglydifficult,especiallywhenconfrontinganewclassofproblem.\n",
            "Letâ€™s\n",
            "lookatsomestrategiesyoucanuseifyouâ€™rehavingthiskindoftrouble. Suppose, for example, that youâ€™re attacking MNIST for the first time. You start out\n",
            "enthusiastic,butarealittlediscouragedwhenyourfirstnetworkfailscompletely,asinthe\n",
            "exampleabove.\n",
            "Thewaytogoistostriptheproblemdown. Getridofallthetraining\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 109\n",
            "(cid:12)\n",
            "andvalidationimagesexceptimageswhichare0sor1s. Thentrytotrainanetworkto\n",
            "distinguish0sfrom1s. Notonlyisthataninherentlyeasierproblemthandistinguishingall\n",
            "tendigits,italsoreducestheamountoftrainingdataby80percent,speedinguptrainingby\n",
            "afactorof5. Thatenablesmuchmorerapidexperimentation,andsogivesyoumorerapid\n",
            "insightintohowtobuildagoodnetwork. Youcanfurtherspeedupexperimentationbystrippingyournetworkdowntothesimplest\n",
            "networklikelytodomeaningfullearning. Ifyoubelievea[784, 10]networkcanlikely 3\n",
            "dobetter-than-chanceclassificationofMNISTdigits,thenbeginyourexperimentationwith\n",
            "suchanetwork. Itâ€™llbemuchfasterthantraininga[784, 30, 10]network,andyoucan\n",
            "buildbackuptothelatter. Youcangetanotherspeedupinexperimentationbyincreasingthefrequencyofmoni-\n",
            "toring. Innetwork2.pywemonitorperformanceattheendofeachtrainingepoch. With\n",
            "50,000imagesperepoch,thatmeanswaitingalittlewhileâ€“abouttensecondsperepoch,\n",
            "onmylaptop,whentraininga[784,30,10]networkâ€“beforegettingfeedbackonhowwell\n",
            "thenetworkislearning. Ofcourse,tensecondsisnâ€™tverylong,butifyouwanttotrialdozens\n",
            "ofhyper-parameterchoicesitâ€™sannoying,andifyouwanttotrialhundredsorthousands\n",
            "ofchoicesitstartstogetdebilitating. Wecangetfeedbackmorequicklybymonitoringthe\n",
            "validationaccuracymoreoften,say,afterevery1,000trainingimages. Furthermore,instead\n",
            "ofusingthefull10,000imagevalidationsettomonitorperformance,wecangetamuch\n",
            "fasterestimateusingjust100validationimages. Allthatmattersisthatthenetworksees\n",
            "enoughimagestodoreallearning,andtogetaprettygoodroughestimateofperformance. Ofcourse,ourprogramnetwork2.pydoesnâ€™tcurrentlydothiskindofmonitoring. Butas\n",
            "akludgetoachieveasimilareffectforthepurposesofillustration, weâ€™llstripdownour\n",
            "trainingdatatojustthefirst1,000MNISTtrainingimages. Letâ€™stryitandseewhathappens.\n",
            "(TokeepthecodebelowsimpleIhavenâ€™timplementedtheideaofusingonly0and1images. Ofcourse,thatcanbedonewithjustalittlemorework.)\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 1000.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "...\n",
            "monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... Weâ€™restillgettingpurenoise!\n",
            "Butthereâ€™sabigwin: weâ€™renowgettingfeedbackinafraction\n",
            "ofasecond,ratherthanonceeverytensecondsorso. Thatmeansyoucanmorequickly\n",
            "experimentwithotherchoicesofhyper-parameter,orevenconductexperimentstrialling\n",
            "manydifferentchoicesofhyper-parameternearlysimultaneously. IntheaboveexampleIleftÎ»asÎ» =1000.0,asweusedearlier. Butsincewechanged\n",
            "thenumberoftrainingexamplesweshouldreallychangeÎ»tokeeptheweightdecaythe\n",
            "same. ThatmeanschangingÎ»to20.0. Ifwedothatthenthisiswhathappens:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "\n",
            "(cid:12)\n",
            "110 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 12 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 14 / 100\n",
            "Epoch 2 training complete\n",
            "3 Accuracy on evaluation data: 25 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 18 / 100\n",
            "... Ahah!\n",
            "Wehaveasignal. Notaterriblygoodsignal,butasignalnonetheless. Thatâ€™ssomething\n",
            "wecanbuildon,modifyingthehyper-parameterstotrytogetfurtherimprovement. Maybe\n",
            "weguessthatourlearningrateneedstobehigher. (Asyouperhapsrealize,thatâ€™sasilly\n",
            "guess,forreasonsweâ€™lldiscussshortly,butpleasebearwithme.) Sototestourguesswetry\n",
            "dialingÎ·upto100.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 100.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... Thatâ€™snogood!\n",
            "Itsuggeststhatourguesswaswrong, andtheproblemwasnâ€™t thatthe\n",
            "learningratewastoolow. SoinsteadwetrydialingÎ·downtoÎ· =1.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 1.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 62 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 42 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 43 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 61 / 100\n",
            "... \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 111\n",
            "(cid:12)\n",
            "Thatâ€™sbetter!\n",
            "Andsowecancontinue,individuallyadjustingeachhyper-parameter,gradually\n",
            "improvingperformance. Onceweâ€™veexploredtofindanimprovedvalueforÎ·,thenwe\n",
            "moveontofindagoodvalueforÎ». Thenexperimentwithamorecomplexarchitecture,\n",
            "sayanetworkwith10hiddenneurons. ThenadjustthevaluesforÎ·andÎ»again. Then\n",
            "increaseto20hiddenneurons. Andthenadjustotherhyper-parameterssomemore.\n",
            "And\n",
            "soon,ateachstageevaluatingperformanceusingourheld-outvalidationdata,andusing\n",
            "thoseevaluationstofindbetterandbetterhyper-parameters. Aswedoso,ittypicallytakes 3\n",
            "longertowitnesstheimpactduetomodificationsofthehyper-parameters,andsowecan\n",
            "graduallydecreasethefrequencyofmonitoring. Thisalllooksverypromisingasabroadstrategy.\n",
            "However,Iwanttoreturntothatinitial\n",
            "stageoffindinghyper-parametersthatenableanetworktolearnanythingatall. Infact,\n",
            "eventheabovediscussionconveystoopositiveanoutlook. Itcanbeimmenselyfrustrating\n",
            "toworkwithanetworkthatâ€™slearningnothing. Youcantweakhyper-parametersfordays,\n",
            "andstillgetnomeaningfulresponse. AndsoIâ€™dliketore-emphasizethatduringtheearly\n",
            "stagesyoushouldmakesureyoucangetquickfeedbackfromexperiments. Intuitively,it\n",
            "mayseemasthoughsimplifyingtheproblemandthearchitecturewillmerelyslowyoudown. Infact,itspeedsthingsup,sinceyoumuchmorequicklyfindanetworkwithameaningful\n",
            "signal. Onceyouâ€™vegotsuchasignal,youcanoftengetrapidimprovementsbytweakingthe\n",
            "hyper-parameters. Aswithmanythingsinlife,gettingstartedcanbethehardestthingtodo. Okay,thatâ€™sthebroadstrategy. Letâ€™snowlookatsomespecificrecommendationsfor\n",
            "settinghyper-parameters. Iwillfocusonthelearningrate,Î·,theL2regularizationparameter,\n",
            "Î», and the mini-batch size. However, many of the remarks apply also to other hyper-\n",
            "parameters,includingthoseassociatedtonetworkarchitecture,otherformsofregularization,\n",
            "andsomehyper-parametersweâ€™llmeetlaterinthebook,suchasthemomentumco-efficient. Learningrate: SupposewerunthreeMNISTnetworkswiththreedifferentlearningrates,\n",
            "Î· =0.025,Î· =0.25andÎ· =2.5,respectively. Weâ€™llsettheotherhyper-parametersasfor\n",
            "theexperimentsinearliersections,runningover30epochs,withamini-batchsizeof10,\n",
            "andwithÎ» =5.0. Weâ€™llalsoreturntousingthefull50,000trainingimages. Hereâ€™sagraph\n",
            "showingthebehaviourofthetrainingcostaswetrain26:\n",
            "26Thegraphwasgeneratedbymultiple_eta.py. \n",
            "(cid:12)\n",
            "112 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "WithÎ· =0.025thecostdecreasessmoothlyuntilthefinalepoch. WithÎ· =0.25thecost\n",
            "initiallydecreases,butafterabout20epochsitisnearsaturation,andthereaftermostofthe\n",
            "changesaremerelysmallandapparentlyrandomoscillations. Finally,withÎ· =2.5thecost\n",
            "makeslargeoscillationsrightfromthestart. Tounderstandthereasonfortheoscillations,\n",
            "recallthatstochasticgradientdescentissupposedtostepusgraduallydownintoavalleyof\n",
            "thecostfunction,\n",
            "3\n",
            "However,ifÎ·istoolargethenthestepswillbesolargethattheymayactuallyovershootthe\n",
            "minimum,causingthealgorithmtoclimbupoutofthevalleyinstead. Thatâ€™slikely27whatâ€™s\n",
            "causingthecosttooscillatewhenÎ· =2.5. WhenwechooseÎ·=0.25theinitialstepsdotake\n",
            "ustowardaminimumofthecostfunction,anditâ€™sonlyoncewegetnearthatminimumthat\n",
            "westarttosufferfromtheovershootingproblem. AndwhenwechooseÎ· =0.025wedonâ€™t\n",
            "sufferfromthisproblematallduringthefirst30epochs. Ofcourse,choosingÎ·sosmall\n",
            "createsanotherproblem,namely,thatitslowsdownstochasticgradientdescent. Aneven\n",
            "betterapproachwouldbetostartwithÎ· =0.25,trainfor20epochs,andthenswitchto\n",
            "Î· =0.025. Weâ€™lldiscusssuchvariablelearningratescheduleslater. Fornow,though,letâ€™s\n",
            "sticktofiguringouthowtofindasinglegoodvalueforthelearningrate,Î·. Withthispictureinmind, wecansetÎ·asfollows. First, weestimatethethreshold\n",
            "valueforÎ·atwhichthecostonthetrainingdataimmediatelybeginsdecreasing,insteadof\n",
            "oscillatingorincreasing. Thisestimatedoesnâ€™tneedtobetooaccurate.\n",
            "Youcanestimate\n",
            "theorderofmagnitudebystartingwithÎ·=0.01. Ifthecostdecreasesduringthefirstfew\n",
            "epochs,thenyoushouldsuccessivelytryÎ· =0.1,1.0,...untilyoufindavalueforÎ·where\n",
            "thecostoscillatesorincreasesduringthefirstfewepochs. Alternately,ifthecostoscillates\n",
            "orincreasesduringthefirstfewepochswhenÎ·=0.01,thentryÎ·=0.001,0.0001,...until\n",
            "youfindavalueforÎ·wherethecostdecreasesduringthefirstfewepochs. Followingthis\n",
            "procedurewillgiveusanorderofmagnitudeestimateforthethresholdvalueofÎ·. You\n",
            "mayoptionallyrefineyourestimate,topickoutthelargestvalueofÎ·atwhichthecost\n",
            "decreasesduringthefirstfewepochs,sayÎ·=0.5orÎ·=0.2(thereâ€™snoneedforthistobe\n",
            "super-accurate). ThisgivesusanestimateforthethresholdvalueofÎ·.\n",
            "Obviously,theactualvalueofÎ·thatyouuseshouldbenolargerthanthethreshold\n",
            "value.\n",
            "Infact,ifthevalueofÎ·istoremainusableovermanyepochsthenyoulikelywantto\n",
            "27Thispictureishelpful,butitâ€™sintendedasanintuition-buildingillustrationofwhatmaygoon,not\n",
            "asacomplete,exhaustiveexplanation. Briefly,amorecompleteexplanationisasfollows: gradient\n",
            "descentusesafirst-orderapproximationtothecostfunctionasaguidetohowtodecreasethecost. ForlargeÎ·,higher-ordertermsinthecostfunctionbecomemoreimportant,andmaydominatethe\n",
            "behaviour,causinggradientdescenttobreakdown.Thisisespeciallylikelyasweapproachminimaand\n",
            "quasi-minimaofthecostfunction,sincenearsuchpointsthegradientbecomessmall,makingiteasier\n",
            "forhigher-ordertermstodominatebehaviour. \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 113\n",
            "(cid:12)\n",
            "useavalueforÎ·thatissmaller,say,afactoroftwobelowthethreshold.\n",
            "Suchachoicewill\n",
            "typicallyallowyoutotrainformanyepochs,withoutcausingtoomuchofaslowdownin\n",
            "learning. InthecaseoftheMNISTdata,followingthisstrategyleadstoanestimateof0.1forthe\n",
            "orderofmagnitudeofthethresholdvalueofÎ·. Aftersomemorerefinement,weobtaina\n",
            "thresholdvalueÎ·=0.5. Followingtheprescriptionabove,thissuggestsusingÎ·=0.25asour\n",
            "valueforthelearningrate. Infact,IfoundthatusingÎ·=0.5workedwellenoughover30 3\n",
            "epochsthatforthemostpartIdidnâ€™tworryaboutusingalowervalueofÎ·. Thisallseemsquitestraightforward. However,usingthetrainingcosttopickÎ·appears\n",
            "tocontradictwhatIsaidearlierinthissection,namely,thatweâ€™dpickhyper-parameters\n",
            "byevaluatingperformanceusingourheld-outvalidationdata. Infact,weâ€™llusevalidation\n",
            "accuracy to pick the regularization hyper-parameter, the mini-batch size, and network\n",
            "parameterssuchasthenumberoflayersandhiddenneurons,andsoon. Whydothings\n",
            "differentlyforthelearningrate?\n",
            "Frankly,thischoiceismypersonalaestheticpreference,\n",
            "andisperhapssomewhatidiosyncratic. Thereasoningisthattheotherhyper-parameters\n",
            "areintendedtoimprovethefinalclassificationaccuracyonthetestset,andsoitmakes\n",
            "sensetoselectthemonthebasisofvalidationaccuracy. However,thelearningrateisonly\n",
            "incidentallymeanttoimpactthefinalclassificationaccuracy. Itsprimarypurposeisreallyto\n",
            "controlthestepsizeingradientdescent,andmonitoringthetrainingcostisthebestway\n",
            "todetectifthestepsizeistoobig. Withthatsaid,thisisapersonalaestheticpreference. Earlyonduringlearningthetrainingcostusuallyonlydecreasesifthevalidationaccuracy\n",
            "improves,andsoinpracticeitâ€™sunlikelytomakemuchdifferencewhichcriterionyouuse. Useearly stoppingto determinethenumber oftraining epochs: Aswediscussed\n",
            "earlierinthechapter,earlystoppingmeansthatattheendofeachepochweshouldcompute\n",
            "theclassificationaccuracyonthevalidationdata. Whenthatstopsimproving,terminate.\n",
            "Thismakessettingthenumberofepochsverysimple. Inparticular,itmeansthatwedonâ€™t\n",
            "needtoworryaboutexplicitlyfiguringouthowthenumberofepochsdependsontheother\n",
            "hyper-parameters. Instead,thatâ€™stakencareofautomatically. Furthermore,earlystopping\n",
            "alsoautomaticallypreventsusfromoverfitting. Thisis,ofcourse,agoodthing,althoughin\n",
            "theearlystagesofexperimentationitcanbehelpfultoturnoffearlystopping,soyoucan\n",
            "seeanysignsofoverfitting,anduseittoinformyourapproachtoregularization. To implement early stopping we need to say more precisely what it means that the\n",
            "classificationaccuracyhasstoppedimproving. Asweâ€™veseen,theaccuracycanjumparound\n",
            "quiteabit,evenwhentheoveralltrendistoimprove.\n",
            "Ifwestopthefirsttimetheaccuracy\n",
            "decreasesthenweâ€™llalmostcertainlystopwhentherearemoreimprovementstobehad. A\n",
            "betterruleistoterminateifthebestclassificationaccuracydoesnâ€™timproveforquitesome\n",
            "time. Suppose,forexample,thatweâ€™redoingMNIST.Thenwemightelecttoterminateifthe\n",
            "classificationaccuracyhasnâ€™timprovedduringthelasttenepochs. Thisensuresthatwedonâ€™t\n",
            "stoptoosoon,inresponsetobadluckintraining,butalsothatweâ€™renotwaitingaround\n",
            "foreverforanimprovementthatnevercomes. This no-improvement-in-ten rule is good for initial exploration of MNIST. However,\n",
            "networkscansometimesplateaunearaparticularclassificationaccuracyforquitesometime,\n",
            "onlytothenbeginimprovingagain. Ifyouâ€™retryingtogetreallygoodperformance,theno-\n",
            "improvement-in-tenrulemaybetooaggressiveaboutstopping. Inthatcase,Isuggestusing\n",
            "theno-improvement-in-tenruleforinitialexperimentation,andgraduallyadoptingmore\n",
            "lenientrules,asyoubetterunderstandthewayyournetworktrains: no-improvement-in-\n",
            "twenty,no-improvement-in-fifty,andsoon. Ofcourse,thisintroducesanewhyper-parameter\n",
            "\n",
            "(cid:12)\n",
            "114 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "tooptimize! Inpractice,however,itâ€™susuallyeasytosetthishyper-parametertogetpretty\n",
            "goodresults. Similarly,forproblemsotherthanMNIST,theno-improvement-in-tenrule\n",
            "maybemuchtooaggressiveornotnearlyaggressiveenough,dependingonthedetailsof\n",
            "theproblem. However,withalittleexperimentationitâ€™susuallyeasytofindaprettygood\n",
            "strategyforearlystopping. Wehavenâ€™tusedearlystoppinginourMNISTexperimentstodate. Thereasonisthat\n",
            "3 weâ€™vebeendoingalotofcomparisonsbetweendifferentapproachestolearning. Forsuch\n",
            "comparisonsitâ€™shelpfultousethesamenumberofepochsineachcase. However,itâ€™swell\n",
            "worthmodifyingnetwork2.pytoimplementearlystopping:\n",
            "Problem\n",
            "Modifynetwork2.pysothatitimplementsearlystoppingusingano-improvement-\n",
            "â€¢ in-nepochsstrategy,wherenisaparameterthatcanbeset. Canyouthinkofaruleforearlystoppingotherthanno-improvement-in-n? Ideally,the\n",
            "â€¢ ruleshouldcompromisebetweengettinghighvalidationaccuraciesandnottraining\n",
            "toolong. Addyourruletonetwork2.py,andrunthreeexperimentscomparingthe\n",
            "validationaccuraciesandnumberofepochsoftrainingtono-improvement-in-10. Learningrateschedule: Weâ€™vebeenholdingthelearningrateÎ·constant. However,itâ€™s\n",
            "oftenadvantageoustovarythelearningrate. Earlyonduringthelearningprocessitâ€™slikely\n",
            "thattheweightsarebadlywrong. Andsoitâ€™sbesttousealargelearningratethatcauses\n",
            "theweightstochangequickly. Later, wecanreducethelearningrateaswemakemore\n",
            "fine-tunedadjustmentstoourweights. Howshouldwesetourlearningrateschedule?\n",
            "Manyapproachesarepossible. One\n",
            "naturalapproachistousethesamebasicideaasearlystopping. Theideaistoholdthe\n",
            "learningrateconstantuntilthevalidationaccuracystartstogetworse. Thendecreasethe\n",
            "learningratebysomeamount,sayafactoroftwoorten. Werepeatthismanytimes,until,\n",
            "say,thelearningrateisafactorof1,024(or1,000)timeslowerthantheinitialvalue. Then\n",
            "weterminate. Avariablelearningschedulecanimproveperformance,butitalsoopensupaworldof\n",
            "possiblechoicesforthelearningschedule. Thosechoicescanbeaheadacheâ€“youcanspend\n",
            "forevertryingtooptimizeyourlearningschedule. Forfirstexperimentsmysuggestionisto\n",
            "useasingle,constantvalueforthelearningrate.\n",
            "Thatâ€™llgetyouagoodfirstapproximation.\n",
            "Later,ifyouwanttoobtainthebestperformancefromyournetwork,itâ€™sworthexperimenting\n",
            "withalearningschedule,alongthelinesIâ€™vedescribed28. Exercise\n",
            "Modify network2.py so that it implements a learning schedule that: halves the\n",
            "â€¢ learningrateeachtimethevalidationaccuracysatisfiestheno-improvement-in-10\n",
            "rule;andterminateswhenthelearningratehasdroppedto1/128ofitsoriginalvalue. Theregularizationparameter,Î»: Isuggeststartinginitiallywithnoregularization(Î»\n",
            "=\n",
            "0.0),anddeterminingavalueforÎ·,asabove.\n",
            "UsingthatchoiceofÎ·,wecanthenusethe\n",
            "validationdatatoselectagoodvalueforÎ». StartbytriallingÎ» =1.029,andthenincreaseor\n",
            "decreasebyfactorsof10,asneededtoimproveperformanceonthevalidationdata. Once\n",
            "youâ€™vefoundagoodorderofmagnitude,youcanfinetuneyourvalueofÎ». Thatdone,you\n",
            "shouldreturnandre-optimizeÎ·again. 28Areadablerecentpaperwhichdemonstratesthebenefitsofvariablelearningratesinattacking\n",
            "MNISTisDeep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCireÂ¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJÃ¼rgenSchmidhuber(2010). 29Idonâ€™thaveagoodprincipledjustificationforusingthisasastartingvalue.Ifanyoneknowsofa\n",
            "goodprincipleddiscussionofwheretostartwithÎ»,Iâ€™dappreciatehearingit(mn@michaelnielsen.org). \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 115\n",
            "(cid:12)\n",
            "Exercise\n",
            "Itâ€™stemptingtousegradientdescenttotrytolearngoodvaluesforhyper-parameters\n",
            "â€¢ suchasÎ»andÎ·. Canyouthinkofanobstacletousinggradientdescenttodetermine\n",
            "Î»? CanyouthinkofanobstacletousinggradientdescenttodetermineÎ·? HowIselectedhyper-parametersearlierinthisbook: Ifyouusetherecommendations\n",
            "inthissectionyouâ€™llfindthatyougetvaluesforÎ·andÎ»whichdonâ€™talwaysexactlymatch 3\n",
            "thevaluesIâ€™veusedearlierinthebook. Thereasonisthatthebookhasnarrativeconstraints\n",
            "thathavesometimesmadeitimpracticaltooptimizethehyper-parameters. Thinkofallthe\n",
            "comparisonsweâ€™vemadeofdifferentapproachestolearning,e.g.,comparingthequadratic\n",
            "andcross-entropycostfunctions,comparingtheoldandnewmethodsofweightinitialization,\n",
            "runningwithandwithoutregularization,andsoon. Tomakesuchcomparisonsmeaningful,\n",
            "Iâ€™veusuallytriedtokeephyper-parametersconstantacrosstheapproachesbeingcompared\n",
            "(ortoscaletheminanappropriateway). Ofcourse,thereâ€™snoreasonforthesamehyper-\n",
            "parameterstobeoptimalforallthedifferentapproachestolearning,sothehyper-parameters\n",
            "Iâ€™veusedaresomethingofacompromise. Asanalternativetothiscompromise,Icouldhavetriedtooptimizetheheckoutofthe\n",
            "hyper-parametersforeverysingleapproachtolearning. Inprinciplethatâ€™dbeabetter,fairer\n",
            "approach,sincethenweâ€™dseethebestfromeveryapproachtolearning. However,weâ€™ve\n",
            "madedozensofcomparisonsalongtheselines,andinpracticeIfoundittoocomputationally\n",
            "expensive. Thatâ€™swhyIâ€™veadoptedthecompromiseofusingprettygood(butnotnecessarily\n",
            "optimal)choicesforthehyper-parameters. Mini-batchsize: Howshouldwesetthemini-batchsize? Toanswerthisquestion,letâ€™s\n",
            "firstsupposethatweâ€™redoingonlinelearning,i.e.,thatweâ€™reusingamini-batchsizeof1. Theobviousworryaboutonlinelearningisthatusingmini-batcheswhichcontainjust\n",
            "asingletrainingexamplewillcausesignificanterrorsinourestimateofthegradient. In\n",
            "fact,though,theerrorsturnouttonotbesuchaproblem. Thereasonisthattheindividual\n",
            "gradientestimatesdonâ€™tneedtobesuper-accurate. Allweneedisanestimateaccurate\n",
            "enoughthatourcostfunctiontendstokeepdecreasing. Itâ€™sasthoughyouaretryingtoget\n",
            "totheNorthMagneticPole,buthaveawonkycompassthatâ€™s10â€“20degreesoffeachtime\n",
            "youlookatit. Providedyoustoptocheckthecompassfrequently,andthecompassgetsthe\n",
            "directionrightonaverage,youâ€™llendupattheNorthMagneticPolejustfine. Basedonthisargument,itsoundsasthoughweshoulduseonlinelearning.\n",
            "Infact,the\n",
            "situationturnsouttobemorecomplicatedthanthat.\n",
            "InaprobleminthelastchapterI\n",
            "pointedoutthatitâ€™spossibletousematrixtechniquestocomputethegradientupdatefor\n",
            "allexamplesinamini-batchsimultaneously,ratherthanloopingoverthem. Dependingon\n",
            "thedetailsofyourhardwareandlinearalgebralibrarythiscanmakeitquiteabitfaster\n",
            "tocomputethegradientestimateforamini-batchof(forexample)size100,ratherthan\n",
            "computing the mini-batch gradient estimate by looping over the 100 training examples\n",
            "separately. Itmighttake(say)only50timesaslong,ratherthan100timesaslong.\n",
            "Now,atfirstitseemsasthoughthisdoesnâ€™thelpusthatmuch. Withourmini-batchof\n",
            "size100thelearningrulefortheweightslookslike:\n",
            "1 (cid:88)\n",
            "w\n",
            "â†’\n",
            "w (cid:48)=w\n",
            "âˆ’\n",
            "Î·\n",
            "100 x âˆ‡\n",
            "C\n",
            "x\n",
            ", (3.45)\n",
            "\n",
            "(cid:12)\n",
            "116 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "wherethesumisovertrainingexamplesinthemini-batch. Thisisversus\n",
            "w w (cid:48)=w Î· C\n",
            "x\n",
            "(3.46)\n",
            "â†’ âˆ’ âˆ‡\n",
            "foronlinelearning. Evenifitonlytakes50timesaslongtodothemini-batchupdate,it\n",
            "stillseemslikelytobebettertodoonlinelearning,becauseweâ€™dbeupdatingsomuchmore\n",
            "3 frequently. Suppose,however,thatinthemini-batchcaseweincreasethelearningratebya\n",
            "factor100,sotheupdaterulebecomes\n",
            "(cid:88)\n",
            "w w (cid:48)=w Î· C\n",
            "x\n",
            ". (3.47)\n",
            "â†’ âˆ’ x âˆ‡\n",
            "Thatâ€™salotlikedoing100separateinstancesofonlinelearningwithalearningrateofÎ·. Butitonlytakes50timesaslongasdoingasingleinstanceofonlinelearning.\n",
            "Ofcourse,itâ€™s\n",
            "nottrulythesameas100instancesofonlinelearning,sinceinthemini-batchthe C â€™sare\n",
            "x\n",
            "allevaluatedforthesamesetofweights,asopposedtothecumulativelearningthâˆ‡atoccurs\n",
            "intheonlinecase. Still,itseemsdistinctlypossiblethatusingthelargermini-batchwould\n",
            "speedthingsup. Withthesefactorsinmind,choosingthebestmini-batchsizeisacompromise. Toosmall,\n",
            "andyoudonâ€™tgettotakefulladvantageofthebenefitsofgoodmatrixlibrariesoptimizedfor\n",
            "fasthardware.\n",
            "Toolargeandyouâ€™resimplynotupdatingyourweightsoftenenough. What\n",
            "youneedistochooseacompromisevaluewhichmaximizesthespeedoflearning.Fortunately,\n",
            "thechoiceofmini-batchsizeatwhichthespeedismaximizedisrelativelyindependent\n",
            "oftheotherhyper-parameters(apartfromtheoverallarchitecture),soyoudonâ€™tneedto\n",
            "haveoptimizedthosehyper-parametersinordertofindagoodmini-batchsize. Theway\n",
            "togoisthereforetousesomeacceptable(butnotnecessarilyoptimal)valuesfortheother\n",
            "hyper-parameters,andthentrialanumberofdifferentmini-batchsizes,scalingÎ·asabove. Plotthevalidationaccuracyversustime(asin,realelapsedtime,notepoch!),andchoose\n",
            "whichevermini-batchsizegivesyouthemostrapidimprovementinperformance. Withthe\n",
            "mini-batchsizechosenyoucanthenproceedtooptimizetheotherhyper-parameters. Ofcourse,asyouâ€™venodoubtrealized,Ihavenâ€™tdonethisoptimizationinourwork. Indeed,ourimplementationdoesnâ€™tusethefasterapproachtomini-batchupdatesatall. Iâ€™ve\n",
            "simplyusedamini-batchsizeof10withoutcommentorexplanationinnearlyallexamples. Becauseofthis,wecouldhavespeduplearningbyreducingthemini-batchsize. Ihavenâ€™t\n",
            "donethis,inpartbecauseIwantedtoillustratetheuseofmini-batchesbeyondsize1,andin\n",
            "partbecausemypreliminaryexperimentssuggestedthespeedupwouldberathermodest. In\n",
            "practicalimplementations,however,wewouldmostcertainlyimplementthefasterapproach\n",
            "tomini-batchupdates,andthenmakeanefforttooptimizethemini-batchsize,inorderto\n",
            "maximizeouroverallspeed. Automatedtechniques: Iâ€™vebeendescribingtheseheuristicsasthoughyouâ€™reoptimiz-\n",
            "ingyourhyper-parametersbyhand. Hand-optimizationisagoodwaytobuildupafeelfor\n",
            "howneuralnetworksbehave. However,andunsurprisingly,agreatdealofworkhasbeen\n",
            "doneonautomatingtheprocess. Acommontechniqueisgridsearch,whichsystematically\n",
            "searchesthroughagridinhyper-parameterspace. Areviewofboththeachievementsand\n",
            "thelimitationsofgridsearch(withsuggestionsforeasily-implementedalternatives)maybe\n",
            "foundina2012paper30byJamesBergstraandYoshuaBengio. Manymoresophisticated\n",
            "30Randomsearchforhyper-parameteroptimization,byJamesBergstraandYoshuaBengio(2012).\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetworkâ€™shyper-parameters? (cid:12) 117\n",
            "(cid:12)\n",
            "approacheshavealsobeenproposed. Iwonâ€™treviewallthatworkhere,butdowanttomen-\n",
            "tionaparticularlypromising2012paperwhichusedaBayesianapproachtoautomatically\n",
            "optimizehyper-parameters31. Thecodefromthepaperispubliclyavailable,andhasbeen\n",
            "usedwithsomesuccessbyotherresearchers. Summingup: Followingtherules-of-thumbIâ€™vedescribedwonâ€™tgiveyoutheabsolute\n",
            "bestpossibleresultsfromyourneuralnetwork. Butitwilllikelygiveyouagoodstartanda\n",
            "basisforfurtherimprovements. Inparticular,Iâ€™vediscussedthehyper-parameterslargely 3\n",
            "independently. Inpractice,therearerelationshipsbetweenthehyper-parameters. Youmay\n",
            "experimentwithÎ·,feelthatyouâ€™vegotitjustright,thenstarttooptimizeforÎ»,onlytofind\n",
            "thatitâ€™smessingupyouroptimizationforÎ·. Inpractice,ithelpstobouncebackwardand\n",
            "forward,graduallyclosingingoodvalues. Aboveall,keepinmindthattheheuristicsIâ€™ve\n",
            "describedarerulesofthumb,notrulescastinstone. Youshouldbeonthelookoutforsigns\n",
            "thatthingsarenâ€™tworking,andbewillingtoexperiment. Inparticular,thismeanscarefully\n",
            "monitoringyournetworkâ€™sbehaviour,especiallythevalidationaccuracy. The difficulty of choosing hyper-parameters is exacerbated by the fact that the lore\n",
            "abouthowtochoosehyper-parametersiswidelyspread,acrossmanyresearchpapersand\n",
            "softwareprograms,andoftenisonlyavailableinsidetheheadsofindividualpractitioners. Therearemany,manypaperssettingout(sometimescontradictory)recommendationsfor\n",
            "howtoproceed.\n",
            "However,thereareafewparticularlyusefulpapersthatsynthesizeand\n",
            "distilloutmuchofthislore. YoshuaBengiohasa2012paper32 thatgivessomepractical\n",
            "recommendationsforusingbackpropagationandgradientdescenttotrainneuralnetworks,\n",
            "includingdeepneuralnets. BengiodiscussesmanyissuesinmuchmoredetailthanIhave,\n",
            "includinghowtodomoresystematichyper-parametersearches.\n",
            "Anothergoodpaperisa\n",
            "1998paper33 byYannLeCun,LÃ©onBottou,GenevieveOrrandKlaus-RobertMÃ¼ller.\n",
            "Both\n",
            "thesepapersappearinanextremelyuseful2012bookthatcollectsmanytrickscommonly\n",
            "usedinneuralnets34. Thebookisexpensive,butmanyofthearticleshavebeenplaced\n",
            "onlinebytheirrespectiveauthorswith,onepresumes,theblessingofthepublisher,andmay\n",
            "belocatedusingasearchengine. Onethingthatbecomesclearasyoureadthesearticlesand,especially,asyouengagein\n",
            "yourownexperiments,isthathyper-parameteroptimizationisnotaproblemthatisever\n",
            "completelysolved. Thereâ€™salwaysanothertrickyoucantrytoimproveperformance. There\n",
            "isasayingcommonamongwritersthatbooksareneverfinished,onlyabandoned. Thesame\n",
            "isalsotrueofneuralnetworkoptimization: thespaceofhyper-parametersissolargethat\n",
            "oneneverreallyfinishesoptimizing,oneonlyabandonsthenetworktoposterity. Soyour\n",
            "goalshouldbetodevelopaworkflowthatenablesyoutoquicklydoaprettygoodjobon\n",
            "theoptimization,whileleavingyoutheflexibilitytotrymoredetailedoptimizations,ifthatâ€™s\n",
            "important. Thechallengeofsettinghyper-parametershasledsomepeopletocomplainthatneural\n",
            "networksrequirealotofworkwhencomparedwithothermachinelearningtechniques. Iâ€™ve\n",
            "heardmanyvariationsonthefollowingcomplaint: â€œYes,awell-tunedneuralnetworkmay\n",
            "getthebestperformanceontheproblem. Ontheotherhand,Icantryarandomforest[or\n",
            "31PracticalBayesianoptimizationofmachinelearningalgorithms,byJasperSnoek,HugoLarochelle,\n",
            "andRyanAdams. 32Practicalrecommendationsforgradient-basedtrainingofdeeparchitectures,byYoshuaBengio\n",
            "(2012). 33EfficientBackProp,byYannLeCun,LÃƒlâ€™onBottou,GenevieveOrrandKlaus-RobertMÃ¼ller(1998)\n",
            "34NeuralNetworks:TricksoftheTrade,editedbyGrÃ©goireMontavon,GeneviÃ¨veOrr,andKlaus-Robert\n",
            "MÃ¼ller.\n",
            "\n",
            "(cid:12)\n",
            "118 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "SVMor... insertyourownfavoritetechnique]anditjustworks.\n",
            "Idonâ€™thavetimetofigure\n",
            "outjusttherightneuralnetwork.â€ Ofcourse,fromapracticalpointofviewitâ€™sgoodto\n",
            "haveeasy-to-applytechniques. Thisisparticularlytruewhenyouâ€™rejustgettingstartedona\n",
            "problem,anditmaynotbeobviouswhethermachinelearningcanhelpsolvetheproblemat\n",
            "all. Ontheotherhand,ifgettingoptimalperformanceisimportant,thenyoumayneedto\n",
            "tryapproachesthatrequiremorespecialistknowledge. Whileitwouldbeniceifmachine\n",
            "3 learningwerealwayseasy,thereisnoapriorireasonitshouldbetriviallysimple. 3.6 Other techniques\n",
            "Eachtechniquedevelopedinthischapterisvaluabletoknowinitsownright,butthatâ€™s\n",
            "nottheonlyreasonIâ€™veexplainedthem. Thelargerpointistofamiliarizeyouwithsomeof\n",
            "theproblemswhichcanoccurinneuralnetworks,andwithastyleofanalysiswhichcan\n",
            "helpovercomethoseproblems. Inasense,weâ€™vebeenlearninghowtothinkaboutneural\n",
            "nets. OvertheremainderofthischapterIbrieflysketchahandfulofothertechniques.\n",
            "These\n",
            "sketchesarelessin-depththantheearlierdiscussions,butshouldconveysomefeelingfor\n",
            "thediversityoftechniquesavailableforuseinneuralnetworks. 3.6.1 Variationsonstochasticgradientdescent\n",
            "StochasticgradientdescentbybackpropagationhasserveduswellinattackingtheMNIST\n",
            "digitclassificationproblem. However,therearemanyotherapproachestooptimizingthecost\n",
            "function,andsometimesthoseotherapproachesofferperformancesuperiortomini-batch\n",
            "stochasticgradientdescent. InthissectionIsketchtwosuchapproaches,theHessianand\n",
            "momentumtechniques. Hessiantechnique: Tobeginourdiscussionithelpstoputneuralnetworksasidefora\n",
            "bit. Instead,weâ€™rejustgoingtoconsidertheabstractproblemofminimizingacostfunction\n",
            "C whichisafunctionofmanyvariables,w=w\n",
            "1\n",
            ",w\n",
            "2\n",
            ",...,soC=C(w). ByTaylorâ€™stheorem,\n",
            "thecostfunctioncanbeapproximatednearapointwby\n",
            "(cid:88) âˆ‚C 1(cid:88) âˆ‚2C\n",
            "C(w+ âˆ†w)=C(w)+ âˆ‚w âˆ†w j+ 2 âˆ†w jâˆ‚w âˆ‚w âˆ†w k+... (3.48)\n",
            "j j jk j k\n",
            "Wecanrewritethismorecompactlyas\n",
            "1\n",
            "C(w+ âˆ†w)=C(w)+ C âˆ†w+ âˆ†wTHâˆ†w+..., (3.49)\n",
            "âˆ‡ Â· 2\n",
            "where Cistheusualgradientvector,andHisamatrixknownastheHessianmatrix,whose\n",
            "jk-theâˆ‡ntryisâˆ‚2C/âˆ‚w âˆ‚w . Supposeweapproximate C bydiscardingthehigher-order\n",
            "j k\n",
            "termsrepresentedby...above,\n",
            "1\n",
            "C(w+ âˆ†w) C(w)+ C âˆ†w+ âˆ†wTHâˆ†w. (3.50)\n",
            "â‰ˆ âˆ‡ Â· 2\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 119\n",
            "(cid:12)\n",
            "Usingcalculuswecanshowthattheexpressionontheright-handsidecanbeminimized35\n",
            "bychoosing\n",
            "âˆ†w= H\n",
            "âˆ’\n",
            "1 C. (3.51)\n",
            "âˆ’ âˆ‡\n",
            "Provided(3.50)isagoodapproximateexpressionforthecostfunction,thenweâ€™dexpect\n",
            "thatmovingfromthepointwtow+ âˆ†w=w H\n",
            "âˆ’\n",
            "1 C shouldsignificantlydecreasethecost 3\n",
            "function. Thatsuggestsapossiblealgorithmâˆ’formiâˆ‡nimizingthecost:\n",
            "Chooseastartingpoint,w. â€¢ Updatewtoanewpointw (cid:48)=w H\n",
            "âˆ’\n",
            "1 C,wheretheHessianHand Carecomputed\n",
            "â€¢ atw. âˆ’ âˆ‡ âˆ‡\n",
            "Update w\n",
            "(cid:48)\n",
            "toanewpoint w\n",
            "(cid:48)(cid:48)\n",
            "=w\n",
            "(cid:48)\n",
            "H\n",
            "(cid:48)âˆ’\n",
            "1\n",
            "(cid:48)\n",
            "C,wheretheHessian H\n",
            "(cid:48)\n",
            "and\n",
            "(cid:48)\n",
            "C are\n",
            "â€¢ computedatw. âˆ’ âˆ‡ âˆ‡\n",
            "(cid:48)\n",
            "... â€¢\n",
            "Inpractice,(3.50)isonlyanapproximation,anditâ€™sbettertotakesmallersteps. Wedothis\n",
            "byrepeatedlychangingwbyanamountâˆ†w= Î·H\n",
            "âˆ’\n",
            "1 C,whereÎ·isknownasthelearning\n",
            "rate. âˆ’ âˆ‡\n",
            "ThisapproachtominimizingacostfunctionisknownastheHessiantechniqueorHessian\n",
            "optimization. TherearetheoreticalandempiricalresultsshowingthatHessianmethods\n",
            "convergeonaminimuminfewerstepsthanstandardgradientdescent. Inparticular,by\n",
            "incorporating information about second-order changes in the cost function itâ€™s possible\n",
            "for the Hessian approach to avoid many pathologies that can occur in gradient descent. Furthermore,thereareversionsofthebackpropagationalgorithmwhichcanbeusedto\n",
            "computetheHessian. If Hessian optimization is so great, why arenâ€™t we using it in our neural networks? Unfortunately,whileithasmanydesirableproperties,ithasoneveryundesirableproperty:\n",
            "itâ€™sverydifficulttoapplyinpractice.PartoftheproblemisthesheersizeoftheHessianmatrix. Supposeyouhaveaneuralnetworkwith107weightsandbiases. Thenthecorresponding\n",
            "Hessianmatrixwillcontain107 107 =1014entries. Thatâ€™salotofentries!\n",
            "Andthatmakes\n",
            "computingH 1 C extremelydÃ—ifficultinpractice.\n",
            "However,thatdoesnâ€™tmeanthatitâ€™snot\n",
            "âˆ’\n",
            "useful to underâˆ‡stand. In fact, there are many variations on gradient descent which are\n",
            "inspiredbyHessianoptimization,butwhichavoidtheproblemwithoverly-largematrices. Letâ€™stakealookatonesuchtechnique,momentum-basedgradientdescent. Momentum-basedgradientdescent: Intuitively,theadvantageHessianoptimization\n",
            "hasisthatitincorporatesnotjustinformationaboutthegradient,butalsoinformationabout\n",
            "how the gradient is changing. Momentum-based gradient descent is based on a similar\n",
            "intuition,butavoidslargematricesofsecondderivatives. Tounderstandthemomentum\n",
            "technique,thinkbacktoouroriginalpictureofgradientdescent1.5,inwhichweconsidered\n",
            "aballrollingdownintoavalley. Atthetime,weobservedthatgradientdescentis,despite\n",
            "itsname,onlylooselysimilartoaballfallingtothebottomofavalley. Themomentum\n",
            "techniquemodifiesgradientdescentintwowaysthatmakeitmoresimilartothephysical\n",
            "picture. First,itintroducesanotionofâ€œvelocityâ€fortheparametersweâ€™retryingtooptimize. Thegradientactstochangethevelocity,not(directly)theâ€œpositionâ€,inmuchthesame\n",
            "wayasphysicalforceschangethevelocity,andonlyindirectlyaffectposition. Second,the\n",
            "35Strictlyspeaking,forthistobeaminimum,andnotmerelyanextremum,weneedtoassumethat\n",
            "theHessianmatrixispositivedefinite. Intuitively,thismeansthatthefunctionC lookslikeavalley\n",
            "locally,notamountainorasaddle.\n",
            "\n",
            "(cid:12)\n",
            "120 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "momentummethodintroducesakindoffrictionterm,whichtendstograduallyreducethe\n",
            "velocity. Letâ€™s give a more precise mathematical description. We introduce velocity variables\n",
            "v = v 1 ,v 2 ,..., one for each corresponding w j variable36. Then we replace the gradient\n",
            "descentupdaterulew w (cid:48)=w Î· C by\n",
            "â†’ âˆ’ âˆ‡\n",
            "3 v v (cid:48)= Âµv Î· C (3.52)\n",
            "â†’ âˆ’ âˆ‡\n",
            "w w (cid:48)=w+v\n",
            "(cid:48)\n",
            ". (3.53)\n",
            "â†’\n",
            "Intheseequations,Âµisahyper-parameterwhichcontrolstheamountofdampingorfriction\n",
            "inthesystem.\n",
            "Tounderstandthemeaningoftheequationsitâ€™shelpfultofirstconsiderthe\n",
            "casewhereÂµ =1,whichcorrespondstonofriction. Whenthatâ€™sthecase,inspectionof\n",
            "theequationsshowsthattheâ€œforceâ€ C isnowmodifyingthevelocity,v,andthevelocity\n",
            "iscontrollingtherateofchangeofwâˆ‡. Intuitively,webuildupthevelocitybyrepeatedly\n",
            "addinggradienttermstoit. Thatmeansthatifthegradientisin(roughly)thesamedirection\n",
            "throughseveralroundsoflearning,wecanbuildupquiteabitofsteammovinginthat\n",
            "direction. Think,forexample,ofwhathappensifweâ€™removingstraightdownaslope:\n",
            "Witheachstepthevelocitygetslargerdowntheslope,sowemovemoreandmorequickly\n",
            "tothebottomofthevalley. Thiscanenablethemomentumtechniquetoworkmuchfaster\n",
            "thanstandardgradientdescent. Ofcourse,aproblemisthatoncewereachthebottomof\n",
            "thevalleywewillovershoot. Or,ifthegradientshouldchangerapidly,thenwecouldfind\n",
            "ourselvesmovinginthewrongdirection. Thatâ€™sthereasonfortheÂµhyper-parameterin\n",
            "(3.52). IsaidearlierthatÂµcontrolstheamountoffrictioninthesystem;tobealittlemore\n",
            "precise,youshouldthinkof1 Âµastheamountoffrictioninthesystem. WhenÂµ =1,as\n",
            "weâ€™veseen,thereisnofrictionâˆ’,andthevelocityiscompletelydrivenbythegradient C. Bycontrast,whenÂµ =0thereâ€™salotoffriction,thevelocitycanâ€™tbuildup,andEquatiâˆ‡ons\n",
            "(3.52)and(3.53)reducetotheusualequationforgradientdescent,w w (cid:48)=w Î· C. In\n",
            "practice,usingavalueofÂµintermediatebetween0and1cangiveusmâ†’uchoftheâˆ’beâˆ‡nefitof\n",
            "beingabletobuildupspeed,butwithoutcausingovershooting. Wecanchoosesuchavalue\n",
            "forÂµusingtheheld-outvalidationdata,inmuchthesamewayasweselectÎ·andÎ».\n",
            "Iâ€™veavoidednamingthehyper-parameterÂµuptonow. Thereasonisthatthestandard\n",
            "name for Âµ is badly chosen: itâ€™s called the momentum co-efficient. This is potentially\n",
            "36Inaneuralnetthew variableswould,ofcourse,includeallweightsandbiases.\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 121\n",
            "(cid:12)\n",
            "confusing,sinceÂµisnotatallthesameasthenotionofmomentumfromphysics. Rather,it\n",
            "ismuchmorecloselyrelatedtofriction.\n",
            "However,thetermmomentumco-efficientiswidely\n",
            "used,sowewillcontinuetouseit. Anicethingaboutthemomentumtechniqueisthatittakesalmostnoworktomod-\n",
            "ify an implementation of gradient descent to incorporate momentum. We can still use\n",
            "backpropagationtocomputethegradients,justasbefore,anduseideassuchassampling\n",
            "stochasticallychosenmini-batches. Inthisway,wecangetsomeoftheadvantagesofthe 3\n",
            "Hessiantechnique,usinginformationabouthowthegradientischanging. Butitâ€™sdone\n",
            "withoutthedisadvantages,andwithonlyminormodificationstoourcode. Inpractice,the\n",
            "momentumtechniqueiscommonlyused,andoftenspeedsuplearning. Exercise\n",
            "WhatwouldgowrongifweusedÂµ>1inthemomentumtechnique?\n",
            "â€¢ WhatwouldgowrongifweusedÂµ<0inthemomentumtechnique? â€¢\n",
            "Problem\n",
            "Addmomentum-basedstochasticgradientdescenttonetwork2.py. â€¢\n",
            "Otherapproachestominimizingthecostfunction: Manyotherapproachestominimizingthe\n",
            "costfunctionhavebeendeveloped,andthereisnâ€™tuniversalagreementonwhichisthebest\n",
            "approach. Asyougodeeperintoneuralnetworksitâ€™sworthdiggingintotheothertechniques,\n",
            "understandinghowtheywork,theirstrengthsandweaknesses,andhowtoapplythemin\n",
            "practice. ApaperImentionedearlier37introducesandcomparesseveralofthesetechniques,\n",
            "includingconjugategradientdescentandtheBFGSmethod(seealsothecloselyrelated\n",
            "limited-memoryBFGSmethod,knownasL-BFGS).Anothertechniquewhichhasrecently\n",
            "shownpromisingresults38isNesterovâ€™sacceleratedgradienttechnique,whichimproveson\n",
            "themomentumtechnique. However,formanyproblems,plainstochasticgradientdescent\n",
            "workswell,especiallyifmomentumisused,andsoweâ€™llsticktostochasticgradientdescent\n",
            "throughtheremainderofthisbook. Othermodelsofartificialneuron\n",
            "Uptonowweâ€™vebuiltourneuralnetworksusingsigmoidneurons. Inprinciple,anetwork\n",
            "builtfromsigmoidneuronscancomputeanyfunction. Inpractice,however,networksbuilt\n",
            "usingothermodelneuronssometimesoutperformsigmoidnetworks. Dependingonthe\n",
            "application,networksbasedonsuchalternatemodelsmaylearnfaster,generalizebetterto\n",
            "testdata,orperhapsdoboth. Letmementionacoupleofalternatemodelneurons,togive\n",
            "youtheflavorofsomevariationsincommonuse. Perhapsthesimplestvariationisthetanh(pronouncedâ€œtanchâ€)neuron,whichreplaces\n",
            "thesigmoidfunctionbythehyperbolictangentfunction. Theoutputofatanhneuronwith\n",
            "input x,weightvectorw,andbias bisgivenby\n",
            "tanh(w x+b), (3.54)\n",
            "Â·\n",
            "wheretanhis,ofcourse,thehyperbolictangentfunction. Itturnsoutthatthisisveryclosely\n",
            "37EfficientBackProp,byYannLeCun,LÃ©onBottou,GenevieveOrrandKlaus-RobertMÃ¼ller(1998).\n",
            "38See,forexample,Ontheimportanceofinitializationandmomentumindeeplearning,byIlya\n",
            "Sutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "122 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "relatedtothesigmoidneuron. Toseethis,recallthatthetanhfunctionisdefinedby\n",
            "ez e z\n",
            "tanh(z)\n",
            "âˆ’\n",
            "âˆ’ . (3.55)\n",
            "â‰¡\n",
            "ez+e z\n",
            "âˆ’\n",
            "Withalittlealgebraitcaneasilybeverifiedthat\n",
            "3\n",
            "Ïƒ (z)=\n",
            "1+tanh(z/2)\n",
            ", (3.56)\n",
            "2\n",
            "thatis,tanhisjustarescaledversionofthesigmoidfunction. Wecanalsoseegraphically\n",
            "thatthetanhfunctionhasthesameshapeasthesigmoidfunction,\n",
            "tanhfunction\n",
            "1\n",
            "0.5\n",
            "4 2 2 4\n",
            "âˆ’ âˆ’\n",
            "0.5\n",
            "âˆ’\n",
            "1\n",
            "âˆ’\n",
            "Onedifferencebetweentanhneuronsandsigmoidneuronsisthattheoutputfromtanh\n",
            "neuronsrangesfrom 1to1,not0to1. Thismeansthatifyouâ€™regoingtobuildanetwork\n",
            "basedontanhneuronâˆ’syoumayneedtonormalizeyouroutputs(and,dependingonthe\n",
            "detailsoftheapplication,possiblyyourinputs)alittledifferentlythaninsigmoidnetworks. Similartosigmoidneurons,anetworkoftanhneuronscan,inprinciple,computeany\n",
            "function39mappinginputstotherange 1to1. Furthermore,ideassuchasbackpropagation\n",
            "andstochasticgradientdescentareasâˆ’easilyappliedtoanetworkoftanhneuronsastoa\n",
            "networkofsigmoidneurons. Exercise\n",
            "ProvetheidentityinEquation(3.56). Whicâ€¢htypeofneuronshouldyouuseinyournetworks,thetanhorsigmoid?\n",
            "Apriorithe\n",
            "answerisnotobvious,toputitmildly! However,therearetheoreticalargumentsandsome\n",
            "empiricalevidencetosuggestthatthetanhsometimesperformsbetter40. Letmebriefly\n",
            "giveyoutheflavorofoneofthetheoreticalargumentsfortanhneurons. Supposeweâ€™re\n",
            "usingsigmoidneurons, soallactivationsinournetworkarepositive. Letâ€™sconsiderthe\n",
            "weightswl\n",
            "j\n",
            "+\n",
            "k\n",
            "1inputtothe j-thneuroninthe(l+1)-thlayer. Therulesforbackpropagation\n",
            "tellusthattheassociatedgradientwillbealÎ´l+1. Becausetheactivationsarepositivethe\n",
            "k j\n",
            "39Therearesometechnicalcaveatstothisstatementforbothtanhandsigmoidneurons,aswellas\n",
            "fortherectifiedlinearneuronsdiscussedbelow.However,informallyitâ€™susuallyfinetothinkofneural\n",
            "networksasbeingabletoapproximateanyfunctiontoarbitraryaccuracy. 40See,forexample,EfficientBackProp,byYannLeCun,LÃ©onBottou,GenevieveOrrandKlaus-Robert\n",
            "MÃ¼ller(1998),andUnderstandingthedifficultyoftrainingdeepfeedforwardnetworks,byXavierGlorot\n",
            "andYoshuaBengio(2010).\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 123\n",
            "(cid:12)\n",
            "signofthisgradientwillbethesameasthesignofÎ´l+1. WhatthismeansisthatifÎ´l+1\n",
            "j j\n",
            "ispositivethenalltheweightswl+1willdecreaseduringgradientdescent,whileifÎ´l+1is\n",
            "jk j\n",
            "negativethenalltheweightswl+1 willincreaseduringgradientdescent. Inotherwords,\n",
            "jk\n",
            "allweightstothesameneuronmusteitherincreasetogetherordecreasetogether. Thatâ€™s\n",
            "aproblem,sincesomeoftheweightsmayneedtoincreasewhileothersneedtodecrease. Thatcanonlyhappenifsomeoftheinputactivationshavedifferentsigns. Thatsuggests\n",
            "replacingthesigmoidbyanactivationfunction,suchastanh,whichallowsbothpositiveand 3\n",
            "negativeactivations. Indeed,becausetanhissymmetricaboutzero,tanh( z)= tanh(z),\n",
            "wemightevenexpectthat, roughlyspeaking, theactivationsinhiddenlâˆ’ayersâˆ’wouldbe\n",
            "equallybalancedbetweenpositiveandnegative. Thatwouldhelpensurethatthereisno\n",
            "systematicbiasfortheweightupdatestobeonewayortheother.\n",
            "Howseriouslyshouldwetakethisargument? Whiletheargumentissuggestive,itâ€™sa\n",
            "heuristic,notarigorousproofthattanhneuronsoutperformsigmoidneurons. Perhapsthere\n",
            "areotherpropertiesofthesigmoidneuronwhichcompensateforthisproblem? Indeed,\n",
            "formanytasksthetanhisfoundempiricallytoprovideonlyasmallornoimprovementin\n",
            "performanceoversigmoidneurons. Unfortunately,wedonâ€™tyethavehard-and-fastrulesto\n",
            "knowwhichneurontypeswilllearnfastest,orgivethebestgeneralizationperformance,for\n",
            "anyparticularapplication. Anothervariationonthesigmoidneuronistherectifiedlinearneuronorrectifiedlinear\n",
            "unit. Theoutputofarectifiedlinearunitwithinput x,weightvectorw,andbias bisgiven\n",
            "by\n",
            "max(0,w x+b). (3.57)\n",
            "Â·\n",
            "Graphically,therectifyingfunctionmax(0,z)lookslikethis:\n",
            "max(0,z)\n",
            "4\n",
            "2\n",
            "z\n",
            "0\n",
            "4 2 0 2 4\n",
            "âˆ’ âˆ’\n",
            "2\n",
            "âˆ’\n",
            "4\n",
            "âˆ’\n",
            "Obviouslysuchneuronsarequitedifferentfrombothsigmoidandtanhneurons. However,\n",
            "likethesigmoidandtanhneurons,rectifiedlinearunitscanbeusedtocomputeanyfunction,\n",
            "andtheycanbetrainedusingideassuchasbackpropagationandstochasticgradientdescent. Whenshouldyouuserectifiedlinearunitsinsteadofsigmoidortanhneurons?\n",
            "Some\n",
            "\n",
            "(cid:12)\n",
            "124 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "recentworkonimagerecognition41hasfoundconsiderablebenefitinusingrectifiedlinear\n",
            "unitsthroughmuchofthenetwork. However,aswithtanhneurons,wedonotyethavea\n",
            "reallydeepunderstandingofwhen,exactly,rectifiedlinearunitsarepreferable,norwhy. To\n",
            "giveyoutheflavorofsomeoftheissues,recallthatsigmoidneuronsstoplearningwhen\n",
            "theysaturate,i.e.,whentheiroutputisneareither0or1. Asweâ€™veseenrepeatedlyinthis\n",
            "chapter,theproblemisthatÏƒ termsreducethegradient,andthatslowsdownlearning. (cid:48)\n",
            "3 Tanhneuronssufferfromasimilarproblemwhentheysaturate. Bycontrast,increasing\n",
            "theweightedinputtoarectifiedlinearunitwillnevercauseittosaturate,andsothere\n",
            "isnocorrespondinglearningslowdown. Ontheotherhand,whentheweightedinputto\n",
            "arectifiedlinearunitisnegative,thegradientvanishes,andsotheneuronstopslearning\n",
            "entirely. Thesearejusttwoofthemanyissuesthatmakeitnon-trivialtounderstandwhen\n",
            "andwhyrectifiedlinearunitsperformbetterthansigmoidortanhneurons. Iâ€™vepaintedapictureofuncertaintyhere,stressingthatwedonotyethaveasolidtheory\n",
            "ofhowactivationfunctionsshouldbechosen. Indeed,theproblemishardereventhanI\n",
            "havedescribed, forthereareinfinitelymanypossibleactivationfunctions. Whichisthe\n",
            "bestforanygivenproblem? Whichwillresultinanetworkwhichlearnsfastest? Which\n",
            "willgivethehighesttestaccuracies? Iamsurprisedhowlittlereallydeepandsystematic\n",
            "investigationhasbeendoneofthesequestions. Ideally,weâ€™dhaveatheorywhichtellsus,in\n",
            "detail,howtochoose(andperhapsmodify-on-the-fly)ouractivationfunctions. Ontheother\n",
            "hand,weshouldnâ€™tletthelackofafulltheorystopus!\n",
            "Wehavepowerfultoolsalreadyat\n",
            "hand,andcanmakealotofprogresswiththosetools. Throughtheremainderofthisbook\n",
            "Iâ€™llcontinuetousesigmoidneuronsasourgo-toneuron,sincetheyâ€™repowerfulandprovide\n",
            "concreteillustrationsofthecoreideasaboutneuralnets. Butkeepinthebackofyourmind\n",
            "thatthesesameideascanbeappliedtoothertypesofneuron,andthattherearesometimes\n",
            "advantagesindoingso. Onstoriesinneuralnetworks\n",
            "Question:Howdoyouapproachutilizingandresearchingmachinelearning\n",
            "techniques that are supported almost entirely empirically, as opposed to\n",
            "mathematically? Alsoinwhatsituationshaveyounoticedsomeofthese\n",
            "techniquesfail?\n",
            "Answer: You have to realize that our theoretical tools are very weak. Sometimes,wehavegoodmathematicalintuitionsforwhyaparticular\n",
            "techniqueshouldwork. Sometimesourintuitionendsupbeingwrong[...]\n",
            "Thequestionsbecome: howwelldoesmymethodworkonthisparticular\n",
            "problem,andhowlargeisthesetofproblemsonwhichitworkswell. â€”QuestionandanswerwithneuralnetworksresearcherYannLeCun\n",
            "Once,attendingaconferenceonthefoundationsofquantummechanics,Inoticedwhat\n",
            "seemedtomeamostcuriousverbalhabit: whentalksfinished,questionsfromtheaudience\n",
            "41See,forexample,WhatistheBestMulti-StageArchitectureforObjectRecognition?,byKevinJarrett,\n",
            "KorayKavukcuoglu,Marcâ€™AurelioRanzatoandYannLeCun(2009),,byXavierGlorot,AntoineBordes,\n",
            "andYoshuaBengio(2011),andImageNetClassificationwithDeepConvolutionalNeuralNetworks,by\n",
            "AlexKrizhevsky,IlyaSutskever,andGeoffreyHinton(2012).Notethatthesepapersfillinimportant\n",
            "detailsabouthowtosetuptheoutputlayer,costfunction,andregularizationinnetworksusingrectified\n",
            "linearunits. Iâ€™veglossedoverallthesedetailsinthisbriefaccount.\n",
            "Thepapersalsodiscussinmore\n",
            "detailthebenefitsanddrawbacksofusingrectifiedlinearunits.AnotherinformativepaperisRectified\n",
            "LinearUnitsImproveRestrictedBoltzmannMachines,byVinodNairandGeoffreyHinton(2010),which\n",
            "demonstratesthebenefitsofusingrectifiedlinearunitsinasomewhatdifferentapproachtoneural\n",
            "networks. \n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 125\n",
            "(cid:12)\n",
            "oftenbeganwithâ€œIâ€™mverysympathetictoyourpointofview,but[...]â€. Quantumfoundations\n",
            "wasnotmyusualfield,andInoticedthisstyleofquestioningbecauseatotherscientific\n",
            "conferencesIâ€™drarelyorneverheardaquestionerexpresstheirsympathyforthepointof\n",
            "viewofthespeaker. Atthetime,Ithoughttheprevalenceofthequestionsuggestedthat\n",
            "littlegenuineprogresswasbeingmadeinquantumfoundations,andpeopleweremerely\n",
            "spinningtheirwheels. Later,Irealizedthatassessmentwastooharsh. Thespeakerswere\n",
            "wrestlingwithsomeofthehardestproblemshumanmindshaveeverconfronted. Ofcourse 3\n",
            "progresswasslow! Buttherewasstillvalueinhearingupdatesonhowpeoplewerethinking,\n",
            "eveniftheydidnâ€™talwayshaveunarguablenewprogresstoreport.\n",
            "Youmayhavenoticedaverbalticsimilartoâ€œIâ€™mverysympathetic[...]â€ inthecurrent\n",
            "book. Toexplainwhatweâ€™reseeingIâ€™veoftenfallenbackonsayingâ€œHeuristically,[...]â€,or\n",
            "â€œRoughlyspeaking,[...]â€,followingupwithastorytoexplainsomephenomenonorother. Thesestoriesareplausible,buttheempiricalevidenceIâ€™vepresentedhasoftenbeenpretty\n",
            "thin. Ifyoulookthroughtheresearchliteratureyouâ€™llseethatstoriesinasimilarstyleappear\n",
            "inmanyresearchpapersonneuralnets,oftenwiththinsupportingevidence. Whatshould\n",
            "wethinkaboutsuchstories? Inmanypartsofscienceâ€“especiallythosepartsthatdealwithsimplephenomenaâ€“itâ€™s\n",
            "possibletoobtainverysolid,veryreliableevidenceforquitegeneralhypotheses. Butin\n",
            "neuralnetworkstherearelargenumbersofparametersandhyper-parameters,andextremely\n",
            "complexinteractionsbetweenthem. Insuchextraordinarilycomplexsystemsitâ€™sexceedingly\n",
            "difficulttoestablishreliablegeneralstatements. Understandingneuralnetworksintheirfull\n",
            "generalityisaproblemthat,likequantumfoundations,teststhelimitsofthehumanmind. Instead,weoftenmakedowithevidencefororagainstafewspecificinstancesofageneral\n",
            "statement. Asaresultthosestatementssometimeslaterneedtobemodifiedorabandoned,\n",
            "whennewevidencecomestolight.\n",
            "Onewayofviewingthissituationisthatanyheuristicstoryaboutneuralnetworkscarries\n",
            "withitanimpliedchallenge. Forexample,considerthestatementIquotedearlier,explaining\n",
            "whydropoutworks42: â€œThistechniquereducescomplexco-adaptationsofneurons,sincea\n",
            "neuroncannotrelyonthepresenceofparticularotherneurons. Itis,therefore,forcedto\n",
            "learnmorerobustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsets\n",
            "oftheotherneurons.â€ Thisisarich,provocativestatement,andonecouldbuildafruitful\n",
            "researchprogramentirelyaroundunpackingthestatement,figuringoutwhatinitistrue,\n",
            "whatisfalse,whatneedsvariationandrefinement. Indeed,thereisnowasmallindustryof\n",
            "researcherswhoareinvestigatingdropout(andmanyvariations),tryingtounderstandhow\n",
            "itworks,andwhatitslimitsare. Andsoitgoeswithmanyoftheheuristicsweâ€™vediscussed. Eachheuristicisnotjusta(potential)explanation,itâ€™salsoachallengetoinvestigateand\n",
            "understandinmoredetail. Of course, there is not time for any single person to investigate all these heuristic\n",
            "explanationsindepth. Itâ€™sgoingtotakedecades(orlonger)forthecommunityofneural\n",
            "networksresearcherstodevelopareallypowerful,evidence-basedtheoryofhowneural\n",
            "networkslearn. Doesthismeanyoushouldrejectheuristicexplanationsasunrigorous,and\n",
            "notsufficientlyevidence-based?\n",
            "No! Infact,weneedsuchheuristicstoinspireandguide\n",
            "ourthinking. Itâ€™slikethegreatageofexploration: theearlyexplorerssometimesexplored\n",
            "(andmadenewdiscoveries)onthebasisofbeliefswhichwerewronginimportantways.\n",
            "Later,thosemistakeswerecorrectedaswefilledinourknowledgeofgeography. Whenyou\n",
            "42FromImageNetClassificationwithDeepConvolutionalNeuralNetworks,byAlexKrizhevsky,Ilya\n",
            "Sutskever,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "126 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "understandsomethingpoorlyâ€“astheexplorersunderstoodgeography,andasweunderstand\n",
            "neuralnetstodayâ€“itâ€™smoreimportanttoexploreboldlythanitistoberigorouslycorrect\n",
            "ineverystepofyourthinking. Andsoyoushouldviewthesestoriesasausefulguideto\n",
            "howtothinkaboutneuralnets,whileretainingahealthyawarenessofthelimitationsof\n",
            "suchstories,andcarefullykeepingtrackofjusthowstrongtheevidenceisforanygivenline\n",
            "ofreasoning. Putanotherway,weneedgoodstoriestohelpmotivateandinspireus,and\n",
            "3 rigorousin-depthinvestigationinordertouncovertherealfactsofthematter.\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 127\n",
            "(cid:12)\n",
            "4444\n",
            "A visual proof that neural nets\n",
            "can compute any function\n",
            "4\n",
            "Oneofthemoststrikingfactsaboutneuralnetworksisthattheycancomputeanyfunction\n",
            "atall. Thatis,supposesomeonehandsyousomecomplicated,wigglyfunction, f(x):\n",
            "f(x)\n",
            "Nomatterwhatthefunction,thereisguaranteedtobeaneuralnetworksothatforevery\n",
            "possibleinput, x,thevalue f(x)(orsomecloseapproximation)isoutputfromthenetwork,\n",
            "e.g.:\n",
            "\n",
            "(cid:12)\n",
            "128 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4 Thisresultholdsevenifthefunctionhasmanyinputs, f = f(x 1 ,...,x m),andmanyoutputs. Forinstance,hereâ€™sanetworkcomputingafunctionwithm=3inputsandn=2outputs:\n",
            "Thisresulttellsusthatneuralnetworkshaveakindofuniversality.\n",
            "Nomatterwhatfunction\n",
            "wewanttocompute,weknowthatthereisaneuralnetworkwhichcandothejob. Whatâ€™smore,thisuniversalitytheoremholdsevenifwerestrictournetworkstohave\n",
            "justasinglelayerintermediatebetweentheinputandtheoutputneuronsâ€“aso-calledsingle\n",
            "hiddenlayer. Soevenverysimplenetworkarchitecturescanbeextremelypowerful. Theuniversalitytheoremiswellknownbypeoplewhouseneuralnetworks. Butwhyitâ€™s\n",
            "trueisnotsowidelyunderstood. Mostoftheexplanationsavailablearequitetechnical. For\n",
            "instance,oneoftheoriginalpapersprovingtheresult1didsousingtheHahn-Banachtheorem,\n",
            "theRieszRepresentationtheorem,andsomeFourieranalysis. Ifyouâ€™reamathematicianthe\n",
            "argumentisnotdifficulttofollow,butitâ€™snotsoeasyformostpeople. Thatâ€™sapity,since\n",
            "theunderlyingreasonsforuniversalityaresimpleandbeautiful. InthischapterIgiveasimpleandmostlyvisualexplanationoftheuniversalitytheorem.\n",
            "Weâ€™llgostepbystepthroughtheunderlyingideas. Youâ€™llunderstandwhyitâ€™struethatneural\n",
            "networkscancomputeanyfunction. Youâ€™llunderstandsomeofthelimitationsoftheresult.\n",
            "Andyouâ€™llunderstandhowtheresultrelatestodeepneuralnetworks. Tofollowthematerialinthechapter, youdonotneedtohavereadearlierchapters\n",
            "inthisbook. Instead, thechapterisstructuredtobeenjoyableasaself-containedessay. Providedyouhavejustalittlebasicfamiliaritywithneuralnetworks,youshouldbeableto\n",
            "followtheexplanation. Iwill,however,provideoccasionallinkstoearliermaterial,tohelp\n",
            "fillinanygapsinyourknowledge. 1Approximationbysuperpositionsofasigmoidalfunction,byGeorgeCybenko(1989).Theresult\n",
            "wasverymuchintheairatthetime,andseveralgroupsprovedcloselyrelatedresults. Cybenkoâ€™s\n",
            "papercontainsausefuldiscussionofmuchofthatwork.AnotherimportantearlypaperisMultilayer\n",
            "feedforwardnetworksareuniversalapproximators,byKurtHornik,MaxwellStinchcombe,andHalbert\n",
            "White(1989).ThispaperusestheStone-Weierstrasstheoremtoarriveatsimilarresults. \n",
            "(cid:12)\n",
            "4.1. Twocaveats (cid:12) 129\n",
            "(cid:12)\n",
            "Universality theorems are a commonplace in computer science, so much so that we\n",
            "sometimesforgethowastonishingtheyare. Butitâ€™sworthremindingourselves: theabilityto\n",
            "computeanarbitraryfunctionistrulyremarkable.\n",
            "Almostanyprocessyoucanimaginecan\n",
            "bethoughtofasfunctioncomputation. Considertheproblemofnamingapieceofmusic\n",
            "basedonashortsampleofthepiece. Thatcanbethoughtofascomputingafunction.\n",
            "Or\n",
            "considertheproblemoftranslatingaChinesetextintoEnglish. Again,thatcanbethought\n",
            "of as computing a function2. Or consider the problem of taking an mp4 movie file and\n",
            "generatingadescriptionoftheplotofthemovie,andadiscussionofthequalityoftheacting. Again,thatcanbethoughtofasakindoffunctioncomputation3Universalitymeansthat,in\n",
            "principle,neuralnetworkscandoallthesethingsandmanymore. 4\n",
            "Ofcourse,justbecauseweknowaneuralnetworkexiststhatcan(say)translateChinese\n",
            "text into English, that doesnâ€™t mean we have good techniques for constructing or even\n",
            "recognizingsuchanetwork. Thislimitationappliesalsototraditionaluniversalitytheorems\n",
            "formodelssuchasBooleancircuits. But,asweâ€™veseenearlierinthebook,neuralnetworks\n",
            "havepowerfulalgorithmsforlearningfunctions. Thatcombinationoflearningalgorithms+\n",
            "universalityisanattractivemix. Uptonow,thebookhasfocusedonthelearningalgorithms.\n",
            "Inthischapter,wefocusonuniversality,andwhatitmeans. 4.1 Two caveats\n",
            "Beforeexplainingwhytheuniversalitytheoremistrue,Iwanttomentiontwocaveatstothe\n",
            "informalstatementâ€œaneuralnetworkcancomputeanyfunctionâ€. First,thisdoesnâ€™tmeanthatanetworkcanbeusedtoexactlycomputeanyfunction. Rather,wecangetanapproximationthatisasgoodaswewant. Byincreasingthenumberof\n",
            "hiddenneuronswecanimprovetheapproximation. Forinstance,earlier(see4)Iillustrated\n",
            "anetworkcomputingsomefunction f(x)usingthreehiddenneurons. Formostfunctions\n",
            "onlyalow-qualityapproximationwillbepossibleusingthreehiddenneurons. Byincreasing\n",
            "thenumberofhiddenneurons(say,tofive)wecantypicallygetabetterapproximation:\n",
            "Andwecandostillbetterbyfurtherincreasingthenumberofhiddenneurons. 2Actually,computingoneofmanyfunctions,sincethereareoftenmanyacceptabletranslationsofa\n",
            "givenpieceoftext. 3Dittotheremarkabouttranslationandtherebeingmanypossiblefunctions.. \n",
            "(cid:12)\n",
            "130 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Tomakethisstatementmoreprecise,supposeweâ€™regivenafunction f(x)whichweâ€™d\n",
            "liketocomputetowithinsomedesiredaccuracyÎµ>0. Theguaranteeisthatbyusing\n",
            "enoughhiddenneuronswecanalwaysfindaneuralnetworkwhoseoutput g(x)satisfies\n",
            "g(x) f(x) <Îµ,forallinputsx. Inotherwords,theapproximationwillbegoodtowithin\n",
            "|thedeâˆ’sireda|ccuracyforeverypossibleinput. Thesecondcaveatisthattheclassoffunctionswhichcanbeapproximatedintheway\n",
            "describedarethecontinuousfunctions. Ifafunctionisdiscontinuous,i.e.,makessudden,\n",
            "sharpjumps,thenitwonâ€™tingeneralbepossibletoapproximateusinganeuralnet. This\n",
            "isnotsurprising,sinceourneuralnetworkscomputecontinuousfunctionsoftheirinput. 4 However,evenifthefunctionweâ€™dreallyliketocomputeisdiscontinuous,itâ€™softenthe\n",
            "casethatacontinuousapproximationisgoodenough. Ifthatâ€™sso,thenwecanuseaneural\n",
            "network.\n",
            "Inpractice,thisisnotusuallyanimportantlimitation. Summingup,amoreprecisestatementoftheuniversalitytheoremisthatneuralnetworks\n",
            "withasinglehiddenlayercanbeusedtoapproximateanycontinuousfunctiontoanydesired\n",
            "precision. Inthischapterweâ€™llactuallyproveaslightlyweakerversionofthisresult,using\n",
            "twohiddenlayersinsteadofone.\n",
            "IntheproblemsIâ€™llbrieflyoutlinehowtheexplanation\n",
            "can,withafewtweaks,beadaptedtogiveaproofwhichusesonlyasinglehiddenlayer. 4.2 Universality with one input and one output\n",
            "Tounderstandwhytheuniversalitytheoremistrue, letâ€™sstartbyunderstandinghowto\n",
            "construct a neural network which approximates a function with just one input and one\n",
            "output:\n",
            "f(x)\n",
            "It turns out that this is the core of the problem of universality. Once weâ€™ve understood\n",
            "thisspecialcaseitâ€™sactuallyprettyeasytoextendtofunctionswithmanyinputsandmany\n",
            "outputs. Tobuildinsightintohowtoconstructanetworktocompute f,letâ€™sstartwithanetwork\n",
            "containingjustasinglehiddenlayer,withtwohiddenneurons,andanoutputlayercontaining\n",
            "asingleoutputneuron:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 131\n",
            "(cid:12)\n",
            "4\n",
            "Togetafeelforhowcomponentsinthenetworkwork,letâ€™sfocusonthetophiddenneuron. Inthediagrambelow,clickontheweight,w,anddragthemousealittlewaystotherightto\n",
            "increasew. Youcanimmediatelyseehowthefunctioncomputedbythetophiddenneuron\n",
            "changes:\n",
            "Outputfromneuron\n",
            "1 w=7,b= 2\n",
            "w=7,b= âˆ’3\n",
            "w=7,b= âˆ’4\n",
            "w=7,b= âˆ’5\n",
            "w=7,b= âˆ’6\n",
            "âˆ’\n",
            "0 1\n",
            "Outputfromneuron\n",
            "1 w=9,b= 4\n",
            "w=8,b= âˆ’4\n",
            "w=7,b= âˆ’4\n",
            "w=6,b= âˆ’4\n",
            "w=5,b= âˆ’4\n",
            "âˆ’\n",
            "0 1\n",
            "Aswelearntearlierinthebook,whatâ€™sbeingcomputedbythehiddenneuronisÏƒ (wx+b),\n",
            "whereÏƒ (z) 1/ (1+e\n",
            "âˆ’\n",
            "z )isthesigmoidfunction. Uptonow,weâ€™vemadefrequentuseofthis\n",
            "algebraicforâ‰¡m.\n",
            "Butfortheproofofuniversalitywewillobtainmoreinsightbyignoringthe\n",
            "algebraentirely,andinsteadmanipulatingandobservingtheshapeshowninthegraph. This\n",
            "\n",
            "(cid:12)\n",
            "132 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "wonâ€™tjustgiveusabetterfeelforwhatâ€™sgoingon,itwillalsogiveusaproof4ofuniversality\n",
            "thatappliestoactivationfunctionsotherthanthesigmoidfunction. Wecansimplifyour\n",
            "analysisquiteabitbyincreasingtheweightsomuchthattheoutputreallyisastepfunction,\n",
            "toaverygoodapproximation. BelowIâ€™veplottedtheoutputfromthetophiddenneuron\n",
            "whentheweightisw=999. Outputfromtophiddenneuron\n",
            "4\n",
            "1\n",
            "x\n",
            "1\n",
            "Itâ€™sactuallyquiteabiteasiertoworkwithstepfunctionsthangeneralsigmoidfunctions. Thereasonisthatintheoutputlayerweaddupcontributionsfromallthehiddenneurons. Itâ€™seasytoanalyzethesumofabunchofstepfunctions,butrathermoredifficulttoreason\n",
            "aboutwhathappenswhenyouaddupabunchofsigmoidshapedcurves. Andsoitmakes\n",
            "thingsmucheasiertoassumethatourhiddenneuronsareoutputtingstepfunctions. More\n",
            "concretely,wedothisbyfixingtheweightwtobesomeverylargevalue,andthensettingthe\n",
            "positionofthestepbymodifyingthebias. Ofcourse,treatingtheoutputasastepfunction\n",
            "isanapproximation,butitâ€™saverygoodapproximation,andfornowweâ€™lltreatitasexact. Iâ€™llcomebacklatertodiscusstheimpactofdeviationsfromthisapproximation. Atwhatvalueof x doesthestepoccur? Putanotherway,howdoesthepositionofthe\n",
            "stepdependupontheweightandbias? Toanswerthisquestion,trymodifyingtheweightandbiasinthediagramabove(you\n",
            "mayneedtoscrollbackabit). Canyoufigureouthowthepositionofthestepdependsonw\n",
            "and b?\n",
            "Withalittleworkyoushouldbeabletoconvinceyourselfthatthepositionofthe\n",
            "stepisproportionalto b,andinverselyproportionaltow. Infact,thestepisatpositions= b/w,asyoucanseebymodifyingtheweightand\n",
            "biasinthefollowingdiagram: âˆ’\n",
            "4Strictlyspeaking,thevisualapproachIâ€™mtakingisnâ€™twhatâ€™straditionallythoughtofasaproof.But\n",
            "Ibelievethevisualapproachgivesmoreinsightintowhytheresultistruethanatraditionalproof.And,\n",
            "ofcourse,thatkindofinsightistherealpurposebehindaproof.Occasionally,therewillbesmallgapsin\n",
            "thereasoningIpresent:placeswhereImakeavisualargumentthatisplausible,butnotquiterigorous. Ifthisbothersyou,thenconsideritachallengetofillinthemissingsteps.Butdonâ€™tlosesightofthe\n",
            "realpurpose:tounderstandwhytheuniversalitytheoremistrue.\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 133\n",
            "(cid:12)\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "b/w=0.4 1\n",
            "4\n",
            "âˆ’\n",
            "Itwillgreatlysimplifyourlivestodescribehiddenneuronsusingjustasingleparameter,s,\n",
            "whichisthestepposition,s= b/w. Trymodifyingsinthefollowingdiagram,inorderto\n",
            "getusedtothenewparameterâˆ’ization:\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "s 1\n",
            "Asnotedabove,weâ€™veimplicitlysettheweightwontheinputtobesomelargevalueâ€“big\n",
            "enoughthatthestepfunctionisaverygoodapproximation. Wecaneasilyconvertaneuron\n",
            "parameterizedinthiswaybackintotheconventionalmodel,bychoosingthebias b= ws. âˆ’\n",
            "Uptonowweâ€™vebeenfocusingontheoutputfromjustthetophiddenneuron.\n",
            "Letâ€™stake\n",
            "alookatthebehavioroftheentirenetwork. Inparticular,weâ€™llsupposethehiddenneurons\n",
            "arecomputingstepfunctionsparameterizedbysteppointss (topneuron)ands (bottom\n",
            "1 2\n",
            "neuron). Andtheyâ€™llhaverespectiveoutputweightsw andw . Hereâ€™sthenetwork:\n",
            "1 2\n",
            "Weightedoutputfromhiddenlayer\n",
            "w 1+w\n",
            "2\n",
            "1\n",
            "w\n",
            "1\n",
            "x\n",
            "s 1 s 2 1\n",
            "Whatâ€™sbeingplottedontherightistheweightedoutputw\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a\n",
            "2\n",
            "fromthehiddenlayer. \n",
            "(cid:12)\n",
            "134 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Here, a and a aretheoutputsfromthetopandbottomhiddenneurons, respectively5. 1 2\n",
            "Theseoutputsaredenotedwithasbecausetheyâ€™reoftenknownastheneuronsâ€™activations. Tryincreasinganddecreasingthesteppoints ofthetophiddenneuron. Getafeel\n",
            "1\n",
            "forhowthischangestheweightedoutputfromthehiddenlayer. Itâ€™sparticularlyworth\n",
            "understandingwhathappenswhens goespasts . Youâ€™llseethatthegraphchangesshape\n",
            "1 2\n",
            "whenthishappens,sincewehavemovedfromasituationwherethetophiddenneuron\n",
            "isthefirsttobeactivatedtoasituationwherethebottomhiddenneuronisthefirsttobe\n",
            "activated. Similarly,trymanipulatingthesteppoints ofthebottomhiddenneuron,andgetafeel\n",
            "2\n",
            "forhowthischangesthecombinedoutputfromthehiddenneurons. 4\n",
            "Tryincreasinganddecreasingeachoftheoutputweights. Noticehowthisrescalesthe\n",
            "contributionfromtherespectivehiddenneurons. Whathappenswhenoneoftheweightsis\n",
            "zero?\n",
            "Finally,trysettingw tobe0.8andw tobe-0.8. Yougetaâ€œbumpâ€function,which\n",
            "1 2\n",
            "startsatpoints ,endsatpoints ,andhasheight0.8. Forinstance,theweightedoutput\n",
            "1 2\n",
            "mightlooklikethis:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "w 1= w 2\n",
            "âˆ’\n",
            "x\n",
            "s 1 s 2 1\n",
            "Ofcourse,wecanrescalethebumptohaveanyheightatall. Letâ€™suseasingleparameter,h,\n",
            "todenotetheheight. ToreduceclutterIâ€™llalsoremovetheâ€œs 1=...â€andâ€œw 1=...â€notations. Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "x\n",
            "s 1 s 2 1\n",
            "1\n",
            "âˆ’\n",
            "Trychangingthevalueofhupanddown,toseehowtheheightofthebumpchanges. Try\n",
            "changingtheheightsoitâ€™snegative,andobservewhathappens.\n",
            "Andtrychangingthestep\n",
            "pointstoseehowthatchangestheshapeofthebump. 5Note,bytheway,thattheoutputfromthewholenetworkisÏƒ (w\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a 2+b),wherebisthe\n",
            "biasontheoutputneuron.Obviously,thisisnâ€™tthesameastheweightedoutputfromthehiddenlayer,\n",
            "whichiswhatweâ€™replottinghere.Weâ€™regoingtofocusontheweightedoutputfromthehiddenlayer\n",
            "rightnow,andonlylaterwillwethinkabouthowthatrelatestotheoutputfromthewholenetwork. \n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 135\n",
            "(cid:12)\n",
            "Youâ€™llnotice,bytheway,thatweâ€™reusingourneuronsinawaythatcanbethought\n",
            "ofnotjustingraphicalterms,butinmoreconventionalprogrammingterms,asakindof\n",
            "if-then-elsestatement,e.g.:\n",
            "if input >= step point:\n",
            "add 1 to the weighted output\n",
            "else:\n",
            "add 0 to the weighted output\n",
            "4\n",
            "ForthemostpartIâ€™mgoingtostickwiththegraphicalpointofview. Butinwhatfollowsyou\n",
            "maysometimesfindithelpfultoswitchpointsofview,andthinkaboutthingsintermsof\n",
            "if-then-else. Wecanuseourbump-makingtricktogettwobumps,bygluingtwopairsofhidden\n",
            "neuronstogetherintothesamenetwork:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "2\n",
            "0\n",
            "1\n",
            "âˆ’h\n",
            "1\n",
            "0 s1 s1s2 s21\n",
            "1 2 1 2\n",
            "Iâ€™vesuppressedtheweightshere,simplywritingthehvaluesforeachpairofhiddenneurons. Tryincreasinganddecreasingbothhvalues,andobservehowitchangesthegraph.\n",
            "Move\n",
            "thebumpsaroundbychangingthesteppoints. Moregenerally,wecanusethisideatogetasmanypeaksaswewant,ofanyheight. In\n",
            "particular,wecandividetheinterval[0,1]upintoalargenumber,N,ofsubintervals,and\n",
            "useN pairsofhiddenneuronstosetuppeaksofanydesiredheight. Letâ€™sseehowthisworks\n",
            "forN=5. Thatâ€™squiteafewneurons,soIâ€™mgoingtopackthingsinabit. Apologiesforthe\n",
            "complexityofthediagram: Icouldhidethecomplexitybyabstractingawayfurther,butI\n",
            "thinkitâ€™sworthputtingupwithalittlecomplexity,forthesakeofgettingamoreconcrete\n",
            "feelforhowthesenetworkswork. \n",
            "(cid:12)\n",
            "136 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Weightedoutput\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "1\n",
            "âˆ’\n",
            "Youcanseethattherearefivepairsofhiddenneurons. Thesteppointsfortherespective\n",
            "pairsofneuronsare0,1/5,then1/5,2/5,andsoon,outto4/5,5/5. Thesevaluesarefixed\n",
            "â€“theymakeitsowegetfiveevenlyspacedbumpsonthegraph.\n",
            "Eachpairofneuronshasavalueofhassociatedtoit. Remember,theconnectionsoutput\n",
            "fromtheneuronshaveweightshand h(notmarked). Clickononeofthehvalues,and\n",
            "dragthemousetotherightorlefttocâˆ’hangethevalue. Asyoudoso,watchthefunction\n",
            "change. Bychangingtheoutputweightsweâ€™reactuallydesigningthefunction! Contrariwise,tryclickingonthegraph,anddraggingupordowntochangetheheight\n",
            "ofanyofthebumpfunctions. Asyouchangetheheights,youcanseethecorresponding\n",
            "changeinhvalues. And,althoughitâ€™snotshown,thereisalsoachangeinthecorresponding\n",
            "outputweights,whichare+hand h. âˆ’\n",
            "Inotherwords,wecandirectlymanipulatethefunctionappearinginthegraphonthe\n",
            "right,andseethatreflectedinthehvaluesontheleft. Afunthingtodoistoholdthemouse\n",
            "buttondownanddragthemousefromonesideofthegraphtotheother.\n",
            "Asyoudothisyou\n",
            "drawoutafunction,andgettowatchtheparametersintheneuralnetworkadapt. Timeforachallenge. Letâ€™sthinkbacktothefunctionIplottedatthebeginningofthechapter:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 137\n",
            "(cid:12)\n",
            "f(x)\n",
            "4\n",
            "Ididnâ€™tsayitatthetime,butwhatIplottedisactuallythefunction\n",
            "f(x)=0.2+0.4x2 +0.3xsin(15x)+0.05cos(50x), (4.1)\n",
            "plottedover x from0to1,andwiththe y axistakingvaluesfrom0to1. Thatâ€™sobviouslynotatrivialfunction.\n",
            "Youâ€™regoingtofigureouthowtocomputeitusinganeuralnetwork. (cid:80)\n",
            "Inournetworksaboveweâ€™vebeenanalyzingtheweightedcombination w a output\n",
            "j j j\n",
            "fromthehiddenneurons. Wenowknowhowtogetalotofcontroloverthisquantity. But,\n",
            "asInotedearlier,thisquantityisnotwhatâ€™soutputfromthenetwork. Whatâ€™soutputfrom\n",
            "thenetworkisÏƒ ( (cid:80) j w j a j+b)where bisthebiasontheoutputneuron. Istheresomeway\n",
            "wecanachievecontrolovertheactualoutputfromthenetwork? Thesolutionistodesignaneuralnetworkwhosehiddenlayerhasaweightedoutput\n",
            "givenbyÏƒ\n",
            "âˆ’\n",
            "1 f(x),whereÏƒ\n",
            "âˆ’\n",
            "1isjusttheinverseoftheÏƒfunction. Thatis,wewantthe\n",
            "weightedoutpâ—¦utfromthehiddenlayertobe:\n",
            "2\n",
            "Ïƒ\n",
            "âˆ’\n",
            "1 f(x)\n",
            "â—¦\n",
            "1\n",
            "1\n",
            "1\n",
            "âˆ’\n",
            "2\n",
            "âˆ’\n",
            "Ifwecandothis,thentheoutputfromthenetworkasawholewillbeagoodapproximation\n",
            "to f(x) 6. Yourchallenge,then,istodesignaneuralnetworktoapproximatethegoalfunction\n",
            "shownjustabove. Tolearnasmuchaspossible,Iwantyoutosolvetheproblemtwice. The\n",
            "firsttime,pleaseclickonthegraph,directlyadjustingtheheightsofthedifferentbump\n",
            "6NotethatIhavesetthebiasontheoutputneuronto0.\n",
            "\n",
            "(cid:12)\n",
            "138 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "functions. Youshouldfinditfairlyeasytogetagoodmatchtothegoalfunction. How\n",
            "wellyouâ€™redoingismeasuredbytheaveragedeviationbetweenthegoalfunctionandthe\n",
            "functionthenetworkisactuallycomputing. Yourchallengeistodrivetheaveragedeviation\n",
            "aslowaspossible. Youcompletethechallengewhenyoudrivetheaveragedeviationto0.40\n",
            "orbelow7\n",
            "4\n",
            "2\n",
            "Weightedoutput\n",
            "Ïƒ =0.38\n",
            "1\n",
            "1\n",
            "1\n",
            "âˆ’\n",
            "2\n",
            "âˆ’\n",
            "Youâ€™venowfiguredoutalltheelementsnecessaryforthenetworktoapproximatelycompute\n",
            "thefunction f(x)!\n",
            "Itâ€™sonlyacoarseapproximation,butwecouldeasilydomuchbetter,\n",
            "merelybyincreasingthenumberofpairsofhiddenneurons,allowingmorebumps. In particular, itâ€™s easy to convert all the data we have found back into the standard\n",
            "parametrizationusedforneuralnetworks.\n",
            "Letmejustrecapquicklyhowthatworks. Thefirstlayerofweightsallhavesomelarge,constantvalue,sayw=1000. Thebiasesonthehiddenneuronsarejust b= ws. So,forinstance,forthesecond\n",
            "hiddenneurons=0.2becomes b= 1000 0.2= âˆ’200. Thefinallayerofweightsaredeâˆ’terminÃ—edbythâˆ’ehvalues. So,forinstance,thevalue\n",
            "youâ€™vechosenaboveforthefirsth,h= 0.6,meansthattheoutputweightsfromthetop\n",
            "twohiddenneuronsare 0.6and0.6,reâˆ’spectively. Andsoon,fortheentirelayerofoutput\n",
            "weights. âˆ’\n",
            "Finally,thebiasontheoutputneuronis0. Thatâ€™severything: wenowhaveacompletedescriptionofaneuralnetworkwhichdoes\n",
            "aprettygoodjobcomputingouroriginalgoalfunction. Andweunderstandhowtoimprove\n",
            "thequalityoftheapproximationbyimprovingthenumberofhiddenneurons. Whatâ€™smore,therewasnothingspecialaboutouroriginalgoalfunction, f(x)=0.2+\n",
            "0.4x2 +0.3sin(15x)+0.05cos(50x). Wecouldhaveusedthisprocedureforanycontinuous\n",
            "functionfrom[0,1]to[0,1]. Inessence,weâ€™reusingoursingle-layerneuralnetworksto\n",
            "7Thisparagraphreferstointeractiveelement,availableonline.Thegraphshowsthefinalresultof\n",
            "manualminimizationofaveragedeviation. \n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 139\n",
            "(cid:12)\n",
            "buildalookuptableforthefunction. Andweâ€™llbeabletobuildonthisideatoprovidea\n",
            "generalproofofuniversality. 4.3 Many input variables\n",
            "Letâ€™sextendourresultstothecaseofmanyinputvariables. Thissoundscomplicated,but\n",
            "alltheideasweneedcanbeunderstoodinthecaseofjusttwoinputs.\n",
            "Soletâ€™saddressthe\n",
            "two-inputcase. Weâ€™llstartbyconsideringwhathappenswhenwehavetwoinputstoaneuron:\n",
            "Here,wehaveinputs x and y,withcorrespondingweightsw andw ,andabias bonthe\n",
            "1 2\n",
            "neuron. Letâ€™ssettheweightw to0,andthenplayaroundwiththefirstweight,w ,andthe\n",
            "2 1\n",
            "bias, b,toseehowtheyaffecttheoutputfromtheneuron:\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 8\n",
            "âˆ’\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "âˆ’\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 2\n",
            "âˆ’\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=11,b= 5\n",
            "âˆ’\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "âˆ’\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w 1=5,b= 5\n",
            "âˆ’\n",
            "\n",
            "(cid:12)\n",
            "140 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Asyoucansee,withw 2=0theinput y makesnodifferencetotheoutputfromtheneuron. Itâ€™sasthough x istheonlyinput. Giventhis,whatdoyouthinkhappenswhenweincreasetheweightw\n",
            "1\n",
            "tow 1=100,\n",
            "withw remaining0? Ifyoudonâ€™timmediatelyseetheanswer,ponderthequestionfora\n",
            "2\n",
            "bit,andseeifyoucanfigureoutwhathappens.\n",
            "Thentryitoutandseeifyouâ€™reright. Iâ€™ve\n",
            "shownwhathappensinthefollowingmovie:\n",
            "Justasinourearlierdiscussion,astheinputweightgetslargertheoutputapproachesa\n",
            "stepfunction. Thedifferenceisthatnowthestepfunctionisinthreedimensions. Alsoas\n",
            "before,wecanmovethelocationofthesteppointaroundbymodifyingthebias.\n",
            "Theactual\n",
            "locationofthesteppointiss b/w . x 1\n",
            "Letâ€™sredotheaboveusingâ‰¡theâˆ’positionofthestepastheparameter:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.7\n",
            "Here,weassumetheweightonthe x inputhassomelargevalueâ€“Iâ€™veusedw 1=1000â€“\n",
            "andtheweightw 2=0. Thenumberontheneuronisthesteppoint,andthelittle x above\n",
            "thenumberremindsusthatthestepisinthe x direction. Ofcourse,itâ€™salsopossibleto\n",
            "getastepfunctioninthe y direction,bymakingtheweightonthe y inputverylarge(say,\n",
            "w 2=1000),andtheweightonthe x equalto0,i.e.,w 1=0:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s y=0.7\n",
            "Thenumberontheneuronisagainthesteppoint,andinthiscasethelittle y abovethe\n",
            "numberremindsusthatthestepisinthe y direction. Icouldhaveexplicitlymarkedthe\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 141\n",
            "(cid:12)\n",
            "weightsonthe x and y inputs,butdecidednotto,sinceitwouldmakethediagramrather\n",
            "cluttered. Butdokeepinmindthatthelittle y markerimplicitlytellsusthatthe y weightis\n",
            "large,andthe x weightis0. Wecanusethestepfunctionsweâ€™vejustconstructedtocomputeathree-dimensional\n",
            "bumpfunction. Todothis,weusetwoneurons,eachcomputingastepfunctioninthe x\n",
            "direction. Thenwecombinethosestepfunctionswithweighthand h,respectively,where\n",
            "histhedesiredheightofthebump. Itâ€™sallillustratedinthefollowinâˆ’gdiagram:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.7,h=1\n",
            "Trychangingthevalueoftheheight,h. Observehowitrelatestotheweightsinthenetwork.\n",
            "Andseehowitchangestheheightofthebumpfunctionontheright. Also,trychangingthesteppoint0.30associatedtothetophiddenneuron. Witnesshow\n",
            "itchangestheshapeofthebump. Whathappenswhenyoumoveitpastthesteppoint0.70\n",
            "associatedtothebottomhiddenneuron? Weâ€™vefiguredouthowtomakeabumpfunctioninthe x direction. Ofcourse,wecan\n",
            "easilymakeabumpfunctioninthe ydirection,byusingtwostepfunctionsinthe ydirection.\n",
            "Recallthatwedothisbymakingtheweightlargeonthe y input,andtheweight0onthe x\n",
            "input. Hereâ€™stheresult:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.4,s2 y=0.6,h=0.8\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.3,s2 y=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "Thislooksnearlyidenticaltotheearliernetwork! Theonlythingexplicitlyshownaschanging\n",
            "isthatthereâ€™snowlittle y markersonourhiddenneurons. Thatremindsusthattheyâ€™re\n",
            "\n",
            "(cid:12)\n",
            "142 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "producing y stepfunctions,not x stepfunctions,andsotheweightisverylargeonthe y\n",
            "input,andzeroonthex input,notviceversa. Asbefore,Idecidednottoshowthisexplicitly,\n",
            "inordertoavoidclutter. Letâ€™sconsiderwhathappenswhenweadduptwobumpfunctions,\n",
            "oneinthe x direction,theotherinthe y direction,bothofheighth:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=0.6\n",
            "TosimplifythediagramIâ€™vedroppedtheconnectionswithzeroweight. Fornow,Iâ€™veleftin\n",
            "thelittlexand ymarkersonthehiddenneurons,toremindyouinwhatdirectionsthebump\n",
            "functionsarebeingcomputed.\n",
            "Weâ€™lldropeventhosemarkerslater,sincetheyâ€™reimplied\n",
            "bytheinputvariable. Tryvaryingtheparameterh. Asyoucansee,thiscausestheoutput\n",
            "weightstochange,andalsotheheightsofboththe x and y bumpfunctions. Whatweâ€™ve\n",
            "builtlooksalittlelikeatowerfunction:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "Towerfunction\n",
            "Ifwecouldbuildsuchtowerfunctions,thenwecouldusethemtoapproximatearbitrary\n",
            "functions,justbyaddingupmanytowersofdifferentheights,andindifferentlocations:\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 143\n",
            "(cid:12)\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Ofcourse,wehavenâ€™tyetfiguredouthowtobuildatowerfunction.Whatwehaveconstructed\n",
            "lookslikeacentraltower,ofheight2h,withasurroundingplateau,ofheighth. Butwecanmakeatowerfunction. Rememberthatearlierwesawneuronscanbeused\n",
            "toimplementatypeofâ€™inlineif-then-elsestatement:\n",
            "if input >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "Thatwasforaneuronwithjustasingleinput. Whatwewantistoapplyasimilarideato\n",
            "thecombinedoutputfromthehiddenneurons:\n",
            "if combined output from hidden neurons >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "If we choose the threshold appropriately â€” say, a value of 3h/2, which is sandwiched\n",
            "betweentheheightoftheplateauandtheheightofthecentraltowerâ€“wecouldsquashthe\n",
            "plateaudowntozero,andleavejustthetowerstanding. Canyouseehowtodothis?\n",
            "Tryexperimentingwiththefollowingnetworktofigureit\n",
            "out. Notethatweâ€™renowplottingtheoutputfromtheentirenetwork,notjusttheweighted\n",
            "outputfromthehiddenlayer. Thismeansweaddabiastermtotheweightedoutputfromthe\n",
            "hiddenlayer,andapplythesigmafunction. Canyoufindvaluesforhand bwhichproduce\n",
            "atower? Thisisabittricky,soifyouthinkaboutthisforawhileandremainstuck,hereâ€™s\n",
            "twohints: (1)Togettheoutputneurontoshowtherightkindofif-then-elsebehaviour,\n",
            "weneedtheinputweights(allhor h)tobelarge;and(2)thevalueof bdeterminesthe\n",
            "scaleoftheif-then-elsethresholdâˆ’. \n",
            "(cid:12)\n",
            "144 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=0.3,b= 0.5\n",
            "âˆ’\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=1.0,b= 2.0\n",
            "âˆ’\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=3.0,b= 5.0\n",
            "âˆ’\n",
            "Withourinitialparameters,theoutputlookslikeaflattenedversionoftheearlierdiagram,\n",
            "withitstowerandplateau. Togetthedesiredbehaviour,weincreasetheparameterhuntil\n",
            "itbecomeslarge.\n",
            "Thatgivestheif-then-elsethresholdingbehaviour. Second, togetthe\n",
            "thresholdright,weâ€™llchoose b 3h/2. Tryit,andseehowitworks! Hereâ€™swhatitlookslike,wâ‰ˆheâˆ’nweuseh=10:\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 5\n",
            "âˆ’\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 7\n",
            "âˆ’\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 12\n",
            "âˆ’\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 15\n",
            "âˆ’\n",
            "Evenforthisrelativelymodestvalueofh,wegetaprettygoodtowerfunction. And,of\n",
            "course,wecanmakeitasgoodaswewantbyincreasinghstillfurther,andkeepingthebias\n",
            "as b= 3h/2. Letâˆ’â€™strygluingtwosuchnetworkstogether,inordertocomputetwodifferenttower\n",
            "functions. To make the respective roles of the two sub-networks clear Iâ€™ve put them in\n",
            "separateboxes,below: eachboxcomputesatowerfunction,usingthetechniquedescribed\n",
            "above. Thegraphontherightshowstheweightedoutputfromthesecondhiddenlayer,that\n",
            "is,itâ€™saweightedcombinationoftowerfunctions. 1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.8,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.2,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w1=0.2,w2=0.9\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 145\n",
            "(cid:12)\n",
            "Inparticular,youcanseethatbymodifyingtheweightsinthefinallayeryoucanchangethe\n",
            "heightoftheoutputtowers. Thesameideacanbeusedtocomputeasmanytowersaswelike.\n",
            "Wecanalsomake\n",
            "themasthinaswelike,andwhateverheightwelike. Asaresult,wecanensurethatthe\n",
            "weightedoutputfromthesecondhiddenlayerapproximatesanydesiredfunctionoftwo\n",
            "variables:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Inparticular,bymakingtheweightedoutputfromthesecondhiddenlayeragoodapproxi-\n",
            "mationtoÏƒ 1 f,weensuretheoutputfromournetworkwillbeagoodapproximationto\n",
            "âˆ’\n",
            "anydesiredfunâ—¦ction, f. Whataboutfunctionsofmorethantwovariables? Letâ€™strythreevariables x ,x ,x . Thefollowingnetworkcanbeusedtocomputea\n",
            "1 2 3\n",
            "towerfunctioninfourdimensions:\n",
            "Here,the x ,x ,x denoteinputstothenetwork. Thes ,t andsoonaresteppointsfor\n",
            "1 2 3 1 1\n",
            "neuronsâ€“thatis,alltheweightsinthefirstlayerarelarge,andthebiasesaresettogivethe\n",
            "steppointss\n",
            "1\n",
            ",t\n",
            "1\n",
            ",s\n",
            "2\n",
            ",.... Theweightsinthesecondlayeralternate+h, h,wherehissome\n",
            "verylargenumber.\n",
            "Andtheoutputbiasis 5h/2. âˆ’\n",
            "Thisnetworkcomputesafunctionwhâˆ’ichis1providedthreeconditionsaremet: x is\n",
            "1\n",
            "betweens andt ; x isbetweens andt ;and x isbetweens andt . Thenetworkis0\n",
            "1 1 2 2 2 3 3 3\n",
            "\n",
            "(cid:12)\n",
            "146 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "everywhereelse. Thatis,itâ€™sakindoftowerwhichis1inalittleregionofinputspace,and\n",
            "0everywhereelse. Bygluingtogethermanysuchnetworkswecangetasmanytowersaswewant,and\n",
            "soapproximateanarbitraryfunctionofthreevariables. Exactlythesameideaworksinm\n",
            "dimensions. Theonlychangeneededistomaketheoutputbias( m+1/2)h,inorderto\n",
            "gettherightkindofsandwichingbehaviortoleveltheplateau. âˆ’\n",
            "Okay,sowenowknowhowtouseneuralnetworkstoapproximateareal-valuedfunction\n",
            "ofmanyvariables. Whataboutvector-valuedfunctions f(x 1 ,...,x m) Rn? Ofcourse,sucha\n",
            "functioncanberegardedasjustnseparatereal-valuedfunctions,f1 (x\n",
            "1\n",
            "âˆˆ,...,x m),f2 (x\n",
            "1\n",
            ",...,x m),\n",
            "andsoon. Sowecreateanetworkapproximating f1,anothernetworkfor f2,andsoon.\n",
            "4\n",
            "Andthenwesimplyglueallthenetworkstogether.\n",
            "Sothatâ€™salsoeasytocopewith. Problem\n",
            "Weâ€™veseenhowtousenetworkswithtwohiddenlayerstoapproximateanarbitrary\n",
            "â€¢ function. Canyoufindaproofshowingthatitâ€™spossiblewithjustasinglehidden\n",
            "layer? Asahint,tryworkinginthecaseofjusttwoinputvariables,andshowingthat:\n",
            "(a)itâ€™spossibletogetstepfunctionsnotjustinthexor ydirections,butinanarbitrary\n",
            "direction;(b)byaddingupmanyoftheconstructionsfrompart(a)itâ€™spossibleto\n",
            "approximateatowerfunctionwhichiscircularinshape, ratherthanrectangular;\n",
            "(c)usingthesecirculartowers,itâ€™spossibletoapproximateanarbitraryfunction. To\n",
            "dopart(c)itmayhelptouseideasfromabitlaterinthischapter. 4.4 Extension beyond sigmoid neurons\n",
            "Weâ€™veprovedthatnetworksmadeupofsigmoidneuronscancomputeanyfunction. Recall\n",
            "thatinasigmoidneurontheinputsx\n",
            "1\n",
            ",x\n",
            "2\n",
            ",...resultintheoutputÏƒ( (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b),wherew\n",
            "j\n",
            "aretheweights, bisthebias,andÏƒisthesigmoidfunction:\n",
            "Whatifweconsideradifferenttypeofneuron,oneusingsomeotheractivationfunction,\n",
            "s(z):\n",
            "\n",
            "(cid:12)\n",
            "4.4. Extensionbeyondsigmoidneurons (cid:12) 147\n",
            "(cid:12)\n",
            "Thatis,weâ€™llassumethatifourneuronshaveinputs x ,x ,...,weightsw ,w ,...andbias\n",
            "1 2 1 2\n",
            "(cid:80)\n",
            "b,thentheoutputiss(\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b). Wecanusethisactivationfunctiontogetastepfunction,justaswedidwiththesigmoid. Tryrampinguptheweightinthefollowing,saytow=100:\n",
            "Outputfromneuron\n",
            "1 w=100,b= 3\n",
            "w=15,b= âˆ’3\n",
            "w=8,b= âˆ’3\n",
            "4\n",
            "w=6,b= âˆ’3\n",
            "w=4,b= âˆ’3\n",
            "âˆ’\n",
            "0 1\n",
            "1 w=6,b= 5\n",
            "w=6,b= âˆ’3\n",
            "w=6,b= âˆ’1\n",
            "âˆ’\n",
            "0 1\n",
            "Justaswiththesigmoid,thiscausestheactivationfunctiontocontract,andultimatelyit\n",
            "becomesaverygoodapproximationtoastepfunction. Trychangingthebias,andyouâ€™llsee\n",
            "thatwecansetthepositionofthesteptobewhereverwechoose.\n",
            "Andsowecanuseallthe\n",
            "sametricksasbeforetocomputeanydesiredfunction. Whatpropertiesdoess(z)needtosatisfyinorderforthistowork? Wedoneedtoassume\n",
            "thats(z)iswell-definedasz andz . Thesetwolimitsarethetwovaluestaken\n",
            "onbyourstepfunction. Weâ†’alsâˆžoneedtoâ†’asâˆžsumethattheselimitsaredifferentfromone\n",
            "another.\n",
            "Iftheywerenâ€™t,thereâ€™dbenostep,simplyaflatgraph! Butprovidedtheactivation\n",
            "functions(z)satisfiestheseproperties,neuronsbasedonsuchanactivationfunctionare\n",
            "universalforcomputation. Problems\n",
            "Earlierinthebookwemetanothertypeofneuronknownasarectifiedlinearunit. â€¢ Explainwhysuchneuronsdonâ€™tsatisfytheconditionsjustgivenforuniversality. Finda\n",
            "proofofuniversalityshowingthatrectifiedlinearunitsareuniversalforcomputation. Supposeweconsiderlinearneurons,i.e.,neuronswiththeactivationfunctions(z)=z. â€¢ Explainwhylinearneuronsdonâ€™tsatisfytheconditionsjustgivenforuniversality.Show\n",
            "thatsuchneuronscanâ€™tbeusedtodouniversalcomputation. \n",
            "(cid:12)\n",
            "148 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4.5 Fixing up the step functions\n",
            "Uptonow,weâ€™vebeenassumingthatourneuronscanproducestepfunctionsexactly. Thatâ€™s\n",
            "aprettygoodapproximation,butitisonlyanapproximation.\n",
            "Infact,therewillbeanarrow\n",
            "windowoffailure,illustratedinthefollowinggraph,inwhichthefunctionbehavesvery\n",
            "differentlyfromastepfunction:\n",
            "4\n",
            "InthesewindowsoffailuretheexplanationIâ€™vegivenforuniversalitywillfail. Now,itâ€™snotaterriblefailure. Bymakingtheweightsinputtotheneuronsbigenough\n",
            "we can make these windows of failure as small as we like. Certainly, we can make the\n",
            "windowmuchnarrowerthanIâ€™veshownaboveâ€“narrower,indeed,thanoureyecouldsee. Soperhapswemightnotworrytoomuchaboutthisproblem.\n",
            "Nonetheless,itâ€™dbenicetohavesomewayofaddressingtheproblem. Infact,theproblemturnsouttobeeasytofix. Letâ€™slookatthefixforneuralnetworks\n",
            "computingfunctionswithjustoneinputandoneoutput. Thesameideasworkalsotoaddress\n",
            "theproblemwhentherearemoreinputsandoutputs. Inparticular,supposewewantournetworktocomputesomefunction, f. Asbefore,we\n",
            "dothisbytryingtodesignournetworksothattheweightedoutputfromourhiddenlayerof\n",
            "neuronsisÏƒ\n",
            "âˆ’\n",
            "1 f(x):\n",
            "â—¦\n",
            "\n",
            "(cid:12)\n",
            "4.5. Fixingupthestepfunctions (cid:12) 149\n",
            "(cid:12)\n",
            "Ifweweretodothisusingthetechniquedescribedearlier,weâ€™dusethehiddenneuronsto\n",
            "produceasequenceofbumpfunctions:\n",
            "4\n",
            "Again,Iâ€™veexaggeratedthesizeofthewindowsoffailure,inordertomakethemeasierto\n",
            "see. Itshouldbeprettyclearthatifweaddallthesebumpfunctionsupweâ€™llendupwitha\n",
            "reasonableapproximationtoÏƒ\n",
            "âˆ’\n",
            "1 f(x),exceptwithinthewindowsoffailure. Supposethatinsteadofusingâ—¦theapproximationjustdescribed,weuseasetofhidden\n",
            "neuronstocomputeanapproximationtohalfouroriginalgoalfunction,i.e.,toÏƒ\n",
            "âˆ’\n",
            "1 f(x) /2. Ofcourse,thislooksjustlikeascaleddownversionofthelastgraph: â—¦\n",
            "AndsupposeweuseanothersetofhiddenneuronstocomputeanapproximationtoÏƒ 1\n",
            "âˆ’\n",
            "f(x) /2,butwiththebasesofthebumpsshiftedbyhalfthewidthofabump: â—¦\n",
            "NowwehavetwodifferentapproximationstoÏƒ\n",
            "âˆ’\n",
            "1 f(x) /2. Ifweaddupthetwoapproxi-\n",
            "mationsweâ€™llgetanoverallapproximationtoÏƒ\n",
            "âˆ’\n",
            "1â—¦f(x).\n",
            "Thatoverallapproximationwill\n",
            "â—¦\n",
            "\n",
            "(cid:12)\n",
            "150 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "stillhavefailuresinsmallwindows. Buttheproblemwillbemuchlessthanbefore.\n",
            "The\n",
            "reasonisthatpointsinafailurewindowforoneapproximationwonâ€™tbeinafailurewindow\n",
            "fortheother. Andsotheapproximationwillbeafactorroughly2betterinthosewindows. Wecoulddoevenbetterbyaddingupalargenumber,M,ofoverlappingapproximations\n",
            "tothefunctionÏƒ\n",
            "âˆ’\n",
            "1 f(x) /M. Providedthewindowsoffailurearenarrowenough,apoint\n",
            "willonlyeverbeinoâ—¦newindowoffailure.\n",
            "Andprovidedweâ€™reusingalargeenoughnumber\n",
            "M ofoverlappingapproximations,theresultwillbeanexcellentoverallapproximation. Conclusion\n",
            "4\n",
            "Theexplanationforuniversalityweâ€™vediscussediscertainlynotapracticalprescriptionfor\n",
            "howtocomputeusingneuralnetworks! Inthis,itâ€™smuchlikeproofsofuniversalityforNAND\n",
            "gatesandthelike. Forthisreason,Iâ€™vefocusedmostlyontryingtomaketheconstruction\n",
            "clearandeasytofollow,andnotonoptimizingthedetailsoftheconstruction. However,you\n",
            "mayfinditafunandinstructiveexercisetoseeifyoucanimprovetheconstruction. Althoughtheresultisnâ€™tdirectlyusefulinconstructingnetworks,itâ€™simportantbecause\n",
            "ittakesoffthetablethequestionofwhetheranyparticularfunctioniscomputableusinga\n",
            "neuralnetwork. Theanswertothatquestionisalwaysâ€œyesâ€. Sotherightquestiontoaskis\n",
            "notwhetheranyparticularfunctioniscomputable,butratherwhatâ€™sagoodwaytocompute\n",
            "thefunction. Theuniversalityconstructionweâ€™vedevelopedusesjusttwohiddenlayerstocomputean\n",
            "arbitraryfunction. Furthermore,asweâ€™vediscussed,itâ€™spossibletogetthesameresultwith\n",
            "justasinglehiddenlayer. Giventhis,youmightwonderwhywewouldeverbeinterested\n",
            "indeepnetworks,i.e.,networkswithmanyhiddenlayers. Canâ€™twesimplyreplacethose\n",
            "networkswithshallow,singlehiddenlayernetworks? Whileinprinciplethatâ€™spossible,therearegoodpracticalreasonstousedeepnetworks.\n",
            "AsarguedinChapter1,deepnetworkshaveahierarchicalstructurewhichmakesthem\n",
            "particularlywelladaptedtolearnthehierarchiesofknowledgethatseemtobeusefulin\n",
            "solvingreal-worldproblems. Putmoreconcretely,whenattackingproblemssuchasimage\n",
            "recognition,ithelpstouseasystemthatunderstandsnotjustindividualpixels,butalso\n",
            "increasinglymorecomplexconcepts: fromedgestosimplegeometricshapes,alltheway\n",
            "upthroughcomplex,multi-objectscenes. Inlaterchapters,weâ€™llseeevidencesuggesting\n",
            "thatdeepnetworksdoabetterjobthanshallownetworksatlearningsuchhierarchiesof\n",
            "knowledge. Tosumup: universalitytellsusthatneuralnetworkscancomputeanyfunction;\n",
            "andempiricalevidencesuggeststhatdeepnetworksarethenetworksbestadaptedtolearn\n",
            "thefunctionsusefulinsolvingmanyreal-worldproblems. 7Chapteracknowledgments:ThankstoJenDoddandChrisOlahformanydiscussionsaboutuniver-\n",
            "salityinneuralnetworks.Mythanks,inparticular,toChrisforsuggestingtheuseofalookuptableto\n",
            "proveuniversality.Theinteractivevisualformofthechapterisinspiredbytheworkofpeoplesuchas\n",
            "MikeBostock,AmitPatel,BretVictor,andStevenWittens. \n",
            "(cid:12)\n",
            "(cid:12) 151\n",
            "(cid:12)\n",
            "5555\n",
            "Why are deep neural networks\n",
            "hard to train? 5\n",
            "Imagineyouâ€™reanengineerwhohasbeenaskedtodesignacomputerfromscratch.\n",
            "Oneday\n",
            "youâ€™reworkingawayinyouroffice,designinglogicalcircuits,settingoutANDgates,ORgates,\n",
            "andsoon,whenyourbosswalksinwithbadnews. Thecustomerhasjustaddedasurprising\n",
            "designrequirement: thecircuitfortheentirecomputermustbejusttwolayersdeep:\n",
            "Youâ€™redumbfounded,andtellyourboss: â€œThecustomeriscrazy!â€\n",
            "Yourbossreplies: â€œIthinktheyâ€™recrazy,too. Butwhatthecustomerwants,theyget.â€\n",
            "Infact,thereâ€™salimitedsenseinwhichthecustomerisnâ€™tcrazy. Supposeyouâ€™reallowed\n",
            "touseaspeciallogicalgatewhichletsyouANDtogetherasmanyinputsasyouwant. And\n",
            "youâ€™realsoallowedamany-inputNANDgate,thatis,agatewhichcanANDmultipleinputs\n",
            "andthennegatetheoutput. Withthesespecialgatesitturnsouttobepossibletocompute\n",
            "anyfunctionatallusingacircuitthatâ€™sjusttwolayersdeep. Butjustbecausesomethingispossibledoesnâ€™tmakeitagoodidea. Inpractice,when\n",
            "solvingcircuitdesignproblems(ormostanykindofalgorithmicproblem),weusuallystart\n",
            "\n",
            "(cid:12)\n",
            "152 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "byfiguringouthowtosolvesub-problems,andthengraduallyintegratethesolutions. In\n",
            "otherwords,webuilduptoasolutionthroughmultiplelayersofabstraction. Forinstance,supposeweâ€™redesigningalogicalcircuittomultiplytwonumbers. Chances\n",
            "arewewanttobuilditupoutofsub-circuitsdoingoperationslikeaddingtwonumbers. Thesub-circuitsforaddingtwonumberswill,inturn,bebuiltupoutofsub-sub-circuitsfor\n",
            "addingtwobits. Veryroughlyspeakingourcircuitwilllooklike:\n",
            "5\n",
            "Thatis,ourfinalcircuitcontainsatleastthreelayersofcircuitelements. Infact,itâ€™llprobably\n",
            "containmorethanthreelayers,aswebreakthesub-tasksdownintosmallerunitsthanIâ€™ve\n",
            "described. Butyougetthegeneralidea.\n",
            "Sodeepcircuitsmaketheprocessofdesigneasier.\n",
            "Buttheyâ€™renotjusthelpfulfordesign. Thereare,infact,mathematicalproofsshowingthatforsomefunctionsveryshallowcircuits\n",
            "requireexponentiallymorecircuitelementstocomputethandodeepcircuits. Forinstance,\n",
            "afamousseriesofpapersintheearly1980s1showedthatcomputingtheparityofasetof\n",
            "bitsrequiresexponentiallymanygates,ifdonewithashallowcircuit. Ontheotherhand,if\n",
            "youusedeepercircuitsitâ€™seasytocomputetheparityusingasmallcircuit: youjustcompute\n",
            "theparityofpairsofbits,thenusethoseresultstocomputetheparityofpairsofpairsofbits,\n",
            "andsoon,buildingupquicklytotheoverallparity. Deepcircuitsthuscanbeintrinsically\n",
            "muchmorepowerfulthanshallowcircuits. Uptonow,thisbookhasapproachedneuralnetworkslikethecrazycustomer. Almostall\n",
            "thenetworksweâ€™veworkedwithhavejustasinglehiddenlayerofneurons(plustheinput\n",
            "andoutputlayers):\n",
            "1Thehistoryissomewhatcomplex,soIwonâ€™tgivedetailedreferences.SeeJohanHÃ¥stadâ€™s2012paper\n",
            "Onthecorrelationofparityandsmall-depthcircuitsforanaccountoftheearlyhistoryandreferences. \n",
            "(cid:12)\n",
            "(cid:12) 153\n",
            "(cid:12)\n",
            "5\n",
            "Thesesimplenetworkshavebeenremarkablyuseful: inearlierchaptersweusednetworks\n",
            "likethistoclassifyhandwrittendigitswithbetterthan98percentaccuracy! Nonetheless,\n",
            "intuitivelyweâ€™dexpectnetworkswithmanymorehiddenlayerstobemorepowerful:\n",
            "Suchnetworkscouldusetheintermediatelayerstobuildupmultiplelayersofabstraction,\n",
            "justaswedoinBooleancircuits. Forinstance,ifweâ€™redoingvisualpatternrecognition,\n",
            "thentheneuronsinthefirstlayermightlearntorecognizeedges,theneuronsinthesecond\n",
            "layercouldlearntorecognizemorecomplexshapes,saytriangleorrectangles,builtupfrom\n",
            "edges. Thethirdlayerwouldthenrecognizestillmorecomplexshapes.\n",
            "Andsoon. These\n",
            "multiplelayersofabstractionseemlikelytogivedeepnetworksacompellingadvantagein\n",
            "learningtosolvecomplexpatternrecognitionproblems. Moreover,justasinthecaseof\n",
            "circuits,therearetheoreticalresultssuggestingthatdeepnetworksareintrinsicallymore\n",
            "powerfulthanshallownetworks2. 2ForcertainproblemsandnetworkarchitecturesthisisprovedinOnthenumberofresponseregionsof\n",
            "deepfeedforwardnetworkswithpiece-wiselinearactivations,byRazvanPascanu,GuidoMontÃºfar,and\n",
            "YoshuaBengio(2014).Seealsothemoreinformaldiscussioninsection2ofLearningdeeparchitectures\n",
            "forAI,byYoshuaBengio(2009). \n",
            "(cid:12)\n",
            "154 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "Howcanwetrainsuchdeepnetworks? Inthischapter,weâ€™lltrytrainingdeepnetworks\n",
            "usingourworkhorselearningalgorithmâ€“stochasticgradientdescentbybackpropagation. Butweâ€™llrunintotrouble,withourdeepnetworksnotperformingmuch(ifatall)better\n",
            "thanshallownetworks. Thatfailureseemssurprisinginthelightofthediscussionabove. Ratherthangiveupon\n",
            "deepnetworks,weâ€™lldigdownandtrytounderstandwhatâ€™smakingourdeepnetworkshard\n",
            "totrain. Whenwelookclosely,weâ€™lldiscoverthatthedifferentlayersinourdeepnetwork\n",
            "arelearningatvastlydifferentspeeds. Inparticular,whenlaterlayersinthenetworkare\n",
            "learningwell,earlylayersoftengetstuckduringtraining,learningalmostnothingatall. This\n",
            "stucknessisnâ€™tsimplyduetobadluck. Rather,weâ€™lldiscovertherearefundamentalreasons\n",
            "thelearningslowdownoccurs,connectedtoouruseofgradient-basedlearningtechniques. Aswedelveintotheproblemmoredeeply,weâ€™lllearnthattheoppositephenomenon\n",
            "canalsooccur: theearlylayersmaybelearningwell,butlaterlayerscanbecomestuck. In\n",
            "5\n",
            "fact,weâ€™llfindthatthereâ€™sanintrinsicinstabilityassociatedtolearningbygradientdescent\n",
            "indeep,many-layerneuralnetworks. Thisinstabilitytendstoresultineithertheearlyor\n",
            "thelaterlayersgettingstuckduringtraining. Thisallsoundslikebadnews. Butbydelvingintothesedifficulties,wecanbegintogain\n",
            "insightintowhatâ€™srequiredtotraindeepnetworkseffectively. Andsotheseinvestigations\n",
            "aregoodpreparationforthenextchapter,whereweâ€™llusedeeplearningtoattackimage\n",
            "recognitionproblems. 5.1 The vanishing gradient problem\n",
            "So,whatgoeswrongwhenwetrytotrainadeepnetwork? Toanswerthatquestion,letâ€™sfirstrevisitthecaseofanetworkwithjustasinglehidden\n",
            "layer. Asperusual,weâ€™llusetheMNISTdigitclassificationproblemasourplaygroundfor\n",
            "learningandexperimentation3. Ifyouwish,youcanfollowalongbytrainingnetworksonyourcomputer.\n",
            "Itisalso,of\n",
            "course,finetojustreadalong. Ifyoudowishtofollowlive,thenyouâ€™llneedPython2.7,\n",
            "Numpy,andacopyofthecode,whichyoucangetbycloningtherelevantrepositoryfrom\n",
            "thecommandline:\n",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
            "Ifyoudonâ€™tusegitthenyoucandownloadthedataandcodehere. Youâ€™llneedtochange\n",
            "intothesrcsubdirectory.\n",
            "Then,fromaPythonshellweloadtheMNISTdata:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            "Wesetupournetwork:\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            "3IintroducedtheMNISTproblemanddatahere1.5andhere1.6. \n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 155\n",
            "(cid:12)\n",
            "Thisnetworkhas784neuronsintheinputlayer,correspondingtothe28 28=784pixels\n",
            "intheinputimage. Weuse30hiddenneurons,aswellas10outputneuroÃ—ns,corresponding\n",
            "tothe10possibleclassificationsfortheMNISTdigits(â€˜0â€™,â€˜1â€™,â€˜2â€™,...,â€˜9â€™). Letâ€™strytrainingournetworkfor30completeepochs,usingmini-batchesof10training\n",
            "examplesatatime,alearningrateÎ· =0.1,andregularizationparameterÎ» =5.0. Aswe\n",
            "trainweâ€™llmonitortheclassificationaccuracyonthevalidation_data4:\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Wegetaclassificationaccuracyof96.48percent(orthereaboutsâ€“itâ€™llvaryabitfromrunto\n",
            "run),comparabletoourearlierresultswithasimilarconfiguration. Now,letâ€™saddanotherhiddenlayer,alsowith30neuronsinit,andtrytrainingwiththe\n",
            "samehyper-parameters:\n",
            "5\n",
            ">>> net = network2.Network([784, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thisgivesanimprovedclassificationaccuracy,96.90percent. Thatâ€™sencouraging: alittle\n",
            "moredepthishelping. Letâ€™saddanother30-neuronhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thatdoesnâ€™thelpatall. Infact,theresultdropsbackdownto96.57percent,closetoour\n",
            "originalshallownetwork. Andsupposeweinsertonefurtherhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Theclassificationaccuracydropsagain,to96.53percent. Thatâ€™sprobablynotastatistically\n",
            "significantdrop,butitâ€™snotencouraging,either.\n",
            "Thisbehaviourseemsstrange. Intuitively,extrahiddenlayersoughttomakethenetwork\n",
            "able to learn more complex classification functions, and thus do a better job classifying. Certainly,thingsshouldnâ€™tgetworse,sincetheextralayerscan,intheworstcase,simplydo\n",
            "nothing5. Butthatâ€™snotwhatâ€™sgoingon.\n",
            "So what is going on? Letâ€™s assume that the extra hidden layers really could help in\n",
            "principle,andtheproblemisthatourlearningalgorithmisnâ€™tfindingtherightweightsand\n",
            "biases. Weâ€™dliketofigureoutwhatâ€™sgoingwronginourlearningalgorithm,andhowtodo\n",
            "better. Togetsomeinsightintowhatâ€™sgoingwrong, letâ€™svisualizehowthenetworklearns. Below,Iâ€™veplottedpartofa[784,30,30,10]network,i.e.,anetworkwithtwohiddenlayers,\n",
            "each containing 30 hidden neurons. Each neuron in the diagram has a little bar on it,\n",
            "4Notethatthenetworksislikelytotakesomeminutestotrain,dependingonthespeedofyour\n",
            "machine.Soifyouâ€™rerunningthecodeyoumaywishtocontinuereadingandreturnlater,notwaitfor\n",
            "thecodetofinishexecuting. 5Seethislaterproblem5.2tounderstandhowtobuildahiddenlayerthatdoesnothing.\n",
            "\n",
            "(cid:12)\n",
            "156 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "representinghowquicklythatneuronischangingasthenetworklearns.\n",
            "Abigbarmeans\n",
            "theneuronâ€™sweightsandbiasarechangingrapidly,whileasmallbarmeanstheweights\n",
            "andbiasarechangingslowly. Moreprecisely,thebarsdenotethegradientâˆ‚C/âˆ‚bforeach\n",
            "neuron,i.e.,therateofchangeofthecostwithrespecttotheneuronâ€™sbias. BackinChapter\n",
            "2wesawthatthisgradientquantitycontrollednotjusthowrapidlythebiaschangesduring\n",
            "learning,butalsohowrapidlytheweightsinputtotheneuronchange,too. Donâ€™tworryif\n",
            "youdonâ€™trecallthedetails: thethingtokeepinmindissimplythatthesebarsshowhow\n",
            "quicklyeachneuronâ€™sweightsandbiasarechangingasthenetworklearns. Tokeepthediagramsimple,Iâ€™veshownjustthetopsixneuronsinthetwohiddenlayers. Iâ€™veomittedtheinputneurons,sincetheyâ€™vegotnoweightsorbiasestolearn. Iâ€™vealso\n",
            "omittedtheoutputneurons,sinceweâ€™redoinglayer-wisecomparisons,anditmakesmost\n",
            "sensetocomparelayerswiththesamenumberofneurons. Theresultsareplottedatthe\n",
            "verybeginningoftraining,i.e.,immediatelyafterthenetworkisinitialized. Heretheyare6:\n",
            "5\n",
            "Thenetworkwasinitializedrandomly,andsoitâ€™snotsurprisingthatthereâ€™salotofvariation\n",
            "inhowrapidlytheneuronslearn. Still,onethingthatjumpsoutisthatthebarsinthesecond\n",
            "hiddenlayeraremostlymuchlargerthanthebarsinthefirsthiddenlayer. Asaresult,the\n",
            "neuronsinthesecondhiddenlayerwilllearnquiteabitfasterthantheneuronsinthefirst\n",
            "hiddenlayer. Isthismerelyacoincidence,oraretheneuronsinthesecondhiddenlayer\n",
            "likelytolearnfasterthanneuronsinthefirsthiddenlayeringeneral? Todeterminewhetherthisisthecase,ithelpstohaveaglobalwayofcomparingthe\n",
            "speedoflearninginthefirstandsecondhiddenlayers. Todothis,letâ€™sdenotethegradient\n",
            "asÎ´l j= âˆ‚C/âˆ‚bl j ,i.e.,thegradientforthe j-thneuroninthel-thlayer7Wecanthinkofthe\n",
            "gradientÎ´1asavectorwhoseentriesdeterminehowquicklythefirsthiddenlayerlearns,\n",
            "andÎ´2 asavectorwhoseentriesdeterminehowquicklythesecondhiddenlayerlearns. 6Thedataplottedisgeneratedusingtheprogramgenerate_gradient.py.Thesameprogramis\n",
            "alsousedtogeneratetheresultsquotedlaterinthissection. 7BackinChapter2wereferredtothisastheerror,buthereweâ€™lladopttheinformaltermâ€œgradientâ€. Isayâ€œinformalâ€becauseofcoursethisdoesnâ€™texplicitlyincludethepartialderivativesofthecostwith\n",
            "respecttotheweights,âˆ‚C/âˆ‚w.\n",
            "\n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 157\n",
            "(cid:12)\n",
            "Weâ€™llthenusethelengthsofthesevectorsas(rough!) globalmeasuresofthespeedatwhich\n",
            "thelayersarelearning. So,forinstance,thelength Î´1 measuresthespeedatwhichthe\n",
            "firsthiddenlayerislearning,whilethelength Î´2 m(cid:107)eas(cid:107)uresthespeedatwhichthesecond\n",
            "hiddenlayerislearning.\n",
            "(cid:107) (cid:107)\n",
            "Withthesedefinitions, andinthesameconfigurationaswasplottedabove, wefind\n",
            "Î´1 =0.07...and Î´2 =0.31....\n",
            "Sothisconfirmsourearliersuspicion: theneuronsin\n",
            "(cid:107)the(cid:107)secondhiddenla(cid:107)yer(cid:107)reallyarelearningmuchfasterthantheneuronsinthefirsthidden\n",
            "layer. What happens if we add more hidden layers? If we have three hidden layers, in a\n",
            "[784,30,30,30,10]network, thentherespectivespeedsoflearningturnouttobe0.012,\n",
            "0.060,and0.283. Again,earlierhiddenlayersarelearningmuchslowerthanlaterhidden\n",
            "layers. Supposeweaddyetanotherlayerwith30hiddenneurons. Inthatcase,therespective\n",
            "speedsoflearningare0.003,0.017,0.070,and0.285. Thepatternholds: earlylayerslearn\n",
            "5\n",
            "slowerthanlaterlayers. Weâ€™vebeenlookingatthespeedoflearningatthestartoftraining,thatis,justafterthe\n",
            "networksareinitialized.\n",
            "Howdoesthespeedoflearningchangeaswetrainournetworks? Letâ€™sreturntolookatthenetworkwithjusttwohiddenlayers. Thespeedoflearningchanges\n",
            "asfollows:\n",
            "Togeneratetheseresults, Iusedbatchgradientdescentwithjust1,000trainingimages,\n",
            "trainedover500epochs. Thisisabitdifferentthanthewayweusuallytrainâ€“Iâ€™veusedno\n",
            "mini-batches,andjust1,000trainingimages,ratherthanthefull50,000imagetrainingset. Iâ€™mnottryingtodoanythingsneaky,orpullthewooloveryoureyes,butitturnsoutthat\n",
            "usingmini-batchstochasticgradientdescentgivesmuchnoisier(albeitverysimilar,when\n",
            "youaverageawaythenoise)results. UsingtheparametersIâ€™vechosenisaneasywayof\n",
            "smoothingtheresultsout,sowecanseewhatâ€™sgoingon. Inanycase,asyoucanseethetwolayersstartoutlearningatverydifferentspeeds(as\n",
            "wealreadyknow). Thespeedinbothlayersthendropsveryquickly,beforerebounding. But\n",
            "throughitall,thefirsthiddenlayerlearnsmuchmoreslowlythanthesecondhiddenlayer. Whataboutmorecomplexnetworks? Hereâ€™stheresultsofasimilarexperiment,butthis\n",
            "timewiththreehiddenlayers(a[784,30,30,30,10]network):\n",
            "\n",
            "(cid:12)\n",
            "158 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Finally,letâ€™sadd\n",
            "afourthhiddenlayer(a[784,30,30,30,30,10]network),andseewhathappenswhenwe\n",
            "train:\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Inthiscase,\n",
            "thefirsthiddenlayerislearningroughly100timesslowerthanthefinalhiddenlayer.\n",
            "No\n",
            "wonderwewerehavingtroubletrainingthesenetworksearlier! Wehavehereanimportantobservation: inatleastsomedeepneuralnetworks,the\n",
            "gradienttendstogetsmalleraswemovebackwardthroughthehiddenlayers. Thismeans\n",
            "thatneuronsintheearlierlayerslearnmuchmoreslowlythanneuronsinlaterlayers. And\n",
            "while weâ€™ve seen this in just a single network, there are fundamental reasons why this\n",
            "happensinmanyneuralnetworks. Thephenomenonisknownasthevanishinggradient\n",
            "\n",
            "(cid:12)\n",
            "5.2. Whatâ€™scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 159\n",
            "(cid:12)\n",
            "problem8. Whydoesthevanishinggradientproblemoccur? Aretherewayswecanavoidit?\n",
            "And\n",
            "howshouldwedealwithitintrainingdeepneuralnetworks? Infact,weâ€™lllearnshortly\n",
            "thatitâ€™snotinevitable,althoughthealternativeisnotveryattractive,either: sometimesthe\n",
            "gradientgetsmuchlargerinearlierlayers! Thisistheexplodinggradientproblem,anditâ€™s\n",
            "notmuchbetternewsthanthevanishinggradientproblem. Moregenerally,itturnsout\n",
            "thatthegradientindeepneuralnetworksisunstable,tendingtoeitherexplodeorvanish\n",
            "inearlierlayers. Thisinstabilityisafundamentalproblemforgradient-basedlearningin\n",
            "deepneuralnetworks. Itâ€™ssomethingweneedtounderstand,and,ifpossible,takestepsto\n",
            "address. Oneresponsetovanishing(orunstable)gradientsistowonderiftheyâ€™rereallysuch\n",
            "aproblem. Momentarilysteppingawayfromneuralnets,imagineweweretryingtonu-\n",
            "mericallyminimizeafunction f(x)ofasinglevariable. Wouldnâ€™titbegoodnewsifthe\n",
            "5\n",
            "derivative f (cid:48)(x)wassmall? Wouldnâ€™tthatmeanwewerealreadynearanextremum? Ina\n",
            "similarway,mightthesmallgradientinearlylayersofadeepnetworkmeanthatwedonâ€™t\n",
            "needtodomuchadjustmentoftheweightsandbiases? Ofcourse,thisisnâ€™tthecase. Recallthatwerandomlyinitializedtheweightandbiases\n",
            "inthenetwork. Itisextremelyunlikelyourinitialweightsandbiaseswilldoagoodjobat\n",
            "whateveritiswewantournetworktodo. Tobeconcrete,considerthefirstlayerofweights\n",
            "ina[784,30,30,30,10]networkfortheMNISTproblem. Therandominitializationmeans\n",
            "thefirstlayerthrowsawaymostinformationabouttheinputimage. Eveniflaterlayershave\n",
            "beenextensivelytrained,theywillstillfinditextremelydifficulttoidentifytheinputimage,\n",
            "simplybecausetheydonâ€™thaveenoughinformation. Andsoitcanâ€™tpossiblybethecasethat\n",
            "notmuchlearningneedstobedoneinthefirstlayer.\n",
            "Ifweâ€™regoingtotraindeepnetworks,\n",
            "weneedtofigureouthowtoaddressthevanishinggradientproblem. 5.2 Whatâ€™s causing the vanishing gradient problem? Unstable\n",
            "gradients in deep neural nets\n",
            "Togetinsightintowhythevanishinggradientproblemoccurs,letâ€™sconsiderthesimplest\n",
            "deepneuralnetwork: onewithjustasingleneuronineachlayer. Hereâ€™sanetworkwith\n",
            "threehiddenlayers:\n",
            "Here,w ,w ,...aretheweights, b ,b ,...arethebiases,andC issomecostfunction. Just\n",
            "1 2 1 2\n",
            "toremindyouhowthisworks,theoutputa\n",
            "j\n",
            "fromthe j-thneuronisÏƒ (z j),whereÏƒisthe\n",
            "usualsigmoidactivationfunction,andz j=w\n",
            "j\n",
            "a\n",
            "j\n",
            "1+b\n",
            "j\n",
            "istheweightedinputtotheneuron. Iâ€™vedrawnthecostC attheendtoemphasizetâˆ’hatthecostisafunctionofthenetworkâ€™s\n",
            "output,a : iftheactualoutputfromthenetworkisclosetothedesiredoutput,thenthecost\n",
            "4\n",
            "willbelow,whileifitâ€™sfaraway,thecostwillbehigh. 8SeeGradientflowinrecurrentnets: thedifficultyoflearninglong-termdependencies,bySepp\n",
            "Hochreiter,YoshuaBengio,PaoloFrasconi,andJÃ¼rgenSchmidhuber(2001).Thispaperstudiedrecurrent\n",
            "neuralnets,buttheessentialphenomenonisthesameasinthefeedforwardnetworkswearestudying. SeealsoSeppHochreiterâ€™searlierDiplomaThesis,UntersuchungenzudynamischenneuronalenNetzen\n",
            "(1991,inGerman).\n",
            "\n",
            "(cid:12)\n",
            "160 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "Weâ€™regoingtostudythegradientâˆ‚C/âˆ‚b associatedtothefirsthiddenneuron. Weâ€™ll\n",
            "1\n",
            "figureoutanexpressionforâˆ‚C/âˆ‚b ,andbystudyingthatexpressionweâ€™llunderstandwhy\n",
            "1\n",
            "thevanishinggradientproblemoccurs. Iâ€™llstartbysimplyshowingyoutheexpressionforâˆ‚C/âˆ‚b .\n",
            "Itlooksforbidding, but\n",
            "1\n",
            "itâ€™sactuallygotasimplestructure,whichIâ€™lldescribeinamoment. Hereâ€™stheexpression\n",
            "(ignorethenetwork,fornow,andnotethatÏƒ isjustthederivativeoftheÏƒfunction):\n",
            "(cid:48)\n",
            "Thestructureintheexpressionisasfollows: thereisaÏƒ (cid:48)(z j)termintheproductforeach\n",
            "5 neuroninthenetwork;aweightw termforeachweightinthenetwork;andafinalâˆ‚C/âˆ‚a\n",
            "j 4\n",
            "term,correspondingtothecostfunctionattheend. NoticethatIâ€™veplacedeachterminthe\n",
            "expressionabovethecorrespondingpartofthenetwork.\n",
            "Sothenetworkitselfisamnemonic\n",
            "fortheexpression. Youâ€™rewelcometotakethisexpressionforgranted,andskiptothediscussionofhowit\n",
            "relatestothevanishinggradientproblem. Thereâ€™snoharmindoingthis,sincetheexpression\n",
            "is a special case of our earlier discussion of backpropagation. But thereâ€™s also a simple\n",
            "explanationofwhytheexpressionistrue,andsoitâ€™sfun(andperhapsenlightening)totake\n",
            "alookatthatexplanation. Imaginewemakeasmallchangeâˆ†b inthebias b .\n",
            "Thatwillsetoffacascadingseries\n",
            "1 1\n",
            "ofchangesintherestofthenetwork. First,itcausesachangeâˆ†a intheoutputfromthe\n",
            "1\n",
            "firsthiddenneuron. That,inturn,willcauseachangeâˆ†z intheweightedinputtothe\n",
            "2\n",
            "secondhiddenneuron. Thenachangeâˆ†a intheoutputfromthesecondhiddenneuron. 2\n",
            "Andsoon,allthewaythroughtoachangeâˆ†C inthecostattheoutput. Wehave\n",
            "âˆ‚C âˆ†C\n",
            ". (5.1)\n",
            "âˆ‚b\n",
            "1 â‰ˆ\n",
            "âˆ†b\n",
            "1\n",
            "Thissuggeststhatwecanfigureoutanexpressionforthegradientâˆ‚C/âˆ‚b bycarefully\n",
            "1\n",
            "trackingtheeffectofeachstepinthiscascade. Todothis,letâ€™sthinkabouthowâˆ†b causestheoutputa fromthefirsthiddenneuron\n",
            "1 1\n",
            "tochange. Wehavea 1= Ïƒ (z 1)= Ïƒ (w 1 a 0+b 1),so\n",
            "âˆ†a 1 â‰ˆ âˆ‚Ïƒ (w âˆ‚ 1 a b 0 1 +b 1)âˆ†b 1= Ïƒ (cid:48)(z 1) âˆ†b 1 . (5.2)\n",
            "ThatÏƒ (cid:48)(z 1)termshouldlookfamiliar: itâ€™sthefirstterminourclaimedexpressionforthe\n",
            "gradientâˆ‚C/âˆ‚b . Intuitively,thistermconvertsachangeâˆ†b inthebiasintoachangeâˆ†a\n",
            "1 1 1\n",
            "intheoutputactivation. Thatchangeâˆ†a inturncausesachangeintheweightedinput\n",
            "1\n",
            "z 2=w\n",
            "2\n",
            "a 1+b\n",
            "2\n",
            "tothesecondhiddenneuron:\n",
            "âˆ‚z\n",
            "âˆ†z\n",
            "2\n",
            "â‰ˆ\n",
            "âˆ‚a\n",
            "2\n",
            "1\n",
            "âˆ†a 1=w\n",
            "2\n",
            "âˆ†a\n",
            "1\n",
            ". (5.3)\n",
            "Combiningourexpressionsforâˆ†z andâˆ†a ,weseehowthechangeinthebiasb propagates\n",
            "2 1 1\n",
            "\n",
            "(cid:12)\n",
            "5.2. Whatâ€™scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 161\n",
            "(cid:12)\n",
            "alongthenetworktoaffectz :\n",
            "2\n",
            "âˆ†z\n",
            "2\n",
            "Ïƒ (cid:48)(z 1)w\n",
            "2\n",
            "âˆ†b\n",
            "1\n",
            ". (5.4)\n",
            "â‰ˆ\n",
            "Again,thatshouldlookfamiliar: weâ€™venowgotthefirsttwotermsinourclaimedexpression\n",
            "forthegradientâˆ‚C/âˆ‚b .\n",
            "1\n",
            "Wecankeepgoinginthisfashion,trackingthewaychangespropagatethroughtherest\n",
            "ofthenetwork. AteachneuronwepickupaÏƒ (cid:48)(z j)term,andthrougheachweightwepick\n",
            "upaw term. Theendresultisanexpressionrelatingthefinalchangeâˆ†C incosttothe\n",
            "j\n",
            "initialchangeâˆ†b inthebias:\n",
            "1\n",
            "âˆ‚C\n",
            "âˆ†C\n",
            "â‰ˆ\n",
            "Ïƒ (cid:48)(z 1)w 2 Ïƒ (cid:48)(z 2)...Ïƒ (cid:48)(z 4)âˆ‚a\n",
            "4\n",
            "âˆ†b 1 . (5.5)\n",
            "5\n",
            "Dividingbyâˆ†b wedoindeedgetthedesiredexpressionforthegradient:\n",
            "1\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ‚b = Ïƒ (cid:48)(z 1)w 2 Ïƒ (cid:48)(z 2)...Ïƒ (cid:48)(z 4)âˆ‚a . (5.6)\n",
            "1 4\n",
            "Whythevanishinggradientproblemoccurs: Tounderstandwhythevanishinggradi-\n",
            "entproblemoccurs,letâ€™sexplicitlywriteouttheentireexpressionforthegradient:\n",
            "âˆ‚C âˆ‚C\n",
            "âˆ‚b = Ïƒ (cid:48)(z 1)w 2 Ïƒ (cid:48)(z 2)w 3 Ïƒ (cid:48)(z 3)w 4 Ïƒ (cid:48)(z 4)âˆ‚a . (5.7)\n",
            "1 4\n",
            "Exceptingtheverylastterm,thisexpressionisaproductoftermsoftheformw\n",
            "j\n",
            "Ïƒ (cid:48)(z j). To\n",
            "understandhoweachofthosetermsbehave,letâ€™slookataplotofthefunctionÏƒ:\n",
            "(cid:48)\n",
            "Derivativeofsigmoidfunction\n",
            "0.2\n",
            "0.1\n",
            "0\n",
            "4 2 0 2 4\n",
            "âˆ’ âˆ’\n",
            "ThederivativereachesamaximumatÏƒ (cid:48)(0)=1/4. Now,ifweuseourstandardapproachto\n",
            "initializingtheweightsinthenetwork,thenweâ€™llchoosetheweightsusingaGaussianwith\n",
            "mean0andstandarddeviation1. Sotheweightswillusuallysatisfy w <1. Puttingthese\n",
            "j\n",
            "observationstogether,weseethatthetermsw j Ïƒ (cid:48)(z j)willusuallysa|tis|fy w j Ïƒ (cid:48)(z j) <1/4. Andwhenwetakeaproductofmanysuchterms,theproductwilltend|toexpone|ntially\n",
            "decrease: themoreterms,thesmallertheproductwillbe.\n",
            "Thisisstartingtosmelllikea\n",
            "possibleexplanationforthevanishinggradientproblem. To make this all a bit more explicit, letâ€™s compare the expression for âˆ‚C/âˆ‚b to an\n",
            "1\n",
            "expressionforthegradientwithrespecttoalaterbias,sayâˆ‚C/âˆ‚b . Ofcourse,wehavenâ€™t\n",
            "3\n",
            "\n",
            "(cid:12)\n",
            "162 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "explicitlyworkedoutanexpressionforâˆ‚C/âˆ‚b ,butitfollowsthesamepatterndescribed\n",
            "3\n",
            "aboveforâˆ‚C/âˆ‚b . Hereâ€™sthecomparisonofthetwoexpressions:\n",
            "1\n",
            "5\n",
            "Thetwoexpressionssharemanyterms. Butthegradientâˆ‚C/âˆ‚b includestwoextraterms\n",
            "1\n",
            "eachoftheformw\n",
            "j\n",
            "Ïƒ (cid:48)(z j). Asweâ€™veseen,suchtermsaretypicallylessthan1/4inmagnitude. Andsothegradientâˆ‚C/âˆ‚b willusuallybeafactorof16(ormore)smallerthanâˆ‚C/âˆ‚b . 1 3\n",
            "Thisistheessentialoriginofthevanishinggradientproblem.\n",
            "Ofcourse,thisisaninformalargument,notarigorousproofthatthevanishinggradient\n",
            "problemwilloccur. Thereareseveralpossibleescapeclauses.\n",
            "Inparticular,wemightwonder\n",
            "whethertheweightsw j couldgrowduringtraining. Iftheydo,itâ€™spossiblethetermsw j Ïƒ (cid:48)(z j)\n",
            "intheproductwillnolongersatisfy w j Ïƒ (cid:48)(z j) <1/4. Indeed,ifthetermsgetlargeenough\n",
            "â€“greaterthan1â€“thenwewillnolo|ngerhav|eavanishinggradientproblem. Instead,the\n",
            "gradientwillactuallygrowexponentiallyaswemovebackwardthroughthelayers. Instead\n",
            "ofavanishinggradientproblem,weâ€™llhaveanexplodinggradientproblem. Theexplodinggradientproblem: Letâ€™slookatanexplicitexamplewhereexploding\n",
            "gradientsoccur. Theexampleissomewhatcontrived: Iâ€™mgoingtofixparametersinthe\n",
            "networkinjusttherightwaytoensurewegetanexplodinggradient. Buteventhoughthe\n",
            "exampleiscontrived,ithasthevirtueoffirmlyestablishingthatexplodinggradientsarenâ€™t\n",
            "merelyahypotheticalpossibility,theyreallycanhappen. Therearetwostepstogettinganexplodinggradient. First,wechoosealltheweights\n",
            "inthenetworktobelarge,sayw 1=w 2=w 3=w 4=100. Second,weâ€™llchoosethebiases\n",
            "sothattheÏƒ (cid:48)(z j)termsarenottoosmall. Thatâ€™sactuallyprettyeasytodo: allweneed\n",
            "doischoosethebiasestoensurethattheweightedinputtoeachneuronisz j=0(andso\n",
            "Ïƒ (cid:48)(z j)=1/4). So,forinstance,wewantz 1=w\n",
            "1\n",
            "a 0+b 1=0. Wecanachievethisbysetting\n",
            "b 1= 100 a 0 . Wecanusethesameideatoselecttheotherbiases. Whenwedothis,we\n",
            "seethâˆ’atallÃ—thetermsw\n",
            "j\n",
            "Ïƒ (cid:48)(z j)areequalto100 1/4=25.\n",
            "Withthesechoiceswegetan\n",
            "explodinggradient. Ã—\n",
            "The unstable gradient problem: The fundamental problem here isnâ€™t so much the\n",
            "vanishinggradientproblemortheexplodinggradientproblem. Itâ€™sthatthegradientin\n",
            "earlylayersistheproductoftermsfromallthelaterlayers.\n",
            "Whentherearemanylayers,\n",
            "thatâ€™sanintrinsicallyunstablesituation. Theonlywayalllayerscanlearnatclosetothe\n",
            "samespeedisifallthoseproductsoftermscomeclosetobalancingout. Withoutsome\n",
            "mechanismorunderlyingreasonforthatbalancingtooccur,itâ€™shighlyunlikelytohappen\n",
            "simplybychance. Inshort,therealproblemhereisthatneuralnetworkssufferfroman\n",
            "unstablegradientproblem. Asaresult,ifweusestandardgradient-basedlearningtechniques,\n",
            "differentlayersinthenetworkwilltendtolearnatwildlydifferentspeeds. Exercise\n",
            "\n",
            "(cid:12)\n",
            "5.3. Unstablegradientsinmorecomplexnetworks (cid:12) 163\n",
            "(cid:12)\n",
            "Inourdiscussionofthevanishinggradientproblem,wemadeuseofthefactthat\n",
            "â€¢ Ïƒ (cid:48)(z) <1/4. Supposeweusedadifferentactivationfunction,onewhosederivative\n",
            "|could|bemuchlarger. Wouldthathelpusavoidtheunstablegradientproblem? The prevalence of the vanishing gradient problem: Weâ€™ve seen that the gradient can\n",
            "eithervanishorexplodeintheearlylayersofadeepnetwork. Infact,whenusingsigmoid\n",
            "neuronsthegradientwillusuallyvanish. Toseewhy,consideragaintheexpression wÏƒ (cid:48)(z). Toavoidthevanishinggradientproblemweneed wÏƒ (cid:48)(z) 1. Youmightthinkth|iscould|\n",
            "happeneasilyif wisverylarge. However,itâ€™smo|rediffic|uâ‰¥ltthanitlooks. Thereasonis\n",
            "thattheÏƒ (cid:48)(z)termalsodependsonw: Ïƒ (cid:48)(z)= Ïƒ (cid:48)(wa+b),whereaistheinputactivation. Sowhenwemake wlarge,weneedtobecarefulthatweâ€™renotsimultaneouslymaking\n",
            "Ïƒ (cid:48)(wa+b)small. Thatturnsouttobeaconsiderableconstraint.\n",
            "Thereasonisthatwhen\n",
            "wemakewlargewetendtomakewa+bverylarge. LookingatthegraphofÏƒ\n",
            "(cid:48)\n",
            "youcansee\n",
            "thatthisputsusoffintheâ€œwingsâ€oftheÏƒ function,whereittakesverysmallvalues. The 5\n",
            "(cid:48)\n",
            "onlywaytoavoidthisisiftheinputactivationfallswithinafairlynarrowrangeofvalues\n",
            "(thisqualitativeexplanationismadequantitativeinthefirstproblembelow). Sometimes\n",
            "thatwillchancetohappen.\n",
            "Moreoften,though,itdoesnothappen.\n",
            "Andsointhegeneric\n",
            "casewehavevanishinggradients. Problems\n",
            "Considertheproduct wÏƒ (cid:48)(wa+b). Suppose wÏƒ (cid:48)(wa+b) 1. (1)Arguethat\n",
            "â€¢ thiscanonlyeveroccu|rif w 4. |(2)Supposi|ngthat w |4â‰¥,considerthesetof\n",
            "inputactivationsaforwhich| w|â‰¥Ïƒ (cid:48)(wa+b) 1. Showtha|t|thâ‰¥esetofasatisfyingthat\n",
            "constraintcanrangeoveran|intervalnogr|eâ‰¥aterinwidththan\n",
            "2 (cid:18) w(1+ (cid:112) 1 4/w) (cid:19)\n",
            "ln | | âˆ’ | | 1 . (5.8)\n",
            "w 2 âˆ’\n",
            "| |\n",
            "(3)Shownumericallythattheaboveexpressionboundingthewidthoftherangeis\n",
            "greatestat w 6.9,whereittakesavalue 0.45. Andsoevengiventhateverything\n",
            "linesupjus|tp|eâ‰ˆrfectly,westillhaveafairlyâ‰ˆnarrowrangeofinputactivationswhich\n",
            "canavoidthevanishinggradientproblem. Identityneuron: Consideraneuronwithasingleinput, x,acorrespondingweight,\n",
            "â€¢ w ,abias b,andaweightw ontheoutput. Showthatbychoosingtheweightsand\n",
            "1 2\n",
            "biasappropriately,wecanensurew\n",
            "2\n",
            "Ïƒ (w\n",
            "1\n",
            "x+b) xforx [0,1]. Suchaneuroncan\n",
            "thusbeusedasakindofidentityneuron,thatisâ‰ˆ,aneuronâˆˆwhoseoutputisthesame\n",
            "(uptorescalingbyaweightfactor)asitsinput. Hint: Ithelpstorewrite x=1/2+ âˆ†,\n",
            "toassumew issmall,andtouseaTaylorseriesexpansioninw âˆ†. 1 1\n",
            "5.3 Unstable gradients in more complex networks\n",
            "Weâ€™vebeenstudyingtoynetworks,withjustoneneuronineachhiddenlayer. Whatabout\n",
            "morecomplexdeepnetworks,withmanyneuronsineachhiddenlayer?\n",
            "\n",
            "(cid:12)\n",
            "164 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "5\n",
            "Infact,muchthesamebehaviouroccursinsuchnetworks. Intheearlierchapteronback-\n",
            "propagationwesawthatthegradientinthel-thlayerofanLlayernetworkisgivenby:\n",
            "Î´l = Î£ (cid:48)(zl )(wl+1 ) TÎ£ (cid:48)(zl+1 )(wl+2 ) T...Î£ (cid:48)(zL ) a C (5.9)\n",
            "âˆ‡\n",
            "Here,Î£ (cid:48)(zl )isadiagonalmatrixwhoseentriesaretheÏƒ (cid:48)(z)valuesfortheweightedinputs\n",
            "tothel-thlayer. Thewl aretheweightmatricesforthedifferentlayers. And C isthe\n",
            "a\n",
            "vectorofpartialderivativesofC withrespecttotheoutputactivations. âˆ‡\n",
            "Thisisamuchmorecomplicatedexpressionthaninthesingle-neuroncase. Still, if\n",
            "youlookclosely,theessentialformisverysimilar,withlotsofpairsoftheform(wj ) TÎ£ (cid:48)(zj ). Whatâ€™smore,thematricesÎ£ (cid:48)(zj )havesmallentriesonthediagonal,nonelargerthan1/4. Providedtheweightmatriceswj arenâ€™ttoolarge,eachadditionalterm(wj ) TÎ£ (cid:48)(zl )tendsto\n",
            "makethegradientvectorsmaller,leadingtoavanishinggradient. Moregenerally,thelarge\n",
            "numberoftermsintheproducttendstoleadtoanunstablegradient,justasinourearlier\n",
            "example. Inpractice,empiricallyitistypicallyfoundinsigmoidnetworksthatgradients\n",
            "vanishexponentiallyquicklyinearlierlayers. Asaresult,learningslowsdowninthoselayers. Thisslowdownisnâ€™tmerelyanaccidentoraninconvenience: itâ€™safundamentalconsequence\n",
            "oftheapproachweâ€™retakingtolearning. 5.4 Other obstacles to deep learning\n",
            "Inthischapterweâ€™vefocusedonvanishinggradientsâ€“and,moregenerally,unstablegra-\n",
            "dientsâ€“asanobstacletodeeplearning. Infact,unstablegradientsarejustoneobstacle\n",
            "todeeplearning,albeitanimportantfundamentalobstacle.\n",
            "Muchongoingresearchaims\n",
            "tobetterunderstandthechallengesthatcanoccurwhentrainingdeepnetworks. Iwonâ€™t\n",
            "comprehensivelysummarizethatworkhere,butjustwanttobrieflymentionacoupleof\n",
            "papers,togiveyoutheflavorofsomeofthequestionspeopleareasking. Asafirstexample,in2010GlorotandBengio9foundevidencesuggestingthattheuseof\n",
            "9Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks,byXavierGlorotand\n",
            "YoshuaBengio(2010).SeealsotheearlierdiscussionoftheuseofsigmoidsinEfficientBackProp,by\n",
            "YannLeCun,LÃ©onBottou,GenevieveOrrandKlaus-RobertMÃ¼ller(1998). \n",
            "(cid:12)\n",
            "5.4. Otherobstaclestodeeplearning (cid:12) 165\n",
            "(cid:12)\n",
            "sigmoidactivationfunctionscancauseproblemstrainingdeepnetworks. Inparticular,they\n",
            "foundevidencethattheuseofsigmoidswillcausetheactivationsinthefinalhiddenlayerto\n",
            "saturatenear0earlyintraining,substantiallyslowingdownlearning. Theysuggestedsome\n",
            "alternativeactivationfunctions,whichappearnottosufferasmuchfromthissaturation\n",
            "problem. Asasecondexample,in2013Sutskever,Martens,DahlandHinton10studiedtheimpact\n",
            "ondeeplearningofboththerandomweightinitializationandthemomentumschedulein\n",
            "momentum-basedstochasticgradientdescent. Inbothcases,makinggoodchoicesmadea\n",
            "substantialdifferenceintheabilitytotraindeepnetworks.\n",
            "Theseexamplessuggestthatâ€œWhatmakesdeepnetworkshardtotrain?â€ isacomplex\n",
            "question. Inthischapter,weâ€™vefocusedontheinstabilitiesassociatedtogradient-based\n",
            "learning in deep networks. The results in the last two paragraphs suggest that there is\n",
            "alsoaroleplayedbythechoiceofactivationfunction,thewayweightsareinitialized,and\n",
            "5\n",
            "evendetailsofhowlearningbygradientdescentisimplemented. And,ofcourse,choiceof\n",
            "networkarchitectureandotherhyper-parametersisalsoimportant. Thus,manyfactorscan\n",
            "playaroleinmakingdeepnetworkshardtotrain,andunderstandingallthosefactorsis\n",
            "stillasubjectofongoingresearch. Thisallseemsratherdownbeatandpessimism-inducing. Butthegoodnewsisthatinthenextchapterweâ€™llturnthataround,anddevelopseveral\n",
            "approachestodeeplearningthattosomeextentmanagetoovercomeorroutearoundall\n",
            "thesechallenges. 10Ontheimportanceofinitializationandmomentumindeeplearning,byIlyaSutskever,James\n",
            "Martens,GeorgeDahlandGeoffreyHinton(2013).\n",
            "\n",
            "(cid:12)\n",
            "166 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 167\n",
            "(cid:12)\n",
            "6666\n",
            "Deep learning\n",
            "6\n",
            "Inthelastchapterwelearnedthatdeepneuralnetworksareoftenmuchhardertotrain\n",
            "thanshallowneuralnetworks. Thatâ€™sunfortunate,sincewehavegoodreasontobelieve\n",
            "thatifwecouldtraindeepnetstheyâ€™dbemuchmorepowerfulthanshallownets. Butwhile\n",
            "thenewsfromthelastchapterisdiscouraging,wewonâ€™tletitstopus. Inthischapter,weâ€™ll\n",
            "developtechniqueswhichcanbeusedtotraindeepnetworks,andapplytheminpractice. Weâ€™llalsolookatthebroaderpicture,brieflyreviewingrecentprogressonusingdeepnets\n",
            "forimagerecognition,speechrecognition,andotherapplications. Andweâ€™lltakeabrief,\n",
            "speculativelookatwhatthefuturemayholdforneuralnets,andforartificialintelligence.\n",
            "Thechapterisalongone.\n",
            "Tohelpyounavigate,letâ€™stakeatour. Thesectionsareonly\n",
            "looselycoupled,soprovidedyouhavesomebasicfamiliaritywithneuralnets,youcanjump\n",
            "towhatevermostinterestsyou. Themainpartofthechapterisanintroductiontooneofthemostwidelyusedtypesof\n",
            "deepnetwork: deepconvolutionalnetworks. Weâ€™llworkthroughadetailedexampleâ€“code\n",
            "andallâ€“ofusingconvolutionalnetstosolvetheproblemofclassifyinghandwrittendigits\n",
            "fromtheMNISTdataset:\n",
            "Weâ€™llstartouraccountofconvolutionalnetworkswiththeshallownetworksusedtoattackthis\n",
            "problemearlierinthebook. Throughmanyiterationsweâ€™llbuildupmoreandmorepowerful\n",
            "networks. Aswegoweâ€™llexploremanypowerfultechniques: convolutions,pooling,theuse\n",
            "ofGPUstodofarmoretrainingthanwedidwithourshallownetworks,thealgorithmic\n",
            "expansionofourtrainingdata(toreduceoverfitting),theuseofthedropouttechnique(also\n",
            "toreduceoverfitting),theuseofensemblesofnetworks,andothers. Theresultwillbea\n",
            "\n",
            "(cid:12)\n",
            "168 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "systemthatoffersnear-humanperformance. Ofthe10,000MNISTtestimagesâ€“images\n",
            "notseenduringtraining! â€“oursystemwillclassify9,967correctly.\n",
            "Hereâ€™sapeekatthe33\n",
            "imageswhicharemisclassified. Notethatthecorrectclassificationisinthetopright;our\n",
            "programâ€™sclassificationisinthebottomright:\n",
            "6\n",
            "Manyofthesearetoughevenforahumantoclassify. Consider,forexample,thethirdimage\n",
            "inthetoprow. Tomeitlooksmorelikeaâ€œ9â€thananâ€œ8â€,whichistheofficialclassification. Ournetworkalsothinksitâ€™saâ€œ9â€.\n",
            "Thiskindofâ€œerrorâ€isattheveryleastunderstandable,\n",
            "andperhapsevencommendable. Weconcludeourdiscussionofimagerecognitionwitha\n",
            "surveyofsomeofthespectacularrecentprogressusingnetworks(particularlyconvolutional\n",
            "nets)todoimagerecognition. Theremainderofthechapterdiscussesdeeplearningfromabroaderandlessdetailed\n",
            "perspective. Weâ€™llbrieflysurveyothermodelsofneuralnetworks,suchasrecurrentneural\n",
            "netsandlongshort-termmemoryunits,andhowsuchmodelscanbeappliedtoproblemsin\n",
            "speechrecognition,naturallanguageprocessing,andotherareas. Andweâ€™llspeculateabout\n",
            "thefutureofneuralnetworksanddeeplearning,rangingfromideaslikeintention-driven\n",
            "userinterfaces,totheroleofdeeplearninginartificialintelligence. Thechapterbuildsontheearlierchaptersinthebook,makinguseofandintegrating\n",
            "ideassuchasbackpropagation,regularization,thesoftmaxfunction,andsoon. However,to\n",
            "readthechapteryoudonâ€™tneedtohaveworkedindetailthroughalltheearlierchapters. It\n",
            "will,however,helptohavereadChapter1,onthebasicsofneuralnetworks. WhenIuse\n",
            "conceptsfromChapters2to5,Iprovidelinkssoyoucanfamiliarizeyourself,ifnecessary. Itâ€™sworthnotingwhatthechapterisnot. Itâ€™snotatutorialonthelatestandgreatest\n",
            "neuralnetworkslibraries. Norarewegoingtobetrainingdeepnetworkswithdozensof\n",
            "layerstosolveproblemsattheveryleadingedge. Rather,thefocusisonunderstanding\n",
            "someofthecoreprinciplesbehinddeepneuralnetworks,andapplyingtheminthesimple,\n",
            "easy-to-understandcontextoftheMNISTproblem. Putanotherway: thechapterisnot\n",
            "goingtobringyourightuptothefrontier. Rather,theintentofthisandearlierchaptersisto\n",
            "focusonfundamentals,andsotoprepareyoutounderstandawiderangeofcurrentwork.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 169\n",
            "(cid:12)\n",
            "6.1 Introducing convolutional networks\n",
            "Inearlierchapters,wetaughtourneuralnetworkstodoaprettygoodjobrecognizingimages\n",
            "ofhandwrittendigits:\n",
            "Wedidthisusingnetworksinwhichadjacentnetworklayersarefullyconnectedtoone\n",
            "another. Thatis, everyneuroninthenetworkisconnectedtoeveryneuroninadjacent\n",
            "layers:\n",
            "6\n",
            "Inparticular,foreachpixelintheinputimage,weencodedthepixelâ€™sintensityasthevalue\n",
            "foracorrespondingneuronintheinputlayer. Forthe28 28pixelimagesweâ€™vebeenusing,\n",
            "thismeansournetworkhas784(=28 28)inputneuroÃ—ns.\n",
            "Wethentrainedthenetworkâ€™s\n",
            "weightsandbiasessothatthenetworkÃ—â€™soutputwouldâ€“wehope! â€“correctlyidentifythe\n",
            "inputimage: â€˜0â€™,â€˜1â€™,â€˜2â€™,...,â€˜8â€™,orâ€˜9â€™.\n",
            "Ourearliernetworksworkprettywell: weâ€™veobtainedaclassificationaccuracybetter\n",
            "than98percent,usingtrainingandtestdatafromtheMNISThandwrittendigitdataset. But\n",
            "uponreflection,itâ€™sstrangetousenetworkswithfully-connectedlayerstoclassifyimages. Thereasonisthatsuchanetworkarchitecturedoesnottakeintoaccountthespatialstructure\n",
            "oftheimages. Forinstance,ittreatsinputpixelswhicharefarapartandclosetogether\n",
            "onexactlythesamefooting. Suchconceptsofspatialstructuremustinsteadbeinferred\n",
            "fromthetrainingdata. Butwhatif,insteadofstartingwithanetworkarchitecturewhichis\n",
            "tabularasa,weusedanarchitecturewhichtriestotakeadvantageofthespatialstructure? \n",
            "(cid:12)\n",
            "170 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "In this section I describe convolutional neural networks1. These networks use a special\n",
            "architecturewhichisparticularlywell-adaptedtoclassifyimages. Usingthisarchitecture\n",
            "makesconvolutionalnetworksfasttotrain. This,inturn,helpsustraindeep,many-layer\n",
            "networks,whichareverygoodatclassifyingimages. Today,deepconvolutionalnetworksor\n",
            "someclosevariantareusedinmostneuralnetworksforimagerecognition. Convolutionalneuralnetworksusethreebasicideas: localreceptivefields,sharedweights,\n",
            "andpooling.\n",
            "Letâ€™slookateachoftheseideasinturn. Local receptive fields: In the fully-connected layers shown earlier, the inputs were\n",
            "depictedasaverticallineofneurons. Inaconvolutionalnet,itâ€™llhelptothinkinsteadof\n",
            "theinputsasa28 28squareofneurons,whosevaluescorrespondtothe28 28pixel\n",
            "intensitiesweâ€™reusÃ—ingasinputs: Ã—\n",
            "6\n",
            "As per usual, weâ€™ll connect the input pixels to a layer of hidden neurons. But we wonâ€™t\n",
            "connecteveryinputpixeltoeveryhiddenneuron. Instead,weonlymakeconnectionsin\n",
            "small,localizedregionsoftheinputimage. Tobemoreprecise,eachneuroninthefirsthiddenlayerwillbeconnectedtoasmall\n",
            "regionoftheinputneurons,say,forexample,a5 5region,correspondingto25input\n",
            "pixels. So,foraparticularhiddenneuron,wemightÃ—haveconnectionsthatlooklikethis:\n",
            "Thatregionintheinputimageiscalledthelocalreceptivefieldforthehiddenneuron. Itâ€™sa\n",
            "littlewindowontheinputpixels.\n",
            "Eachconnectionlearnsaweight. Andthehiddenneuron\n",
            "1Theoriginsofconvolutionalneuralnetworksgobacktothe1970s.Buttheseminalpaperestablishing\n",
            "themodernsubjectofconvolutionalnetworkswasa1998paper,Gradient-basedlearningappliedto\n",
            "documentrecognition,byYannLeCun,LÃ©onBottou,YoshuaBengio,andPatrickHaffner.LeCunhas\n",
            "sincemadeaninterestingremarkontheterminologyforconvolutionalnets:â€œThe[biological]neural\n",
            "inspirationinmodelslikeconvolutionalnetsisverytenuous. Thatâ€™swhyIcallthemâ€˜convolutional\n",
            "netsâ€™notâ€˜convolutionalneuralnetsâ€™,andwhywecallthenodesâ€˜unitsâ€™andnotâ€˜neuronsâ€™â€.Despitethis\n",
            "remark,convolutionalnetsusemanyofthesameideasastheneuralnetworksweâ€™vestudieduptonow:\n",
            "ideassuchasbackpropagation,gradientdescent,regularization,non-linearactivationfunctions,andso\n",
            "on.Andsowewillfollowcommonpractice,andconsiderthematypeofneuralnetwork.Iwillusethe\n",
            "termsâ€œconvolutionalneuralnetworkâ€andâ€œconvolutionalnet(work)â€interchangeably.Iwillalsouse\n",
            "thetermsâ€œ[artificial]neuronâ€andâ€œunitâ€interchangeably. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 171\n",
            "(cid:12)\n",
            "learnsanoverallbiasaswell. Youcanthinkofthatparticularhiddenneuronaslearningto\n",
            "analyzeitsparticularlocalreceptivefield. We then slide the local receptive field across the entire input image. For each local\n",
            "receptivefield,thereisadifferenthiddenneuroninthefirsthiddenlayer. Toillustratethis\n",
            "concretely,letâ€™sstartwithalocalreceptivefieldinthetop-leftcorner:\n",
            "Thenweslidethelocalreceptivefieldoverbyonepixeltotheright(i.e.,byoneneuron),to\n",
            "connecttoasecondhiddenneuron: 6\n",
            "Andsoon,buildingupthefirsthiddenlayer. Notethatifwehavea28 28inputimage,and\n",
            "5 5localreceptivefields,thentherewillbe24 24neuronsintheÃ—hiddenlayer. Thisis\n",
            "beÃ—causewecanonlymovethelocalreceptivefieldÃ—23neuronsacross(or23neuronsdown),\n",
            "beforecollidingwiththeright-handside(orbottom)oftheinputimage. Iâ€™veshownthelocalreceptivefieldbeingmovedbyonepixelatatime. Infact,sometimes\n",
            "adifferentstridelengthisused. Forinstance,wemightmovethelocalreceptivefield2\n",
            "pixelstotheright(ordown),inwhichcaseweâ€™dsayastridelengthof2isused. Inthis\n",
            "chapterweâ€™llmostlystickwithstridelength1,butitâ€™sworthknowingthatpeoplesometimes\n",
            "experimentwithdifferentstridelengths2. Sharedweightsandbiases: Iâ€™vesaidthateachhiddenneuronhasabiasand5 5\n",
            "weightsconnectedtoitslocalreceptivefield. WhatIdidnotyetmentionisthatweâ€™regoÃ—ing\n",
            "tousethesameweightsandbiasforeachofthe24 24hiddenneurons.\n",
            "Inotherwords,\n",
            "forthe j,k-thhiddenneuron,theoutputis: Ã—\n",
            "(cid:130) 4 4 (cid:140)\n",
            "(cid:88)(cid:88)\n",
            "Ïƒ b+ w\n",
            "l,m\n",
            "a\n",
            "j+l,k+m\n",
            ". (6.1)\n",
            "l=0m=0\n",
            "2Aswasdoneinearlierchapters,ifweâ€™reinterestedintryingdifferentstridelengthsthenwecanuse\n",
            "validationdatatopickoutthestridelengthwhichgivesthebestperformance.Formoredetails,seethe\n",
            "earlierdiscussionofhowtochoosehyper-parametersinaneuralnetwork.Thesameapproachmayalso\n",
            "beusedtochoosethesizeofthelocalreceptivefieldâ€“thereis,ofcourse,nothingspecialaboutusinga\n",
            "5 5localreceptivefield. Ingeneral,largerlocalreceptivefieldstendtobehelpfulwhentheinput\n",
            "imÃ—agesaresignificantlylargerthanthe28 28pixelMNISTimages.\n",
            "Ã—\n",
            "\n",
            "(cid:12)\n",
            "172 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Here,Ïƒistheneuralactivationfunctionâ€“perhapsthesigmoidfunctionweusedinearlier\n",
            "chapters. bisthesharedvalueforthebias.\n",
            "w isa5 5arrayofsharedweights. And,\n",
            "l,m\n",
            "finally,weusea todenotetheinputactivationatpositÃ—ion x,y. x,y\n",
            "Thismeansthatalltheneuronsinthefirsthiddenlayerdetectexactlythesamefeature3,\n",
            "justatdifferentlocationsintheinputimage. Toseewhythismakessense,supposethe\n",
            "weightsandbiasaresuchthatthehiddenneuroncanpickout, say, averticaledgeina\n",
            "particularlocalreceptivefield. Thatabilityisalsolikelytobeusefulatotherplacesinthe\n",
            "image. Andsoitisusefultoapplythesamefeaturedetectoreverywhereintheimage. Toput\n",
            "itinslightlymoreabstractterms,convolutionalnetworksarewelladaptedtothetranslation\n",
            "invarianceofimages: moveapictureofacat(say)alittleways,anditâ€™sstillanimageofa\n",
            "cat4. Forthisreason,wesometimescallthemapfromtheinputlayertothehiddenlayera\n",
            "featuremap. Wecalltheweightsdefiningthefeaturemapthesharedweights.\n",
            "Andwecall\n",
            "thebiasdefiningthefeaturemapinthiswaythesharedbias. Thesharedweightsandbias\n",
            "areoftensaidtodefineakernelorfilter. Intheliterature,peoplesometimesusetheseterms\n",
            "inslightlydifferentways,andforthatreasonIâ€™mnotgoingtobemoreprecise;rather,ina\n",
            "6\n",
            "moment,weâ€™lllookatsomeconcreteexamples. ThenetworkstructureIâ€™vedescribedsofarcandetectjustasinglekindoflocalized\n",
            "feature. Todoimagerecognitionweâ€™llneedmorethanonefeaturemap. Andsoacomplete\n",
            "convolutionallayerconsistsofseveraldifferentfeaturemaps:\n",
            "Intheexampleshown,thereare3featuremaps. Eachfeaturemapisdefinedbyasetof\n",
            "5 5sharedweights,andasinglesharedbias. Theresultisthatthenetworkcandetect\n",
            "3Ã—differentkindsoffeatures, witheachfeaturebeingdetectableacrosstheentireimage. Iâ€™veshownjust3featuremaps,tokeepthediagramabovesimple. However,inpractice\n",
            "convolutionalnetworksmayusemore(andperhapsmanymore)featuremaps. Oneofthe\n",
            "earlyconvolutionalnetworks,LeNet-5,used6featuremaps,eachassociatedtoa5 5local\n",
            "receptivefield,torecognizeMNISTdigits. SotheexampleillustratedaboveisactualÃ—lypretty\n",
            "closetoLeNet-5. Intheexampleswedeveloplaterinthechapterweâ€™lluseconvolutional\n",
            "layerswith20and40featuremaps. Letâ€™stakeaquickpeekatsomeofthefeatureswhich\n",
            "arelearned. 3Ihavenâ€™tpreciselydefinedthenotionofafeature. Informally,thinkofthefeaturedetectedbya\n",
            "hiddenneuronasthekindofinputpatternthatwillcausetheneurontoactivate:itmightbeanedgein\n",
            "theimage,forinstance,ormaybesomeothertypeofshape. 4Infact,fortheMNISTdigitclassificationproblemweâ€™vebeenstudying,theimagesarecenteredand\n",
            "size-normalized.SoMNISThaslesstranslationinvariancethanimagesfoundâ€œinthewildâ€,sotospeak. Still,featureslikeedgesandcornersarelikelytobeusefulacrossmuchoftheinputspace.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 173\n",
            "(cid:12)\n",
            "The20imagescorrespondto20differentfeaturemaps(orfilters,orkernels). Eachmapis\n",
            "representedasa5 5blockimage,correspondingtothe5 5weightsinthelocalreceptive\n",
            "field. WhiterblockÃ—smeanasmaller(typically,morenegaÃ—tive)weight,sothefeaturemap 6\n",
            "respondslesstocorrespondinginputpixels. Darkerblocksmeanalargerweight,sothe\n",
            "featuremaprespondsmoretothecorrespondinginputpixels. Veryroughlyspeaking,the\n",
            "imagesaboveshowthetypeoffeaturestheconvolutionallayerrespondsto. Sowhatcanweconcludefromthesefeaturemaps? Itâ€™sclearthereisspatialstructure\n",
            "here beyond what weâ€™d expect at random: many of the features have clear sub-regions\n",
            "oflightanddark. Thatshowsournetworkreallyislearningthingsrelatedtothespatial\n",
            "structure.\n",
            "However,beyondthat,itâ€™sdifficulttoseewhatthesefeaturedetectorsarelearning. Certainly,weâ€™renotlearning(say)theGaborfilterswhichhavebeenusedinmanytraditional\n",
            "approachestoimagerecognition. Infact,thereâ€™snowalotofworkonbetterunderstanding\n",
            "the features learnt by convolutional networks. If youâ€™re interested in following up on\n",
            "thatwork,IsuggeststartingwiththepaperVisualizingandUnderstandingConvolutional\n",
            "NetworksbyMatthewZeilerandRobFergus(2013). Abigadvantageofsharingweightsandbiasesisthatitgreatlyreducesthenumberof\n",
            "parametersinvolvedinaconvolutionalnetwork. Foreachfeaturemapweneed25=5 5\n",
            "sharedweights,plusasinglesharedbias. Soeachfeaturemaprequires26parameters. IfÃ—we\n",
            "have20featuremapsthatâ€™satotalof20 26=520parametersdefiningtheconvolutional\n",
            "layer. Bycomparison,supposewehadafuÃ—llyconnectedfirstlayer,with784=28 28input\n",
            "neurons,andarelativelymodest30hiddenneurons,asweusedinmanyoftheÃ—examples\n",
            "earlierinthebook. Thatâ€™satotalof784 30weights,plusanextra30biases,foratotal\n",
            "of23,550parameters.\n",
            "Inotherwords,theÃ—fully-connectedlayerwouldhavemorethan40\n",
            "timesasmanyparametersastheconvolutionallayer.\n",
            "Ofcourse,wecanâ€™treallydoadirectcomparisonbetweenthenumberofparameters,\n",
            "sincethetwomodelsaredifferentinessentialways. But,intuitively,itseemslikelythatthe\n",
            "useoftranslationinvariancebytheconvolutionallayerwillreducethenumberofparameters\n",
            "it needs to get the same performance as the fully-connected model. That, in turn, will\n",
            "resultinfastertrainingfortheconvolutionalmodel,and,ultimately,willhelpusbuilddeep\n",
            "networksusingconvolutionallayers. Incidentally,thenameconvolutionalcomesfromthefactthattheoperationinEquation\n",
            "(125)issometimesknownasaconvolution. Alittlemoreprecisely,peoplesometimeswrite\n",
            "thatequationasa1 = Ïƒ (b+w a0 ),wherea1 denotesthesetofoutputactivationsfrom\n",
            "onefeaturemap,a0isthesetoâˆ—finputactivations,and iscalledaconvolutionoperation. âˆ—\n",
            "\n",
            "(cid:12)\n",
            "174 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Weâ€™renotgoingtomakeanydeepuseofthemathematicsofconvolutions,soyoudonâ€™tneed\n",
            "toworrytoomuchaboutthisconnection. Butitâ€™sworthatleastknowingwherethename\n",
            "comesfrom. Poolinglayers: Inadditiontotheconvolutionallayersjustdescribed,convolutional\n",
            "neuralnetworksalsocontainpoolinglayers. Poolinglayersareusuallyusedimmediately\n",
            "afterconvolutionallayers. Whatthepoolinglayersdoissimplifytheinformationinthe\n",
            "outputfromtheconvolutionallayer. Indetail,apoolinglayertakeseachfeaturemap5outputfromtheconvolutionallayer\n",
            "andpreparesacondensedfeaturemap. Forinstance,eachunitinthepoolinglayermay\n",
            "summarizearegionof(say)2 2neuronsinthepreviouslayer. Asaconcreteexample,\n",
            "onecommonprocedureforpooÃ—lingisknownasmax-pooling. Inmax-pooling,apooling\n",
            "unitsimplyoutputsthemaximumactivationinthe2 2inputregion,asillustratedinthe\n",
            "followingdiagram: Ã—\n",
            "6\n",
            "Notethatsincewehave24 24neuronsoutputfromtheconvolutionallayer,afterpooling\n",
            "wehave12 12neurons. Ã—\n",
            "AsmentÃ—ionedabove,theconvolutionallayerusuallyinvolvesmorethanasinglefeature\n",
            "map. Weapplymax-poolingtoeachfeaturemapseparately. Soiftherewerethreefeature\n",
            "maps,thecombinedconvolutionalandmax-poolinglayerswouldlooklike:\n",
            "Wecanthinkofmax-poolingasawayforthenetworktoaskwhetheragivenfeatureisfound\n",
            "anywhereinaregionoftheimage. Itthenthrowsawaytheexactpositionalinformation.\n",
            "Theintuitionisthatonceafeaturehasbeenfound,itsexactlocationisnâ€™tasimportantasits\n",
            "roughlocationrelativetootherfeatures. Abigbenefitisthattherearemanyfewerpooled\n",
            "features,andsothishelpsreducethenumberofparametersneededinlaterlayers.\n",
            "Max-poolingisnâ€™ttheonlytechniqueusedforpooling. Anothercommonapproachis\n",
            "knownasL2pooling. Here,insteadoftakingthemaximumactivationofa2 2regionof\n",
            "Ã—\n",
            "5Thenomenclatureisbeingusedlooselyhere.Inparticular,Iâ€™musingâ€œfeaturemapâ€tomeannotthe\n",
            "functioncomputedbytheconvolutionallayer,butrathertheactivationofthehiddenneuronsoutput\n",
            "fromthelayer.Thiskindofmildabuseofnomenclatureisprettycommonintheresearchliterature. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 175\n",
            "(cid:12)\n",
            "neurons,wetakethesquarerootofthesumofthesquaresoftheactivationsinthe2 2\n",
            "region. Whilethedetailsaredifferent,theintuitionissimilartomax-pooling: L2poolingÃ—is\n",
            "awayofcondensinginformationfromtheconvolutionallayer. Inpractice,bothtechniques\n",
            "havebeenwidelyused. Andsometimespeopleuseothertypesofpoolingoperation. If\n",
            "youâ€™rereallytryingtooptimizeperformance,youmayusevalidationdatatocompareseveral\n",
            "differentapproachestopooling,andchoosetheapproachwhichworksbest. Butweâ€™renot\n",
            "goingtoworryaboutthatkindofdetailedoptimization.\n",
            "Puttingitalltogether: Wecannowputalltheseideastogethertoformacomplete\n",
            "convolutionalneuralnetwork. Itâ€™ssimilartothearchitecturewewerejustlookingat,but\n",
            "hastheadditionofalayerof10outputneurons,correspondingtothe10possiblevaluesfor\n",
            "MNISTdigits(â€˜0â€™,â€˜1â€™,â€˜2â€™,etc):\n",
            "6\n",
            "Thenetworkbeginswith28 28inputneurons,whichareusedtoencodethepixelintensities\n",
            "fortheMNISTimage. ThisÃ—isthenfollowedbyaconvolutionallayerusinga5 5local\n",
            "receptivefieldand3featuremaps. Theresultisalayerof3 24 24hiddenfeatureÃ—neurons. Thenextstepisamax-poolinglayer,appliedto2 2regioÃ—ns,Ã—acrosseachofthe3feature\n",
            "maps. Theresultisalayerof3 12 12hiddenfeÃ—atureneurons. Ã— Ã—\n",
            "Thefinallayerofconnectionsinthenetworkisafully-connectedlayer. Thatis,thislayer\n",
            "connectseveryneuronfromthemax-pooledlayertoeveryoneofthe10outputneurons. Thisfully-connectedarchitectureisthesameasweusedinearlierchapters. Note,however,\n",
            "thatinthediagramabove,Iâ€™veusedasinglearrow,forsimplicity,ratherthanshowingall\n",
            "theconnections. Ofcourse,youcaneasilyimaginetheconnections. This convolutional architecture is quite different to the architectures used in earlier\n",
            "chapters. Buttheoverallpictureissimilar: anetworkmadeofmanysimpleunits,whose\n",
            "behaviorsaredeterminedbytheirweightsandbiases.\n",
            "Andtheoverallgoalisstillthesame:\n",
            "tousetrainingdatatotrainthenetworkâ€™sweightsandbiasessothatthenetworkdoesa\n",
            "goodjobclassifyinginputdigits. In particular, just as earlier in the book, we will train our network using stochastic\n",
            "gradientdescentandbackpropagation. Thismostlyproceedsinexactlythesamewayasin\n",
            "earlierchapters.\n",
            "However,wedoneedtomakeafewmodificationstothebackpropagation\n",
            "procedure. Thereasonisthatourearlierderivationofbackpropagationwasfornetworks\n",
            "withfully-connectedlayers. Fortunately,itâ€™sstraightforwardtomodifythederivationfor\n",
            "convolutionalandmax-poolinglayers. Ifyouâ€™dliketounderstandthedetails,thenIinvite\n",
            "youtoworkthroughthefollowingproblem. Bewarnedthattheproblemwilltakesometime\n",
            "toworkthrough,unlessyouâ€™vereallyinternalizedtheearlierderivationofbackpropagation\n",
            "(inwhichcaseitâ€™seasy). Problem\n",
            "\n",
            "(cid:12)\n",
            "176 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "BackpropagationinaconvolutionalnetworkThecoreequationsofbackpropagationin\n",
            "â€¢ anetworkwithfully-connectedlayersare(BP1)â€“(BP4). Supposewehaveanetwork\n",
            "containingaconvolutionallayer,amax-poolinglayer,andafully-connectedoutput\n",
            "layer,asinthenetworkdiscussedabove. Howaretheequationsofbackpropagation\n",
            "modified? 6.2 Convolutional neural networks in practice\n",
            "Weâ€™venowseenthecoreideasbehindconvolutionalneuralnetworks. Letâ€™slookathowthey\n",
            "workinpractice,byimplementingsomeconvolutionalnetworks,andapplyingthemtothe\n",
            "MNISTdigitclassificationproblem. Theprogramweâ€™llusetodothisiscallednetwork3.py,\n",
            "anditâ€™sanimprovedversionoftheprogramsnetwork.pyandnetwork2.pydevelopedin\n",
            "earlierchapters6. Ifyouwishtofollowalong,thecodeisavailableonGitHub. Notethat\n",
            "weâ€™llworkthroughthecodefornetwork3.pyitselfinthenextsection.\n",
            "Inthissection,weâ€™ll\n",
            "usenetwork3.pyasalibrarytobuildconvolutionalnetworks. 6 The programs network.py and network2.py were implemented using Python and\n",
            "the matrix library Numpy. Those programs worked from first principles, and got right\n",
            "downintothedetailsofbackpropagation,stochasticgradientdescent,andsoon. Butnow\n",
            "thatweunderstandthosedetails,fornetwork3.pyweâ€™regoingtouseamachinelearning\n",
            "libraryknownasTheano7. UsingTheanomakesiteasytoimplementbackpropagationfor\n",
            "convolutionalneuralnetworks,sinceitautomaticallycomputesallthemappingsinvolved. Theano is also quite a bit faster than our earlier code (which was written to be easy to\n",
            "understand, not fast), and this makes it practical to train more complex networks. In\n",
            "particular,onegreatfeatureofTheanoisthatitcanruncodeoneitheraCPUor,ifavailable,\n",
            "aGPU.RunningonaGPUprovidesasubstantialspeedupand,again,helpsmakeitpractical\n",
            "totrainmorecomplexnetworks.\n",
            "Ifyouwishtofollowalong, thenyouâ€™llneedtogetTheanorunningonyoursystem. ToinstallTheano,followtheinstructionsattheprojectâ€™shomepage. Theexampleswhich\n",
            "followwererunusingTheano0.68. SomewererununderMacOSXYosemite,withnoGPU. SomewererunonUbuntu14.04,withanNVIDIAGPU.Andsomeoftheexperimentswere\n",
            "rununderboth. Togetnetwork3.pyrunningyouâ€™llneedtosettheGPUflagtoeitherTrue\n",
            "orFalse(asappropriate)inthenetwork3.pysource.\n",
            "Beyondthat,togetTheanoupand\n",
            "runningonaGPUyoumayfindtheinstructionsherehelpful. Therearealsotutorialsonthe\n",
            "web,easilyfoundusingGoogle,whichcanhelpyougetthingsworking. Ifyoudonâ€™thavea\n",
            "GPUavailablelocally,thenyoumaywishtolookintoAmazonWebServicesEC2G2spot\n",
            "instances. NotethatevenwithaGPUthecodewilltakesometimetoexecute.\n",
            "Manyofthe\n",
            "experimentstakefromminutestohourstorun. OnaCPUitmaytakedaystorunthemost\n",
            "complexoftheexperiments. Asinearlierchapters,Isuggestsettingthingsrunning,and\n",
            "continuingtoread,occasionallycomingbacktochecktheoutputfromthecode. Ifyouâ€™re\n",
            "6Notealsothatnetwork3.pyincorporatesideasfromtheTheanolibraryâ€™sdocumentationonconvo-\n",
            "lutionalneuralnets(notablytheimplementationofLeNet-5),fromMishaDenilâ€™simplementationof\n",
            "dropout,andfromChrisOlah. 7SeeTheano: ACPUandGPUMathExpressionCompilerinPython,byJamesBergstra,Olivier\n",
            "Breuleux,FredericBastien,PascalLamblin,RavzanPascanu,GuillaumeDesjardins,JosephTurian,David\n",
            "Warde-Farley,andYoshuaBengio(2010).TheanoisalsothebasisforthepopularPylearn2andKeras\n",
            "neuralnetworkslibraries.OtherpopularneuralnetslibrariesatthetimeofthiswritingincludeCaffe\n",
            "andTorch. 8AsIreleasethischapter,thecurrentversionofTheanohaschangedtoversion0.7. Iâ€™veactually\n",
            "reruntheexamplesunderTheano0.7andgetextremelysimilarresultstothosereportedinthetext. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 177\n",
            "(cid:12)\n",
            "usingaCPU,youmaywishtoreducethenumberoftrainingepochsforthemorecomplex\n",
            "experiments,orperhapsomitthementirely. Togetabaseline,weâ€™llstartwithashallowarchitectureusingjustasinglehiddenlayer,\n",
            "containing100hiddenneurons.\n",
            "Weâ€™lltrainfor60epochs,usingalearningrateofÎ· =0.1,a\n",
            "mini-batchsizeof10,andnoregularization. Herewego9:\n",
            ">>> import network3\n",
            ">>> from network3 import Network\n",
            ">>> from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
            ">>> training_data, validation_data, test_data = network3.load_data_shared()\n",
            ">>> mini_batch_size = 10\n",
            ">>> net = Network([FullyConnectedLayer(n_in=784, n_out=100),SoftmaxLayer(n_in\n",
            "=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Iobtainedabestclassificationaccuracyof97.80percent. Thisistheclassificationaccuracy\n",
            "on the test_data, evaluated at the training epoch where we get the best classification\n",
            "accuracyonthevalidation_data.\n",
            "Usingthevalidationdatatodecidewhentoevaluate\n",
            "6\n",
            "thetestaccuracyhelpsavoidoverfittingtothetestdata(seethisearlierdiscussionoftheuse\n",
            "ofvalidationdata). Wewillfollowthispracticebelow. Yourresultsmayvaryslightly,since\n",
            "thenetworkâ€™sweightsandbiasesarerandomlyinitialized10. This97.80percentaccuracyisclosetothe98.04percentaccuracyobtainedbackin\n",
            "Chapter3,usingasimilarnetworkarchitectureandlearninghyper-parameters. Inparticular,\n",
            "bothexamplesusedashallownetwork,withasinglehiddenlayercontaining100hidden\n",
            "neurons. Bothalsotrainedfor60epochs,usedamini-batchsizeof10,andalearningrate\n",
            "ofÎ· =0.1. Therewere,however,twodifferencesintheearliernetwork. First,weregularizedthe\n",
            "earliernetwork,tohelpreducetheeffectsofoverfitting. Regularizingthecurrentnetwork\n",
            "doesimprovetheaccuracies,butthegainisonlysmall,andsoweâ€™llholdoffworryingabout\n",
            "regularizationuntillater. Second,whilethefinallayerintheearliernetworkusedsigmoid\n",
            "activationsandthecross-entropycostfunction,thecurrentnetworkusesasoftmaxfinal\n",
            "layer,andthelog-likelihoodcostfunction. AsexplainedinChapter3thisisnâ€™tabigchange. Ihavenâ€™tmadethisswitchforanyparticularlydeepreasonâ€“mostly,Iâ€™vedoneitbecause\n",
            "softmaxpluslog-likelihoodcostismorecommoninmodernimageclassificationnetworks. Canwedobetterthantheseresultsusingadeepernetworkarchitecture? Letâ€™sbeginbyinsertingaconvolutionallayer,rightatthebeginningofthenetwork. Weâ€™ll\n",
            "use5by5localreceptivefields,astridelengthof1,and20featuremaps. Weâ€™llalsoinsert\n",
            "amax-poolinglayer,whichcombinesthefeaturesusing2by2poolingwindows.\n",
            "Sothe\n",
            "overallnetworkarchitecturelooksmuchlikethearchitecturediscussedinthelastsection,\n",
            "butwithanextrafully-connectedlayer:\n",
            "9Codefortheexperimentsinthissectionmaybefoundinthisscript.Notethatthecodeinthescript\n",
            "simplyduplicatesandparallelsthediscussioninthissection. NotealsothatthroughoutthesectionIâ€™veexplicitlyspecifiedthenumberoftrainingepochs.Iâ€™vedone\n",
            "thisforclarityabouthowweâ€™retraining.Inpractice,itâ€™sworthusingearlystopping,thatis,tracking\n",
            "accuracyonthevalidationset,andstoppingtrainingwhenweareconfidentthevalidationaccuracyhas\n",
            "stoppedimproving. 10Infact,inthisexperimentIactuallydidthreeseparaterunstraininganetworkwiththisarchitecture. Ithenreportedthetestaccuracywhichcorrespondedtothebestvalidationaccuracyfromanyofthe\n",
            "threeruns.Usingmultiplerunshelpsreducevariationinresults,whichisusefulwhencomparingmany\n",
            "architectures,aswearedoing.Iâ€™vefollowedthisprocedurebelow,exceptwherenoted.Inpractice,it\n",
            "madelittledifferencetotheresultsobtained. \n",
            "(cid:12)\n",
            "178 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Inthisarchitecture,wecanthinkoftheconvolutionalandpoolinglayersaslearningabout\n",
            "localspatialstructureintheinputtrainingimage,whilethelater,fully-connectedlayerlearns\n",
            "atamoreabstractlevel,integratingglobalinformationfromacrosstheentireimage. Thisis\n",
            "acommonpatterninconvolutionalneuralnetworks. Letâ€™strainsuchanetwork,andseehowitperforms11:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "6\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Thatgetsusto98.78percentaccuracy,whichisaconsiderableimprovementoveranyof\n",
            "ourpreviousresults. Indeed,weâ€™vereducedourerrorratebybetterthanathird,whichisa\n",
            "greatimprovement.\n",
            "Inspecifyingthenetworkstructure,Iâ€™vetreatedtheconvolutionalandpoolinglayersas\n",
            "asinglelayer. Whethertheyâ€™reregardedasseparatelayersorasasinglelayeristosome\n",
            "extentamatteroftaste. network3.pytreatsthemasasinglelayerbecauseitmakesthe\n",
            "codefornetwork3.pyalittlemorecompact. However,itiseasytomodifynetwork3.pyso\n",
            "thelayerscanbespecifiedseparately,ifdesired. Exercise\n",
            "Whatclassificationaccuracydoyougetifyouomitthefully-connectedlayer,and\n",
            "â€¢ justusetheconvolutional-poolinglayerandsoftmaxlayer? Doestheinclusionofthe\n",
            "fully-connectedlayerhelp? Canweimproveonthe98.78percentclassificationaccuracy? Letâ€™stryinsertingasecondconvolutional-poolinglayer. Weâ€™llmaketheinsertionbetween\n",
            "theexistingconvolutional-poolinglayerandthefully-connectedhiddenlayer.\n",
            "Again,weâ€™ll\n",
            "usea5 5localreceptivefield,andpoolover2 2regions. Letâ€™sseewhathappenswhen\n",
            "wetrainÃ—usingsimilarhyper-parameterstobeforeÃ—:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "ConvPoolLayer(\n",
            "11Iâ€™vecontinuedtouseamini-batchsizeof10here.Infact,aswediscussedearlieritmaybepossible\n",
            "tospeeduptrainingusinglargermini-batches.Iâ€™vecontinuedtousethesamemini-batchsizemostlyfor\n",
            "consistencywiththeexperimentsinearlierchapters. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 179\n",
            "(cid:12)\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Onceagain,wegetanimprovement: weâ€™renowat99.06percentclassificationaccuracy! Thereâ€™stwonaturalquestionstoaskatthispoint. Thefirstquestionis: whatdoesiteven\n",
            "meantoapplyasecondconvolutional-poolinglayer? Infact,youcanthinkofthesecond\n",
            "convolutional-poolinglayerashavingasinput12 12â€œimagesâ€,whoseâ€œpixelsâ€represent\n",
            "thepresence(orabsence)ofparticularlocalizedfeaÃ—turesintheoriginalinputimage. Soyou\n",
            "canthinkofthislayerashavingasinputaversionoftheoriginalinputimage. Thatversion\n",
            "isabstractedandcondensed,butstillhasalotofspatialstructure,andsoitmakessenseto\n",
            "useasecondconvolutional-poolinglayer. Thatâ€™sasatisfyingpointofview,butgivesrisetoasecondquestion. Theoutputfrom\n",
            "thepreviouslayerinvolves20separatefeaturemaps,andsothereare20 12 12inputs\n",
            "to the second convolutional-pooling layer. Itâ€™s as though weâ€™ve got 20Ã—separÃ—ate images 6\n",
            "inputtotheconvolutional-poolinglayer,notasingleimage,aswasthecaseforthefirst\n",
            "convolutional-poolinglayer. Howshouldneuronsinthesecondconvolutional-poolinglayer\n",
            "respondtothesemultipleinputimages? Infact,weâ€™llalloweachneuroninthislayertolearn\n",
            "fromall20 5 5inputneuronsinitslocalreceptivefield. Moreinformally: thefeature\n",
            "detectorsinÃ—theÃ—secondconvolutional-poolinglayerhaveaccesstoallthefeaturesfromthe\n",
            "previouslayer,butonlywithintheirparticularlocalreceptivefield12. Problem\n",
            "UsingthetanhactivationfunctionSeveraltimesearlierinthebookIâ€™vementioned\n",
            "â€¢ argumentsthatthetanhfunctionmaybeabetteractivationfunctionthanthesigmoid\n",
            "function. Weâ€™veneveractedonthosesuggestions, sincewewerealreadymaking\n",
            "plentyofprogresswiththesigmoid.\n",
            "Butnowletâ€™strysomeexperimentswithtanh\n",
            "as our activation function. Try training the network with tanh activations in the\n",
            "convolutionalandfully-connectedlayers13. Beginwiththesamehyper-parametersas\n",
            "forthesigmoidnetwork,buttrainfor20epochsinsteadof60. Howwelldoesyour\n",
            "networkperform? Whatifyoucontinueoutto60epochs? Tryplottingtheper-epoch\n",
            "validationaccuraciesforbothtanh-andsigmoid-basednetworks,allthewayoutto60\n",
            "epochs. Ifyourresultsaresimilartomine,youâ€™llfindthetanhnetworkstrainalittle\n",
            "faster,butthefinalaccuraciesareverysimilar. Canyouexplainwhythetanhnetwork\n",
            "mighttrainfaster? Canyougetasimilartrainingspeedwiththesigmoid,perhaps\n",
            "bychangingthelearningrate,ordoingsomerescaling14? Tryahalf-dozeniterations\n",
            "onthelearninghyper-parametersornetworkarchitecture,searchingforwaysthat\n",
            "tanhmaybesuperiortothesigmoid. Note: Thisisanopen-endedproblem.\n",
            "Personally,\n",
            "Ididnotfindmuchadvantageinswitchingtotanh,althoughIhavenâ€™texperimented\n",
            "exhaustively,andperhapsyoumayfindaway. Inanycase,inamomentwewillfindan\n",
            "advantageinswitchingtotherectifiedlinearactivationfunction,andsowewonâ€™tgoany\n",
            "12Thisissuewouldhaveariseninthefirstlayeriftheinputimageswereincolor.Inthatcaseweâ€™d\n",
            "have3inputfeaturesforeachpixel,correspondingtored,greenandbluechannelsintheinputimage. Soweâ€™dallowthefeaturedetectorstohaveaccesstoallcolorinformation,butonlywithinagivenlocal\n",
            "receptivefield. 13Note that you can pass activation_fn=tanh as a parameter to the ConvPoolLayer and\n",
            "FullyConnectedLayerclasses. 14YoumayperhapsfindinspirationinrecallingthatÏƒ (z)=(1+tanh(z/2)) /2.\n",
            "\n",
            "(cid:12)\n",
            "180 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "deeperintotheuseoftanh. Usingrectifiedlinearunits: Thenetworkweâ€™vedevelopedatthispointisactuallyavariant\n",
            "ofoneofthenetworksusedintheseminal1998paper15introducingtheMNISTproblem,\n",
            "anetworkknownasLeNet-5. Itâ€™sagoodfoundationforfurtherexperimentation,andfor\n",
            "buildingupunderstandingandintuition. Inparticular,therearemanywayswecanvarythe\n",
            "networkinanattempttoimproveourresults. Asabeginning,letâ€™schangeourneuronssothatinsteadofusingasigmoidactivation\n",
            "function, we use rectified linear units. That is, weâ€™ll use the activation function f(z)\n",
            "max(0,z).\n",
            "Weâ€™lltrainfor60epochs,withalearningrateofÎ· =0.03. Ialsofoundthatâ‰¡it\n",
            "helpsalittletousesomel2regularization,withregularizationparameterÎ» =0.1:\n",
            ">>> from network3 import ReLU\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5), poolsize=(2, 2), activation_fn=ReLU),\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12), filter_shape=(40, 20, 5, 5),\n",
            "6\n",
            "poolsize=(2, 2), activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4, n_out\n",
            "=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.03, validation_data, test_data,\n",
            "lmbda=0.1)\n",
            "Iobtainedaclassificationaccuracyof99.23percent. Itâ€™samodestimprovementoverthe\n",
            "sigmoidresults(99.06).\n",
            "However,acrossallmyexperimentsIfoundthatnetworksbased\n",
            "onrectifiedlinearunitsconsistentlyoutperformednetworksbasedonsigmoidactivation\n",
            "functions. Thereappearstobearealgaininmovingtorectifiedlinearunitsforthisproblem. What makes the rectified linear activation function better than the sigmoid or tanh\n",
            "functions? Atpresent,wehaveapoorunderstandingoftheanswertothisquestion.\n",
            "Indeed,\n",
            "rectifiedlinearunitshaveonlybeguntobewidelyusedinthepastfewyears. Thereason\n",
            "forthatrecentadoptionisempirical: afewpeopletriedrectifiedlinearunits,oftenonthe\n",
            "basisofhunchesorheuristicarguments16. Theygotgoodresultsclassifyingbenchmarkdata\n",
            "sets,andthepracticehasspread. Inanidealworldweâ€™dhaveatheorytellinguswhich\n",
            "activationfunctiontopickforwhichapplication. Butatpresentweâ€™realongwayfromsuch\n",
            "aworld. Ishouldnotbeatallsurprisediffurthermajorimprovementscanbeobtainedby\n",
            "anevenbetterchoiceofactivationfunction. AndIalsoexpectthatincomingdecadesa\n",
            "powerfultheoryofactivationfunctionswillbedeveloped. Today,westillhavetorelyon\n",
            "poorlyunderstoodrulesofthumbandexperience. Expandingthetrainingdata: Anotherwaywemayhopetoimproveourresultsisby\n",
            "algorithmicallyexpandingthetrainingdata. Asimplewayofexpandingthetrainingdatais\n",
            "todisplaceeachtrainingimagebyasinglepixel,eitheruponepixel,downonepixel,left\n",
            "onepixel,orrightonepixel. Wecandothisbyrunningtheprogramexpand_mnist.py\n",
            "fromtheshellprompt17:\n",
            "15Gradient-basedlearningappliedtodocumentrecognition,byYannLeCun,LÃ©onBottou,Yoshua\n",
            "Bengio,andPatrickHaffner(1998). Therearemanydifferencesofdetail,butbroadlyspeakingour\n",
            "networkisquitesimilartothenetworksdescribedinthepaper. 16Acommonjustificationisthatmax(0,z)doesnâ€™tsaturateinthelimitoflargez,unlikesigmoid\n",
            "neurons,andthishelpsrectifiedlinearunitscontinuelearning.Theargumentisfine,asfaritgoes,but\n",
            "itâ€™shardlyadetailedjustification,moreofajust-sostory. Notethatwediscussedtheproblemswith\n",
            "saturationbackinChapter2. 17Thecodeforexpand_mnist.pyisavailablehere. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 181\n",
            "(cid:12)\n",
            "$ python expand_mnist.py\n",
            "Runningthisprogramtakesthe50,000MNISTtrainingimages,andpreparesanexpanded\n",
            "trainingset,with250,000trainingimages. Wecanthenusethosetrainingimagestotrain\n",
            "ournetwork. Weâ€™llusethesamenetworkasabove,withrectifiedlinearunits. Inmyinitial\n",
            "experimentsIreducedthenumberoftrainingepochsâ€“thismadesense,sinceweâ€™retraining\n",
            "with5timesasmuchdata. But, infact, expandingthedataturnedouttoconsiderably\n",
            "reducetheeffectofoverfitting.\n",
            "Andso,aftersomeexperimentation,Ieventuallywentback\n",
            "totrainingfor60epochs. Inanycase,letâ€™strain:\n",
            ">>> expanded_training_data, _, _ = network3.load_data_shared(\"../data/\n",
            "mnist_expanded.pkl.gz\")\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU), 6\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "UsingtheexpandedtrainingdataIobtaineda99.37percenttrainingaccuracy. Sothisalmost\n",
            "trivial change gives a substantial improvement in classification accuracy.\n",
            "Indeed, as we\n",
            "discussedearlierthisideaofalgorithmicallyexpandingthedatacanbetakenfurther. Justto\n",
            "remindyouoftheflavourofsomeoftheresultsinthatearlierdiscussion: in2003Simard,\n",
            "SteinkrausandPlatt18improvedtheirMNISTperformanceto99.6percentusinganeural\n",
            "networkotherwiseverysimilartoours,usingtwoconvolutional-poolinglayers,followedby\n",
            "ahiddenfully-connectedlayerwith100neurons. Therewereafewdifferencesofdetailin\n",
            "theirarchitectureâ€“theydidnâ€™thavetheadvantageofusingrectifiedlinearunits,forinstanceâ€“\n",
            "butthekeytotheirimprovedperformancewasexpandingthetrainingdata.\n",
            "Theydidthisby\n",
            "rotating,translating,andskewingtheMNISTtrainingimages. Theyalsodevelopedaprocess\n",
            "ofâ€œelasticdistortionâ€,awayofemulatingtherandomoscillationshandmusclesundergo\n",
            "whenapersoniswriting. Bycombiningalltheseprocessestheysubstantiallyincreasedthe\n",
            "effectivesizeoftheirtrainingdata,andthatâ€™showtheyachieved99.6percentaccuracy. Problem\n",
            "Theideaofconvolutionallayersistobehaveinaninvariantwayacrossimages. It\n",
            "â€¢ mayseemsurprising,then,thatournetworkcanlearnmorewhenallweâ€™vedoneis\n",
            "translatetheinputdata. Canyouexplainwhythisisactuallyquitereasonable? Insertinganextrafully-connectedlayer: Canwedoevenbetter? Onepossibilityistouse\n",
            "exactlythesameprocedureasabove,buttoexpandthesizeofthefully-connectedlayer. I\n",
            "triedwith300and1,000neurons,obtainingresultsof99.46and99.43percent,respectively. Thatâ€™sinteresting,butnotreallyaconvincingwinovertheearlierresult(99.37percent)\n",
            "18BestPracticesforConvolutionalNeuralNetworksAppliedtoVisualDocumentAnalysis,byPatrice\n",
            "Simard,DaveSteinkraus,andJohnPlatt(2003). \n",
            "(cid:12)\n",
            "182 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Whataboutaddinganextrafully-connectedlayer? Letâ€™stryinsertinganextrafully-\n",
            "connectedlayer,sothatwehavetwo100-hiddenneuronfully-connectedlayers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=100, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "Doingthis,Iobtainedatestaccuracyof99.43percent. Again,theexpandednetisnâ€™thelping\n",
            "6\n",
            "somuch. Runningsimilarexperimentswithfully-connectedlayerscontaining300and1,000\n",
            "neuronsyieldsresultsof99.48and99.47percent. Thatâ€™sencouraging,butstillfallsshortof\n",
            "areallydecisivewin. Whatâ€™sgoingonhere? Isitthattheexpandedorextrafully-connectedlayersreally\n",
            "donâ€™thelpwithMNIST?Ormightitbethatournetworkhasthecapacitytodobetter,but\n",
            "weâ€™regoingaboutlearningthewrongway? Forinstance, maybewecouldusestronger\n",
            "regularizationtechniquestoreducethetendencytooverfit. Onepossibilityisthedropout\n",
            "techniqueintroducedbackinChapter3. Recallthatthebasicideaofdropoutistoremove\n",
            "individualactivationsatrandomwhiletrainingthenetwork.\n",
            "Thismakesthemodelmore\n",
            "robusttothelossofindividualpiecesofevidence,andthuslesslikelytorelyonparticular\n",
            "idiosyncraciesofthetrainingdata. Letâ€™stryapplyingdropouttothefinalfully-connected\n",
            "layers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(\n",
            "n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "FullyConnectedLayer(\n",
            "n_in=1000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
            "mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 40, mini_batch_size, 0.03,\n",
            "validation_data, test_data)\n",
            "Usingthis,weobtainanaccuracyof99.60percent,whichisasubstantialimprovementover\n",
            "ourearlierresults,especiallyourmainbenchmark,thenetworkwith100hiddenneurons,\n",
            "whereweachieved99.37percent. Therearetwochangesworthnoting.\n",
            "\n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 183\n",
            "(cid:12)\n",
            "First,Ireducedthenumberoftrainingepochsto40: dropoutreducedoverfitting,and\n",
            "sowelearnedfaster. Second,thefully-connectedhiddenlayershave1,000neurons,notthe100usedearlier. Ofcourse,dropouteffectivelyomitsmanyoftheneuronswhiletraining,sosomeexpansion\n",
            "istobeexpected. Infact,Itriedexperimentswithboth300and1,000hiddenneurons,and\n",
            "obtained(veryslightly)bettervalidationperformancewith1,000hiddenneurons. Usinganensembleofnetworks: Aneasywaytoimproveperformancestillfurtheristo\n",
            "createseveralneuralnetworks,andthengetthemtovotetodeterminethebestclassification. Suppose,forexample,thatwetrained5differentneuralnetworksusingtheprescription\n",
            "above, with each achieving accuracies near to 99.6 percent. Even though the networks\n",
            "wouldallhavesimilaraccuracies,theymightwellmakedifferenterrors,duetothedifferent\n",
            "randominitializations. Itâ€™splausiblethattakingavoteamongstour5networksmightyield\n",
            "aclassificationbetterthananyindividualnetwork. Thissoundstoogoodtobetrue,butthiskindofensemblingisacommontrickwithboth\n",
            "neuralnetworksandothermachinelearningtechniques. Anditdoesinfactyieldfurther\n",
            "improvements: weendupwith99.67percentaccuracy. Inotherwords,ourensembleof\n",
            "networksclassifiesallbut33ofthe10,000testimagescorrectly. 6\n",
            "Theremainingerrorsinthetestsetareshownbelow. Thelabelinthetoprightisthe\n",
            "correctclassification,accordingtotheMNISTdata,whileinthebottomrightisthelabel\n",
            "outputbyourensembleofnets:\n",
            "Itâ€™sworthlookingthroughtheseindetail. Thefirsttwodigits,a6anda5,aregenuineerrors\n",
            "byourensemble.\n",
            "However, theyâ€™realsounderstandableerrors, thekindahumancould\n",
            "plausiblymake. That6reallydoeslookalotlikea0,andthe5looksalotlikea3. Thethird\n",
            "image,supposedlyan8,actuallylookstomemorelikea9. SoIâ€™msidingwiththenetwork\n",
            "ensemblehere: Ithinkitâ€™sdoneabetterjobthanwhoeveroriginallydrewthedigit. Onthe\n",
            "otherhand,thefourthimage,the6,reallydoesseemtobeclassifiedbadlybyournetworks.\n",
            "Andsoon. Inmostcasesournetworksâ€™choicesseematleastplausible,andinsomecases\n",
            "theyâ€™vedoneabetterjobclassifyingthantheoriginalpersondidwritingthedigit. Overall,\n",
            "ournetworksofferexceptionalperformance,especiallywhenyouconsiderthattheycorrectly\n",
            "classified9,967imageswhicharenâ€™tshown.\n",
            "Inthatcontext,thefewclearerrorshereseem\n",
            "quiteunderstandable. Evenacarefulhumanmakestheoccasionalmistake. AndsoIexpect\n",
            "\n",
            "(cid:12)\n",
            "184 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "thatonlyanextremelycarefulandmethodicalhumanwoulddomuchbetter. Ournetwork\n",
            "isgettingneartohumanperformance. Whyweonlyapplieddropouttothefully-connectedlayers: Ifyoulookcarefullyat\n",
            "thecodeabove,youâ€™llnoticethatweapplieddropoutonlytothefully-connectedsectionof\n",
            "thenetwork,nottotheconvolutionallayers. Inprinciplewecouldapplyasimilarprocedure\n",
            "totheconvolutionallayers. But, infact, thereâ€™snoneed: theconvolutionallayershave\n",
            "considerableinbuiltresistancetooverfitting. Thereasonisthatthesharedweightsmean\n",
            "thatconvolutionalfiltersareforcedtolearnfromacrosstheentireimage. Thismakesthem\n",
            "lesslikelytopickuponlocalidiosyncraciesinthetrainingdata. Andsothereislessneedto\n",
            "applyotherregularizers,suchasdropout. Goingfurther: Itâ€™spossibletoimproveperformanceonMNISTstillfurther. Rodrigo\n",
            "Benensonhascompiledaninformativesummarypage,showingprogressovertheyears,with\n",
            "linkstopapers. Manyofthesepapersusedeepconvolutionalnetworksalonglinessimilarto\n",
            "thenetworksweâ€™vebeenusing. Ifyoudigthroughthepapersyouâ€™llfindmanyinteresting\n",
            "techniques,andyoumayenjoyimplementingsomeofthem. Ifyoudosoitâ€™swisetostart\n",
            "implementationwithasimplenetworkthatcanbetrainedquickly,whichwillhelpyoumore\n",
            "6\n",
            "rapidlyunderstandwhatisgoingon. Forthemostpart,Iwonâ€™ttrytosurveythisrecentwork. ButIcanâ€™tresistmakingone\n",
            "exception. Itâ€™sa2010paperbyCireÂ¸san,Meier,Gambardella,andSchmidhuber19.\n",
            "WhatI\n",
            "likeaboutthispaperishowsimpleitis. Thenetworkisamany-layerneuralnetwork,using\n",
            "onlyfully-connectedlayers(noconvolutions). Theirmostsuccessfulnetworkhadhidden\n",
            "layerscontaining2,500,2,000,1,500,1,000,and500neurons,respectively. Theyusedideas\n",
            "similartoSimardetaltoexpandtheirtrainingdata. Butapartfromthat,theyusedfew\n",
            "othertricks,includingnoconvolutionallayers: itwasaplain,vanillanetwork,ofthekind\n",
            "that,withenoughpatience,couldhavebeentrainedinthe1980s(iftheMNISTdatasethad\n",
            "existed),givenenoughcomputingpower. Theyachievedaclassificationaccuracyof99.65\n",
            "percent,moreorlessthesameasours. Thekeywastouseaverylarge,verydeepnetwork,\n",
            "andtouseaGPUtospeeduptraining. Thisletthemtrainformanyepochs. Theyalsotook\n",
            "advantageoftheirlongtrainingtimestograduallydecreasethelearningratefrom10 3to\n",
            "âˆ’\n",
            "10 6. Itâ€™safunexercisetotrytomatchtheseresultsusinganarchitectureliketheirs. âˆ’\n",
            "Why are we able to train? We saw in the last chapter that there are fundamental\n",
            "obstructionstotrainingindeep,many-layerneuralnetworks. Inparticular,wesawthatthe\n",
            "gradienttendstobequiteunstable: aswemovefromtheoutputlayertoearlierlayersthe\n",
            "gradienttendstoeithervanish(thevanishinggradientproblem)orexplode(theexploding\n",
            "gradientproblem). Sincethegradientisthesignalweusetotrain,thiscausesproblems.\n",
            "Howhaveweavoidedthoseresults? Ofcourse, theansweristhatwehavenâ€™tavoidedtheseresults.\n",
            "Instead, weâ€™vedone\n",
            "afewthingsthathelpusproceedanyway. Inparticular: (1)Usingconvolutionallayers\n",
            "greatlyreducesthenumberofparametersinthoselayers, makingthelearningproblem\n",
            "much easier; (2) Using more powerful regularization techniques (notably dropout and\n",
            "convolutionallayers)toreduceoverfitting,whichisotherwisemoreofaprobleminmore\n",
            "complexnetworks;(3)Usingrectifiedlinearunitsinsteadofsigmoidneurons,tospeedup\n",
            "trainingâ€“empirically,oftenbyafactorof3â€“5;(4)UsingGPUsandbeingwillingtotrainfor\n",
            "alongperiodoftime. Inparticular,inourfinalexperimentswetrainedfor40epochsusing\n",
            "adataset5timeslargerthantherawMNISTtrainingdata. Earlierinthebookwemostly\n",
            "19Deep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCireÂ¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJÃ¼rgenSchmidhuber(2010).\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 185\n",
            "(cid:12)\n",
            "trainedfor30epochsusingjusttherawtrainingdata. Combiningfactors(3)and(4)itâ€™sas\n",
            "thoughweâ€™vetrainedafactorperhaps30timeslongerthanbefore. Yourresponsemaybeâ€œIsthatit?\n",
            "Isthatallwehadtodototraindeepnetworks? Whatâ€™s\n",
            "allthefussabout?â€\n",
            "Ofcourse,weâ€™veusedotherideas,too: makinguseofsufficientlylargedatasets(to\n",
            "helpavoidoverfitting);usingtherightcostfunction(toavoidalearningslowdown);using\n",
            "goodweightinitializations(alsotoavoidalearningslowdown,duetoneuronsaturation);\n",
            "algorithmicallyexpandingthetrainingdata. Wediscussedtheseandotherideasinearlier\n",
            "chapters,andhaveforthemostpartbeenabletoreusetheseideaswithlittlecommentin\n",
            "thischapter. Withthatsaid,thisreallyisarathersimplesetofideas. Simple,butpowerful,when\n",
            "usedinconcert. Gettingstartedwithdeeplearninghasturnedouttobeprettyeasy!\n",
            "Howdeeparethesenetworks,anyway? Countingtheconvolutional-poolinglayers\n",
            "as single layers, our final architecture has 4 hidden layers. Does such a network really\n",
            "deserve to be called a deep network? Of course, 4 hidden layers is many more than in\n",
            "6\n",
            "theshallownetworkswestudiedearlier. Mostofthosenetworksonlyhadasinglehidden\n",
            "layer,oroccasionally2hiddenlayers. Ontheotherhand,asof2015state-of-the-artdeep\n",
            "networkssometimeshavedozensofhiddenlayers. Iâ€™veoccasionallyheardpeopleadopta\n",
            "deeper-than-thouattitude,holdingthatifyouâ€™renotkeeping-up-with-the-Jonesesinterms\n",
            "ofnumberofhiddenlayers,thenyouâ€™renotreallydoingdeeplearning. Iâ€™mnotsympathetic\n",
            "tothisattitude, inpartbecauseitmakesthedefinitionofdeeplearningintosomething\n",
            "whichdependsupontheresult-of-the-moment. Therealbreakthroughindeeplearningwas\n",
            "torealizethatitâ€™spracticaltogobeyondtheshallow1-and2-hiddenlayernetworksthat\n",
            "dominatedworkuntilthemid-2000s. Thatreallywasasignificantbreakthrough,opening\n",
            "uptheexplorationofmuchmoreexpressivemodels. Butbeyondthat,thenumberoflayers\n",
            "isnotofprimaryfundamentalinterest. Rather,theuseofdeepernetworksisatooltouseto\n",
            "helpachieveothergoalsâ€“likebetterclassificationaccuracies. Awordonprocedure: Inthissection,weâ€™vesmoothlymovedfromsinglehidden-layer\n",
            "shallownetworkstomany-layerconvolutionalnetworks. Itallseemedsoeasy!\n",
            "Wemakea\n",
            "changeand,forthemostpart,wegetanimprovement. Ifyoustartexperimenting,Ican\n",
            "guaranteethingswonâ€™talwaysbesosmooth. ThereasonisthatIâ€™vepresentedacleaned-up\n",
            "narrative,omittingmanyexperimentsâ€“includingmanyfailedexperiments. Thiscleaned-up\n",
            "narrativewillhopefullyhelpyougetclearonthebasicideas. Butitalsorunstheriskof\n",
            "conveyinganincompleteimpression. Gettingagood,workingnetworkcaninvolvealotof\n",
            "trialanderror,andoccasionalfrustration. Inpractice,youshouldexpecttoengageinquite\n",
            "abitofexperimentation.\n",
            "TospeedthatprocessupyoumayfindithelpfultorevisitChapter\n",
            "3â€™sdiscussionofhowtochooseaneuralnetworkâ€™shyper-parameters,andperhapsalsoto\n",
            "lookatsomeofthefurtherreadingsuggestedinthatsection. 6.3 The code for our convolutional networks\n",
            "Alright,letâ€™stakealookatthecodeforourprogram,network3.py. Structurally,itâ€™ssimilar\n",
            "tonetwork2.py,theprogramwedevelopedinChapter3,althoughthedetailsdiffer,due\n",
            "totheuseofTheano. Weâ€™llstartbylookingattheFullyConnectedLayerclass,whichis\n",
            "\n",
            "(cid:12)\n",
            "186 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "similartothelayersstudiedearlierinthebook. Hereâ€™sthecode(discussionbelow)20:\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=â€™wâ€™, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=â€™bâ€™, borrow=True)\n",
            "6 self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "Muchofthe__init__methodisself-explanatory,butafewremarksmayhelpclarifythe\n",
            "code. Asperusual,werandomlyinitializetheweightsandbiasesasnormalrandomvariables\n",
            "withsuitablestandarddeviations.\n",
            "Thelinesdoingthislookalittleforbidding. However,\n",
            "mostofthecomplicationisjustloadingtheweightsandbiasesintowhatTheanocallsshared\n",
            "variables. ThisensuresthatthesevariablescanbeprocessedontheGPU,ifoneisavailable. Wewonâ€™tgettoomuchintothedetailsofthis. Ifyouâ€™reinterested,youcandigintothe\n",
            "Theanodocumentation. Notealsothatthisweightandbiasinitializationisdesignedforthe\n",
            "sigmoidactivationfunction(asdiscussedearlier). Ideally,weâ€™dinitializetheweightsand\n",
            "biasessomewhatdifferentlyforactivationfunctionssuchasthetanhandrectifiedlinear\n",
            "function. Thisisdiscussedfurtherinproblemsbelow.\n",
            "The__init__methodfinisheswith\n",
            "self.params = [self.w, self.b]. Thisisahandywaytobundleupallthelearnable\n",
            "parametersassociatedtothelayer. Lateron, theNetwork.SGDmethodwilluseparams\n",
            "attributestofigureoutwhatvariablesinaNetworkinstancecanlearn. Theset_inptmethodisusedtosettheinputtothelayer,andtocomputethecorre-\n",
            "spondingoutput. Iusethenameinptratherthaninputbecauseinputisabuilt-infunction\n",
            "20NoteaddedNovember2016:severalreadershavenotedthatinthelineinitializingself.w,Iset\n",
            "scale=np.sqrt(1.0/n_out),whentheargumentsofChapter3suggestabetterinitializationmaybe\n",
            "scale=np.sqrt(1.0/n_in).Thiswassimplyamistakeonmypart.InanidealworldIâ€™drerunallthe\n",
            "examplesinthischapterwiththecorrectcode.Still,Iâ€™vemovedontootherprojects,soamgoingtolet\n",
            "theerrorgo. \n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 187\n",
            "(cid:12)\n",
            "inPython,andmessingwithbuilt-instendstocauseunpredictablebehavioranddifficult-to-\n",
            "diagnosebugs. Notethatweactuallysettheinputintwoseparateways: asself.inptand\n",
            "self.inpt_dropout. Thisisdonebecauseduringtrainingwemaywanttousedropout. If\n",
            "thatâ€™sthecasethenwewanttoremoveafractionself.p_dropoutoftheneurons. Thatâ€™s\n",
            "whatthefunctiondropout_layerinthesecond-lastlineoftheset_inptmethodisdo-\n",
            "ing. Soself.inpt_dropoutandself.output_dropoutareusedduringtraining,while\n",
            "self.inptandself.outputareusedforallotherpurposes,e.g.,evaluatingaccuracyonthe\n",
            "validationandtestdata. TheConvPoolLayerandSoftmaxLayerclassdefinitionsaresimilartoFullyConnectedLayer\n",
            ". Indeed,theyâ€™resoclosethatIwonâ€™texcerptthecodehere.\n",
            "Ifyouâ€™reinterestedyoucanlook\n",
            "atthefulllistingfornetwork3.py,laterinthissection. However,acoupleofminordifferencesofdetailareworthmentioning. Mostobviously,\n",
            "inbothConvPoolLayerandSoftmaxLayerwecomputetheoutputactivationsintheway\n",
            "appropriate to that layer type. Fortunately, Theano makes that easy, providing built-in\n",
            "operationstocomputeconvolutions,max-pooling,andthesoftmaxfunction. Less obviously, when we introduced the softmax layer, we never discussed how to\n",
            "initializetheweightsandbiases. Elsewhereweâ€™vearguedthatforsigmoidlayersweshould 6\n",
            "initialize the weights using suitably parameterized normal random variables. But that\n",
            "heuristicargumentwasspecifictosigmoidneurons(and,withsomeamendment,totanh\n",
            "neurons). However,thereâ€™snoparticularreasontheargumentshouldapplytosoftmaxlayers. Sothereâ€™snoapriorireasontoapplythatinitializationagain. Ratherthandothat,Ishall\n",
            "initializealltheweightsandbiasestobe0. Thisisaratheradhocprocedure,butworks\n",
            "wellenoughinpractice. Okay,weâ€™velookedatallthelayerclasses.\n",
            "WhatabouttheNetworkclass? Letâ€™sstartby\n",
            "lookingatthe__init__method:\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of â€˜layersâ€˜, describing the network architecture, and\n",
            "a value for the â€˜mini_batch_sizeâ€˜ to be used during training\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "Mostofthisisself-explanatory,ornearlyso. Thelineself.params = [param for layer\n",
            "in ...]bundlesuptheparametersforeachlayerintoasinglelist.\n",
            "Asanticipatedabove,\n",
            "theNetwork.SGDmethodwilluseself.paramstofigureoutwhatvariablesintheNetwork\n",
            "canlearn. Thelinesself.x = T.matrix(\"x\")andself.y = T.ivector(\"y\")define\n",
            "Theanosymbolicvariablesnamedxandy. Thesewillbeusedtorepresenttheinputand\n",
            "\n",
            "(cid:12)\n",
            "188 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "desiredoutputfromthenetwork. Now,thisisnâ€™taTheanotutorial,andsowewonâ€™tgettoodeeplyintowhatitmeans\n",
            "thatthesearesymbolicvariables21. Buttheroughideaisthattheserepresentmathematical\n",
            "variables,notexplicitvalues. Wecandoalltheusualthingsonewoulddowithsuchvariables:\n",
            "add, subtract, andmultiplythem, applyfunctions, andsoon. Indeed, Theanoprovides\n",
            "manywaysofmanipulatingsuchsymbolicvariables,doingthingslikeconvolutions,max-\n",
            "pooling,andsoon. Butthebigwinistheabilitytodofastsymbolicdifferentiation,usinga\n",
            "verygeneralformofthebackpropagationalgorithm. Thisisextremelyusefulforapplying\n",
            "stochasticgradientdescenttoawidevarietyofnetworkarchitectures. Inparticular,thenext\n",
            "fewlinesofcodedefinesymbolicoutputsfromthenetwork. Westartbysettingtheinputto\n",
            "theinitiallayer,withtheline\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "Notethattheinputsaresetonemini-batchatatime,whichiswhythemini-batchsizeis\n",
            "there. Notealsothatwepasstheinputself.xintwice: thisisbecausewemayusethe\n",
            "networkintwodifferentways(withorwithoutdropout). Theforloopthenpropagatesthe\n",
            "6\n",
            "symbolicvariableself.xforwardthroughthelayersoftheNetwork. Thisallowsustodefine\n",
            "thefinaloutputandoutput_dropoutattributes,whichsymbolicallyrepresenttheoutput\n",
            "fromtheNetwork. Nowthatweâ€™veunderstoodhowaNetworkisinitialized,letâ€™slookathowitistrained,\n",
            "usingtheSGDmethod. Thecodelookslengthy,butitsstructureisactuallyrathersimple.\n",
            "Explanatorycommentsafterthecode. def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches. i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "21TheTheanodocumentationprovidesagoodintroductiontoTheano.Andifyougetstuck,youmay\n",
            "findithelpfultolookatoneoftheothertutorialsavailableonline. Forinstance,thistutorialcovers\n",
            "manybasics.\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 189\n",
            "(cid:12)\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out, 6\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(â€™The corresponding test accuracy is {0:.2%}â€™.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "Thefirstfewlinesarestraightforward,separatingthedatasetsinto x and y components,\n",
            "andcomputingthenumberofmini-batchesusedineachdataset. Thenextfewlinesare\n",
            "moreinteresting,andshowsomeofwhatmakesTheanofuntoworkwith.\n",
            "Letâ€™sexplicitly\n",
            "excerptthelineshere:\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "\n",
            "(cid:12)\n",
            "190 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)]\n",
            "Intheselineswesymbolicallysetuptheregularizedlog-likelihoodcostfunction,computethe\n",
            "correspondingderivativesinthegradientfunction,aswellasthecorrespondingparameter\n",
            "updates. Theanoletsusachieveallofthisinjustthesefewlines.\n",
            "Theonlythinghiddenis\n",
            "thatcomputingthecostinvolvesacalltothecostmethodfortheoutputlayer;thatcodeis\n",
            "elsewhereinnetwork3.py. Butthatcodeisshortandsimple,anyway. Withallthesethings\n",
            "defined,thestageissettodefinethetrain_mbfunction,aTheanosymbolicfunctionwhich\n",
            "usestheupdatestoupdatetheNetworkparameters,givenamini-batchindex. Similarly,\n",
            "validate_mb_accuracyandtest_mb_accuracycomputetheaccuracyoftheNetworkon\n",
            "anygivenmini-batchofvalidationortestdata. Byaveragingoverthesefunctions,wewill\n",
            "beabletocomputeaccuraciesontheentirevalidationandtestdatasets. The remainder of the SGD method is self-explanatory â€“ we simply iterate over the\n",
            "epochs,repeatedlytrainingthenetworkonmini-batchesoftrainingdata,andcomputing\n",
            "thevalidationandtestaccuracies. 6\n",
            "Okay,weâ€™venowunderstoodthemostimportantpiecesofcodeinnetwork3.py.\n",
            "Letâ€™s\n",
            "takeabrieflookattheentireprogram. Youdonâ€™tneedtoreadthroughthisindetail,butyou\n",
            "mayenjoyglancingoverit,andperhapsdivingdownintoanypiecesthatstrikeyourfancy. Thebestwaytoreallyunderstanditis,ofcourse,bymodifyingit,addingextrafeatures,or\n",
            "refactoringanythingyouthinkcouldbedonemoreelegantly. Afterthecode,therearesome\n",
            "problemswhichcontainafewstartersuggestionsforthingstodo. Hereâ€™sthecode22:\n",
            "\"\"\"network3.py\n",
            "~~~~~~~~~~~~~~\n",
            "A Theano-based program for training and running simple neural\n",
            "networks. Supports several layer types (fully connected, convolutional, max\n",
            "pooling, softmax), and activation functions (sigmoid, tanh, and\n",
            "rectified linear units, with more easily added). When run on a CPU, this program is much faster than network.py and\n",
            "network2.py. However, unlike network.py and network2.py it can also\n",
            "be run on a GPU, which makes it faster still. Because the code is based on Theano, the code is different in many\n",
            "ways from network.py and network2.py. However, where possible I have\n",
            "tried to maintain consistency with the earlier programs. In\n",
            "particular, the API is similar to network2.py. Note that I have\n",
            "focused on making the code simple, easily readable, and easily\n",
            "modifiable.\n",
            "It is not optimized, and omits many desirable features. This program incorporates ideas from the Theano documentation on\n",
            "convolutional neural nets (notably,\n",
            "http://deeplearning.net/tutorial/lenet.html ), from Misha Denilâ€™s\n",
            "implementation of dropout (https://github.com/mdenil/dropout ), and\n",
            "from Chris Olah (http://colah.github.io ). 22UsingTheanoonaGPUcanbealittletricky.Inparticular,itâ€™seasytomakethemistakeofpulling\n",
            "dataofftheGPU,whichcanslowthingsdownalot.Iâ€™vetriedtoavoidthis.Withthatsaid,thiscode\n",
            "cancertainlybespedupquiteabitfurtherwithcarefuloptimizationofTheanoâ€™sconfiguration.Seethe\n",
            "Theanodocumentationformoredetails. \n",
            "(cid:12)\n",
            "6.3.\n",
            "Thecodeforourconvolutionalnetworks (cid:12) 191\n",
            "(cid:12)\n",
            "Written for Theano 0.6 and 0.7, needs some changes for more recent\n",
            "versions of Theano. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import cPickle\n",
            "import gzip\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "import theano\n",
            "import theano.tensor as T\n",
            "from theano.tensor.nnet import conv\n",
            "from theano.tensor.nnet import softmax\n",
            "from theano.tensor import shared_randomstreams\n",
            "from theano.tensor.signal import downsample\n",
            "# Activation functions for neurons\n",
            "def linear(z): return z 6\n",
            "def ReLU(z): return T.maximum(0.0, z)\n",
            "from theano.tensor.nnet import sigmoid\n",
            "from theano.tensor import tanh\n",
            "#### Constants\n",
            "GPU = True\n",
            "if GPU:\n",
            "print \"Trying to run under a GPU. If this is not desired, then modify \"+\\\n",
            "\"network3.py\\nto set the GPU flag to False.\"\n",
            "try: theano.config.device = â€™gpuâ€™\n",
            "except: pass # itâ€™s already set\n",
            "theano.config.floatX = â€™float32â€™\n",
            "else:\n",
            "print \"Running with a CPU. If this is not desired, then the modify \"+\\\n",
            "\"network3.py to set\\nthe GPU flag to True.\"\n",
            "#### Load the MNIST data\n",
            "def load_data_shared(filename=\"../data/mnist.pkl.gz\"):\n",
            "f = gzip.open(filename, â€™rbâ€™)\n",
            "training_data, validation_data, test_data = cPickle.load(f)\n",
            "f.close()\n",
            "def shared(data):\n",
            "\"\"\"Place the data into shared variables. This allows Theano to copy\n",
            "the data to the GPU, if one is available. \"\"\"\n",
            "shared_x = theano.shared(\n",
            "np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
            "shared_y = theano.shared(\n",
            "np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
            "return shared_x, T.cast(shared_y, \"int32\")\n",
            "return [shared(training_data), shared(validation_data), shared(test_data)]\n",
            "#### Main class used to construct and train networks\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of â€˜layersâ€˜, describing the network architecture, and\n",
            "a value for the â€˜mini_batch_sizeâ€˜ to be used during training\n",
            "\n",
            "(cid:12)\n",
            "192 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "6 training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches.\n",
            "i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 193\n",
            "(cid:12)\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out,\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format( 6\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(â€™The corresponding test accuracy is {0:.2%}â€™.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "#### Define layer types\n",
            "class ConvPoolLayer(object):\n",
            "\"\"\"Used to create a combination of a convolutional and a max-pooling\n",
            "layer. A more sophisticated implementation would separate the\n",
            "two, but for our purposes weâ€™ll always use them together, and it\n",
            "simplifies the code, so it makes sense to combine them.\n",
            "\"\"\"\n",
            "def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
            "activation_fn=sigmoid):\n",
            "\"\"\"â€˜filter_shapeâ€˜ is a tuple of length 4, whose entries are the number\n",
            "of filters, the number of input feature maps, the filter height, and the\n",
            "filter width. â€˜image_shapeâ€˜ is a tuple of length 4, whose entries are the\n",
            "mini-batch size, the number of input feature maps, the image\n",
            "height, and the image width. â€˜poolsizeâ€˜ is a tuple of length 2, whose entries are the y and\n",
            "x pooling sizes. \"\"\"\n",
            "self.filter_shape = filter_shape\n",
            "\n",
            "(cid:12)\n",
            "194 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "self.image_shape = image_shape\n",
            "self.poolsize = poolsize\n",
            "self.activation_fn=activation_fn\n",
            "# initialize weights and biases\n",
            "n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape(self.image_shape)\n",
            "conv_out = conv.conv2d(\n",
            "6 input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
            "image_shape=self.image_shape)\n",
            "pooled_out = downsample.max_pool_2d(\n",
            "input=conv_out, ds=self.poolsize, ignore_border=True)\n",
            "self.output = self.activation_fn(\n",
            "pooled_out + self.b.dimshuffle(â€™xâ€™, 0, â€™xâ€™, â€™xâ€™))\n",
            "self.output_dropout = self.output # no dropout in the convolutional layers\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=â€™wâ€™, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=â€™bâ€™, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 195\n",
            "(cid:12)\n",
            "class SoftmaxLayer(object):\n",
            "def __init__(self, n_in, n_out, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
            "name=â€™wâ€™, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.zeros((n_out,), dtype=theano.config.floatX),\n",
            "name=â€™bâ€™, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer( 6\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def cost(self, net):\n",
            "\"Return the log-likelihood cost.\"\n",
            "return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "#### Miscellanea\n",
            "def size(data):\n",
            "\"Return the size of the dataset â€˜dataâ€˜.\"\n",
            "return data[0].get_value(borrow=True).shape[0]\n",
            "def dropout_layer(layer, p_dropout):\n",
            "srng = shared_randomstreams.RandomStreams(\n",
            "np.random.RandomState(0).randint(999999))\n",
            "mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
            "return layer*T.cast(mask, theano.config.floatX)\n",
            "Problems\n",
            "At present, the SGD method requires the user to manually choose the number of\n",
            "â€¢ epochstotrainfor. Earlierinthebookwediscussedanautomatedwayofselecting\n",
            "thenumberofepochstotrainfor,knownasearlystopping.\n",
            "Modifynetwork3.pyto\n",
            "implementearlystopping. AddaNetworkmethodtoreturntheaccuracyonanarbitrarydataset. â€¢ ModifytheSGDmethodtoallowthelearningrateÎ·tobeafunctionoftheepoch\n",
            "â€¢ number. Hint: Afterworkingonthisproblemforawhile,youmayfinditusefultosee\n",
            "thediscussionatthislink. EarlierinthechapterIdescribedatechniqueforexpandingthetrainingdatabyapply-\n",
            "â€¢ ing(small)rotations,skewing,andtranslation. Modifynetwork3.pytoincorporate\n",
            "allthesetechniques. Note: Unlessyouhaveatremendousamountofmemory, itis\n",
            "notpracticaltoexplicitlygeneratetheentireexpandeddataset. Soyoushouldconsider\n",
            "\n",
            "(cid:12)\n",
            "196 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "alternateapproaches. Addtheabilitytoloadandsavenetworkstonetwork3.py. â€¢ Ashortcomingofthecurrentcodeisthatitprovidesfewdiagnostictools. Canyou\n",
            "â€¢ thinkofanydiagnosticstoaddthatwouldmakeiteasiertounderstandtowhatextent\n",
            "anetworkisoverfitting? Addthem. Weâ€™veusedthesameinitializationprocedureforrectifiedlinearunitsasforsigmoid\n",
            "â€¢ (andtanh)neurons. Ourargumentforthatinitializationwasspecifictothesigmoid\n",
            "function.Consideranetworkmadeentirelyofrectifiedlinearunits(includingoutputs). Showthatrescalingalltheweightsinthenetworkbyaconstantfactorc>0simply\n",
            "rescalestheoutputsbyafactor cL 1, where L isthenumberoflayers. Howdoes\n",
            "âˆ’\n",
            "thischangeifthefinallayerisasoftmax? Whatdoyouthinkofusingthesigmoid\n",
            "initialization procedure for the rectified linear units? Can you think of a better\n",
            "initialization procedure?\n",
            "Note: This is a very open-ended problem, not something\n",
            "withasimpleself-containedanswer.\n",
            "Still,consideringtheproblemwillhelpyoubetter\n",
            "understandnetworkscontainingrectifiedlinearunits. Ouranalysisoftheunstablegradientproblemwasforsigmoidneurons. Howdoes\n",
            "6 â€¢ theanalysischangefornetworksmadeupofrectifiedlinearunits? Canyouthinkofa\n",
            "goodwayofmodifyingsuchanetworksoitdoesnâ€™tsufferfromtheunstablegradient\n",
            "problem? Note: Thewordgoodinthesecondpartofthismakestheproblemaresearch\n",
            "problem. Itâ€™sactuallyeasytothinkofwaysofmakingsuchmodifications.\n",
            "ButIhavenâ€™t\n",
            "investigatedinenoughdepthtoknowofareallygoodtechnique. 6.4 Recent progress in image recognition\n",
            "In1998,theyearMNISTwasintroduced,ittookweekstotrainastate-of-the-artworkstation\n",
            "toachieveaccuraciessubstantiallyworsethanthosewecanachieveusingaGPUandless\n",
            "thananhouroftraining. Thus, MNISTisnolongeraproblemthatpushesthelimitsof\n",
            "availabletechnique;rather,thespeedoftrainingmeansthatitisaproblemgoodforteaching\n",
            "andlearningpurposes. Meanwhile,thefocusofresearchhasmovedon,andmodernwork\n",
            "involvesmuchmorechallengingimagerecognitionproblems.Inthissection,Ibrieflydescribe\n",
            "somerecentworkonimagerecognitionusingneuralnetworks. Thesectionisdifferenttomostofthebook. ThroughthebookIâ€™vefocusedonideaslikely\n",
            "tobeoflastinginterestâ€“ideassuchasbackpropagation,regularization,andconvolutional\n",
            "networks. Iâ€™vetriedtoavoidresultswhicharefashionableasIwrite,butwhoselong-term\n",
            "valueisunknown. Inscience,suchresultsaremoreoftenthannotephemerawhichfadeand\n",
            "havelittlelastingimpact. Giventhis,askepticmightsay: â€œwell,surelytherecentprogress\n",
            "inimagerecognitionisanexampleofsuchephemera? Inanothertwoorthreeyears,things\n",
            "willhavemovedon. Sosurelytheseresultsareonlyofinteresttoafewspecialistswhowant\n",
            "tocompeteattheabsolutefrontier? Whybotherdiscussingit?â€\n",
            "Such a skeptic is right that some of the finer details of recent papers will gradually\n",
            "diminishinperceivedimportance. Withthatsaid,thepastfewyearshaveseenextraordinary\n",
            "improvementsusingdeepnetstoattackextremelydifficultimagerecognitiontasks.\n",
            "Imagine\n",
            "ahistorianofsciencewritingaboutcomputervisionintheyear2100. Theywillidentifythe\n",
            "years2011to2015(andprobablyafewyearsbeyond)asatimeofhugebreakthroughs,\n",
            "drivenbydeepconvolutionalnets. Thatdoesnâ€™tmeandeepconvolutionalnetswillstillbe\n",
            "usedin2100,muchlessdetailedideassuchasdropout,rectifiedlinearunits,andsoon. But\n",
            "itdoesmeanthatanimportanttransitionistakingplace,rightnow,inthehistoryofideas. \n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 197\n",
            "(cid:12)\n",
            "Itâ€™sabitlikewatchingthediscoveryoftheatom,ortheinventionofantibiotics: invention\n",
            "anddiscoveryonahistoricscale. Andsowhilewewonâ€™tdigdowndeepintodetails,itâ€™s\n",
            "worthgettingsomeideaoftheexcitingdiscoveriescurrentlybeingmade.\n",
            "The2012LRMDpaper: Letmestartwitha2012paper23fromagroupofresearchers\n",
            "from Stanford and Google. Iâ€™ll refer to this paper as LRMD, after the last names of the\n",
            "firstfourauthors. LRMDusedaneuralnetworktoclassifyimagesfromImageNet,avery\n",
            "challengingimagerecognitionproblem. The2011ImageNetdatathattheyusedincluded\n",
            "16millionfullcolorimages,in20thousandcategories. Theimageswerecrawledfromthe\n",
            "opennet,andclassifiedbyworkersfromAmazonâ€™sMechanicalTurkservice. Hereâ€™safew\n",
            "ImageNetimages24:\n",
            "6\n",
            "Theseare,respectively,inthecategoriesforbeadingplane,brownrootrotfungus,scalded\n",
            "milk,andthecommonroundworm. Ifyouâ€™relookingforachallenge,Iencourageyouto\n",
            "visitImageNetâ€™slistofhandtools,whichdistinguishesbetweenbeadingplanes,blockplanes,\n",
            "chamferplanes,andaboutadozenothertypesofplane,amongstothercategories. Idonâ€™t\n",
            "knowaboutyou,butIcannotconfidentlydistinguishbetweenallthesetooltypes. Thisis\n",
            "obviouslyamuchmorechallengingimagerecognitiontaskthanMNIST!LRMDâ€™snetwork\n",
            "obtainedarespectable15.8percentaccuracyforcorrectlyclassifyingImageNetimages. That\n",
            "maynotsoundimpressive,butitwasahugeimprovementoverthepreviousbestresult\n",
            "of9.3percentaccuracy. Thatjumpsuggestedthatneuralnetworksmightofferapowerful\n",
            "approachtoverychallengingimagerecognitiontasks,suchasImageNet. The2012KSHpaper: TheworkofLRMDwasfollowedbya2012paperofKrizhevsky,\n",
            "SutskeverandHinton(KSH)25.KSHtrainedandtestedadeepconvolutionalneuralnetwork\n",
            "usingarestrictedsubsetoftheImageNetdata. Thesubsettheyusedcamefromapopular\n",
            "machinelearningcompetitionâ€“theImageNetLarge-ScaleVisualRecognitionChallenge\n",
            "(ILSVRC).Usingacompetitiondatasetgavethemagoodwayofcomparingtheirapproach\n",
            "tootherleadingtechniques. TheILSVRC-2012trainingsetcontainedabout1.2million\n",
            "ImageNet images, drawn from 1,000 categories. The validation and test sets contained\n",
            "50,000and150,000images,respectively,drawnfromthesame1,000categories. OnedifficultyinrunningtheILSVRCcompetitionisthatmanyImageNetimagescontain\n",
            "multipleobjects. Supposeanimageshowsalabradorretrieverchasingasoccerball. The\n",
            "so-calledâ€œcorrectâ€ImageNetclassificationoftheimagemightbeasalabradorretriever. 23Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning,byQuocLe,Marcâ€™Aurelio\n",
            "Ranzato,RajatMonga,MatthieuDevin,KaiChen,GregCorrado,JeffDean,andAndrewNg(2012).\n",
            "Notethatthedetailedarchitectureofthenetworkusedinthepaperdifferedinmanydetailsfromthe\n",
            "deepconvolutionalnetworksweâ€™vebeenstudying.Broadlyspeaking,however,LRMDisbasedonmany\n",
            "similarideas. 24Thesearefromthe2014dataset,whichissomewhatchangedfrom2011.Qualitatively,however,\n",
            "thedatasetisextremelysimilar.DetailsaboutImageNetareavailableintheoriginalImageNetpaper,\n",
            "ImageNet:alarge-scalehierarchicalimagedatabase,byJiaDeng,WeiDong,RichardSocher,Li-JiaLi,\n",
            "KaiLi,andLiFei-Fei(2009). 25ImageNetclassificationwithdeepconvolutionalneuralnetworks,byAlexKrizhevsky,IlyaSutskever,\n",
            "andGeoffreyE.Hinton(2012).\n",
            "\n",
            "(cid:12)\n",
            "198 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Shouldanalgorithmbepenalizedifitlabelstheimageasasoccerball? Becauseofthis\n",
            "ambiguity,analgorithmwasconsideredcorrectiftheactualImageNetclassificationwas\n",
            "amongthe5classificationsthealgorithmconsideredmostlikely. Bythistop-5criterion,\n",
            "KSHâ€™sdeepconvolutionalnetworkachievedanaccuracyof84.7percent,vastlybetterthan\n",
            "thenext-bestcontestentry,whichachievedanaccuracyof73.8percent. Usingthemore\n",
            "restrictivemetricofgettingthelabelexactlyright,KSHâ€™snetworkachievedanaccuracyof\n",
            "63.3percent. Itâ€™sworthbrieflydescribingKSHâ€™snetwork,sinceithasinspiredmuchsubsequentwork. Itâ€™salso,asweshallsee,closelyrelatedtothenetworkswetrainedearlierinthischapter,\n",
            "albeitmoreelaborate. KSHusedadeepconvolutionalneuralnetwork,trainedontwoGPUs. TheyusedtwoGPUsbecausetheparticulartypeofGPUtheywereusing(anNVIDIAGeForce\n",
            "GTX580)didnâ€™thaveenoughon-chipmemorytostoretheirentirenetwork. Sotheysplit\n",
            "thenetworkintotwoparts,partitionedacrossthetwoGPUs.\n",
            "TheKSHnetworkhas7layersofhiddenneurons. Thefirst5hiddenlayersareconvolu-\n",
            "tionallayers(somewithmax-pooling),whilethenext2layersarefully-connectedlayers. Theoutputlayerisa1,000-unitsoftmaxlayer,correspondingtothe1,000imageclasses. 6 Hereâ€™sasketchofthenetwork,takenfromtheKSHpaper26.\n",
            "Thedetailsareexplainedbelow. Notethatmanylayersaresplitinto2parts,correspondingtothe2GPUs. Theinputlayercontains3 224 224neurons,representingtheRGBvaluesfora224 224\n",
            "image. Recallthat,asmenÃ—tioneÃ—dearlier,ImageNetcontainsimagesofvaryingresolÃ—ution. Thisposesaproblem,sinceaneuralnetworkâ€™sinputlayerisusuallyofafixedsize. KSHdealt\n",
            "withthisbyrescalingeachimagesotheshortersidehadlength256. Theythencropped\n",
            "outa256 256areainthecenteroftherescaledimage. Finally,KSHextractedrandom\n",
            "224 224Ã—subimages(andhorizontalreflections)fromthe256 256images. Theydidthis\n",
            "randÃ—omcroppingasawayofexpandingthetrainingdata,anÃ—dthusreducingoverfitting. ThisisparticularlyhelpfulinalargenetworksuchasKSHâ€™s. Itwasthese224 224images\n",
            "whichwereusedasinputstothenetwork. InmostcasesthecroppedimagestilÃ—lcontainsthe\n",
            "mainobjectfromtheuncroppedimage. MovingontothehiddenlayersinKSHâ€™snetwork,thefirsthiddenlayerisaconvolutional\n",
            "layer,withamax-poolingstep. Ituseslocalreceptivefieldsofsize11 11,andastride\n",
            "lengthof4pixels. Thereareatotalof96featuremaps. ThefeaturemapsÃ—aresplitintotwo\n",
            "groupsof48each,withthefirst48featuremapsresidingononeGPU,andthesecond48\n",
            "featuremapsresidingontheotherGPU.Themax-poolinginthisandlaterlayersisdonein\n",
            "3 3regions,butthepoolingregionsareallowedtooverlap,andarejust2pixelsapart. Ã—\n",
            "26ThankstoIlyaSutskever.\n",
            "\n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 199\n",
            "(cid:12)\n",
            "Thesecondhiddenlayerisalsoaconvolutionallayer,withamax-poolingstep. Ituses\n",
            "5 5localreceptivefields,andthereâ€™satotalof256featuremaps,splitinto128oneach\n",
            "GPÃ—U.Notethatthefeaturemapsonlyuse48inputchannels,notthefull96outputfromthe\n",
            "previouslayer(aswouldusuallybethecase). Thisisbecauseanysinglefeaturemaponly\n",
            "usesinputsfromthesameGPU.Inthissensethenetworkdepartsfromtheconvolutional\n",
            "architecturewedescribedearlierinthechapter,thoughobviouslythebasicideaisstillthe\n",
            "same. Thethird,fourthandfifthhiddenlayersareconvolutionallayers,butunliketheprevious\n",
            "layers,theydonotinvolvemax-pooling. Theirrespectivesparametersare: (3)384feature\n",
            "maps,with3 3localreceptivefields,and256inputchannels;(4)384featuremaps,with\n",
            "3 3localreceÃ—ptivefields,and192inputchannels;and(5)256featuremaps,with3 3local\n",
            "reÃ—ceptivefields,and192inputchannels. NotethatthethirdlayerinvolvessomeinÃ—ter-GPU\n",
            "communication(asdepictedinthefigure)inorderthatthefeaturemapsuseall256input\n",
            "channels. Thesixthandseventhhiddenlayersarefully-connectedlayers,with4,096neuronsin\n",
            "eachlayer. Theoutputlayerisa1,000-unitsoftmaxlayer. 6\n",
            "TheKSHnetworktakesadvantageofmanytechniques. Insteadofusingthesigmoidor\n",
            "tanhactivationfunctions,KSHuserectifiedlinearunits,whichspeduptrainingsignificantly. KSHâ€™snetworkhadroughly60millionlearnedparameters,andwasthus,evenwiththelarge\n",
            "trainingset,susceptibletooverfitting. Toovercomethis,theyexpandedthetrainingsetusing\n",
            "therandomcroppingstrategywediscussedabove. Theyalsofurtheraddressedoverfitting\n",
            "byusingavariantofl2regularization,anddropout. Thenetworkitselfwastrainedusing\n",
            "momentum-basedmini-batchstochasticgradientdescent. Thatâ€™s an overview of many of the core ideas in the KSH paper.\n",
            "Iâ€™ve omitted some\n",
            "details, for which you should look at the paper.\n",
            "You can also look at Alex Krizhevskyâ€™s\n",
            "cuda-convnet(andsuccessors),whichcontainscodeimplementingmanyoftheideas. A\n",
            "Theano-basedimplementationhasalsobeendeveloped27,withthecodeavailablehere. The\n",
            "codeisrecognizablyalongsimilarlinestothatdevelopedinthischapter,althoughtheuseof\n",
            "multipleGPUscomplicatesthingssomewhat. TheCaffeneuralnetsframeworkalsoincludes\n",
            "aversionoftheKSHnetwork,seetheirModelZoofordetails. The 2014 ILSVRC competition: Since 2012, rapid progress continues to be made. Considerthe2014ILSVRCcompetition. Asin2012,itinvolvedatrainingsetof1.2million\n",
            "images, in 1,000 categories, and the figure of merit was whether the top 5 predictions\n",
            "includedthecorrectcategory. Thewinningteam,basedprimarilyatGoogle28,usedadeep\n",
            "convolutionalnetworkwith22layersofneurons. TheycalledtheirnetworkGoogLeNet,\n",
            "asahomagetoLeNet-5.\n",
            "GoogLeNetachievedatop-5accuracyof93.33percent,agiant\n",
            "improvementoverthe2013winner(Clarifai,with88.3percent),andthe2012winner(KSH,\n",
            "with84.7percent). JusthowgoodisGoogLeNetâ€™s93.33percentaccuracy?\n",
            "In2014ateamofresearchers\n",
            "wroteasurveypaperabouttheILSVRCcompetition29. Oneofthequestionstheyaddressis\n",
            "howwellhumansperformonILSVRC.Todothis,theybuiltasystemwhichletshumans\n",
            "27Theano-basedlarge-scalevisualrecognitionwithmultipleGPUs,byWeiguangDing,RuoyanWang,\n",
            "FeiMao,andGrahamTaylor(2014). 28Goingdeeperwithconvolutions,byChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,Scott\n",
            "Reed,DragomirAnguelov,DumitruErhan,VincentVanhoucke,andAndrewRabinovich(2014). 29ImageNetlargescalevisualrecognitionchallenge,byOlgaRussakovsky,JiaDeng,HaoSu,Jonathan\n",
            "Krause,SanjeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,\n",
            "AlexanderC.Berg,andLiFei-Fei(2014). \n",
            "(cid:12)\n",
            "200 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "classifyILSVRCimages. Asoneoftheauthors,AndrejKarpathy,explainsinaninformative\n",
            "blogpost,itwasalotoftroubletogetthehumansuptoGoogLeNetâ€™sperformance:\n",
            "...thetaskoflabelingimageswith5outof1000categoriesquicklyturned\n",
            "outtobeextremelychallenging,evenforsomefriendsinthelabwhohave\n",
            "beenworkingonILSVRCanditsclassesforawhile. Firstwethought\n",
            "we would put it up on [Amazon Mechanical Turk]. Then we thought\n",
            "wecouldrecruitpaidundergrads. ThenIorganizedalabelingpartyof\n",
            "intenselabelingeffortonlyamongthe(expertlabelers)inourlab. ThenI\n",
            "developedamodifiedinterfacethatusedGoogLeNetpredictionstoprune\n",
            "thenumberofcategoriesfrom1000toonlyabout100. Itwasstilltoo\n",
            "hardâ€“peoplekeptmissingcategoriesandgettinguptorangesof13â€“15%\n",
            "errorrates. IntheendIrealizedthattogetanywherecompetitivelyclose\n",
            "toGoogLeNet,itwasmostefficientifIsatdownandwentthroughthe\n",
            "painfullylongtrainingprocessandthesubsequentcarefulannotation\n",
            "processmyself... Thelabelinghappenedatarateofabout1perminute,\n",
            "butthisdecreasedovertime... Someimagesareeasilyrecognized,while\n",
            "6\n",
            "some images (such as those of fine-grained breeds of dogs, birds, or\n",
            "monkeys)canrequiremultipleminutesofconcentratedeffort. Ibecame\n",
            "verygoodatidentifyingbreedsofdogs...\n",
            "Basedonthesampleofimages\n",
            "Iworkedon,theGoogLeNetclassificationerrorturnedouttobe6.8%... My own error in the end turned out to be 5.1%, approximately 1.7%\n",
            "better. In other words, an expert human, working painstakingly, was with great effort able to\n",
            "narrowlybeatthedeepneuralnetwork. Infact, Karpathyreportsthatasecondhuman\n",
            "expert,trainedonasmallersampleofimages,wasonlyabletoattaina12.0percenttop-5\n",
            "errorrate,significantlybelowGoogLeNetâ€™sperformance. Abouthalftheerrorsweredueto\n",
            "theexpertâ€œfailingtospotandconsiderthegroundtruthlabelasanoptionâ€.\n",
            "These are astonishing results. Indeed, since this work, several teams have reported\n",
            "systems whose top-5 error rate is actually better than 5.1%. This has sometimes been\n",
            "reportedinthemediaasthesystemshavingbetter-than-humanvision.\n",
            "Whiletheresultsare\n",
            "genuinelyexciting,therearemanycaveatsthatmakeitmisleadingtothinkofthesystems\n",
            "ashavingbetter-than-humanvision. TheILSVRCchallengeisinmanywaysaratherlimited\n",
            "problem â€“ a crawl of the open web is not necessarily representative of images found in\n",
            "applications! And,ofcourse,thetop-5criterionisquiteartificial. Wearestillalongway\n",
            "fromsolvingtheproblemofimagerecognitionor,morebroadly,computervision. Still,itâ€™s\n",
            "extremelyencouragingtoseesomuchprogressmadeonsuchachallengingproblem,over\n",
            "justafewyears. Otheractivity: Iâ€™vefocusedonImageNet,butthereâ€™saconsiderableamountofother\n",
            "activityusingneuralnetstodoimagerecognition. Letmebrieflydescribeafewinteresting\n",
            "recentresults,justtogivetheflavourofsomecurrentwork. OneencouragingpracticalsetofresultscomesfromateamatGoogle,whoapplieddeep\n",
            "convolutionalnetworkstotheproblemofrecognizingstreetnumbersinGoogleâ€™sStreetView\n",
            "imagery30. Intheirpaper,theyreportdetectingandautomaticallytranscribingnearly100\n",
            "millionstreetnumbersatanaccuracysimilartothatofahumanoperator. Thesystemisfast:\n",
            "theirsystemtranscribedallofStreetViewâ€™simagesofstreetnumbersinFranceinlessthan\n",
            "30Multi-digitNumberRecognitionfromStreetViewImageryusingDeepConvolutionalNeuralNet-\n",
            "works,byIanJ.Goodfellow,YaroslavBulatov,JulianIbarz,SachaArnoud,andVinayShet(2013). \n",
            "(cid:12)\n",
            "6.4.\n",
            "Recentprogressinimagerecognition (cid:12) 201\n",
            "(cid:12)\n",
            "anhour! Theysay: â€œHavingthisnewdatasetsignificantlyincreasedthegeocodingqualityof\n",
            "GoogleMapsinseveralcountriesespeciallytheonesthatdidnotalreadyhaveothersources\n",
            "ofgoodgeocoding.â€ Andtheygoontomakethebroaderclaim: â€œWebelievewiththismodel\n",
            "wehavesolved[opticalcharacterrecognition]forshortsequences[ofcharacters]formany\n",
            "applications.â€\n",
            "Iâ€™veperhapsgiventheimpressionthatitâ€™sallaparadeofencouragingresults. Ofcourse,\n",
            "someofthemostinterestingworkreportsonfundamentalthingswedonâ€™tyetunderstand. Forinstance,a2013paper31showedthatdeepnetworksmaysufferfromwhatareeffectively\n",
            "blindspots. Considerthelinesofimagesbelow. OntheleftisanImageNetimageclassified\n",
            "correctlybytheirnetwork. Ontherightisaslightlyperturbedimage(theperturbationisin\n",
            "themiddle)whichisclassifiedincorrectlybythenetwork. Theauthorsfoundthatthereare\n",
            "suchâ€œadversarialâ€imagesforeverysampleimage,notjustafewspecialones. 6\n",
            "Thisisadisturbingresult.ThepaperusedanetworkbasedonthesamecodeasKSHâ€™snetwork\n",
            "â€“thatis,justthetypeofnetworkthatisbeingincreasinglywidelyused. Whilesuchneural\n",
            "networkscomputefunctionswhichare,inprinciple,continuous,resultslikethissuggestthat\n",
            "inpracticetheyâ€™relikelytocomputefunctionswhichareverynearlydiscontinuous. Worse,\n",
            "theyâ€™llbediscontinuousinwaysthatviolateourintuitionaboutwhatisreasonablebehavior. Thatâ€™sconcerning. Furthermore,itâ€™snotyetwellunderstoodwhatâ€™scausingthediscontinuity:\n",
            "isitsomethingaboutthelossfunction? Theactivationfunctionsused? Thearchitectureof\n",
            "thenetwork?\n",
            "Somethingelse? Wedonâ€™tyetknow.\n",
            "Now,theseresultsarenotquiteasbadastheysound. Althoughsuchadversarialimages\n",
            "arecommon,theyâ€™realsounlikelyinpractice. Asthepapernotes:\n",
            "Theexistenceoftheadversarialnegativesappearstobeincontradiction\n",
            "withthenetworkâ€™sabilitytoachievehighgeneralizationperformance. Indeed,ifthenetworkcangeneralizewell,howcanitbeconfusedby\n",
            "theseadversarialnegatives,whichareindistinguishablefromtheregular\n",
            "examples? Theexplanationisthatthesetofadversarialnegativesisof\n",
            "extremelylowprobability,andthusisnever(orrarely)observedinthe\n",
            "31Intriguingpropertiesofneuralnetworks,byChristianSzegedy,WojciechZaremba,IlyaSutskever,\n",
            "JoanBruna,DumitruErhan,IanGoodfellow,andRobFergus(2013)\n",
            "\n",
            "(cid:12)\n",
            "202 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "testset,yetitisdense(muchliketherationalnumbers),andsoitisfound\n",
            "nearvirtuallyeverytestcase. Nonetheless, it is distressing that we understand neural nets so poorly that this kind of\n",
            "resultshouldbearecentdiscovery. Ofcourse,amajorbenefitoftheresultsisthatthey\n",
            "havestimulatedmuchfollowupwork. Forexample,onerecentpaper32 showsthatgiven\n",
            "atrainednetworkitâ€™spossibletogenerateimageswhichlooktoahumanlikewhitenoise,\n",
            "butwhichthenetworkclassifiesasbeinginaknowncategorywithaveryhighdegreeof\n",
            "confidence. Thisisanotherdemonstrationthatwehavealongwaytogoinunderstanding\n",
            "neuralnetworksandtheiruseinimagerecognition. Despiteresultslikethis,theoverallpictureisencouraging.\n",
            "Weâ€™reseeingrapidprogress\n",
            "onextremelydifficultbenchmarks,likeImageNet. Weâ€™realsoseeingrapidprogressinthe\n",
            "solutionofreal-worldproblems,likerecognizingstreetnumbersinStreetView. Butwhilethis\n",
            "isencouragingitâ€™snotenoughjusttoseeimprovementsonbenchmarks,orevenreal-world\n",
            "applications. Therearefundamentalphenomenawhichwestillunderstandpoorly,such\n",
            "astheexistenceofadversarialimages. Whensuchfundamentalproblemsarestillbeing\n",
            "discovered(nevermindsolved),itisprematuretosaythatweâ€™renearsolvingtheproblemof\n",
            "6\n",
            "imagerecognition. Atthesametimesuchproblemsareanexcitingstimulustofurtherwork. 6.5 Other approaches to deep neural nets\n",
            "Throughthisbook,weâ€™veconcentratedonasingleproblem: classifyingtheMNISTdigits. Itâ€™sajuicyproblemwhichforcedustounderstandmanypowerfulideas: stochasticgradient\n",
            "descent,backpropagation,convolutionalnets,regularization,andmore.Butitâ€™salsoanarrow\n",
            "problem. Ifyoureadtheneuralnetworksliterature,youâ€™llrunintomanyideaswehavenâ€™t\n",
            "discussed: recurrentneuralnetworks,Boltzmannmachines,generativemodels,transfer\n",
            "learning,reinforcementlearning,andsoon,onandonÃ¢AË˜Ë›eandon! Neuralnetworksisa\n",
            "vastfield. However,manyimportantideasarevariationsonideasweâ€™vealreadydiscussed,\n",
            "andcanbeunderstoodwithalittleeffort.\n",
            "InthissectionIprovideaglimpseoftheseasyet\n",
            "unseenvistas. Thediscussionisnâ€™tdetailed,norcomprehensiveâ€“thatwouldgreatlyexpand\n",
            "thebook. Rather,itâ€™simpressionistic,anattempttoevoketheconceptualrichnessofthe\n",
            "field,andtorelatesomeofthoserichestowhatweâ€™vealreadyseen. Throughthesection,Iâ€™ll\n",
            "provideafewlinkstoothersources,asentreestolearnmore. Ofcourse,manyoftheselinks\n",
            "willsoonbesuperseded,andyoumaywishtosearchoutmorerecentliterature.\n",
            "Thatpoint\n",
            "notwithstanding,Iexpectmanyoftheunderlyingideastobeoflastinginterest. Recurrentneuralnetworks(RNNs): Inthefeedforwardnetsweâ€™vebeenusingthere\n",
            "isasingleinputwhichcompletelydeterminestheactivationsofalltheneuronsthrough\n",
            "theremaininglayers. Itâ€™saverystaticpicture: everythinginthenetworkisfixed,witha\n",
            "frozen,crystallinequalitytoit. Butsupposeweallowtheelementsinthenetworktokeep\n",
            "changinginadynamicway. Forinstance,thebehaviourofhiddenneuronsmightnotjustbe\n",
            "determinedbytheactivationsinprevioushiddenlayers,butalsobytheactivationsatearlier\n",
            "times. Indeed,aneuronâ€™sactivationmightbedeterminedinpartbyitsownactivationatan\n",
            "earliertime. Thatâ€™scertainlynotwhathappensinafeedforwardnetwork. Orperhapsthe\n",
            "activationsofhiddenandoutputneuronswonâ€™tbedeterminedjustbythecurrentinputto\n",
            "thenetwork,butalsobyearlierinputs. 32DeepNeuralNetworksareEasilyFooled:HighConfidencePredictionsforUnrecognizableImages,\n",
            "byAnhNguyen,JasonYosinski,andJeffClune(2014). \n",
            "(cid:12)\n",
            "6.5. Otherapproachestodeepneuralnets (cid:12) 203\n",
            "(cid:12)\n",
            "Neuralnetworkswiththiskindoftime-varyingbehaviourareknownasrecurrentneural\n",
            "networksorRNNs. Therearemanydifferentwaysofmathematicallyformalizingtheinformal\n",
            "descriptionofrecurrentnetsgiveninthelastparagraph.\n",
            "Youcangettheflavourofsomeof\n",
            "thesemathematicalmodelsbyglancingattheWikipediaarticleonRNNs. AsIwrite,that\n",
            "pagelistsnofewerthan13differentmodels. Butmathematicaldetailsaside,thebroadidea\n",
            "isthatRNNsareneuralnetworksinwhichthereissomenotionofdynamicchangeovertime. And,notsurprisingly,theyâ€™reparticularlyusefulinanalysingdataorprocessesthatchange\n",
            "overtime. Suchdataandprocessesarisenaturallyinproblemssuchasspeechornatural\n",
            "language,forexample. OnewayRNNsarecurrentlybeingusedistoconnectneuralnetworksmorecloselyto\n",
            "traditionalwaysofthinkingaboutalgorithms, waysofthinkingbasedonconceptssuch\n",
            "asTuringmachinesand(conventional)programminglanguages. A2014paperdeveloped\n",
            "an RNN which could take as input a character-by-character description of a (very, very\n",
            "simple!) Pythonprogram,andusethatdescriptiontopredicttheoutput. Informally,the\n",
            "networkislearningtoâ€œunderstandâ€certainPythonprograms. Asecondpaper,alsofrom\n",
            "2014,usedRNNsasastartingpointtodevelopwhattheycalledaneuralTuringmachine\n",
            "6\n",
            "(NTM).Thisisauniversalcomputerwhoseentirestructurecanbetrainedusinggradient\n",
            "descent. TheytrainedtheirNTMtoinferalgorithmsforseveralsimpleproblems,suchas\n",
            "sortingandcopying. Asitstands,theseareextremelysimpletoymodels. LearningtoexecutethePythonpro-\n",
            "gramprint(398345+42598)doesnâ€™tmakeanetworkintoafull-fledgedPythoninterpreter! Itâ€™snotclearhowmuchfurtheritwillbepossibletopushtheideas.\n",
            "Still,theresultsare\n",
            "intriguing. Historically,neuralnetworkshavedonewellatpatternrecognitionproblems\n",
            "whereconventionalalgorithmicapproacheshavetrouble. Viceversa,conventionalalgorith-\n",
            "micapproachesaregoodatsolvingproblemsthatneuralnetsarenâ€™tsogoodat. No-one\n",
            "todayimplementsawebserveroradatabaseprogramusinganeuralnetwork! Itâ€™dbegreat\n",
            "todevelopunifiedmodelsthatintegratethestrengthsofbothneuralnetworksandmore\n",
            "traditionalapproachestoalgorithms. RNNsandideasinspiredbyRNNsmayhelpusdothat. RNNshavealsobeenusedinrecentyearstoattackmanyotherproblems. Theyâ€™vebeen\n",
            "particularlyusefulinspeechrecognition. ApproachesbasedonRNNshave,forexample,\n",
            "setrecordsfortheaccuracyofphonemerecognition. Theyâ€™vealsobeenusedtodevelop\n",
            "improvedmodelsofthelanguagepeopleusewhilespeaking. Betterlanguagemodelshelp\n",
            "disambiguateutterancesthatotherwisesoundalike.Agoodlanguagemodelwill,forexample,\n",
            "tellusthatâ€œtoinfinityandbeyondâ€ismuchmorelikelythanâ€œtwoinfinityandbeyondâ€,\n",
            "despitethefactthatthephrasessoundidentical. RNNshavebeenusedtosetnewrecords\n",
            "forcertainlanguagebenchmarks. Thisworkis,incidentally,partofabroaderuseofdeepneuralnetsofalltypes,notjust\n",
            "RNNs,inspeechrecognition.\n",
            "Forexample,anapproachbasedondeepnetshasachieved\n",
            "outstandingresultsonlargevocabularycontinuousspeechrecognition. Andanothersystem\n",
            "basedondeepnetshasbeendeployedinGoogleâ€™sAndroidoperatingsystem(forrelated\n",
            "technicalwork,seeVincentVanhouckeâ€™s2012â€“2015papers). Iâ€™vesaidalittleaboutwhatRNNscando,butnotsomuchabouthowtheywork.\n",
            "It\n",
            "perhapswonâ€™tsurpriseyoutolearnthatmanyoftheideasusedinfeedforwardnetworkscan\n",
            "alsobeusedinRNNs. Inparticular,wecantrainRNNsusingstraightforwardmodificationsto\n",
            "gradientdescentandbackpropagation. Manyotherideasusedinfeedforwardnets,ranging\n",
            "fromregularizationtechniquestoconvolutionstotheactivationandcostfunctionsused,are\n",
            "alsousefulinrecurrentnets. Andsomanyofthetechniquesweâ€™vedevelopedinthebook\n",
            "\n",
            "(cid:12)\n",
            "204 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "canbeadaptedforusewithRNNs. Longshort-termmemoryunits(LSTMs): OnechallengeaffectingRNNsisthatearly\n",
            "modelsturnedouttobeverydifficulttotrain,hardereventhandeepfeedforwardnetworks. ThereasonistheunstablegradientproblemdiscussedinChapter5. Recallthattheusual\n",
            "manifestationofthisproblemisthatthegradientgetssmallerandsmallerasitispropagated\n",
            "back through layers. This makes learning in early layers extremely slow. The problem\n",
            "actuallygetsworseinRNNs,sincegradientsarenâ€™tjustpropagatedbackwardthroughlayers,\n",
            "theyâ€™repropagatedbackwardthroughtime. Ifthenetworkrunsforalongtimethatcan\n",
            "makethegradientextremelyunstableandhardtolearnfrom.\n",
            "Fortunately,itâ€™spossibleto\n",
            "incorporateanideaknownaslongshort-termmemoryunits(LSTMs)intoRNNs. Theunits\n",
            "wereintroducedbyHochreiterandSchmidhuberin1997withtheexplicitpurposeofhelping\n",
            "addresstheunstablegradientproblem.\n",
            "LSTMsmakeitmucheasiertogetgoodresultswhen\n",
            "trainingRNNs,andmanyrecentpapers(includingmanythatIlinkedabove)makeuseof\n",
            "LSTMsorrelatedideas. Deepbeliefnets,generativemodels,andBoltzmannmachines: Moderninterestin\n",
            "deeplearningbeganin2006,withpapersexplaininghowtotrainatypeofneuralnetwork\n",
            "6 knownasadeepbeliefnetwork(DBN)33.DBNswereinfluentialforseveralyears,buthave\n",
            "sincelessenedinpopularity,whilemodelssuchasfeedforwardnetworksandrecurrentneural\n",
            "netshavebecomefashionable. Despitethis,DBNshaveseveralpropertiesthatmakethem\n",
            "interesting. OnereasonDBNsareinterestingisthattheyâ€™reanexampleofwhatâ€™scalledagenerative\n",
            "model. Inafeedforwardnetwork,wespecifytheinputactivations,andtheydeterminethe\n",
            "activationsofthefeatureneuronslaterinthenetwork. AgenerativemodellikeaDBNcan\n",
            "beusedinasimilarway,butitâ€™salsopossibletospecifythevaluesofsomeofthefeature\n",
            "neuronsandthenâ€œrunthenetworkbackwardâ€,generatingvaluesfortheinputactivations. Moreconcretely,aDBNtrainedonimagesofhandwrittendigitscan(potentially,andwith\n",
            "somecare)alsobeusedtogenerateimagesthatlooklikehandwrittendigits. Inotherwords,\n",
            "theDBNwouldinsomesensebelearningtowrite. Inthis,agenerativemodelismuchlike\n",
            "thehumanbrain: notonlycanitreaddigits,itcanalsowritethem. InGeoffreyHintonâ€™s\n",
            "memorablephrase,torecognizeshapes,firstlearntogenerateimages. A second reason DBNs are interesting is that they can do unsupervised and semi-\n",
            "supervisedlearning. Forinstance,whentrainedwithimagedata,DBNscanlearnuseful\n",
            "featuresforunderstandingotherimages,evenifthetrainingimagesareunlabelled. Andthe\n",
            "abilitytodounsupervisedlearningisextremelyinterestingbothforfundamentalscientific\n",
            "reasons,andâ€“ifitcanbemadetoworkwellenoughâ€“forpracticalapplications. Giventheseattractivefeatures,whyhaveDBNslessenedinpopularityasmodelsfor\n",
            "deeplearning? Partofthereasonisthatmodelssuchasfeedforwardandrecurrentnets\n",
            "haveachievedmanyspectacularresults,suchastheirbreakthroughsonimageandspeech\n",
            "recognitionbenchmarks. Itâ€™snotsurprisingandquiterightthatthereâ€™snowlotsofattention\n",
            "beingpaidtothesemodels. Thereâ€™sanunfortunatecorollary,however. Themarketplace\n",
            "ofideasoftenfunctionsinawinner-take-allfashion,withnearlyallattentiongoingtothe\n",
            "currentfashion-of-the-momentinanygivenarea. Itcanbecomeextremelydifficultforpeople\n",
            "toworkonmomentarilyunfashionableideas,evenwhenthoseideasareobviouslyofreal\n",
            "long-terminterest. MypersonalopinionisthatDBNsandothergenerativemodelslikely\n",
            "deservemoreattentionthantheyarecurrentlyreceiving. AndIwonâ€™tbesurprisedifDBNs\n",
            "33SeeAfastlearningalgorithmfordeepbeliefnets,byGeoffreyHinton,SimonOsindero,andYee-Whye\n",
            "Teh(2006),aswellastherelatedworkinReducingthedimensionalityofdatawithneuralnetworks,by\n",
            "GeoffreyHintonandRuslanSalakhutdinov(2006).\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 205\n",
            "(cid:12)\n",
            "orarelatedmodelonedaysurpassthecurrentlyfashionablemodels. Foranintroductionto\n",
            "DBNs,seethisoverview. Iâ€™vealsofoundthisarticlehelpful. Itisnâ€™tprimarilyaboutdeep\n",
            "beliefnets,perse,butdoescontainmuchusefulinformationaboutrestrictedBoltzmann\n",
            "machines,whichareakeycomponentofDBNs. Otherideas: Whatelseisgoingoninneuralnetworksanddeeplearning?\n",
            "Well,thereâ€™s\n",
            "ahugeamountofotherfascinatingwork. Activeareasofresearchincludeusingneural\n",
            "networkstodonaturallanguageprocessing(seealsothisinformativereviewpaper),machine\n",
            "translation,aswellasperhapsmoresurprisingapplicationssuchasmusicinformatics. There\n",
            "are,ofcourse,manyotherareastoo. Inmanycases,havingreadthisbookyoushouldbeable\n",
            "tobeginfollowingrecentwork,although(ofcourse)youâ€™llneedtofillingapsinpresumed\n",
            "backgroundknowledge.\n",
            "Let me finish this section by mentioning a particularly fun paper. It combines deep\n",
            "convolutionalnetworkswithatechniqueknownasreinforcementlearninginordertolearn\n",
            "to play video games well (see also this followup). The idea is to use the convolutional\n",
            "networktosimplifythepixeldatafromthegamescreen,turningitintoasimplersetof\n",
            "features,whichcanbeusedtodecidewhichactiontotake: â€œgoleftâ€,â€œgodownâ€,â€œfireâ€,and\n",
            "6\n",
            "soon. Whatisparticularlyinterestingisthatasinglenetworklearnedtoplaysevendifferent\n",
            "classicvideogamesprettywell,outperforminghumanexpertsonthreeofthegames. Now,\n",
            "thisallsoundslikeastunt,andthereâ€™snodoubtthepaperwaswellmarketed,withthe\n",
            "titleâ€œPlayingAtariwithreinforcementlearningâ€.\n",
            "Butlookingpastthesurfacegloss,consider\n",
            "thatthissystemistakingrawpixeldataâ€“itdoesnâ€™tevenknowthegamerules! â€“andfrom\n",
            "that data learning to do high-quality decision-making in several very different and very\n",
            "adversarialenvironments,eachwithitsowncomplexsetofrules. Thatâ€™sprettyneat. 6.6 On the future of neural networks\n",
            "Intention-drivenuserinterfaces: Thereâ€™sanoldjokeinwhichanimpatientprofessortellsa\n",
            "confusedstudent: â€œdonâ€™tlistentowhatIsay;listentowhatImeanâ€. Historically,computers\n",
            "haveoftenbeen,liketheconfusedstudent,inthedarkaboutwhattheirusersmean. Butthis\n",
            "ischanging.IstillremembermysurprisethefirsttimeImisspelledaGooglesearchquery,only\n",
            "tohaveGooglesayâ€œDidyoumean[correctedquery]?â€ andtoofferthecorrespondingsearch\n",
            "results. GoogleCEOLarryPageoncedescribedtheperfectsearchengineasunderstanding\n",
            "exactlywhat[yourqueries]meanandgivingyoubackexactlywhatyouwant. Thisisavisionofanintention-drivenuserinterface. Inthisvision,insteadofresponding\n",
            "tousersâ€™literalqueries,searchwillusemachinelearningtotakevagueuserinput,discern\n",
            "preciselywhatwasmeant,andtakeactiononthebasisofthoseinsights. Theideaofintention-driveninterfacescanbeappliedfarmorebroadlythansearch. Overthenextfewdecades,thousandsofcompanieswillbuildproductswhichusemachine\n",
            "learningtomakeuserinterfacesthatcantolerateimprecision,whilediscerningandactingon\n",
            "theuserâ€™strueintent. Weâ€™realreadyseeingearlyexamplesofsuchintention-driveninterfaces:\n",
            "Appleâ€™sSiri;WolframAlpha;IBMâ€™sWatson;systemswhichcanannotatephotosandvideos;\n",
            "andmuchmore. Mostoftheseproductswillfail. Inspireduserinterfacedesignishard, andIexpect\n",
            "manycompanieswilltakepowerfulmachinelearningtechnologyanduseittobuildinsipid\n",
            "userinterfaces. Thebestmachinelearningintheworldwonâ€™thelpifyouruserinterface\n",
            "conceptstinks. Buttherewillbearesidueofproductswhichsucceed.\n",
            "Overtimethatwill\n",
            "causeaprofoundchangeinhowwerelatetocomputers. Notsolongagoâ€“letâ€™ssay,2005\n",
            "\n",
            "(cid:12)\n",
            "206 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "â€“userstookitforgrantedthattheyneededprecisioninmostinteractionswithcomputers. Indeed,computerliteracytoagreatextentmeantinternalizingtheideathatcomputersare\n",
            "extremelyliteral;asinglemisplacedsemi-colonmaycompletelychangethenatureofan\n",
            "interactionwithacomputer. ButoverthenextfewdecadesIexpectweâ€™lldevelopmany\n",
            "successfulintention-drivenuserinterfaces,andthatwilldramaticallychangewhatweexpect\n",
            "wheninteractingwithcomputers. Machine learning, data science, and the virtuous circle of innovation: Of course,\n",
            "machinelearningisnâ€™tjustbeingusedtobuildintention-driveninterfaces. Anothernotable\n",
            "applicationisindatascience,wheremachinelearningisusedtofindtheâ€œknownunknownsâ€\n",
            "hiddenindata. Thisisalreadyafashionablearea,andmuchhasbeenwrittenaboutit,so\n",
            "Iwonâ€™tsaymuch. ButIdowanttomentiononeconsequenceofthisfashionthatisnot\n",
            "so often remarked: over the long run itâ€™s possible the biggest breakthrough in machine\n",
            "learningwonâ€™tbeanysingleconceptualbreakthrough. Rather,thebiggestbreakthroughwill\n",
            "bethatmachinelearningresearchbecomesprofitable,throughapplicationstodatascience\n",
            "andotherareas. Ifacompanycaninvest1dollarinmachinelearningresearchandget1\n",
            "dollarand10centsbackreasonablyrapidly,thenalotofmoneywillendupinmachine\n",
            "6\n",
            "learningresearch. Putanotherway,machinelearningisanenginedrivingthecreationof\n",
            "severalmajornewmarketsandareasofgrowthintechnology. Theresultwillbelargeteams\n",
            "ofpeoplewithdeepsubjectexpertise, andwithaccesstoextraordinaryresources.\n",
            "That\n",
            "willpropelmachinelearningfurtherforward,creatingmoremarketsandopportunities,a\n",
            "virtuouscircleofinnovation. The role of neural networks and deep learning: Iâ€™ve been talking broadly about\n",
            "machinelearningasacreatorofnewopportunitiesfortechnology. Whatwillbethespecific\n",
            "roleofneuralnetworksanddeeplearninginallthis?\n",
            "Toanswerthequestion,ithelpstolookathistory. Backinthe1980stherewasagreat\n",
            "dealofexcitementandoptimismaboutneuralnetworks,especiallyafterbackpropagation\n",
            "becamewidelyknown. Thatexcitementfaded,andinthe1990sthemachinelearningbaton\n",
            "passedtoothertechniques,suchassupportvectormachines. Today,neuralnetworksare\n",
            "againridinghigh,settingallsortsofrecords,defeatingallcomersonmanyproblems. But\n",
            "whoistosaythattomorrowsomenewapproachwonâ€™tbedevelopedthatsweepsneural\n",
            "networksawayagain? Orperhapsprogresswithneuralnetworkswillstagnate,andnothing\n",
            "willimmediatelyarisetotaketheirplace? Forthisreason,itâ€™smucheasiertothinkbroadlyaboutthefutureofmachinelearning\n",
            "thanaboutneuralnetworksspecifically. Partoftheproblemisthatweunderstandneural\n",
            "networkssopoorly. Whyisitthatneuralnetworkscangeneralizesowell? Howisitthat\n",
            "theyavoidoverfittingaswellastheydo,giventheverylargenumberofparametersthey\n",
            "learn? Whyisitthatstochasticgradientdescentworksaswellasitdoes? Howwellwill\n",
            "neuralnetworksperformasdatasetsarescaled? Forinstance,ifImageNetwasexpanded\n",
            "byafactorof10,wouldneuralnetworksâ€™performanceimprovemoreorlessthanother\n",
            "machinelearningtechniques? Theseareallsimple,fundamentalquestions.\n",
            "And,atpresent,\n",
            "weunderstandtheanswerstothesequestionsverypoorly. Whilethatâ€™sthecase,itâ€™sdifficult\n",
            "tosaywhatroleneuralnetworkswillplayinthefutureofmachinelearning. Iwillmakeoneprediction: Ibelievedeeplearningisheretostay.\n",
            "Theabilitytolearn\n",
            "hierarchiesofconcepts,buildingupmultiplelayersofabstraction,seemstobefundamental\n",
            "tomakingsenseoftheworld. Thisdoesnâ€™tmeantomorrowâ€™sdeeplearnerswonâ€™tberadically\n",
            "differentthantodayâ€™s. Wecouldseemajorchangesintheconstituentunitsused,inthe\n",
            "architectures,orinthelearningalgorithms. Thosechangesmaybedramaticenoughthatwe\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 207\n",
            "(cid:12)\n",
            "nolongerthinkoftheresultingsystemsasneuralnetworks. Buttheyâ€™dstillbedoingdeep\n",
            "learning. Willneuralnetworksanddeeplearningsoonleadtoartificialintelligence? Inthis\n",
            "bookweâ€™vefocusedonusingneuralnetstodospecifictasks, suchasclassifyingimages. Letâ€™sbroadenourambitions, andask: whataboutgeneral-purposethinkingcomputers? Canneuralnetworksanddeeplearninghelpussolvetheproblemof(general)artificial\n",
            "intelligence(AI)?And,ifso,giventherapidrecentprogressofdeeplearning,canweexpect\n",
            "generalAIanytimesoon? Addressingthesequestionscomprehensivelywouldtakeaseparatebook.\n",
            "Instead,let\n",
            "meofferoneobservation. Itâ€™sbasedonanideaknownasConwayâ€™slaw:\n",
            "Anyorganizationthatdesignsasystem... willinevitablyproduceadesign\n",
            "whosestructureisacopyoftheorganizationâ€™scommunicationstructure. So,forexample,Conwayâ€™slawsuggeststhatthedesignofaBoeing747aircraftwillmirror\n",
            "theextendedorganizationalstructureofBoeinganditscontractorsatthetimethe747was\n",
            "designed. Orforasimple,specificexample,consideracompanybuildingacomplexsoftware\n",
            "application. Iftheapplicationâ€™sdashboardissupposedtobeintegratedwithsomemachine 6\n",
            "learningalgorithm,thepersonbuildingthedashboardbetterbetalkingtothecompanyâ€™s\n",
            "machinelearningexpert. Conwayâ€™slawismerelythatobservation,writlarge. UponfirsthearingConwayâ€™slaw, manypeoplerespondeitherâ€œWell, isnâ€™tthatbanal\n",
            "andobvious?â€ orâ€œIsnâ€™tthatwrong?â€ Letmestartwiththeobjectionthatitâ€™swrong. Asan\n",
            "instanceofthisobjection,considerthequestion: wheredoesBoeingâ€™saccountingdepartment\n",
            "showupinthedesignofthe747?\n",
            "Whatabouttheirjanitorialdepartment?\n",
            "Theirinternal\n",
            "catering? Andtheansweristhatthesepartsoftheorganizationprobablydonâ€™tshowup\n",
            "explicitlyanywhereinthe747. SoweshouldunderstandConwayâ€™slawasreferringonlyto\n",
            "thosepartsofanorganizationconcernedexplicitlywithdesignandengineering. Whatabouttheotherobjection,thatConwayâ€™slawisbanalandobvious? Thismayper-\n",
            "hapsbetrue,butIdonâ€™tthinkso,fororganizationstoooftenactwithdisregardforConwayâ€™s\n",
            "law. Teamsbuildingnewproductsareoftenbloatedwithlegacyhiresor,contrariwise,lacka\n",
            "personwithsomecrucialexpertise. Thinkofalltheproductswhichhaveuselesscomplicating\n",
            "features. Orthinkofalltheproductswhichhaveobviousmajordeficienciesâ€“e.g.,aterrible\n",
            "userinterface. Problemsinbothclassesareoftencausedbyamismatchbetweentheteam\n",
            "that was needed to produce a good product, and the team that was actually assembled. Conwayâ€™slawmaybeobvious,butthatdoesnâ€™tmeanpeopledonâ€™troutinelyignoreit. Conwayâ€™slawappliestothedesignandengineeringofsystemswherewestartoutwith\n",
            "aprettygoodunderstandingofthelikelyconstituentparts,andhowtobuildthem. Itcanâ€™t\n",
            "beapplieddirectlytothedevelopmentofartificialintelligence,becauseAIisnâ€™t(yet)sucha\n",
            "problem: wedonâ€™tknowwhattheconstituentpartsare. Indeed,weâ€™renotevensurewhat\n",
            "basicquestionstobeasking. Inotherswords,atthispointAIismoreaproblemofscience\n",
            "thanofengineering. Imaginebeginningthedesignofthe747withoutknowingaboutjet\n",
            "enginesortheprinciplesofaerodynamics. Youwouldnâ€™tknowwhatkindsofexpertstohire\n",
            "intoyourorganization. AsWernhervonBraunputit,â€œbasicresearchiswhatIâ€™mdoingwhen\n",
            "Idonâ€™tknowwhatIâ€™mdoingâ€. IsthereaversionofConwayâ€™slawthatappliestoproblems\n",
            "whicharemoresciencethanengineering? Togaininsightintothisquestion,considerthehistoryofmedicine. Intheearlydays,\n",
            "medicine was the domain of practitioners like Galen and Hippocrates, who studied the\n",
            "entirebody. Butasourknowledgegrew,peoplewereforcedtospecialize.\n",
            "Wediscovered\n",
            "\n",
            "(cid:12)\n",
            "208 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "manydeepnewideas34: thinkofthingslikethegermtheoryofdisease,forinstance,orthe\n",
            "understandingofhowantibodieswork,ortheunderstandingthattheheart,lungs,veins\n",
            "andarteriesformacompletecardiovascularsystem. Suchdeepinsightsformedthebasisfor\n",
            "subfieldssuchasepidemiology,immunology,andtheclusterofinter-linkedfieldsaroundthe\n",
            "cardiovascularsystem. Andsothestructureofourknowledgehasshapedthesocialstructure\n",
            "ofmedicine. Thisisparticularlystrikinginthecaseofimmunology: realizingtheimmune\n",
            "systemexistsandisasystemworthyofstudyisanextremelynon-trivialinsight. Sowehave\n",
            "anentirefieldofmedicineâ€“withspecialists,conferences,evenprizes,andsoonâ€“organized\n",
            "aroundsomethingwhichisnotjustinvisible,itâ€™sarguablynotadistinctthingatall. Thisisacommonpatternthathasbeenrepeatedinmanywell-establishedsciences:\n",
            "not just medicine, but physics, mathematics, chemistry, and others. The fields start out\n",
            "monolithic,withjustafewdeepideas.\n",
            "Earlyexpertscanmasterallthoseideas.\n",
            "Butastime\n",
            "passesthatmonolithiccharacterchanges. Wediscovermanydeepnewideas,toomanyfor\n",
            "anyonepersontoreallymaster. Asaresult,thesocialstructureofthefieldre-organizes\n",
            "anddividesaroundthoseideas. Insteadofamonolith,wehavefieldswithinfieldswithin\n",
            "fields,acomplex,recursive,self-referentialsocialstructure,whoseorganizationmirrorsthe\n",
            "6\n",
            "connectionsbetweenourdeepestinsights. Andsothestructureofourknowledgeshapesthe\n",
            "socialorganizationofscience.\n",
            "Butthatsocialshapeinturnconstrainsandhelpsdetermine\n",
            "whatwecandiscover. ThisisthescientificanalogueofConwayâ€™slaw. Sowhatâ€™sthisgottodowithdeeplearningorAI? Well,sincetheearlydaysofAItherehavebeenargumentsaboutitthatgo,ononeside,\n",
            "â€œHey,itâ€™snotgoingtobesohard,weâ€™vegot[super-specialweapon]onoursideâ€,counteredby\n",
            "â€œ[super-specialweapon]wonâ€™tbeenoughâ€. Deeplearningisthelatestsuper-specialweapon\n",
            "Iâ€™veheardusedinsucharguments35;earlierversionsoftheargumentusedlogic,orProlog,\n",
            "orexpertsystems,orwhateverthemostpowerfultechniqueofthedaywas. Theproblem\n",
            "withsuchargumentsisthattheydonâ€™tgiveyouanygoodwayofsayingjusthowpowerful\n",
            "anygivencandidatesuper-specialweaponis. Ofcourse,weâ€™vejustspentachapterreviewing\n",
            "evidencethatdeeplearningcansolveextremelychallengingproblems. Itcertainlylooksvery\n",
            "excitingandpromising. ButthatwasalsotrueofsystemslikePrologorEuriskoorexpert\n",
            "systemsintheirday. Andsothemerefactthatasetofideaslooksverypromisingdoesnâ€™t\n",
            "meanmuch. Howcanwetellifdeeplearningistrulydifferentfromtheseearlierideas? Is\n",
            "theresomewayofmeasuringhowpowerfulandpromisingasetofideasis? Conwayâ€™slaw\n",
            "suggeststhatasaroughandheuristicproxymetricwecanevaluatethecomplexityofthe\n",
            "socialstructureassociatedtothoseideas. So,therearetwoquestionstoask. First,howpowerfulasetofideasareassociatedto\n",
            "deeplearning,accordingtothismetricofsocialcomplexity? Second,howpowerfulatheory\n",
            "willweneed,inordertobeabletobuildageneralartificialintelligence? Astothefirstquestion: whenwelookatdeeplearningtoday,itâ€™sanexcitingandfast-\n",
            "paced but also relatively monolithic field. There are a few deep ideas, and a few main\n",
            "conferences,withsubstantialoverlapbetweenseveraloftheconferences. Andthereispaper\n",
            "afterpaperleveragingthesamebasicsetofideas: usingstochasticgradientdescent(ora\n",
            "closevariation)tooptimizeacostfunction. Itâ€™sfantasticthoseideasaresosuccessful. But\n",
            "34Myapologiesforoverloadingâ€œdeepâ€.Iwonâ€™tdefineâ€œdeepideasâ€precisely,butlooselyImeanthe\n",
            "kindofideawhichisthebasisforarichfieldofenquiry.Thebackpropagationalgorithmandthegerm\n",
            "theoryofdiseasearebothgoodexamples. 35Interestingly,oftennotbyleadingexpertsindeeplearning,whohavebeenquiterestrained.See,\n",
            "forexample,thisthoughtfulpostbyYannLeCun.Thisisadifferencefrommanyearlierincarnationsof\n",
            "theargument.\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 209\n",
            "(cid:12)\n",
            "whatwedonâ€™tyetseeislotsofwell-developedsubfields,eachexploringtheirownsetsof\n",
            "deepideas,pushingdeeplearninginmanydirections. Andso,accordingtothemetricof\n",
            "socialcomplexity,deeplearningis,ifyouâ€™llforgivetheplayonwords,stillarathershallow\n",
            "field. Itâ€™sstillpossibleforonepersontomastermostofthedeepestideasinthefield. On the second question: how complex and powerful a set of ideas will be needed\n",
            "to obtain AI? Of course, the answer to this question is: no-one knows for sure.\n",
            "But in\n",
            "theappendixIexaminesomeoftheexistingevidenceonthisquestion. Iconcludethat,\n",
            "evenratheroptimistically,itâ€™sgoingtotakemany,manydeepideastobuildanAI.Andso\n",
            "Conwayâ€™slawsuggeststhattogettosuchapointwewillnecessarilyseetheemergence\n",
            "ofmanyinterrelatingdisciplines, withacomplexandsurprisingstructuremirroringthe\n",
            "structureinourdeepestinsights. Wedonâ€™tyetseethisrichsocialstructureintheuseof\n",
            "neuralnetworksanddeeplearning. Andso,Ibelievethatweareseveraldecades(atleast)\n",
            "fromusingdeeplearningtodevelopgeneralAI. Iâ€™vegonetoalotoftroubletoconstructanargumentwhichistentative,perhapsseems\n",
            "ratherobvious,andwhichhasanindefiniteconclusion. Thiswillnodoubtfrustratepeople\n",
            "who crave certainty. Reading around online, I see many people who loudly assert very\n",
            "definite,verystronglyheldopinionsaboutAI,oftenonthebasisofflimsyreasoningand\n",
            "non-existentevidence. Myfrankopinionisthis: itâ€™stooearlytosay. Astheoldjokegoes,if\n",
            "youaskascientisthowfarawaysomediscoveryisandtheysayâ€œ10yearsâ€(ormore),what\n",
            "theymeanisâ€œIâ€™vegotnoideaâ€. AI,likecontrolledfusionandafewothertechnologies,has\n",
            "been10yearsawayfor60plusyears. Ontheflipside,whatwedefinitelydohaveindeep\n",
            "learningisapowerfultechniquewhoselimitshavenotyetbeenfound,andmanywide-open\n",
            "fundamentalproblems. Thatâ€™sanexcitingcreativeopportunity.\n",
            "\n",
            "(cid:12)\n",
            "210 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 211\n",
            "(cid:12)\n",
            "AAAA\n",
            "Is there a simple algorithm for\n",
            "intelligence? A\n",
            "Inthisbook,weâ€™vefocusedonthenutsandboltsofneuralnetworks: howtheywork,and\n",
            "howtheycanbeusedtosolvepatternrecognitionproblems.\n",
            "Thisismaterialwithmany\n",
            "immediatepracticalapplications. But,ofcourse,onereasonforinterestinneuralnetsisthe\n",
            "hopethatonedaytheywillgofarbeyondsuchbasicpatternrecognitionproblems. Perhaps\n",
            "they,orsomeotherapproachbasedondigitalcomputers,willeventuallybeusedtobuild\n",
            "thinkingmachines,machinesthatmatchorsurpasshumanintelligence? Thisnotionfar\n",
            "exceedsthematerialdiscussedinthebookâ€“orwhatanyoneintheworldknowshowtodo. Butitâ€™sfuntospeculate. Therehasbeenmuchdebateaboutwhetheritâ€™sevenpossibleforcomputerstomatch\n",
            "humanintelligence. Iâ€™mnotgoingtoengagewiththatquestion. Despiteongoingdispute,I\n",
            "believeitâ€™snotinseriousdoubtthatanintelligentcomputerispossibleâ€“althoughitmaybe\n",
            "extremelycomplicated,andperhapsfarbeyondcurrenttechnologyâ€“andcurrentnaysayers\n",
            "willonedayseemmuchlikethevitalists. Rather,thequestionIexplorehereiswhetherthereisasimplesetofprincipleswhich\n",
            "canbeusedtoexplainintelligence? Inparticular,andmoreconcretely,isthereasimple\n",
            "algorithmforintelligence? Theideathatthereisatrulysimplealgorithmforintelligenceisaboldidea.\n",
            "Itperhaps\n",
            "soundstoooptimistictobetrue. Manypeoplehaveastrongintuitivesensethatintelligence\n",
            "hasconsiderableirreduciblecomplexity. Theyâ€™resoimpressedbytheamazingvarietyand\n",
            "flexibilityofhumanthoughtthattheyconcludethatasimplealgorithmforintelligencemust\n",
            "beimpossible. Despitethisintuition,Idonâ€™tthinkitâ€™swisetorushtojudgement. Thehistory\n",
            "ofscienceisfilledwithinstanceswhereaphenomenoninitiallyappearedextremelycomplex,\n",
            "butwaslaterexplainedbysomesimplebutpowerfulsetofideas. Consider,forexample,theearlydaysofastronomy. Humanshaveknownsinceancient\n",
            "timesthatthereisamenagerieofobjectsinthesky: thesun,themoon,theplanets,the\n",
            "comets,andthestars. Theseobjectsbehaveinverydifferentwaysâ€“starsmoveinastately,\n",
            "\n",
            "(cid:12)\n",
            "212 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A regularwayacrossthesky,forexample,whilecometsappearasifoutofnowhere,streak\n",
            "acrossthesky,andthendisappear.\n",
            "Inthe16thcenturyonlyafoolishoptimistcouldhave\n",
            "imaginedthatalltheseobjectsâ€™motionscouldbeexplainedbyasimplesetofprinciples. Butinthe17thcenturyNewtonformulatedhistheoryofuniversalgravitation,whichnot\n",
            "onlyexplainedallthesemotions,butalsoexplainedterrestrialphenomenasuchasthetides\n",
            "andthebehaviourofEarth-boundprojecticles. The16thcenturyâ€™sfoolishoptimistseemsin\n",
            "retrospectlikeapessimist,askingfortoolittle.\n",
            "Ofcourse,sciencecontainsmanymoresuchexamples. Considerthemyriadchemical\n",
            "substancesmakingupourworld,sobeautifullyexplainedbyMendeleevâ€™speriodictable,\n",
            "whichis,inturn,explainedbyafewsimpleruleswhichmaybeobtainedfromquantum\n",
            "mechanics. Orthepuzzleofhowthereissomuchcomplexityanddiversityinthebiological\n",
            "world,whoseoriginturnsouttolieintheprincipleofevolutionbynaturalselection. These\n",
            "andmanyotherexamplessuggestthatitwouldnotbewisetoruleoutasimpleexplanation\n",
            "ofintelligencemerelyonthegroundsthatwhatourbrainsâ€“currentlythebestexamplesof\n",
            "intelligenceâ€“aredoingappearstobeverycomplicated1. Contrariwise, anddespitetheseoptimisticexamples, itisalsologicallypossiblethat\n",
            "intelligencecanonlybeexplainedbyalargenumberoffundamentallydistinctmechanisms. Inthecaseofourbrains,thosemanymechanismsmayperhapshaveevolvedinresponse\n",
            "tomanydifferentselectionpressuresinourspeciesâ€™evolutionaryhistory. Ifthispointof\n",
            "viewiscorrect,thenintelligenceinvolvesconsiderableirreduciblecomplexity,andnosimple\n",
            "algorithmforintelligenceispossible. Whichofthesetwopointsofviewiscorrect? Togetinsightintothisquestion,letâ€™saskacloselyrelatedquestion,whichiswhether\n",
            "thereâ€™sasimpleexplanationofhowhumanbrainswork. Inparticular,letâ€™slookatsomeways\n",
            "ofquantifyingthecomplexityofthebrain. Ourfirstapproachistheviewofthebrainfrom\n",
            "connectomics. Thisisallabouttherawwiring: howmanyneuronsthereareinthebrain,\n",
            "howmanyglialcells,andhowmanyconnectionstherearebetweentheneurons. Youâ€™ve\n",
            "probablyheardthenumbersbeforeâ€“thebraincontainsontheorderof100billionneurons,\n",
            "100billionglialcells,and100trillionconnectionsbetweenneurons. Thosenumbersare\n",
            "staggering. Theyâ€™re also intimidating. If we need to understand the details of all those\n",
            "connections(nottomentiontheneuronsandglialcells)inordertounderstandhowthe\n",
            "brainworks,thenweâ€™recertainlynotgoingtoendupwithasimplealgorithmforintelligence. Thereâ€™sasecond,moreoptimisticpointofview,theviewofthebrainfrommolecular\n",
            "biology. Theideaistoaskhowmuchgeneticinformationisneededtodescribethebrainâ€™s\n",
            "architecture.Togetahandleonthisquestion,weâ€™llstartbyconsideringthegeneticdifferences\n",
            "betweenhumansandchimpanzees. Youâ€™veprobablyheardthesoundbitethatâ€œhumanbeings\n",
            "are98percentchimpanzeeâ€. Thissayingissometimesvariedâ€“popularvariationsalsogive\n",
            "thenumberas95or99percent. Thevariationsoccurbecausethenumberswereoriginally\n",
            "estimatedbycomparingsamplesofthehumanandchimpgenomes,nottheentiregenomes. However,in2007theentirechimpanzeegenomewassequenced(seealsohere),andwe\n",
            "nowknowthathumanandchimpDNAdifferatroughly125millionDNAbasepairs. Thatâ€™s\n",
            "outofatotalofroughly3billionDNAbasepairsineachgenome.\n",
            "Soitâ€™snotrighttosay\n",
            "humanbeingsare98percentchimpanzeeâ€“weâ€™remorelike96percentchimpanzee. 1ThroughthisappendixIassumethatforacomputertobeconsideredintelligentitscapabilitiesmust\n",
            "matchorexceedhumanthinkingability.AndsoIâ€™llregardthequestionâ€œIsthereasimplealgorithmfor\n",
            "intelligence?â€asequivalenttoâ€œIsthereasimplealgorithmwhichcanâ€˜thinkâ€™alongessentiallythesame\n",
            "linesasthehumanbrain?â€Itâ€™sworthnoting,however,thattheremaywellbeformsofintelligencethat\n",
            "donâ€™tsubsumehumanthought,butnonethelessgobeyonditininterestingways. \n",
            "(cid:12)\n",
            "(cid:12) 213\n",
            "(cid:12)\n",
            "Howmuchinformationisinthat125millionbasepairs? Eachbasepaircanbelabelled A\n",
            "byoneoffourpossibilitiesâ€“theâ€œlettersâ€ofthegeneticcode,thebasesadenine,cytosine,\n",
            "guanine,andthymine. Soeachbasepaircanbedescribedusingtwobitsofinformation\n",
            "â€“justenoughinformationtospecifyoneofthefourlabels.\n",
            "So125millionbasepairsis\n",
            "equivalentto250millionbitsofinformation. Thatâ€™sthegeneticdifferencebetweenhumans\n",
            "andchimps! Ofcourse,that250millionbitsaccountsforallthegeneticdifferencesbetweenhumans\n",
            "andchimps. Weâ€™reonlyinterestedinthedifferenceassociatedtothebrain. Unfortunately,\n",
            "no-oneknowswhatfractionofthetotalgeneticdifferenceisneededtoexplainthedifference\n",
            "between the brains. But letâ€™s assume for the sake of argument that about half that 250\n",
            "millionbitsaccountsforthebraindifferences. Thatâ€™satotalof125millionbits.\n",
            "125millionbitsisanimpressivelylargenumber. Letâ€™sgetasenseforhowlargeitis\n",
            "bytranslatingitintomorehumanterms. Inparticular,howmuchwouldbeanequivalent\n",
            "amountofEnglishtext? ItturnsoutthattheinformationcontentofEnglishtextisabout\n",
            "1 bit per letter. That sounds low â€“ after all, the alphabet has 26 letters â€“ but there is a\n",
            "tremendousamountofredundancyinEnglishtext. Ofcourse,youmightarguethatour\n",
            "genomesareredundant,too,sotwobitsperbasepairisanoverestimate. Butweâ€™llignore\n",
            "that,sinceatworstitmeansthatweâ€™reoverestimatingourbrainâ€™sgeneticcomplexity. With\n",
            "theseassumptions,weseethatthegeneticdifferencebetweenourbrainsandchimpbrains\n",
            "isequivalenttoabout125millionletters,orabout25millionEnglishwords. Thatâ€™sabout\n",
            "30timesasmuchastheKingJamesBible. Thatâ€™salotofinformation. Butitâ€™snotincomprehensiblylarge.\n",
            "Itâ€™sonahumanscale. Maybenosinglehumancouldeverunderstandallthatâ€™swritteninthatcode,butagroup\n",
            "ofpeoplecouldperhapsunderstanditcollectively,throughappropriatespecialization. And\n",
            "althoughitâ€™salotofinformation,itâ€™sminusculewhencomparedtotheinformationrequired\n",
            "todescribethe100billionneurons,100billionglialcells,and100trillionconnectionsin\n",
            "ourbrains. Evenifweuseasimple,coarsedescriptionâ€“say,10floatingpointnumbersto\n",
            "characterizeeachconnectionâ€“thatwouldrequireabout70quadrillionbits. Thatmeansthe\n",
            "geneticdescriptionisafactorofabouthalfabillionlesscomplexthanthefullconnectome\n",
            "forthehumanbrain. Whatwelearnfromthisisthatourgenomecannotpossiblycontainadetaileddescription\n",
            "ofallourneuralconnections. Rather,itmustspecifyjustthebroadarchitectureandbasic\n",
            "principlesunderlyingthebrain. Butthatarchitectureandthoseprinciplesseemtobeenough\n",
            "toguaranteethatwehumanswillgrowuptobeintelligent. Ofcourse,therearecaveats\n",
            "â€“growingchildrenneedahealthy,stimulatingenvironmentandgoodnutritiontoachieve\n",
            "theirintellectualpotential. Butprovidedwegrowupinareasonableenvironment,ahealthy\n",
            "human will have remarkable intelligence. In some sense, the information in our genes\n",
            "containstheessenceofhowwethink. Andfurthermore,theprinciplescontainedinthat\n",
            "geneticinformationseemlikelytobewithinourabilitytocollectivelygrasp. Allthenumbersaboveareveryroughestimates. Itâ€™spossiblethat125millionbitsis\n",
            "atremendousoverestimate,thatthereissomemuchmorecompactsetofcoreprinciples\n",
            "underlyinghumanthought.Maybemostofthat125millionbitsisjustfine-tuningofrelatively\n",
            "minordetails. Ormaybewewereoverlyconservativeinhowwecomputedthenumbers.\n",
            "Obviously,thatâ€™dbegreatifitweretrue! Forourcurrentpurposes,thekeypointisthis: the\n",
            "architectureofthebrainiscomplicated,butitâ€™snotnearlyascomplicatedasyoumightthink\n",
            "basedonthenumberofconnectionsinthebrain. Theviewofthebrainfrommolecular\n",
            "biologysuggestswehumansoughttoonedaybeabletounderstandthebasicprinciples\n",
            "\n",
            "(cid:12)\n",
            "214 (cid:12) Isthereasimplealgorithmforintelligence?\n",
            "(cid:12)\n",
            "A behindthebrainâ€™sarchitecture. InthelastfewparagraphsIâ€™veignoredthefactthat125millionbitsmerelyquantifies\n",
            "the genetic difference between human and chimp brains. Not all our brain function is\n",
            "duetothose125millionbits. Chimpsareremarkablethinkersintheirownright. Maybe\n",
            "the key to intelligence lies mostly in the mental abilities (and genetic information) that\n",
            "chimpsandhumanshaveincommon. Ifthisiscorrect,thenhumanbrainsmightbejusta\n",
            "minorupgradetochimpanzeebrains,atleastintermsofthecomplexityoftheunderlying\n",
            "principles.\n",
            "Despitetheconventionalhumanchauvinismaboutouruniquecapabilities,this\n",
            "isnâ€™tinconceivable: thechimpanzeeandhumangeneticlinesdivergedjust5millionyears\n",
            "ago, a blink in evolutionary timescales. However, in the absence of a more compelling\n",
            "argument,Iâ€™msympathetictotheconventionalhumanchauvinism: myguessisthatthe\n",
            "mostinterestingprinciplesunderlyinghumanthoughtlieinthat125millionbits,notinthe\n",
            "partofthegenomewesharewithchimpanzees. Adoptingtheviewofthebrainfrommolecularbiologygaveusareductionofroughly\n",
            "nineordersofmagnitudeinthecomplexityofourdescription. Whileencouraging,itdoesnâ€™t\n",
            "telluswhetherornotatrulysimplealgorithmforintelligenceispossible.\n",
            "Canwegetany\n",
            "furtherreductionsincomplexity? And, moretothepoint, canwesettlethequestionof\n",
            "whetherasimplealgorithmforintelligenceispossible? Unfortunately,thereisnâ€™tyetanyevidencestrongenoughtodecisivelysettlethisques-\n",
            "tion. Letmedescribesomeoftheavailableevidence, withthecaveatthatthisisavery\n",
            "briefandincompleteoverview,meanttoconveytheflavourofsomerecentwork,notto\n",
            "comprehensivelysurveywhatisknown. Amongtheevidencesuggestingthattheremaybeasimplealgorithmforintelligence\n",
            "isanexperimentreportedinApril2000inthejournalNature. Ateamofscientistsledby\n",
            "MrigankaSurâ€œrewiredâ€thebrainsofnewbornferrets. Usually,thesignalfromaferretâ€™s\n",
            "eyesistransmittedtoapartofthebrainknownasthevisualcortex. Butfortheseferrets\n",
            "thescientiststookthesignalfromtheeyesandrerouteditsoitinsteadwenttotheauditory\n",
            "cortex,i.e,thebrainregionthatâ€™susuallyusedforhearing. Tounderstandwhathappenedwhentheydidthis,weneedtoknowabitaboutthevisual\n",
            "cortex. Thevisualcortexcontainsmanyorientationcolumns. Thesearelittleslabsofneurons,\n",
            "eachofwhichrespondstovisualstimulifromsomeparticulardirection. Youcanthinkofthe\n",
            "orientationcolumnsastinydirectionalsensors: whensomeoneshinesabrightlightfrom\n",
            "someparticulardirection,acorrespondingorientationcolumnisactivated. Ifthelightis\n",
            "moved,adifferentorientationcolumnisactivated. Oneofthemostimportanthigh-level\n",
            "structures in the visual cortex is the orientation map, which charts how the orientation\n",
            "columnsarelaidout. Whatthescientistsfoundisthatwhenthevisualsignalfromtheferretsâ€™eyeswasrerouted\n",
            "totheauditorycortex,theauditorycortexchanged. Orientationcolumnsandanorientation\n",
            "mapbegantoemergeintheauditorycortex. Itwasmoredisorderlythantheorientationmap\n",
            "usuallyfoundinthevisualcortex,butunmistakablysimilar. Furthermore,thescientistsdid\n",
            "somesimpletestsofhowtheferretsrespondedtovisualstimuli,trainingthemtorespond\n",
            "differently when lights flashed from different directions. These tests suggested that the\n",
            "ferretscouldstilllearntoâ€œseeâ€,atleastinarudimentaryfashion,usingtheauditorycortex. Thisisanastonishingresult.\n",
            "Itsuggeststhattherearecommonprinciplesunderlying\n",
            "howdifferentpartsofthebrainlearntorespondtosensorydata. Thatcommonalitypro-\n",
            "videsatleastsomesupportfortheideathatthereisasetofsimpleprinciplesunderlying\n",
            "intelligence. However,weshouldnâ€™tkidourselvesabouthowgoodtheferretsâ€™visionwasin\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 215\n",
            "(cid:12)\n",
            "theseexperiments. Thebehaviouralteststestedonlyverygrossaspectsofvision. And,of A\n",
            "course,wecanâ€™tasktheferretsiftheyâ€™veâ€œlearnedtoseeâ€. Sotheexperimentsdonâ€™tprove\n",
            "thattherewiredauditorycortexwasgivingtheferretsahigh-fidelityvisualexperience. And\n",
            "sotheyprovideonlylimitedevidenceinfavouroftheideathatcommonprinciplesunderlie\n",
            "howdifferentpartsofthebrainlearn. Whatevidenceisthereagainsttheideaofasimplealgorithmforintelligence? Some\n",
            "evidencecomesfromthefieldsofevolutionarypsychologyandneuroanatomy. Sincethe\n",
            "1960sevolutionarypsychologistshavediscoveredawiderangeofhumanuniversals,complex\n",
            "behaviourscommontoallhumans,acrossculturesandupbringing. Thesehumanuniversals\n",
            "include the incest taboo between mother and son, the use of music and dance, as well\n",
            "asmuchcomplexlinguisticstructure,suchastheuseofswearwords(i.e.,taboowords),\n",
            "pronouns,andevenstructuresasbasicastheverb. Complementingtheseresults,agreat\n",
            "dealofevidencefromneuroanatomyshowsthatmanyhumanbehavioursarecontrolled\n",
            "byparticularlocalizedareasofthebrain,andthoseareasseemtobesimilarinallpeople.\n",
            "Takentogether,thesefindingssuggestthatmanyveryspecializedbehavioursarehardwired\n",
            "intoparticularpartsofourbrains. Somepeopleconcludefromtheseresultsthatseparateexplanationsmustberequiredfor\n",
            "thesemanybrainfunctions,andthatasaconsequencethereisanirreduciblecomplexityto\n",
            "thebrainâ€™sfunction,acomplexitythatmakesasimpleexplanationforthebrainâ€™soperation\n",
            "(and, perhaps, a simple algorithm for intelligence) impossible. For example, one well-\n",
            "knownartificialintelligenceresearcherwiththispointofviewisMarvinMinsky. Inthe\n",
            "1970sand1980sMinskydevelopedhisâ€œSocietyofMindâ€theory,basedontheideathat\n",
            "humanintelligenceistheresultofalargesocietyofindividuallysimple(butverydifferent)\n",
            "computationalprocesseswhichMinskycallsagents. Inhisbookdescribingthetheory,Minsky\n",
            "sumsupwhatheseesasthepowerofthispointofview:\n",
            "Whatmagicaltrickmakesusintelligent? Thetrickisthatthereisnotrick.\n",
            "Thepower\n",
            "ofintelligencestemsfromourvastdiversity,notfromanysingle,perfectprinciple. Ina\n",
            "response 2 to reviews of his book, Minsky elaborated on the motivation for the Society\n",
            "of Mind, giving an argument similar to that stated above, based on neuroanatomy and\n",
            "evolutionarypsychology:\n",
            "Wenowknowthatthebrainitselfiscomposedofhundredsofdifferent\n",
            "regionsandnuclei,eachwithsignificantlydifferentarchitecturalelements\n",
            "andarrangements,andthatmanyofthemareinvolvedwithdemonstrably\n",
            "differentaspectsofourmentalactivities. Thismodernmassofknowledge\n",
            "showsthatmanyphenomenatraditionallydescribedbycommonsense\n",
            "terms like â€œintelligenceâ€ or â€œunderstandingâ€ actually involve complex\n",
            "assembliesofmachinery. Minsky is, of course, not the only person to hold a point of view along these lines; Iâ€™m\n",
            "merelygivinghimasanexampleofasupporterofthislineofargument. Ifindtheargument\n",
            "interesting, but donâ€™t believe the evidence is compelling. While itâ€™s true that the brain\n",
            "iscomposedofalargenumberofdifferentregions, withdifferentfunctions, itdoesnot\n",
            "thereforefollowthatasimpleexplanationforthebrainâ€™sfunctionisimpossible. Perhaps\n",
            "those architectural differences arise out of common underlying principles, much as the\n",
            "motionofcomets,theplanets,thesunandthestarsallarisefromasinglegravitationalforce. NeitherMinskynoranyoneelsehasarguedconvincinglyagainstsuchunderlyingprinciples.\n",
            "2InContemplatingMinds:AForumforArtificialIntelligence,editedbyWilliamJ.Clancey,Stephen\n",
            "W.Smoliar,andMarkStefik(MITPress,1994). \n",
            "(cid:12)\n",
            "216 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A Myownprejudiceisinfavouroftherebeingasimplealgorithmforintelligence. And\n",
            "themainreasonIliketheidea,aboveandbeyondthe(inconclusive)argumentsabove,is\n",
            "thatitâ€™sanoptimisticidea. Whenitcomestoresearch,anunjustifiedoptimismisoftenmore\n",
            "productivethanaseeminglybetterjustifiedpessimism,foranoptimisthasthecourageto\n",
            "setoutandtrynewthings. Thatâ€™sthepathtodiscovery,evenifwhatisdiscoveredisperhaps\n",
            "notwhatwasoriginallyhoped. Apessimistmaybemoreâ€œcorrectâ€insomenarrowsense,\n",
            "butwilldiscoverlessthantheoptimist. Thispointofviewisinstarkcontrasttothewayweusuallyjudgeideas: byattempting\n",
            "tofigureoutwhethertheyarerightorwrong. Thatâ€™sasensiblestrategyfordealingwith\n",
            "theroutineminutiaeofday-to-dayresearch. Butitcanbethewrongwayofjudgingabig,\n",
            "boldidea,thesortofideathatdefinesanentireresearchprogram. Sometimes,wehaveonly\n",
            "weakevidenceaboutwhethersuchanideaiscorrectornot. Wecanmeeklyrefusetofollow\n",
            "theidea,insteadspendingallourtimesquintingattheavailableevidence,tryingtodiscern\n",
            "whatâ€™strue. Orwecanacceptthatno-oneyetknows,andinsteadworkhardondeveloping\n",
            "thebig,boldidea,intheunderstandingthatwhilewehavenoguaranteeofsuccess,itis\n",
            "onlythusthatourunderstandingadvances. Withallthatsaid,initsmostoptimisticform,Idonâ€™tbelieveweâ€™lleverfindasimple\n",
            "algorithmforintelligence. Tobemoreconcrete,Idonâ€™tbelieveweâ€™lleverfindareallyshort\n",
            "Python(orCorLisp,orwhatever)programâ€“letâ€™ssay,anywhereuptoathousandlines\n",
            "ofcodeâ€“whichimplementsartificialintelligence. NordoIthinkweâ€™lleverfindareally\n",
            "easily-describedneuralnetworkthatcanimplementartificialintelligence.\n",
            "ButIdobelieve\n",
            "itâ€™sworthactingasthoughwecouldfindsuchaprogramornetwork. Thatâ€™sthepathto\n",
            "insight,andbypursuingthatpathwemayonedayunderstandenoughtowritealonger\n",
            "programorbuildamoresophisticatednetworkwhichdoesexhibitintelligence. Andsoitâ€™s\n",
            "worthactingasthoughanextremelysimplealgorithmforintelligenceexists. Inthe1980s,theeminentmathematicianandcomputerscientistJackSchwartzwas\n",
            "invited to a debate between artificial intelligence proponents and artificial intelligence\n",
            "skeptics. Thedebatebecameunruly,withtheproponentsmakingover-the-topclaimsabout\n",
            "theamazingthingsjustroundthecorner,andtheskepticsdoublingdownontheirpessimism,\n",
            "claimingartificialintelligencewasoutrightimpossible.\n",
            "Schwartzwasanoutsidertothe\n",
            "debate,andremainedsilentasthediscussionheatedup. Duringalull,hewasaskedto\n",
            "speakupandstatehisthoughtsontheissuesunderdiscussion. Hesaid: â€œWell,someof\n",
            "thesedevelopmentsmaylieonehundredNobelprizesawayâ€(ref,page22). Itseemstome\n",
            "aperfectresponse.\n",
            "Thekeytoartificialintelligenceissimple,powerfulideas,andwecan\n",
            "andshouldsearchoptimisticallyforthoseideas. Butweâ€™regoingtoneedmanysuchideas,\n",
            "andweâ€™vestillgotalongwaytogo! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chatbot interface\n",
        "print(\"Chat with the PDF content (type 'exit', 'quit', or 'bye' to end):\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = qa.invoke({\"query\": query})\n",
        "    print(\"\\nAssistant:\", response['result'])"
      ],
      "metadata": {
        "id": "oc9D164m8zcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}