{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAwdPxXLYzRx"
      },
      "outputs": [],
      "source": [
        "# https://openrouter.ai/settings/keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~ensorflow*\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/tensorflow*\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/keras*\n",
        "!pip install --upgrade --force-reinstall protobuf==3.20.*\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "W9x7bwJw2xjf",
        "outputId": "a68f4027-8201-4c25-d738-e94c4f85a0d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.20.*\n",
            "  Using cached protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Using cached protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~qdm (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires tqdm, which is not installed.\n",
            "kaggle 1.7.4.5 requires tqdm, which is not installed.\n",
            "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow-probability>=0.13.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tqdm>=4.64.1, which is not installed.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "41a6a967f1b848c69c21914a7a41d7b3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~qdm*\n"
      ],
      "metadata": {
        "id": "Gl8F0P4M3Her"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35cylDd3Mi9",
        "outputId": "2846cb52-cd3e-46c3-cb01-68bdccfee569"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "3WlsNSphZdsT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7be6c9c4-7463-4802-f11d-7efb9bad885d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.9)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-experimental"
      ],
      "metadata": {
        "id": "yfkUbrGAbC50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8adc8901-d438-4324-e84c-460a9d9dbd85"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.3.4)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.28 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.3.74)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.11.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.9)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<0.4.0,>=0.3.28->langchain-experimental) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "id": "yH1N0y8RbSmc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "fb853eac-7c5f-4a4b-ed85-1882a5120b06"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "4v0uipM7b5lq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8bd7be09-0cf8-45aa-ac11-dfa0aeaad1bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Jaish19/GenAI---RAG-using-LangChain.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIe8qK4kFlmm",
        "outputId": "b50dd790-8cbd-4561-b126-16520b632749"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'GenAI---RAG-using-LangChain' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "from google.colab import userdata\n",
        "\n",
        "# Step 1: Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\") # Replace with your actual OpenRouter API key\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"\n",
        "\n",
        "# Step 2: Initialize the LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"mistralai/mistral-7b-instruct\",  # or try \"meta-llama/llama-3-8b-instruct\", etc.\n",
        "    temperature=0.7,\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Step 3: Interact with the model (normal chat mode)\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = llm([HumanMessage(content=query)])\n",
        "    print(\"\\nAssistant:\", response.content)\n"
      ],
      "metadata": {
        "id": "ZHYUOORrfR4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a28d2364-b305-42a3-c712-5dc4465f4107"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3908245900.py:11: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
            "  llm = ChatOpenAI(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You: hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3908245900.py:27: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm([HumanMessage(content=query)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Assistant:  Hello! How can I help you today? If you have any questions or need assistance with something, feel free to ask. I'm here to help!\n",
            "\n",
            "You: bye\n",
            "Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA # RAG - To retrieve the contents from the pdfs or any docs\n",
        "from langchain.chains.llm import LLMChain  # It's LangChain function to wrap the tools/agents to create a seemless workflow\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.prompts import PromptTemplate  # To provide the equipped prompt\n",
        "from langchain_community.document_loaders import PDFPlumberLoader # To read the pdf\n",
        "from langchain_experimental.text_splitter import SemanticChunker # Text chunks\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        " # Embed the texts or contents\n",
        "from langchain_community.vectorstores import FAISS  # Vector DB\n",
        "from google.colab import userdata\n",
        "from langchain_experimental.text_splitter import SemanticChunker\n"
      ],
      "metadata": {
        "id": "LwAeB3X03ozM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenRouter API key and endpoint\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENROUTER_OPENAI\")\n",
        "os.environ[\"OPENAI_API_BASE\"] = \"https://openrouter.ai/api/v1\"  # Endpoint of the openRouter"
      ],
      "metadata": {
        "id": "wldmxxtr4DJK"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load and process PDF\n",
        "loader = PDFPlumberLoader(\"/content/GenAI---RAG-using-LangChain/neural_network.pdf\")\n",
        "docs = loader.load()\n",
        "print(\"Pages loaded:\", len(docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqIJxIGy4FHQ",
        "outputId": "2f493c0c-6df7-415a-b91c-8f1f7d0d3cac"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pages loaded: 224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-WFpT_B5V55",
        "outputId": "ea2b77ba-0923-4c6d-952c-8c09b2efecff"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.55.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y transformers protobuf\n",
        "!pip install transformers sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MJTbbGq6Lib",
        "outputId": "bd540b98-429b-4fc1-e0ec-10e38b4fe1ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.55.2\n",
            "Uninstalling transformers-4.55.2:\n",
            "  Successfully uninstalled transformers-4.55.2\n",
            "\u001b[33mWARNING: Skipping protobuf as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers\n",
            "  Using cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (5.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Using cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.55.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# PROCESS 1: CHUNK process\n",
        "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Semantic splitter with custom\n",
        "# With SemanticChunker (in LangChain), you don't define chunk_size directly like in other splitters\n",
        "# (CharacterTextSplitter, RecursiveCharacterTextSplitter, etc.).\n",
        "# Instead, semantic chunking decides where to split based on semantic similarity between segments, not on fixed sizes.\n",
        "splitter = SemanticChunker(\n",
        "    embeddings=HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    ),\n",
        "    breakpoint_threshold_type=\"percentile\", # or standard_deviation\n",
        "    breakpoint_threshold_amount=90, # Value for the threshold (e.g. 95 = 95th percentile distance between embeddings)\n",
        "    buffer_size=1 # (Optional) Adds context from neighboring chunks\n",
        ")\n",
        "\n",
        "# Split document\n",
        "chunks = splitter.split_documents(docs)\n",
        "for chunk in chunks:\n",
        "    print(chunk.page_content)\n",
        "\n",
        "\n",
        "# PROCESS 2: Vector DB\n",
        "embedder = HuggingFaceEmbeddings()\n",
        "vector = FAISS.from_documents(chunks, embedder)\n",
        "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "\n",
        "'''\n",
        "# Try with some queries for similarity search\n",
        "results = retriever.get_relevant_documents(\"What is the remedy for cold and flu?\")\n",
        "\n",
        "for doc in results:\n",
        "    print(doc.page_content)\n",
        "'''\n",
        "\n",
        "# PROCESS 3: LLM MODEL CHOOSING\n",
        "# Use ChatOpenAI with OpenRouter backend\n",
        "llm = ChatOpenAI(\n",
        "    model=\"meta-llama/llama-3-8b-instruct\",   #\"mistralai/mistral-7b-instruct\"\n",
        "    temperature=1.3, # It's a less temp -> so it'll give you some relative information about the content.\n",
        "    openai_api_base=os.environ[\"OPENAI_API_BASE\"],\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        "    request_timeout=60,\n",
        ")\n",
        "\n",
        "# PROCESS 4: PROMPT CREATION\n",
        "prompt = \"\"\"\n",
        "You are a helpful assistant.\n",
        "Use the following pieces of context to answer the question at the end.\n",
        "Answer only using the context and be concise (3–4 sentences).\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt)\n",
        "\n",
        "\n",
        "# PROCESS 5: LANGCHAIN CREATION - LLM & PROMPT\n",
        "llm_chain = LLMChain(llm=llm, prompt=QA_CHAIN_PROMPT, verbose=True)\n",
        "\n",
        "document_prompt = PromptTemplate(\n",
        "    input_variables=[\"page_content\", \"source\"],\n",
        "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
        ")\n",
        "\n",
        "# DOCUMENT CHAIN PROCESS\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=llm_chain,\n",
        "    document_variable_name=\"context\",\n",
        "    document_prompt=document_prompt,\n",
        "    callbacks=None,\n",
        ")\n",
        "\n",
        "# PROCESS 6: WRAPPING ALL TOGETHER ALONG WITH RETRIEVER\n",
        "qa = RetrievalQA(\n",
        "    combine_documents_chain=combine_documents_chain,\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# Example query\n"
      ],
      "metadata": {
        "id": "6DWOwM-JY03M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b5ba9063-2a9f-4a73-c278-d021a45c786b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural Networks and Deep Learning\n",
            "MichaelNielsen\n",
            "Theoriginalonlinebookcanbefoundat\n",
            "http://neuralnetworksanddeeplearning.com\n",
            "\n",
            "\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) i\n",
            "(cid:12)\n",
            "Contents\n",
            "Whatthisbookisabout iii\n",
            "Ontheexercisesandproblems v\n",
            "1 Usingneuralnetstorecognizehandwrittendigits 1\n",
            "1.1 Perceptrons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 2\n",
            "1.2 Sigmoidneurons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 7\n",
            "1.3 Thearchitectureofneuralnetworks.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 10\n",
            "1.4 Asimplenetworktoclassifyhandwrittendigits. .\n",
            ". . . . . . . . . . . . . . . .\n",
            ". 12\n",
            "1.5 Learningwithgradientdescent. .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 15\n",
            "1.6 Implementingournetworktoclassifydigits . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 24\n",
            "1.7 Towarddeeplearning .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 35\n",
            "2 Howthebackpropagationalgorithmworks 39\n",
            "2.1 Warmup:afastmatrix-basedapproachtocomputingtheoutputfromaneural\n",
            "network . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 40\n",
            "2.2 Thetwoassumptionsweneedaboutthecostfunction .\n",
            ".\n",
            ". . . . . . . . . . .\n",
            ". 42\n",
            "2.3 TheHadamardproduct,s t .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 43\n",
            "2.4 Thefourfundamentalequ(cid:12)ationsbehindbackpropagation .\n",
            ".\n",
            ". . . . . . . . .\n",
            ". 43\n",
            "2.5 Proofofthefourfundamentalequations(optional) .\n",
            ".\n",
            ". . . . . . . . . . . . .\n",
            ". 48\n",
            "2.6 Thebackpropagationalgorithm . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 49\n",
            "2.7 Thecodeforbackpropagation . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 50\n",
            "2.8 Inwhatsenseisbackpropagationafastalgorithm? .\n",
            ". . . . . . . . . . . . . .\n",
            ". 52\n",
            "2.9 Backpropagation: thebigpicture .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 53\n",
            "3 Improvingthewayneuralnetworkslearn 59\n",
            "3.1 Thecross-entropycostfunction . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 60\n",
            "3.1.1 Introducingthecross-entropycostfunction . .\n",
            ". . . . . . . . . . . . .\n",
            ". 62\n",
            "3.1.2 Usingthecross-entropytoclassifyMNISTdigits. .\n",
            ". . . . . . . . . . .\n",
            ".\n",
            "67\n",
            "3.1.3 Whatdoesthecross-entropymean?\n",
            "Wheredoesitcomefrom?\n",
            ".\n",
            ". .\n",
            ". 68\n",
            "3.1.4 Softmax . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 70\n",
            "3.2 Overfittingandregularization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 73\n",
            "3.2.1 Regularization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 78\n",
            "3.2.2 Whydoesregularizationhelpreduceoverfitting? .\n",
            ". . . . . . . . . . .\n",
            ". 83\n",
            "3.2.3 Othertechniquesforregularization . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 87\n",
            "3.3 Weightinitialization .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 94\n",
            "3.4 Handwritingrecognitionrevisited: thecode. .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 98\n",
            "3.5 Howtochooseaneuralnetwork’shyper-parameters?. .\n",
            ". . . . . . . . . . . .\n",
            ". 107\n",
            "3.6 Othertechniques .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "118\n",
            "\n",
            "(cid:12)\n",
            "ii (cid:12) Contents\n",
            "(cid:12)\n",
            "3.6.1 Variationsonstochasticgradientdescent . .\n",
            ". . . . . . . . . . . . . . .\n",
            ". 118\n",
            "4 Avisualproofthatneuralnetscancomputeanyfunction 127\n",
            "4.1 Twocaveats .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 129\n",
            "4.2 Universalitywithoneinputandoneoutput . .\n",
            ". . . . . . . . . . . . . . . . . .\n",
            ". 130\n",
            "4.3 Manyinputvariables .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 139\n",
            "4.4 Extensionbeyondsigmoidneurons . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 146\n",
            "4.5 Fixingupthestepfunctions.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "148\n",
            "5 Whyaredeepneuralnetworkshardtotrain?\n",
            "151\n",
            "5.1 Thevanishinggradientproblem .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "154\n",
            "5.2 What’scausingthevanishinggradientproblem? Unstablegradientsindeep\n",
            "neuralnets .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 159\n",
            "5.3 Unstablegradientsinmorecomplexnetworks . .\n",
            ". . . . . . . . . . . . . . . .\n",
            ". 163\n",
            "5.4 Otherobstaclestodeeplearning .\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 164\n",
            "6 Deeplearning 167\n",
            "6.1 Introducingconvolutionalnetworks. .\n",
            ". . . . . . . . . . . . . . . . . . . . . . .\n",
            ". 169\n",
            "6.2 Convolutionalneuralnetworksinpractice . .\n",
            ". . . . . . . . . . . . . . . . . . .\n",
            ". 176\n",
            "6.3 Thecodeforourconvolutionalnetworks. .\n",
            ". . . . . . . . . . . . . . . . . . . .\n",
            ". 185\n",
            "6.4 Recentprogressinimagerecognition . .\n",
            ". . . . . . . . . . . . . . . . . . . . . .\n",
            ". 196\n",
            "6.5 Otherapproachestodeepneuralnets.\n",
            ".\n",
            ". . . . . . . . . . . . . . . . . . . . . .\n",
            ". 202\n",
            "6.6 Onthefutureofneuralnetworks . .\n",
            ". . . . . . . . . . . . . . . . . . . . . . . .\n",
            ".\n",
            "205\n",
            "A Isthereasimplealgorithmforintelligence?\n",
            "211\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) iii\n",
            "(cid:12)\n",
            "What this book is about\n",
            "Neuralnetworksareoneofthemostbeautifulprogrammingparadigmseverinvented. In\n",
            "theconventionalapproachtoprogramming,wetellthecomputerwhattodo,breakingbig\n",
            "problemsupintomanysmall,preciselydefinedtasksthatthecomputercaneasilyperform.\n",
            "Bycontrast,inaneuralnetworkwedon’ttellthecomputerhowtosolveourproblem. Instead,\n",
            "itlearnsfromobservationaldata,figuringoutitsownsolutiontotheproblemathand. Automaticallylearningfromdatasoundspromising. However, until2006wedidn’t\n",
            "know how to train neural networks to surpass more traditional approaches, except for\n",
            "afewspecializedproblems. Whatchangedin2006wasthediscoveryoftechniquesfor\n",
            "learning in so-called deep neural networks. These techniques are now known as deep\n",
            "learning. They’vebeendevelopedfurther,andtodaydeepneuralnetworksanddeeplearning\n",
            "achieveoutstandingperformanceonmanyimportantproblemsincomputervision,speech\n",
            "recognition,andnaturallanguageprocessing. They’rebeingdeployedonalargescaleby\n",
            "companiessuchasGoogle,Microsoft,andFacebook. Thepurposeofthisbookistohelpyoumasterthecoreconceptsofneuralnetworks,\n",
            "includingmoderntechniquesfordeeplearning. Afterworkingthroughthebookyouwill\n",
            "havewrittencodethatusesneuralnetworksanddeeplearningtosolvecomplexpattern\n",
            "recognitionproblems. Andyouwillhaveafoundationtouseneuralnetworksanddeep\n",
            "learningtoattackproblemsofyourowndevising. A principle-oriented approach\n",
            "Oneconvictionunderlyingthebookisthatit’sbettertoobtainasolidunderstandingofthe\n",
            "coreprinciplesofneuralnetworksanddeeplearning,ratherthanahazyunderstanding\n",
            "ofalonglaundrylistofideas.\n",
            "Ifyou’veunderstoodthecoreideaswell,youcanrapidly\n",
            "understandothernewmaterial. Inprogramminglanguageterms,thinkofitasmastering\n",
            "thecoresyntax,librariesanddatastructuresofanewlanguage. Youmaystillonly“know”a\n",
            "tinyfractionofthetotallanguage–manylanguageshaveenormousstandardlibraries–but\n",
            "newlibrariesanddatastructurescanbeunderstoodquicklyandeasily. Thismeansthebookisemphaticallynotatutorialinhowtousesomeparticularneural\n",
            "networklibrary.\n",
            "Ifyoumostlywanttolearnyourwayaroundalibrary,don’treadthisbook! Findthelibraryyouwishtolearn,andworkthroughthetutorialsanddocumentation. But\n",
            "bewarned. Whilethishasanimmediateproblem-solvingpayoff,ifyouwanttounderstand\n",
            "what’sreallygoingoninneuralnetworks,ifyouwantinsightsthatwillstillberelevant\n",
            "yearsfromnow,thenit’snotenoughjusttolearnsomehotlibrary. Youneedtounderstand\n",
            "thedurable,lastinginsightsunderlyinghowneuralnetworkswork. Technologiescomeand\n",
            "technologiesgo,butinsightisforever. \n",
            "(cid:12)\n",
            "iv (cid:12) Whatthisbookisabout\n",
            "(cid:12)\n",
            "A hands-on approach\n",
            "We’lllearnthecoreprinciplesbehindneuralnetworksanddeeplearningbyattackinga\n",
            "concreteproblem: theproblemofteachingacomputertorecognizehandwrittendigits. This\n",
            "problemisextremelydifficulttosolveusingtheconventionalapproachtoprogramming. Andyet,aswe’llsee,itcanbesolvedprettywellusingasimpleneuralnetwork,withjusta\n",
            "fewtensoflinesofcode,andnospeciallibraries. What’smore,we’llimprovetheprogram\n",
            "throughmanyiterations,graduallyincorporatingmoreandmoreofthecoreideasabout\n",
            "neuralnetworksanddeeplearning. Thishands-onapproachmeansthatyou’llneedsomeprogrammingexperiencetoread\n",
            "thebook.\n",
            "Butyoudon’tneedtobeaprofessionalprogrammer. I’vewrittenthecodeinPython\n",
            "(version2.7),which,evenifyoudon’tprograminPython,shouldbeeasytounderstandwith\n",
            "justalittleeffort. Throughthecourseofthebookwewilldevelopalittleneuralnetwork\n",
            "library,whichyoucanusetoexperimentandtobuildunderstanding.\n",
            "Allthecodeisavailable\n",
            "fordownloadhere. Onceyou’vefinishedthebook,orasyoureadit,youcaneasilypickup\n",
            "oneofthemorefeature-completeneuralnetworklibrariesintendedforuseinproduction. Onarelatednote,themathematicalrequirementstoreadthebookaremodest. There\n",
            "issomemathematicsinmostchapters,butit’susuallyjustelementaryalgebraandplotsof\n",
            "functions,whichIexpectmostreaderswillbeokaywith. Ioccasionallyusemoreadvanced\n",
            "mathematics,buthavestructuredthematerialsoyoucanfollowevenifsomemathematical\n",
            "detailseludeyou. TheonechapterwhichusesheaviermathematicsextensivelyisChapter2,\n",
            "whichrequiresalittlemultivariablecalculusandlinearalgebra. Ifthosearen’tfamiliar,I\n",
            "beginChapter2withadiscussionofhowtonavigatethemathematics. Ifyou’refindingit\n",
            "reallyheavygoing,youcansimplyskiptothesummaryofthechapter’smainresults. Inany\n",
            "case,there’snoneedtoworryaboutthisattheoutset. It’srareforabooktoaimtobebothprinciple-orientedandhands-on. ButIbelieve\n",
            "you’lllearnbestifwebuildoutthefundamentalideasofneuralnetworks. We’lldevelop\n",
            "livingcode,notjustabstracttheory,codewhichyoucanexploreandextend. Thiswayyou’ll\n",
            "understandthefundamentals,bothintheoryandpractice,andbewellsettoaddfurtherto\n",
            "yourknowledge. \n",
            "(cid:12)\n",
            "(cid:12) v\n",
            "(cid:12)\n",
            "On the exercises and problems\n",
            "It’snotuncommonfortechnicalbookstoincludeanadmonitionfromtheauthorthatreaders\n",
            "mustdotheexercisesandproblems. IalwaysfeelalittlepeculiarwhenIreadsuchwarnings.\n",
            "WillsomethingbadhappentomeifIdon’tdotheexercisesandproblems? Ofcoursenot. I’llgainsometime,butattheexpenseofdepthofunderstanding. Sometimesthat’sworthit. Sometimesit’snot. Sowhat’sworthdoinginthisbook? Myadviceisthatyoureallyshouldattemptmostof\n",
            "theexercises,andyoushouldaimnottodomostoftheproblems. Youshoulddomostoftheexercisesbecausethey’rebasicchecksthatyou’veunderstood\n",
            "thematerial. Ifyoucan’tsolveanexerciserelativelyeasily,you’veprobablymissedsomething\n",
            "fundamental. Ofcourse,ifyoudogetstuckonanoccasionalexercise,justmoveon–chances\n",
            "areit’sjustasmallmisunderstandingonyourpart,ormaybeI’vewordedsomethingpoorly. Butifmostexercisesareastruggle,thenyouprobablyneedtorereadsomeearliermaterial. Theproblemsareanothermatter. They’remoredifficultthantheexercises,andyou’ll\n",
            "likelystruggletosolvesomeproblems. That’sannoying,but,ofcourse,patienceintheface\n",
            "ofsuchfrustrationistheonlywaytotrulyunderstandandinternalizeasubject. With that said, I don’t recommend working through all the problems.\n",
            "What’s even\n",
            "betteristofindyourownproject. Maybeyouwanttouseneuralnetstoclassifyyourmusic\n",
            "collection. Ortopredictstockprices. Orwhatever.\n",
            "Butfindaprojectyoucareabout. Then\n",
            "youcanignoretheproblemsinthebook,orusethemsimplyasinspirationforworkonyour\n",
            "ownproject. Strugglingwithaprojectyoucareaboutwillteachyoufarmorethanworking\n",
            "throughanynumberofsetproblems. Emotionalcommitmentisakeytoachievingmastery. Ofcourse,youmaynothavesuchaprojectinmind,atleastupfront. That’sfine. Work\n",
            "throughthoseproblemsyoufeelmotivatedtoworkon. Andusethematerialinthebookto\n",
            "helpyousearchforideasforcreativepersonalprojects. \n",
            "(cid:12)\n",
            "vi (cid:12) Ontheexercisesandproblems\n",
            "(cid:12)\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 1\n",
            "(cid:12)\n",
            "1111\n",
            "Using neural nets to recognize\n",
            "handwritten digits\n",
            "1\n",
            "Thehumanvisualsystemisoneofthewondersoftheworld. Considerthefollowingsequence\n",
            "ofhandwrittendigits:\n",
            "Mostpeopleeffortlesslyrecognizethosedigitsas504192. Thateaseisdeceptive. Ineach\n",
            "hemisphereofourbrain,humanshaveaprimaryvisualcortex,alsoknownasV ,containing\n",
            "1\n",
            "140millionneurons,withtensofbillionsofconnectionsbetweenthem. Andyethuman\n",
            "visioninvolvesnotjustV ,butanentireseriesofvisualcortices–V ,V ,V ,andV –doing\n",
            "1 2 3 4 5\n",
            "progressivelymorecompleximageprocessing. Wecarryinourheadsasupercomputer,tuned\n",
            "byevolutionoverhundredsofmillionsofyears,andsuperblyadaptedtounderstandthe\n",
            "visualworld. Recognizinghandwrittendigitsisn’teasy. Rather,wehumansarestupendously,\n",
            "astoundinglygoodatmakingsenseofwhatoureyesshowus. Butnearlyallthatworkis\n",
            "doneunconsciously. Andsowedon’tusuallyappreciatehowtoughaproblemourvisual\n",
            "systemssolve. Thedifficultyofvisualpatternrecognitionbecomesapparentifyouattempttowrite\n",
            "acomputerprogramtorecognizedigitslikethoseabove.\n",
            "Whatseemseasywhenwedoit\n",
            "ourselvessuddenlybecomesextremelydifficult. Simpleintuitionsabouthowwerecognize\n",
            "shapes–“a9hasaloopatthetop,andaverticalstrokeinthebottomright”–turnoutto\n",
            "benotsosimpletoexpressalgorithmically. Whenyoutrytomakesuchrulesprecise,you\n",
            "quicklygetlostinamorassofexceptionsandcaveatsandspecialcases.\n",
            "Itseemshopeless. Neuralnetworksapproachtheprobleminadifferentway. Theideaistotakealarge\n",
            "numberofhandwrittendigits,knownastrainingexamples,\n",
            "\n",
            "(cid:12)\n",
            "2 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "andthendevelopasystemwhichcanlearnfromthosetrainingexamples. Inotherwords,the\n",
            "neuralnetworkusestheexamplestoautomaticallyinferrulesforrecognizinghandwritten\n",
            "digits. Furthermore,byincreasingthenumberoftrainingexamples,thenetworkcanlearn\n",
            "moreabouthandwriting,andsoimproveitsaccuracy. SowhileI’veshownjust100training\n",
            "digitsabove,perhapswecouldbuildabetterhandwritingrecognizerbyusingthousandsor\n",
            "evenmillionsorbillionsoftrainingexamples. Inthischapterwe’llwriteacomputerprogramimplementinganeuralnetworkthat\n",
            "learnstorecognizehandwrittendigits. Theprogramisjust74lineslong,andusesnospecial\n",
            "neuralnetworklibraries. Butthisshortprogramcanrecognizedigitswithanaccuracyover\n",
            "96percent,withouthumanintervention.\n",
            "Furthermore,inlaterchapterswe’lldevelopideas\n",
            "whichcanimproveaccuracytoover99percent.\n",
            "Infact,thebestcommercialneuralnetworks\n",
            "arenowsogoodthattheyareusedbybankstoprocesscheques, andbypostofficesto\n",
            "recognizeaddresses. We’refocusingonhandwritingrecognitionbecauseit’sanexcellentprototypeproblemfor\n",
            "learningaboutneuralnetworksingeneral. Asaprototypeithitsasweetspot:it’schallenging\n",
            "–it’snosmallfeattorecognizehandwrittendigits–butit’snotsodifficultastorequirean\n",
            "extremelycomplicatedsolution,ortremendouscomputationalpower. Furthermore,it’sa\n",
            "greatwaytodevelopmoreadvancedtechniques,suchasdeeplearning. Andsothroughout\n",
            "thebookwe’llreturnrepeatedlytotheproblemofhandwritingrecognition. Laterinthe\n",
            "book,we’lldiscusshowtheseideasmaybeappliedtootherproblemsincomputervision,\n",
            "andalsoinspeech,naturallanguageprocessing,andotherdomains. Ofcourse,ifthepointofthechapterwasonlytowriteacomputerprogramtorecognize\n",
            "handwrittendigits,thenthechapterwouldbemuchshorter! Butalongthewaywe’lldevelop\n",
            "manykeyideasaboutneuralnetworks,includingtwoimportanttypesofartificialneuron\n",
            "(theperceptronandthesigmoidneuron),andthestandardlearningalgorithmforneural\n",
            "networks,knownasstochasticgradientdescent. Throughout,Ifocusonexplainingwhy\n",
            "thingsaredonethewaytheyare,andonbuildingyourneuralnetworksintuition. That\n",
            "requiresalengthierdiscussionthanifIjustpresentedthebasicmechanicsofwhat’sgoingon,\n",
            "butit’sworthitforthedeeperunderstandingyou’llattain. Amongstthepayoffs,bytheend\n",
            "ofthechapterwe’llbeinpositiontounderstandwhatdeeplearningis,andwhyitmatters. 1.1 Perceptrons\n",
            "Whatisaneuralnetwork? Togetstarted,I’llexplainatypeofartificialneuroncalleda\n",
            "perceptron. Perceptrons were developed in the 1950s and 1960s by the scientist Frank\n",
            "\n",
            "(cid:12)\n",
            "1.1. Perceptrons (cid:12) 3\n",
            "(cid:12)\n",
            "Rosenblatt,inspiredbyearlierworkbyWarrenMcCullochandWalterPitts. Today,it’smore 1\n",
            "commontouseothermodelsofartificialneurons–inthisbook,andinmuchmodernwork\n",
            "onneuralnetworks,themainneuronmodelusedisonecalledthesigmoidneuron. We’llget\n",
            "tosigmoidneuronsshortly. Buttounderstandwhysigmoidneuronsaredefinedtheway\n",
            "theyare,it’sworthtakingthetimetofirstunderstandperceptrons. Sohowdoperceptronswork? Aperceptrontakesseveralbinaryinputs, x ,x ,...,and\n",
            "1 2\n",
            "producesasinglebinaryoutput:\n",
            "In the example shown the perceptron has three inputs, x , x , x . In general it could\n",
            "1 2 3\n",
            "havemoreorfewerinputs. Rosenblattproposedasimpleruletocomputetheoutput. He\n",
            "introducedweights,w ,w ,...,realnumbersexpressingtheimportanceoftherespective\n",
            "1 2\n",
            "inputstotheoutput. Theneuron’soutput,0or1,isdeterminedbywhethertheweighted\n",
            "(cid:80)\n",
            "sum w x islessthanorgreaterthansomethresholdvalue. Justliketheweights,the\n",
            "j j j\n",
            "thresholdisarealnumberwhichisaparameteroftheneuron. Toputitinmoreprecise\n",
            "algebraicterms:\n",
            "(cid:168) (cid:80)\n",
            "0 if w x threshold\n",
            "output=\n",
            "1 if\n",
            "(cid:80) j\n",
            "w\n",
            "j\n",
            "x\n",
            "j >≤\n",
            "threshold\n",
            "(1.1)\n",
            "j j j\n",
            "That’sallthereistohowaperceptronworks! That’sthebasicmathematicalmodel.\n",
            "Awayyoucanthinkabouttheperceptronisthat\n",
            "it’sadevicethatmakesdecisionsbyweighingupevidence. Letmegiveanexample. It’s\n",
            "notaveryrealisticexample,butit’seasytounderstand,andwe’llsoongettomorerealistic\n",
            "examples. Supposetheweekendiscomingup,andyou’veheardthatthere’sgoingtobea\n",
            "cheesefestivalinyourcity. Youlikecheese,andaretryingtodecidewhetherornottogoto\n",
            "thefestival.\n",
            "Youmightmakeyourdecisionbyweighingupthreefactors:\n",
            "1. Istheweathergood? 2. Doesyourboyfriendorgirlfriendwanttoaccompanyyou? 3.\n",
            "Isthefestivalnearpublictransit?\n",
            "(Youdon’townacar). Wecanrepresentthesethreefactorsbycorrespondingbinaryvariables x , x and x . For\n",
            "1 2 3\n",
            "instance,we’dhavex 1=1iftheweatherisgood,andx 1=0iftheweatherisbad. Similarly,\n",
            "x 2=1ifyourboyfriendorgirlfriendwantstogo,and x 2=0ifnot. Andsimilarlyagainfor\n",
            "x andpublictransit. 3\n",
            "Now,supposeyouabsolutelyadorecheese,somuchsothatyou’rehappytogotothe\n",
            "festivalevenifyourboyfriendorgirlfriendisuninterestedandthefestivalishardtogetto. Butperhapsyoureallyloathebadweather,andthere’snowayyou’dgotothefestivalif\n",
            "theweatherisbad. Youcanuseperceptronstomodelthiskindofdecision-making. One\n",
            "waytodothisistochooseaweightw\n",
            "1\n",
            "=6fortheweather,andw\n",
            "2\n",
            "=2andw\n",
            "3\n",
            "=2for\n",
            "theotherconditions. Thelargervalueofw indicatesthattheweathermattersalottoyou,\n",
            "1\n",
            "muchmorethanwhetheryourboyfriendorgirlfriendjoinsyou,orthenearnessofpublic\n",
            "transit. Finally,supposeyouchooseathresholdof5fortheperceptron. Withthesechoices,\n",
            "theperceptronimplementsthedesireddecision-makingmodel,outputting1wheneverthe\n",
            "\n",
            "(cid:12)\n",
            "4 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 weatherisgood,and0whenevertheweatherisbad. Itmakesnodifferencetotheoutput\n",
            "whetheryourboyfriendorgirlfriendwantstogo,orwhetherpublictransitisnearby.\n",
            "Byvaryingtheweightsandthethreshold,wecangetdifferentmodelsofdecision-making. Forexample,supposeweinsteadchoseathresholdof3. Thentheperceptronwoulddecide\n",
            "thatyoushouldgotothefestivalwhenevertheweatherwasgoodorwhenboththefestival\n",
            "wasnearpublictransitandyourboyfriendorgirlfriendwaswillingtojoinyou. Inother\n",
            "words,it’dbeadifferentmodelofdecision-making. Droppingthethresholdmeansyou’re\n",
            "morewillingtogotothefestival. Obviously,theperceptronisn’tacompletemodelofhumandecision-making!\n",
            "Butwhat\n",
            "theexampleillustratesishowaperceptroncanweighupdifferentkindsofevidenceinorder\n",
            "tomakedecisions. Anditshouldseemplausiblethatacomplexnetworkofperceptrons\n",
            "couldmakequitesubtledecisions:\n",
            "Inthisnetwork,thefirstcolumnofperceptrons–whatwe’llcallthefirstlayerofperceptrons\n",
            "–ismakingthreeverysimpledecisions,byweighingtheinputevidence. Whataboutthe\n",
            "perceptronsinthesecondlayer? Eachofthoseperceptronsismakingadecisionbyweighing\n",
            "uptheresultsfromthefirstlayerofdecision-making. Inthiswayaperceptroninthesecond\n",
            "layercanmakeadecisionatamorecomplexandmoreabstractlevelthanperceptronsin\n",
            "thefirstlayer. Andevenmorecomplexdecisionscanbemadebytheperceptroninthethird\n",
            "layer. Inthisway,amany-layernetworkofperceptronscanengageinsophisticateddecision\n",
            "making. Incidentally,whenIdefinedperceptronsIsaidthataperceptronhasjustasingleoutput. Inthenetworkabovetheperceptronslookliketheyhavemultipleoutputs. Infact,they’re\n",
            "stillsingleoutput. Themultipleoutputarrowsaremerelyausefulwayofindicatingthatthe\n",
            "outputfromaperceptronisbeingusedastheinputtoseveralotherperceptrons. It’sless\n",
            "unwieldythandrawingasingleoutputlinewhichthensplits.\n",
            "Let’ssimplifythewaywedescribeperceptrons. Thecondition (cid:80) w x >thresholdis\n",
            "j j j\n",
            "cumbersome, and we can make two notational changes to simplify it. The first change\n",
            "(cid:80) (cid:80)\n",
            "istowrite j w j x j asadotproduct, w x = j w j x j ,wherewand x arevectorswhose\n",
            "componentsaretheweightsandinputs·,respectively. Thesecondchangeistomovethe\n",
            "threshold to the other side of the inequality, and to replace it by what’s known as the\n",
            "perceptron’sbias,b threshold. Usingthebiasinsteadofthethreshold,theperceptron\n",
            "rulecanberewritten≡:−\n",
            "(cid:168)\n",
            "0 if w x+b 0\n",
            "output=\n",
            "1 if w\n",
            "·\n",
            "x+b\n",
            "≤>0 (1.2)\n",
            "·\n",
            "Youcanthinkofthebiasasameasureofhoweasyitistogettheperceptrontooutput\n",
            "a1. Ortoputitinmorebiologicalterms, thebiasisameasureofhoweasyitistoget\n",
            "\n",
            "(cid:12)\n",
            "1.1. Perceptrons (cid:12) 5\n",
            "(cid:12)\n",
            "theperceptrontofire. Foraperceptronwithareallybigbias,it’sextremelyeasyforthe 1\n",
            "perceptrontooutputa1. Butifthebiasisverynegative,thenit’sdifficultfortheperceptron\n",
            "tooutputa1. Obviously,introducingthebiasisonlyasmallchangeinhowwedescribe\n",
            "perceptrons,butwe’llseelaterthatitleadstofurthernotationalsimplifications. Becauseof\n",
            "this,intheremainderofthebookwewon’tusethethreshold,we’llalwaysusethebias. I’vedescribedperceptronsasamethodforweighingevidencetomakedecisions. Another\n",
            "wayperceptronscanbeusedistocomputetheelementarylogicalfunctionsweusuallythink\n",
            "ofasunderlyingcomputation,functionssuchasAND,OR,andNAND.Forexample,suppose\n",
            "wehaveaperceptronwithtwoinputs,eachwithweight–2,andanoverallbiasof3. Here’s\n",
            "ourperceptron:\n",
            "Thenweseethatinput00producesoutput1,since( 2) 0+( 2) 0+3=3ispositive. Here,I’veintroducedthe symboltomakethemultipl−icati∗onsex−plici∗t. Similarcalculations\n",
            "showthattheinputs01an∗d10produceoutput1. Buttheinput11producesoutput0,since\n",
            "( 2) 1+( 2) 1+3= 1isnegative.\n",
            "AndsoourperceptronimplementsaNANDgate! − T∗he NA−ND∗example−shows that we can use perceptrons to compute simple logical\n",
            "functions. Infact,wecanusenetworksofperceptronstocomputeanylogicalfunctionat\n",
            "all. ThereasonisthattheNANDgateisuniversalforcomputation,thatis,wecanbuildany\n",
            "computationupoutofNANDgates. Forexample,wecanuseNANDgatestobuildacircuit\n",
            "(cid:76)\n",
            "whichaddstwobits, x and x . Thisrequirescomputingthebitwisesum, x x ,aswell\n",
            "1 2 1 2\n",
            "asacarrybitwhichissetto1whenbothx andx are1,i.e.,thecarrybitisjustthebitwise\n",
            "1 2\n",
            "product x x :\n",
            "1 2\n",
            "TogetanequivalentnetworkofperceptronswereplacealltheNANDgatesbyperceptrons\n",
            "withtwoinputs,eachwithweight–2,andanoverallbiasof3. Here’stheresultingnetwork.\n",
            "NotethatI’vemovedtheperceptroncorrespondingtothebottomrightNANDgatealittle,\n",
            "justtomakeiteasiertodrawthearrowsonthediagram:\n",
            "\n",
            "(cid:12)\n",
            "6 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 Onenotableaspectofthisnetworkofperceptronsisthattheoutputfromtheleftmostper-\n",
            "ceptronisusedtwiceasinputtothebottommostperceptron. WhenIdefinedtheperceptron\n",
            "modelIdidn’tsaywhetherthiskindofdouble-output-to-the-same-placewasallowed.\n",
            "Actu-\n",
            "ally,itdoesn’tmuchmatter.\n",
            "Ifwedon’twanttoallowthiskindofthing,thenit’spossible\n",
            "tosimplymergethetwolines,intoasingleconnectionwithaweightof–4insteadoftwo\n",
            "connectionswith–2weights. (Ifyoudon’tfindthisobvious,youshouldstopandproveto\n",
            "yourselfthatthisisequivalent.) Withthatchange,thenetworklooksasfollows,withall\n",
            "unmarkedweightsequalto–2,allbiasesequalto3,andasingleweightof–4,asmarked:\n",
            "UptonowI’vebeendrawinginputslike x and x asvariablesfloatingtotheleftofthe\n",
            "1 2\n",
            "networkofperceptrons. Infact,it’sconventionaltodrawanextralayerofperceptrons–the\n",
            "inputlayer–toencodetheinputs:\n",
            "Thisnotationforinputperceptrons,inwhichwehaveanoutput,butnoinputs,\n",
            "isashorthand. Itdoesn’tactuallymeanaperceptronwithnoinputs. Toseethis,suppose\n",
            "(cid:80)\n",
            "wedidhaveaperceptronwithnoinputs. Thentheweightedsum w x wouldalwaysbe\n",
            "j j j\n",
            "zero,andsotheperceptronwouldoutput1if b>0,and0if b 0. Thatis,theperceptron\n",
            "wouldsimplyoutputafixedvalue,notthedesiredvalue(x ,≤intheexampleabove). It’s\n",
            "1\n",
            "bettertothinkoftheinputperceptronsasnotreallybeingperceptronsatall,butrather\n",
            "specialunitswhicharesimplydefinedtooutputthedesiredvalues, x ,x ,.... 1 2\n",
            "Theadderexampledemonstrateshowanetworkofperceptronscanbeusedtosimulatea\n",
            "circuitcontainingmanyNANDgates. AndbecauseNANDgatesareuniversalforcomputation,\n",
            "itfollowsthatperceptronsarealsouniversalforcomputation. Thecomputationaluniversalityofperceptronsissimultaneouslyreassuringanddisap-\n",
            "pointing. It’sreassuringbecauseittellsusthatnetworksofperceptronscanbeaspowerfulas\n",
            "\n",
            "(cid:12)\n",
            "1.2. Sigmoidneurons (cid:12) 7\n",
            "(cid:12)\n",
            "anyothercomputingdevice. Butit’salsodisappointing,becauseitmakesitseemasthough 1\n",
            "perceptronsaremerelyanewtypeofNANDgate.\n",
            "That’shardlybignews!\n",
            "However,thesituationisbetterthanthisviewsuggests. Itturnsoutthatwecandevise\n",
            "learning algorithms which can automatically tune the weights and biases of a network\n",
            "ofartificialneurons. Thistuninghappensinresponsetoexternalstimuli,withoutdirect\n",
            "interventionbyaprogrammer. Theselearningalgorithmsenableustouseartificialneurons\n",
            "inawaywhichisradicallydifferenttoconventionallogicgates. Insteadofexplicitlylaying\n",
            "outacircuitofNANDandothergates,ourneuralnetworkscansimplylearntosolveproblems,\n",
            "sometimesproblemswhereitwouldbeextremelydifficulttodirectlydesignaconventional\n",
            "circuit. 1.2 Sigmoid neurons\n",
            "Learningalgorithmssoundterrific. Buthowcanwedevisesuchalgorithmsforaneural\n",
            "network? Supposewehaveanetworkofperceptronsthatwe’dliketousetolearntosolve\n",
            "someproblem. Forexample,theinputstothenetworkmightbetherawpixeldatafrom\n",
            "ascanned,handwrittenimageofadigit. Andwe’dlikethenetworktolearnweightsand\n",
            "biasessothattheoutputfromthenetworkcorrectlyclassifiesthedigit. Toseehowlearning\n",
            "mightwork,supposewemakeasmallchangeinsomeweight(orbias)inthenetwork. What\n",
            "we’dlikeisforthissmallchangeinweighttocauseonlyasmallcorrespondingchangein\n",
            "theoutputfromthenetwork. Aswe’llseeinamoment,thispropertywillmakelearning\n",
            "possible. Schematically,here’swhatwewant(obviouslythisnetworkistoosimpletodo\n",
            "handwritingrecognition!):\n",
            "Ifitweretruethatasmallchangeinaweight(orbias)causesonlyasmallchangeinoutput,\n",
            "thenwecouldusethisfacttomodifytheweightsandbiasestogetournetworktobehave\n",
            "moreinthemannerwewant. Forexample,supposethenetworkwasmistakenlyclassifying\n",
            "animageasan“8”whenitshouldbea“9”. Wecouldfigureouthowtomakeasmallchange\n",
            "intheweightsandbiasessothenetworkgetsalittleclosertoclassifyingtheimageasa“9”. Andthenwe’drepeatthis,changingtheweightsandbiasesoverandovertoproducebetter\n",
            "andbetteroutput.\n",
            "Thenetworkwouldbelearning. Theproblemisthatthisisn’twhathappenswhenournetworkcontainsperceptrons. Infact,asmallchangeintheweightsorbiasofanysingleperceptroninthenetworkcan\n",
            "sometimescausetheoutputofthatperceptrontocompletelyflip,sayfrom0to1. That\n",
            "flipmaythencausethebehaviouroftherestofthenetworktocompletelychangeinsome\n",
            "\n",
            "(cid:12)\n",
            "8 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 verycomplicatedway. Sowhileyour“9”mightnowbeclassifiedcorrectly,thebehaviourof\n",
            "thenetworkonalltheotherimagesislikelytohavecompletelychangedinsomehard-to-\n",
            "controlway. Thatmakesitdifficulttoseehowtograduallymodifytheweightsandbiasesso\n",
            "thatthenetworkgetsclosertothedesiredbehaviour.\n",
            "Perhapsthere’ssomecleverwayof\n",
            "gettingaroundthisproblem. Butit’snotimmediatelyobvioushowwecangetanetworkof\n",
            "perceptronstolearn. Wecanovercomethisproblembyintroducinganewtypeofartificialneuroncalleda\n",
            "sigmoidneuron. Sigmoidneuronsaresimilartoperceptrons,butmodifiedsothatsmall\n",
            "changesintheirweightsandbiascauseonlyasmallchangeintheiroutput. That’sthecrucial\n",
            "factwhichwillallowanetworkofsigmoidneuronstolearn. Okay,letmedescribethesigmoidneuron. We’lldepictsigmoidneuronsinthesameway\n",
            "wedepictedperceptrons:\n",
            "Justlikeaperceptron,thesigmoidneuronhasinputs, x ,x ,.... Butinsteadofbeingjust0\n",
            "1 2\n",
            "or1,theseinputscanalsotakeonanyvaluesbetween0and1. So,forinstance,0.638...isa\n",
            "validinputforasigmoidneuron. Alsojustlikeaperceptron,thesigmoidneuronhasweights\n",
            "foreachinput,w ,w ,...,andanoverallbias, b.\n",
            "Buttheoutputisnot0or1. Instead,it’s\n",
            "1 2\n",
            "σ (wx+b),whereσiscalledthesigmoidfunction1,andisdefinedby:\n",
            "1\n",
            "σ (z) . (1.3)\n",
            "≡ 1+e z\n",
            "−\n",
            "Toputitallalittlemoreexplicitly,theoutputofasigmoidneuronwithinputs x ,x ,...,\n",
            "1 2\n",
            "weightsw ,w ,...,andbias bis\n",
            "1 2\n",
            "1\n",
            ". (1.4)\n",
            "(cid:128) (cid:80) (cid:138)\n",
            "1+exp\n",
            "j\n",
            "w\n",
            "j\n",
            "x\n",
            "j\n",
            "b\n",
            "− −\n",
            "Atfirstsight,sigmoidneuronsappearverydifferenttoperceptrons. Thealgebraicformof\n",
            "thesigmoidfunctionmayseemopaqueandforbiddingifyou’renotalreadyfamiliarwith\n",
            "it. Infact,therearemanysimilaritiesbetweenperceptronsandsigmoidneurons,andthe\n",
            "algebraicformofthesigmoidfunctionturnsouttobemoreofatechnicaldetailthanatrue\n",
            "barriertounderstanding. Tounderstandthesimilaritytotheperceptronmodel,supposez w x+bisalarge\n",
            "positivenumber. Thene\n",
            "−\n",
            "z 0andsoσ (z) 1. Inotherwords,when≡z= ·w x+bislarge\n",
            "andpositive,theoutputfro≈mthesigmoidn≈euronisapproximately1,justas·itwouldhave\n",
            "beenforaperceptron. Supposeontheotherhandthatz=w x+bisverynegative.\n",
            "Then\n",
            "e\n",
            "−\n",
            "z ,andσ (z) 0. Sowhenz=w x+bisverynegativ·e,thebehaviourofasigmoid\n",
            "→∞ ≈ ·\n",
            "1Incidentally,σissometimescalledthelogisticfunction,andthisnewclassofneuronscalledlogistic\n",
            "neurons.It’susefultorememberthisterminology,sincethesetermsareusedbymanypeopleworking\n",
            "withneuralnets.However,we’llstickwiththesigmoidterminology. \n",
            "(cid:12)\n",
            "1.2. Sigmoidneurons (cid:12) 9\n",
            "(cid:12)\n",
            "neuronalsocloselyapproximatesaperceptron. It’sonlywhenw x+bisofmodestsize 1\n",
            "thatthere’smuchdeviationfromtheperceptronmodel. ·\n",
            "Whataboutthealgebraicformofσ?\n",
            "Howcanweunderstandthat? Infact,theexact\n",
            "formofσisn’tsoimportant–whatreallymattersistheshapeofthefunctionwhenplotted. Here’stheshape:\n",
            "Sigmoidfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "− − −\n",
            "Thisshapeisasmoothedoutversionofastepfunction:\n",
            "Stepfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "− − −\n",
            "Ifσhadinfactbeenastepfunction,thenthesigmoidneuronwouldbeaperceptron,since\n",
            "theoutputwouldbe1or0dependingonwhetherw x+bwaspositiveornegative2. By\n",
            "usingtheactualσfunctionweget,asalreadyimplied· above,asmoothedoutperceptron. Indeed,it’sthesmoothnessoftheσfunctionthatisthecrucialfact,notitsdetailedform. Thesmoothnessofσmeansthatsmallchanges∆w intheweightsand∆binthebiaswill\n",
            "j\n",
            "produceasmallchange∆outputintheoutputfromtheneuron. Infact,calculustellsus\n",
            "that∆outputiswellapproximatedby\n",
            "(cid:88)∂output ∂output\n",
            "∆output\n",
            "≈ j\n",
            "∂w\n",
            "j\n",
            "∆w j+ ∂b ∆b (1.5)\n",
            "2Actually,whenw x+b=0theperceptronoutputs0,whilethestepfunctionoutputs1.So,strictly\n",
            "speaking,we’dneedt·omodifythestepfunctionatthatonepoint.Butyougettheidea. \n",
            "(cid:12)\n",
            "10 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 wherethesumisoveralltheweights,w ,and∂output/∂w and∂output/∂bdenotepartial\n",
            "j j\n",
            "derivativesoftheoutputwithrespecttow and b,respectively. Don’tpanicifyou’renot\n",
            "j\n",
            "comfortablewithpartialderivatives! Whiletheexpressionabovelookscomplicated,withall\n",
            "thepartialderivatives,it’sactuallysayingsomethingverysimple(andwhichisverygood\n",
            "news): ∆outputisalinearfunctionofthechanges∆w and∆bintheweightsandbias. j\n",
            "Thislinearitymakesiteasytochoosesmallchangesintheweightsandbiasestoachieve\n",
            "anydesiredsmallchangeintheoutput. Sowhilesigmoidneuronshavemuchofthesame\n",
            "qualitativebehaviorasperceptrons,theymakeitmucheasiertofigureouthowchanging\n",
            "theweightsandbiaseswillchangetheoutput. Ifit’stheshapeofσ whichreallymatters, andnotitsexactform, thenwhyusethe\n",
            "particularformusedforσinEquation1.3? Infact,laterinthebookwewilloccasionally\n",
            "considerneuronswheretheoutputis f(w x+b)forsomeotheractivationfunction f(). The\n",
            "mainthingthatchangeswhenweusead·ifferentactivationfunctionisthatthepar·ticular\n",
            "valuesforthepartialderivativesinEquation1.5change. Itturnsoutthatwhenwecompute\n",
            "thosepartialderivativeslater,usingσwillsimplifythealgebra,simplybecauseexponentials\n",
            "havelovelypropertieswhendifferentiated. Inanycase,σiscommonly-usedinworkon\n",
            "neuralnets,andistheactivationfunctionwe’llusemostofteninthisbook. Howshouldweinterprettheoutputfromasigmoidneuron?Obviously,onebigdifference\n",
            "betweenperceptronsandsigmoidneuronsisthatsigmoidneuronsdon’tjustoutput0or\n",
            "1. Theycanhaveasoutputanyrealnumberbetween0and1,sovaluessuchas0.173...\n",
            "and0.689...arelegitimateoutputs. Thiscanbeuseful,forexample,ifwewanttousethe\n",
            "outputvaluetorepresenttheaverageintensityofthepixelsinanimageinputtoaneural\n",
            "network. Butsometimesitcanbeanuisance. Supposewewanttheoutputfromthenetwork\n",
            "toindicateeither“theinputimageisa9”or“theinputimageisnota9”. Obviously,it’dbe\n",
            "easiesttodothisiftheoutputwasa0ora1,asinaperceptron. Butinpracticewecan\n",
            "setupaconventiontodealwiththis,forexample,bydecidingtointerpretanyoutputofat\n",
            "least0.5asindicatinga“9”,andanyoutputlessthan0.5asindicating“nota9”. I’llalways\n",
            "explicitlystatewhenwe’reusingsuchaconvention,soitshouldn’tcauseanyconfusion. Exercises\n",
            "Sigmoidneuronssimulatingperceptrons,partISupposewetakealltheweights\n",
            "• andbiasesinanetworkofperceptrons,andmultiplythembyapositiveconstant,c>0. Showthatthebehaviorofthenetworkdoesn’tchange. Sigmoidneuronssimulatingperceptrons,partIISupposewehavethesamesetup\n",
            "• asthelastproblem–anetworkofperceptrons. Supposealsothattheoverallinputto\n",
            "thenetworkofperceptronshasbeenchosen.\n",
            "Wewon’tneedtheactualinputvalue,we\n",
            "justneedtheinputtohavebeenfixed. Supposetheweightsandbiasesaresuchthat\n",
            "w x+b=0fortheinputxtoanyparticularperceptroninthenetwork. Nowreplace\n",
            "all·thepe(cid:54)rceptronsinthenetworkbysigmoidneurons,andmultiplytheweightsand\n",
            "biasesbyapositiveconstantc>0. Showthatinthelimitasc thebehaviourof\n",
            "thisnetworkofsigmoidneuronsisexactlythesameasthen→etw∞orkofperceptrons. Howcanthisfailwhenw x+b=0foroneoftheperceptrons? ·\n",
            "1.3 The architecture of neural networks\n",
            "InthenextsectionI’llintroduceaneuralnetworkthatcandoaprettygoodjobclassifying\n",
            "handwrittendigits. Inpreparationforthat,ithelpstoexplainsometerminologythatletsus\n",
            "namedifferentpartsofanetwork.\n",
            "Supposewehavethenetwork:\n",
            "\n",
            "(cid:12)\n",
            "1.3. Thearchitectureofneuralnetworks (cid:12) 11\n",
            "(cid:12)\n",
            "1\n",
            "Asmentionedearlier,theleftmostlayerinthisnetworkiscalledtheinputlayer,andthe\n",
            "neuronswithinthelayerarecalledinputneurons. Therightmostoroutputlayercontains\n",
            "theoutputneurons,or,asinthiscase,asingleoutputneuron. Themiddlelayeriscalleda\n",
            "hiddenlayer,sincetheneuronsinthislayerareneitherinputsnoroutputs. Theterm“hidden”\n",
            "perhapssoundsalittlemysterious–thefirsttimeIheardthetermIthoughtitmusthave\n",
            "somedeepphilosophicalormathematicalsignificance–butitreallymeansnothingmore\n",
            "than“notaninputoranoutput”. Thenetworkabovehasjustasinglehiddenlayer,butsome\n",
            "networkshavemultiplehiddenlayers. Forexample,thefollowingfour-layernetworkhas\n",
            "twohiddenlayers:\n",
            "Somewhatconfusingly,andforhistoricalreasons,suchmultiplelayernetworksaresome-\n",
            "times called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons,\n",
            "notperceptrons. I’mnotgoingtousetheMLPterminologyinthisbook,sinceIthinkit’s\n",
            "confusing,butwantedtowarnyouofitsexistence.\n",
            "Thedesignoftheinputandoutputlayersinanetworkisoftenstraightforward.\n",
            "For\n",
            "example,supposewe’retryingtodeterminewhetherahandwrittenimagedepictsa“9”ornot. Anaturalwaytodesignthenetworkistoencodetheintensitiesoftheimagepixelsintothe\n",
            "inputneurons. Iftheimageisa64by64greyscaleimage,thenwe’dhave4,096=64 64\n",
            "inputneurons,withtheintensitiesscaledappropriatelybetween0and1. Theoutputl×ayer\n",
            "willcontainjustasingleneuron,withoutputvaluesoflessthan0.5indicating“inputimage\n",
            "isnota9”,andvaluesgreaterthan0.5indicating“inputimageisa9”. Whilethedesignoftheinputandoutputlayersofaneuralnetworkisoftenstraight-\n",
            "forward,therecanbequiteanarttothedesignofthehiddenlayers. Inparticular,it’snot\n",
            "\n",
            "(cid:12)\n",
            "12 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 possibletosumupthedesignprocessforthehiddenlayerswithafewsimplerulesofthumb. Instead,neuralnetworksresearchershavedevelopedmanydesignheuristicsforthehidden\n",
            "layers,whichhelppeoplegetthebehaviourtheywantoutoftheirnets. Forexample,such\n",
            "heuristicscanbeusedtohelpdeterminehowtotradeoffthenumberofhiddenlayersagainst\n",
            "thetimerequiredtotrainthenetwork. We’llmeetseveralsuchdesignheuristicslaterinthis\n",
            "book. Uptonow,we’vebeendiscussingneuralnetworkswheretheoutputfromonelayeris\n",
            "usedasinputtothenextlayer. Suchnetworksarecalledfeedforwardneuralnetworks. This\n",
            "meanstherearenoloopsinthenetwork–informationisalwaysfedforward,neverfed\n",
            "back. Ifwedidhaveloops,we’dendupwithsituationswheretheinputtotheσfunction\n",
            "dependedontheoutput. That’dbehardtomakesenseof,andsowedon’tallowsuchloops. However,thereareothermodelsofartificialneuralnetworksinwhichfeedbackloops\n",
            "arepossible. Thesemodelsarecalledrecurrentneuralnetworks.\n",
            "Theideainthesemodelsis\n",
            "tohaveneuronswhichfireforsomelimiteddurationoftime,beforebecomingquiescent. Thatfiringcanstimulateotherneurons,whichmayfirealittlewhilelater,alsoforalimited\n",
            "duration. Thatcausesstillmoreneuronstofire,andsoovertimewegetacascadeofneurons\n",
            "firing.\n",
            "Loopsdon’tcauseproblemsinsuchamodel,sinceaneuron’soutputonlyaffectsits\n",
            "inputatsomelatertime,notinstantaneously. Recurrent neural nets have been less influential than feedforward networks, in part\n",
            "becausethelearningalgorithmsforrecurrentnetsare(atleasttodate)lesspowerful. But\n",
            "recurrentnetworksarestillextremelyinteresting. They’remuchcloserinspirittohowour\n",
            "brainsworkthanfeedforwardnetworks. Andit’spossiblethatrecurrentnetworkscansolve\n",
            "importantproblemswhichcanonlybesolvedwithgreatdifficultybyfeedforwardnetworks.\n",
            "However,tolimitourscope,inthisbookwe’regoingtoconcentrateonthemorewidely-used\n",
            "feedforwardnetworks. 1.4 A simple network to classify handwritten digits\n",
            "Havingdefinedneuralnetworks,let’sreturntohandwritingrecognition. Wecansplitthe\n",
            "problemofrecognizinghandwrittendigitsintotwosub-problems. First,we’dlikeaway\n",
            "of breaking an image containing many digits into a sequence of separate images, each\n",
            "containingasingledigit. Forexample,we’dliketobreaktheimage\n",
            "intosixseparateimages,\n",
            "Wehumanssolvethissegmentationproblemwithease,butit’schallengingforacomputer\n",
            "programtocorrectlybreakuptheimage. Oncetheimagehasbeensegmented,theprogram\n",
            "then needs to classify each individual digit. So, for instance, we’d like our program to\n",
            "recognizethatthefirstdigitabove,\n",
            "\n",
            "(cid:12)\n",
            "1.4. Asimplenetworktoclassifyhandwrittendigits (cid:12) 13\n",
            "(cid:12)\n",
            "isa5. 1\n",
            "We’llfocusonwritingaprogramtosolvethesecondproblem,thatis,classifyingindividual\n",
            "digits. Wedothisbecauseitturnsoutthatthesegmentationproblemisnotsodifficultto\n",
            "solve,onceyouhaveagoodwayofclassifyingindividualdigits. Therearemanyapproaches\n",
            "to solving the segmentation problem. One approach is to trial many different ways of\n",
            "segmentingtheimage,usingtheindividualdigitclassifiertoscoreeachtrialsegmentation. A trial segmentation gets a high score if the individual digit classifier is confident of its\n",
            "classificationinallsegments,andalowscoreiftheclassifierishavingalotoftroubleinone\n",
            "ormoresegments. Theideaisthatiftheclassifierishavingtroublesomewhere,thenit’s\n",
            "probablyhavingtroublebecausethesegmentationhasbeenchosenincorrectly.\n",
            "Thisidea\n",
            "andothervariationscanbeusedtosolvethesegmentationproblemquitewell. Soinsteadof\n",
            "worryingaboutsegmentationwe’llconcentrateondevelopinganeuralnetworkwhichcan\n",
            "solvethemoreinterestinganddifficultproblem,namely,recognizingindividualhandwritten\n",
            "digits. Torecognizeindividualdigitswewilluseathree-layerneuralnetwork:\n",
            "Theinputlayerofthenetworkcontainsneuronsencodingthevaluesoftheinputpixels. As\n",
            "discussedinthenextsection,ourtrainingdataforthenetworkwillconsistofmany28by28\n",
            "pixelimagesofscannedhandwrittendigits,andsotheinputlayercontains784=28 28\n",
            "neurons. ForsimplicityI’veomittedmostofthe784inputneuronsinthediagramabove.×The\n",
            "inputpixelsaregreyscale,withavalueof0.0representingwhite,avalueof1.0representing\n",
            "black,andinbetweenvaluesrepresentinggraduallydarkeningshadesofgrey. Thesecondlayerofthenetworkisahiddenlayer.\n",
            "Wedenotethenumberofneuronsin\n",
            "\n",
            "(cid:12)\n",
            "14 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 thishiddenlayerbyn,andwe’llexperimentwithdifferentvaluesforn. Theexampleshown\n",
            "illustratesasmallhiddenlayer,containingjustn=15neurons. Theoutputlayerofthenetworkcontains10neurons. Ifthefirstneuronfires,i.e.,has\n",
            "anoutput 1,thenthatwillindicatethatthenetworkthinksthedigitisa0. Ifthesecond\n",
            "neuronfire≈sthenthatwillindicatethatthenetworkthinksthedigitisa1. Andsoon. A\n",
            "littlemoreprecisely,wenumbertheoutputneuronsfrom0through9,andfigureoutwhich\n",
            "neuronhasthehighestactivationvalue. Ifthatneuronis,say,neuronnumber6,thenour\n",
            "networkwillguessthattheinputdigitwasa6. Andsoonfortheotheroutputneurons.\n",
            "Youmightwonderwhyweuse10outputneurons. Afterall,thegoalofthenetwork\n",
            "istotelluswhichdigit(0,1,2,...,9)correspondstotheinputimage. Aseeminglynatural\n",
            "wayofdoingthatistousejust4outputneurons,treatingeachneuronastakingonabinary\n",
            "value,dependingonwhethertheneuron’soutputiscloserto0orto1. Fourneuronsare\n",
            "enoughtoencodetheanswer,since24 =16ismorethanthe10possiblevaluesfortheinput\n",
            "digit. Whyshouldournetworkuse10neuronsinstead?\n",
            "Isn’tthatinefficient? Theultimate\n",
            "justificationisempirical: wecantryoutbothnetworkdesigns,anditturnsoutthat,forthis\n",
            "particularproblem,thenetworkwith10outputneuronslearnstorecognizedigitsbetter\n",
            "thanthenetworkwith4outputneurons. Butthatleavesuswonderingwhyusing10output\n",
            "neuronsworksbetter.\n",
            "Istheresomeheuristicthatwouldtellusinadvancethatweshould\n",
            "usethe10-outputencodinginsteadofthe4-outputencoding? To understand why we do this, it helps to think about what the neural network is\n",
            "doingfromfirstprinciples. Considerfirstthecasewhereweuse10outputneurons. Let’s\n",
            "concentrateonthefirstoutputneuron,theonethat’stryingtodecidewhetherornotthe\n",
            "digitisa0. Itdoesthisbyweighingupevidencefromthehiddenlayerofneurons. What\n",
            "arethosehiddenneuronsdoing? Well,justsupposeforthesakeofargumentthatthefirst\n",
            "neuroninthehiddenlayerdetectswhetherornotanimagelikethefollowingispresent:\n",
            "Itcandothisbyheavilyweightinginputpixelswhichoverlapwiththeimage, andonly\n",
            "lightlyweightingtheotherinputs. Inasimilarway,let’ssupposeforthesakeofargument\n",
            "thatthesecond,third,andfourthneuronsinthehiddenlayerdetectwhetherornotthe\n",
            "followingimagesarepresent:\n",
            "Asyoumayhaveguessed,thesefourimagestogethermakeupthe0imagethatwesawin\n",
            "thelineofdigitsshownearlier:\n",
            "Soifallfourofthesehiddenneuronsarefiringthenwecanconcludethatthedigitisa0. Of\n",
            "course,that’snottheonlysortofevidencewecanusetoconcludethattheimagewasa0\n",
            "–wecouldlegitimatelygeta0inmanyotherways(say,throughtranslationsoftheabove\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 15\n",
            "(cid:12)\n",
            "images,orslightdistortions). Butitseemssafetosaythatatleastinthiscasewe’dconclude 1\n",
            "thattheinputwasa0.\n",
            "Supposingtheneuralnetworkfunctionsinthisway,wecangiveaplausibleexplanation\n",
            "forwhyit’sbettertohave10outputsfromthenetwork,ratherthan4. Ifwehad4outputs,\n",
            "then the first output neuron would be trying to decide what the most significant bit of\n",
            "thedigitwas. Andthere’snoeasywaytorelatethatmostsignificantbittosimpleshapes\n",
            "likethoseshownabove. It’shardtoimaginethatthere’sanygoodhistoricalreasonthe\n",
            "componentshapesofthedigitwillbecloselyrelatedto(say)themostsignificantbitinthe\n",
            "output. Now, with all that said, this is all just a heuristic. Nothing says that the three-layer\n",
            "neuralnetworkhastooperateinthewayIdescribed,withthehiddenneuronsdetecting\n",
            "simplecomponentshapes. Maybeacleverlearningalgorithmwillfindsomeassignmentof\n",
            "weightsthatletsususeonly4outputneurons. ButasaheuristicthewayofthinkingI’ve\n",
            "describedworksprettywell,andcansaveyoualotoftimeindesigninggoodneuralnetwork\n",
            "architectures. Exercise\n",
            "Thereisawayofdeterminingthebitwiserepresentationofadigitbyaddinganextra\n",
            "• layertothethree-layernetworkabove. Theextralayerconvertstheoutputfromthe\n",
            "previouslayerintoabinaryrepresentation,asillustratedinthefigurebelow. Finda\n",
            "setofweightsandbiasesforthenewoutputlayer. Assumethatthefirst3layersof\n",
            "neuronsaresuchthatthecorrectoutputinthethirdlayer(i.e.,theoldoutputlayer)\n",
            "hasactivationatleast0.99,andincorrectoutputshaveactivationlessthan0.01. 1.5 Learning with gradient descent\n",
            "Nowthatwehaveadesignforourneuralnetwork,howcanitlearntorecognizedigits? The\n",
            "firstthingwe’llneedisadatasettolearnfrom–aso-calledtrainingdataset.\n",
            "We’llusethe\n",
            "MNISTdataset,whichcontainstensofthousandsofscannedimagesofhandwrittendigits,\n",
            "togetherwiththeircorrectclassifications. MNIST’snamecomesfromthefactthatitisa\n",
            "modifiedsubsetoftwodatasetscollectedbyNIST,theUnitedStates’NationalInstituteof\n",
            "StandardsandTechnology. Here’safewimagesfromMNIST:\n",
            "\n",
            "(cid:12)\n",
            "16 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Asyoucansee,thesedigitsare,infact,thesameasthoseshownatthebeginningofthis\n",
            "chapterasachallengetorecognize. Ofcourse,whentestingournetworkwe’llaskitto\n",
            "recognizeimageswhicharen’tinthetrainingset! TheMNISTdatacomesintwoparts.\n",
            "Thefirstpartcontains60,000imagestobeused\n",
            "astrainingdata. Theseimagesarescannedhandwritingsamplesfrom250people,halfof\n",
            "whomwereUSCensusBureauemployees,andhalfofwhomwerehighschoolstudents. Theimagesaregreyscaleand28by28pixelsinsize. ThesecondpartoftheMNISTdata\n",
            "setis10,000imagestobeusedastestdata. Again,theseare28by28greyscaleimages. We’llusethetestdatatoevaluatehowwellourneuralnetworkhaslearnedtorecognize\n",
            "digits. Tomakethisagoodtestofperformance,thetestdatawastakenfromadifferent\n",
            "setof250peoplethantheoriginaltrainingdata(albeitstillagroupsplitbetweenCensus\n",
            "Bureauemployeesandhighschoolstudents). Thishelpsgiveusconfidencethatoursystem\n",
            "canrecognizedigitsfrompeoplewhosewritingitdidn’tseeduringtraining. We’llusethenotation x todenoteatraininginput. It’llbeconvenienttoregardeach\n",
            "traininginput x asa28 28=784-dimensionalvector. Eachentryinthevectorrepresents\n",
            "thegreyvalueforasingl×epixelintheimage. We’lldenotethecorrespondingdesiredoutput\n",
            "by y= y(x),where yisa10-dimensionalvector. Forexample,ifaparticulartrainingimage,\n",
            "x,depictsa6,then y(x)=(0,0,0,0,0,0,1,0,0,0) T isthedesiredoutputfromthenetwork. NotethatT hereisthetransposeoperation,turningarowvectorintoanordinary(column)\n",
            "vector. Whatwe’dlikeisanalgorithmwhichletsusfindweightsandbiasessothattheoutput\n",
            "fromthenetworkapproximates y(x)foralltraininginputs x. Toquantifyhowwellwe’re\n",
            "achievingthisgoalwedefineacostfunction3:\n",
            "1 (cid:88)\n",
            "C(w,b) y(x) a 2 (1.6)\n",
            "≡ 2n x (cid:107) − (cid:107)\n",
            "Here,wdenotesthecollectionofallweightsinthenetwork, ballthebiases,nisthetotal\n",
            "numberoftraininginputs,aisthevectorofoutputsfromthenetworkwhen x isinput,and\n",
            "thesumisoveralltraininginputs, x. Ofcourse,theoutputadependson x,wand b,butto\n",
            "keepthenotationsimpleIhaven’texplicitlyindicatedthisdependence.\n",
            "Thenotation v\n",
            "justdenotestheusuallengthfunctionforavectorv. We’llcallC thequadraticcostfunct(cid:107)ion(cid:107);\n",
            "it’salsosometimesknownasthemeansquarederrororjustMSE.Inspectingtheformofthe\n",
            "quadraticcostfunction,weseethatC(w,b)isnon-negative,sinceeveryterminthesum\n",
            "isnon-negative. Furthermore,thecostC(w,b)becomessmall,i.e.,C(w,b) 0,precisely\n",
            "when y(x)isapproximatelyequaltotheoutput,a,foralltraininginputs,x. S≈oourtraining\n",
            "algorithmhasdoneagoodjobifitcanfindweightsandbiasessothat C(w,b) 0. By\n",
            "contrast,it’snotdoingsowellwhenC(w,b)islarge–thatwouldmeanthat y(x≈ )isnot\n",
            "closetotheoutputaforalargenumberofinputs. Sotheaimofourtrainingalgorithmwill\n",
            "betominimizethecostC(w,b)asafunctionoftheweightsandbiases. Inotherwords,we\n",
            "wanttofindasetofweightsandbiaseswhichmakethecostassmallaspossible. We’lldo\n",
            "thatusinganalgorithmknownasgradientdescent. 3Sometimesreferredtoasalossorobjectivefunction. Weusethetermcostfunctionthroughout\n",
            "thisbook,butyoushouldnotetheotherterminology,sinceit’softenusedinresearchpapersandother\n",
            "discussionsofneuralnetworks.\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 17\n",
            "(cid:12)\n",
            "Whyintroducethequadraticcost? Afterall,aren’tweprimarilyinterestedinthenumber 1\n",
            "ofimagescorrectlyclassifiedbythenetwork?\n",
            "Whynottrytomaximizethatnumberdirectly,\n",
            "ratherthanminimizingaproxymeasurelikethequadraticcost? Theproblemwiththat\n",
            "isthatthenumberofimagescorrectlyclassifiedisnotasmoothfunctionoftheweights\n",
            "andbiasesinthenetwork. Forthemostpart,makingsmallchangestotheweightsand\n",
            "biaseswon’tcauseanychangeatallinthenumberoftrainingimagesclassifiedcorrectly. Thatmakesitdifficulttofigureouthowtochangetheweightsandbiasestogetimproved\n",
            "performance. Ifweinsteaduseasmoothcostfunctionlikethequadraticcostitturnsoutto\n",
            "beeasytofigureouthowtomakesmallchangesintheweightsandbiasessoastogetan\n",
            "improvementinthecost. That’swhywefocusfirstonminimizingthequadraticcost,and\n",
            "onlyafterthatwillweexaminetheclassificationaccuracy. Evengiventhatwewanttouseasmoothcostfunction,youmaystillwonderwhywe\n",
            "choosethequadraticfunctionusedinEquation1.6. Isn’tthisaratheradhocchoice? Perhaps\n",
            "ifwechoseadifferentcostfunctionwe’dgetatotallydifferentsetofminimizingweights\n",
            "andbiases? Thisisavalidconcern,andlaterwe’llrevisitthecostfunction,andmakesome\n",
            "modifications. However,thequadraticcostfunctionofEquation1.6worksperfectlywellfor\n",
            "understandingthebasicsoflearninginneuralnetworks,sowe’llstickwithitfornow. Recapping,ourgoalintraininganeuralnetworkistofindweightsandbiaseswhich\n",
            "minimizethequadraticcostfunctionC(w,b). Thisisawell-posedproblem,butit’sgotalot\n",
            "ofdistractingstructureascurrentlyposed–theinterpretationofwand basweightsand\n",
            "biases,theσfunctionlurkinginthebackground,thechoiceofnetworkarchitecture,MNIST,\n",
            "andsoon. Itturnsoutthatwecanunderstandatremendousamountbyignoringmostof\n",
            "thatstructure,andjustconcentratingontheminimizationaspect.\n",
            "Sofornowwe’regoingto\n",
            "forgetallaboutthespecificformofthecostfunction,theconnectiontoneuralnetworks,\n",
            "andsoon. Instead,we’regoingtoimaginethatwe’vesimplybeengivenafunctionofmany\n",
            "variablesandwewanttominimizethatfunction. We’regoingtodevelopatechniquecalled\n",
            "gradientdescentwhichcanbeusedtosolvesuchminimizationproblems. Thenwe’llcome\n",
            "backtothespecificfunctionwewanttominimizeforneuralnetworks. Okay,let’ssupposewe’retryingtominimizesomefunction, C(v). Thiscouldbeany\n",
            "real-valuedfunctionofmanyvariables,v=v1,v2,.... NotethatI’vereplacedthewand b\n",
            "notationbyvtoemphasizethatthiscouldbeanyfunction–we’renotspecificallythinkingin\n",
            "theneuralnetworkscontextanymore. TominimizeC(v)ithelpstoimagineC asafunction\n",
            "ofjusttwovariables,whichwe’llcallv andv :\n",
            "1 2\n",
            "Whatwe’dlikeistofindwhere C achievesitsglobalminimum. Now, ofcourse, forthe\n",
            "functionplottedabove,wecaneyeballthegraphandfindtheminimum.\n",
            "Inthatsense,I’ve\n",
            "\n",
            "(cid:12)\n",
            "18 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 perhapsshownslightlytoosimpleafunction! Ageneralfunction,C,maybeacomplicated\n",
            "functionofmanyvariables,anditwon’tusuallybepossibletojusteyeballthegraphtofind\n",
            "theminimum. Onewayofattackingtheproblemistousecalculustotrytofindtheminimumanalytically. WecouldcomputederivativesandthentryusingthemtofindplaceswhereCisanextremum. WithsomeluckthatmightworkwhenC isafunctionofjustoneorafewvariables. But\n",
            "it’llturnintoanightmarewhenwehavemanymorevariables. Andforneuralnetworks\n",
            "we’lloftenwantfarmorevariables–thebiggestneuralnetworkshavecostfunctionswhich\n",
            "dependonbillionsofweightsandbiasesinanextremelycomplicatedway. Usingcalculusto\n",
            "minimizethatjustwon’twork! (Afterassertingthatwe’llgaininsightbyimaginingC asafunctionofjusttwovariables,\n",
            "I’veturnedaroundtwiceintwoparagraphsandsaid, “hey, butwhatifit’safunctionof\n",
            "manymorethantwovariables?” Sorryaboutthat. PleasebelievemewhenIsaythatit\n",
            "reallydoeshelptoimagineC asafunctionoftwovariables. Itjusthappensthatsometimes\n",
            "thatpicturebreaksdown,andthelasttwoparagraphsweredealingwithsuchbreakdowns. Goodthinkingaboutmathematicsofteninvolvesjugglingmultipleintuitivepictures,learning\n",
            "whenit’sappropriatetouseeachpicture,andwhenit’snot.)\n",
            "Okay,socalculusdoesn’twork. Fortunately,thereisabeautifulanalogywhichsuggests\n",
            "analgorithmwhichworksprettywell. Westartbythinkingofourfunctionasakindofa\n",
            "valley.\n",
            "Ifyousquintjustalittleattheplotabove,thatshouldn’tbetoohard. Andweimagine\n",
            "aballrollingdowntheslopeofthevalley. Oureverydayexperiencetellsusthattheball\n",
            "willeventuallyrolltothebottomofthevalley.\n",
            "Perhapswecanusethisideaasawayto\n",
            "findaminimumforthefunction? We’drandomlychooseastartingpointforan(imaginary)\n",
            "ball,andthensimulatethemotionoftheballasitrolleddowntothebottomofthevalley. Wecoulddothissimulationsimplybycomputingderivatives(andperhapssomesecond\n",
            "derivatives)ofC –thosederivativeswouldtelluseverythingweneedtoknowaboutthe\n",
            "local“shape”ofthevalley,andthereforehowourballshouldroll. BasedonwhatI’vejustwritten,youmightsupposethatwe’llbetryingtowritedown\n",
            "Newton’sequationsofmotionfortheball,consideringtheeffectsoffrictionandgravity,\n",
            "andsoon. Actually,we’renotgoingtotaketheball-rollinganalogyquitethatseriously–\n",
            "we’redevisinganalgorithmtominimizeC,notdevelopinganaccuratesimulationofthe\n",
            "lawsofphysics! Theball’s-eyeviewismeanttostimulateourimagination,notconstrainour\n",
            "thinking. Soratherthangetintoallthemessydetailsofphysics,let’ssimplyaskourselves:\n",
            "ifweweredeclaredGodforaday,andcouldmakeupourownlawsofphysics,dictatingto\n",
            "theballhowitshouldroll,whatlaworlawsofmotioncouldwepickthatwouldmakeitso\n",
            "theballalwaysrolledtothebottomofthevalley?\n",
            "Tomakethisquestionmoreprecise,let’sthinkaboutwhathappenswhenwemovethe\n",
            "ballasmallamount∆v inthe v direction,andasmallamount∆v inthe v direction. 1 1 2 2\n",
            "CalculustellsusthatC changesasfollows:\n",
            "∂C ∂C\n",
            "∆C\n",
            "≈\n",
            "∂v\n",
            "1\n",
            "∆v 1+∂v\n",
            "2\n",
            "∆v 2 . (1.7)\n",
            "We’regoingtofindawayofchoosing∆v and∆v soastomake∆C negative;i.e.,we’ll\n",
            "1 2\n",
            "choosethemsotheballisrollingdownintothevalley. Tofigureouthowtomakesucha\n",
            "choiceithelpstodefine∆vtobethevectorofchangesinv,∆v ( ∆v 1 ,∆v 2) T,whereT is\n",
            "againthetransposeoperation,turningrowvectorsintocolumn≡vectors. We’llalsodefine\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 19\n",
            "(cid:12)\n",
            "thegradientofC tobethevectorofpartialderivatives,\n",
            "(cid:128)∂C, ∂C (cid:138)T\n",
            ". Wedenotethegradient\n",
            "1\n",
            "∂v1 ∂v2\n",
            "vectorby C,i.e.:\n",
            "∇ (cid:129)∂C ∂C(cid:139)T\n",
            "C , . (1.8)\n",
            "∇ ≡\n",
            "∂v\n",
            "1\n",
            "∂v\n",
            "2\n",
            "Inamomentwe’llrewritethechange∆C intermsof∆v andthegradient, C. Before\n",
            "gettingtothat,though,Iwanttoclarifysomethingthatsometimesgetspeople∇hungupon\n",
            "thegradient.\n",
            "Whenmeetingthe C notationforthefirsttime,peoplesometimeswonder\n",
            "howtheyshouldthinkaboutthe∇ symbol. What, exactly, does C mean? Infact, it’s\n",
            "perfectlyfinetothinkof C asas∇inglemathematicalobject–the∇vectordefinedabove–\n",
            "whichhappenstobewritt∇enusingtwosymbols. Inthispointofview, C isjustapieceof\n",
            "notationalflag-waving,tellingyou“hey, C isagradientvector”. There∇aremoreadvanced\n",
            "pointsofviewwhere C canbeviewed∇asanindependentmathematicalentityinitsown\n",
            "right(forexample,as∇adifferentialoperator),butwewon’tneedsuchpointsofview. Withthesedefinitions,theexpression1.7for∆C canberewrittenas\n",
            "∆C C ∆v (1.9)\n",
            "≈∇ ·\n",
            "Thisequationhelpsexplainwhy C iscalledthegradientvector: C relateschangesin\n",
            "vtochangesinC,justaswe’dexp∇ectsomethingcalledagradientto∇do.\n",
            "Butwhat’sreally\n",
            "excitingabouttheequationisthatitletsusseehowtochoose∆vsoastomake∆Cnegative. Inparticular,supposewechoose\n",
            "∆v= η C, (1.10)\n",
            "− ∇\n",
            "whereηisasmall,positiveparameter(knownasthelearningrate). ThenEquation1.9tells\n",
            "usthat∆C η C C= η C 2. Because C 2 0,thisguaranteesthat∆C 0,\n",
            "i.e.,C willa≈lw−ays∇dec·r∇ease,n−eve(cid:107)r∇inc(cid:107)rease,ifwe(cid:107)c∇han(cid:107)ge≥vaccordingtotheprescription≤in\n",
            "1.10. (Within,ofcourse,thelimitsoftheapproximationinEquation1.9).\n",
            "Thisisexactlythe\n",
            "propertywewanted! Andsowe’lltakeEquation1.10todefinethe“lawofmotion”forthe\n",
            "ballinourgradientdescentalgorithm. Thatis,we’lluseEquation1.10tocomputeavalue\n",
            "for∆v,thenmovetheball’spositionvbythatamount:\n",
            "v v (cid:48)=v η C. (1.11)\n",
            "→ − ∇\n",
            "Thenwe’llusethisupdateruleagain,tomakeanothermove. Ifwekeepdoingthis,over\n",
            "andover,we’llkeepdecreasingC until–wehope–wereachaglobalminimum. Summingup,thewaythegradientdescentalgorithmworksistorepeatedlycompute\n",
            "thegradient C,andthentomoveintheoppositedirection,“fallingdown”theslopeofthe\n",
            "valley. Weca∇nvisualizeitlikethis:\n",
            "\n",
            "(cid:12)\n",
            "20 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Noticethatwiththisrulegradientdescentdoesn’treproducerealphysicalmotion. Inreal\n",
            "lifeaballhasmomentum, andthatmomentummayallowittorollacrosstheslope, or\n",
            "even(momentarily)rolluphill.\n",
            "It’sonlyaftertheeffectsoffrictionsetinthattheballis\n",
            "guaranteedtorolldownintothevalley. Bycontrast,ourruleforchoosing∆vjustsays“go\n",
            "down,rightnow”. That’sstillaprettygoodruleforfindingtheminimum! Tomakegradientdescentworkcorrectly,weneedtochoosethelearningrateηtobe\n",
            "smallenoughthatEquation1.9isagoodapproximation. Ifwedon’t,wemightendupwith\n",
            "∆C>0,whichobviouslywouldnotbegood! Atthesametime,wedon’twantηtobetoo\n",
            "small,sincethatwillmakethechanges∆vtiny,andthusthegradientdescentalgorithm\n",
            "willworkveryslowly. Inpracticalimplementations,ηisoftenvariedsothatEquation1.9\n",
            "remainsagoodapproximation,butthealgorithmisn’ttooslow.\n",
            "We’llseelaterhowthis\n",
            "works. I’veexplainedgradientdescentwhenC isafunctionofjusttwovariables. But,infact,\n",
            "everythingworksjustaswellevenwhenC isafunctionofmanymorevariables. Supposein\n",
            "particularthatC isafunctionofmvariables,v ,...,v . Thenthechange∆C inC produced\n",
            "1 m\n",
            "byasmallchange∆v=( ∆v 1 ,...,∆v m) T is\n",
            "∆C C ∆v, (1.12)\n",
            "≈∇ ·\n",
            "wherethegradient C isthevector\n",
            "∇\n",
            "(cid:129)∂C ∂C (cid:139)T\n",
            "C ,..., . (1.13)\n",
            "∇ ≡\n",
            "∂v\n",
            "1\n",
            "∂v\n",
            "m\n",
            "Justasforthetwovariablecase,wecanchoose\n",
            "∆v= η C, (1.14)\n",
            "− ∇\n",
            "andwe’reguaranteedthatour(approximate)expression1.12for∆C willbenegative. This\n",
            "givesusawayoffollowingthegradienttoaminimum,evenwhenC isafunctionofmany\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 21\n",
            "(cid:12)\n",
            "variables,byrepeatedlyapplyingtheupdaterule 1\n",
            "v v (cid:48)=v η C. (1.15)\n",
            "→ − ∇\n",
            "Youcanthinkofthisupdateruleasdefiningthegradientdescentalgorithm. Itgivesusa\n",
            "wayofrepeatedlychangingthepositionvinordertofindaminimumofthefunctionC. The\n",
            "ruledoesn’talwayswork–severalthingscangowrongandpreventgradientdescentfrom\n",
            "findingtheglobalminimumofC,apointwe’llreturntoexploreinlaterchapters. But,in\n",
            "practicegradientdescentoftenworksextremelywell,andinneuralnetworkswe’llfindthat\n",
            "it’sapowerfulwayofminimizingthecostfunction,andsohelpingthenetlearn. Indeed,there’sevenasenseinwhichgradientdescentistheoptimalstrategyforsearching\n",
            "foraminimum. Let’ssupposethatwe’retryingtomakeamove∆v inpositionsoasto\n",
            "decrease C asmuchaspossible. Thisisequivalenttominimizing∆C C ∆v. We’ll\n",
            "constrainthesizeofthemovesothat ∆v = εforsomesmallfixedε>≈0.∇Ino·therwords,\n",
            "wewantamovethatisasmallstepof(cid:107)afi(cid:107)xedsize,andwe’retryingtofindthemovement\n",
            "directionwhichdecreasesC asmuchaspossible. Itcanbeprovedthatthechoiceof∆v\n",
            "whichminimizes C ∆v is∆v = η C,whereη = ε/ C isdeterminedbythesize\n",
            "constraint ∆v = ∇ε. ·Sogradientde−sce∇ntcanbevieweda(cid:107)s∇aw(cid:107)ayoftakingsmallstepsin\n",
            "thedirectio(cid:107)nwh(cid:107)ichdoesthemosttoimmediatelydecreaseC.\n",
            "Exercises\n",
            "Provetheassertionofthelastparagraph. Hint: Ifyou’renotalreadyfamiliarwiththe\n",
            "• Cauchy-Schwarzinequality,youmayfindithelpfultofamiliarizeyourselfwithit. Iexplainedgradientdescentwhen C isafunctionoftwovariables,andwhenit’s\n",
            "• afunctionofmorethantwovariables.\n",
            "WhathappenswhenC isafunctionofjust\n",
            "onevariable? Canyouprovideageometricinterpretationofwhatgradientdescentis\n",
            "doingintheone-dimensionalcase? People have investigated many variations of gradient descent, including variations\n",
            "thatmorecloselymimicarealphysicalball. Theseball-mimickingvariationshavesome\n",
            "advantages,butalsohaveamajordisadvantage: itturnsouttobenecessarytocompute\n",
            "secondpartialderivativesofC,andthiscanbequitecostly. Toseewhyit’scostly,suppose\n",
            "wewanttocomputeallthesecondpartialderivatives∂2C/∂v∂v . Ifthereareamillion\n",
            "j k\n",
            "suchv variablesthenwe’dneedtocomputesomethinglikeatrillion(i.e.,amillionsquared)\n",
            "j\n",
            "secondpartialderivatives4! That’sgoingtobecomputationallycostly.\n",
            "Withthatsaid,there\n",
            "aretricksforavoidingthiskindofproblem,andfindingalternativestogradientdescentis\n",
            "anactiveareaofinvestigation. Butinthisbookwe’llusegradientdescent(andvariations)\n",
            "asourmainapproachtolearninginneuralnetworks. Howcanweapplygradientdescenttolearninaneuralnetwork? Theideaistouse\n",
            "gradientdescenttofindtheweightsw andbiases b whichminimizethecostinEquation\n",
            "k l\n",
            "1.6. Toseehowthisworks,let’srestatethegradientdescentupdaterule,withtheweights\n",
            "andbiasesreplacingthevariablesv. Inotherwords,our“position”nowhascomponentsw\n",
            "j k\n",
            "and b,andthegradientvector C hascorrespondingcomponents∂C/∂w and∂C/∂b. l k l\n",
            "∇\n",
            "4Actually,morelikehalfatrillion,since∂2C/∂v j ∂v k= ∂2C/∂v k ∂v j .Still,yougetthepoint. \n",
            "(cid:12)\n",
            "22 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 Writingoutthegradientdescentupdateruleintermsofcomponents,wehave\n",
            "∂C\n",
            "w\n",
            "k\n",
            "→\n",
            "w (cid:48)k=w\n",
            "k\n",
            "−\n",
            "η\n",
            "∂w\n",
            "k\n",
            "(1.16)\n",
            "∂C\n",
            "b\n",
            "l\n",
            "→\n",
            "b l(cid:48)=b\n",
            "l\n",
            "−\n",
            "η\n",
            "∂b\n",
            "l\n",
            ". (1.17)\n",
            "Byrepeatedlyapplyingthisupdaterulewecan“rolldownthehill”,andhopefullyfinda\n",
            "minimumofthecostfunction. Inotherwords,thisisarulewhichcanbeusedtolearnina\n",
            "neuralnetwork. There are a number of challenges in applying the gradient descent rule. We’ll look\n",
            "into those in depth in later chapters.\n",
            "But for now I just want to mention one problem. Tounderstandwhattheproblemis,let’slookbackatthequadraticcostinEquation1.6. NoticethatthiscostfunctionhastheformC= 1\n",
            "n\n",
            "(cid:80)\n",
            "x\n",
            "C\n",
            "x\n",
            ",thatis,it’sanaverageovercosts\n",
            "C y(x) a 2 forindividualtrainingexamples. Inpractice,tocomputethegradient C we\n",
            "x (cid:107) 2− (cid:107)\n",
            "nee≡dtocomputethegradients C separatelyforeachtraininginput,x,andthena∇verage\n",
            "x\n",
            "them, C= 1\n",
            "n\n",
            "(cid:80)\n",
            "x\n",
            "C\n",
            "x\n",
            ". Unfortu∇nately,whenthenumberoftraininginputsisverylargethis\n",
            "cantak∇ealongtim∇e,andlearningthusoccursslowly. Anideacalledstochasticgradientdescentcanbeusedtospeeduplearning. Theidea\n",
            "istoestimatethegradient C bycomputing C forasmallsampleofrandomlychosen\n",
            "x\n",
            "traininginputs. Byaveragin∇goverthissmalls∇ampleitturnsoutthatwecanquicklygeta\n",
            "goodestimateofthetruegradient C,andthishelpsspeedupgradientdescent,andthus\n",
            "learning. ∇\n",
            "Tomaketheseideasmoreprecise,stochasticgradientdescentworksbyrandomlypicking\n",
            "outasmallnumbermofrandomlychosentraininginputs.\n",
            "We’lllabelthoserandomtraining\n",
            "inputsX ,X ,...,X ,andrefertothemasamini-batch. Providedthesamplesizemislarge\n",
            "1 2 m\n",
            "enoughweexpectthattheaveragevalueofthe C willberoughlyequaltotheaverage\n",
            "Xj\n",
            "overall C ,thatis, ∇\n",
            "∇ x (cid:80)m j=1 ∇ C Xj (cid:80) x ∇ C x = C, (1.18)\n",
            "m ≈ n ∇\n",
            "wherethesecondsumisovertheentiresetoftrainingdata. Swappingsidesweget\n",
            "1 (cid:88) m\n",
            "C C , (1.19)\n",
            "∇ ≈ m j=1∇ Xj\n",
            "confirmingthatwecanestimatetheoverallgradientbycomputinggradientsjustforthe\n",
            "randomlychosenmini-batch. Toconnectthisexplicitlytolearninginneuralnetworks,supposewkandbldenotethe\n",
            "weightsandbiasesinourneuralnetwork. Thenstochasticgradientdescentworksbypicking\n",
            "outarandomlychosenmini-batchoftraininginputs,andtrainingwiththose,\n",
            "w\n",
            "k →\n",
            "w (cid:48)k=w\n",
            "k − m\n",
            "η (cid:88)\n",
            "j\n",
            "∂\n",
            "∂\n",
            "C\n",
            "w\n",
            "X\n",
            "k\n",
            "j (1.20)\n",
            "b\n",
            "l →\n",
            "b l(cid:48)=b\n",
            "l − m\n",
            "η (cid:88)\n",
            "j\n",
            "∂\n",
            "∂\n",
            "C\n",
            "b\n",
            "X\n",
            "l\n",
            "j, (1.21)\n",
            "\n",
            "(cid:12)\n",
            "1.5. Learningwithgradientdescent (cid:12) 23\n",
            "(cid:12)\n",
            "wherethesumsareoverallthetrainingexamplesX inthecurrentmini-batch. Thenwe 1\n",
            "j\n",
            "pickoutanotherrandomlychosenmini-batchandtrainwiththose. Andsoon,untilwe’ve\n",
            "exhaustedthetraininginputs,whichissaidtocompleteanepochoftraining. Atthatpoint\n",
            "westartoverwithanewtrainingepoch. Incidentally,it’sworthnotingthatconventionsvaryaboutscalingofthecostfunctionand\n",
            "ofmini-batchupdatestotheweightsandbiases. InEquation1.6wescaledtheoverallcost\n",
            "functionbyafactor 1. Peoplesometimesomitthe 1,summingoverthecostsofindividual\n",
            "n n\n",
            "trainingexamplesinsteadofaveraging. Thisisparticularlyusefulwhenthetotalnumber\n",
            "oftrainingexamplesisn’tknowninadvance. Thiscanoccurifmoretrainingdataisbeing\n",
            "generatedinrealtime,forinstance. And,inasimilarway,themini-batchupdaterules1.20\n",
            "and1.21sometimesomitthe 1 termoutthefrontofthesums. Conceptuallythismakeslittle\n",
            "m\n",
            "difference,sinceit’sequivalenttorescalingthelearningrateη. Butwhendoingdetailed\n",
            "comparisonsofdifferentworkit’sworthwatchingoutfor. Wecanthinkofstochasticgradientdescentasbeinglikepoliticalpolling: it’smucheasier\n",
            "tosampleasmallmini-batchthanitistoapplygradientdescenttothefullbatch,justas\n",
            "carryingoutapolliseasierthanrunningafullelection. Forexample,ifwehaveatraining\n",
            "setofsizen=60,000,asinMNIST,andchooseamini-batchsizeof(say)m=10,thismeans\n",
            "we’llgetafactorof6,000speedupinestimatingthegradient! Ofcourse,theestimatewon’t\n",
            "beperfect–therewillbestatisticalfluctuations–butitdoesn’tneedtobeperfect: allwe\n",
            "reallycareaboutismovinginageneraldirectionthatwillhelpdecreaseC,andthatmeans\n",
            "wedon’tneedanexactcomputationofthegradient. Inpractice,stochasticgradientdescent\n",
            "isacommonlyusedandpowerfultechniqueforlearninginneuralnetworks,andit’sthe\n",
            "basisformostofthelearningtechniqueswe’lldevelopinthisbook. Exercize\n",
            "Anextremeversionofgradientdescentistouseamini-batchsizeofjust1. Thatis,\n",
            "• givenatraininginput, x,weupdateourweightsandbiasesaccordingtotherules\n",
            "w\n",
            "k\n",
            "w (cid:48)k=w\n",
            "k\n",
            "η∂C\n",
            "x\n",
            "/∂w\n",
            "k\n",
            "and b\n",
            "l\n",
            "b l(cid:48)=b\n",
            "l\n",
            "η∂C\n",
            "x\n",
            "/∂b\n",
            "l\n",
            ".\n",
            "Thenwechooseanother\n",
            "trai→ninginput,−andupdatetheweigh→tsandbia−sesagain.\n",
            "Andsoon,repeatedly. This\n",
            "procedureisknownasonline,on-line,orincrementallearning. Inonlinelearning,a\n",
            "neuralnetworklearnsfromjustonetraininginputatatime(justashumanbeings\n",
            "do). Nameoneadvantageandonedisadvantageofonlinelearning,comparedto\n",
            "stochasticgradientdescentwithamini-batchsizeof,say,20. Letmeconcludethissectionbydiscussingapointthatsometimesbugspeoplenewtogradient\n",
            "descent. InneuralnetworksthecostC is,ofcourse,afunctionofmanyvariables–allthe\n",
            "weights and biases – andso in some sense defines a surface in avery high-dimensional\n",
            "space. Somepeoplegethungupthinking: “Hey, Ihavetobeabletovisualizeallthese\n",
            "extra dimensions”.\n",
            "And they may start to worry: “I can’t think in four dimensions, let\n",
            "alonefive(orfivemillion)”. Istheresomespecialabilitythey’remissing,someabilitythat\n",
            "“real” supermathematicians have? Of course, the answer is no.\n",
            "Even most professional\n",
            "mathematicianscan’tvisualizefourdimensionsespeciallywell,ifatall. Thetricktheyuse,\n",
            "instead,istodevelopotherwaysofrepresentingwhat’sgoingon. That’sexactlywhatwe\n",
            "didabove: weusedanalgebraic(ratherthanvisual)representationof∆C tofigureouthow\n",
            "tomovesoastodecreaseC. Peoplewhoaregoodatthinkinginhighdimensionshavea\n",
            "mentallibrarycontainingmanydifferenttechniquesalongtheselines;ouralgebraictrickis\n",
            "justoneexample. Thosetechniquesmaynothavethesimplicitywe’reaccustomedtowhen\n",
            "visualizingthreedimensions,butonceyoubuildupalibraryofsuchtechniques,youcanget\n",
            "prettygoodatthinkinginhighdimensions. Iwon’tgointomoredetailhere,butifyou’re\n",
            "\n",
            "(cid:12)\n",
            "24 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 interestedthenyoumayenjoyreadingthisdiscussionofsomeofthetechniquesprofessional\n",
            "mathematiciansusetothinkinhighdimensions. Whilesomeofthetechniquesdiscussedare\n",
            "quitecomplex,muchofthebestcontentisintuitiveandaccessible,andcouldbemastered\n",
            "byanyone. 1.6 Implementing our network to classify digits\n",
            "Alright,let’swriteaprogramthatlearnshowtorecognizehandwrittendigits,usingstochastic\n",
            "gradientdescentandtheMNISTtrainingdata.We’lldothiswithashortPython(2.7)program,\n",
            "just74linesofcode! ThefirstthingweneedistogettheMNISTdata. Ifyou’reagituser\n",
            "thenyoucanobtainthedatabycloningthecoderepositoryforthisbook,\n",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
            "Ifyoudon’tusegitthenyoucandownloadthedataandcodehere. Incidentally,whenIdescribedtheMNISTdataearlier,Isaiditwassplitinto60,000\n",
            "trainingimages,and10,000testimages. That’stheofficialMNISTdescription. Actually,\n",
            "we’regoingtosplitthedataalittledifferently. We’llleavethetestimagesasis,butsplitthe\n",
            "60,000-imageMNISTtrainingsetintotwoparts: asetof50,000images,whichwe’lluse\n",
            "totrainourneuralnetwork,andaseparate10,000imagevalidationset. Wewon’tusethe\n",
            "validationdatainthischapter,butlaterinthebookwe’llfinditusefulinfiguringouthowto\n",
            "setcertainhyper-parametersoftheneuralnetwork–thingslikethelearningrate,andsoon,\n",
            "whicharen’tdirectlyselectedbyourlearningalgorithm. Althoughthevalidationdataisn’t\n",
            "partoftheoriginalMNISTspecification,manypeopleuseMNISTinthisfashion,andthe\n",
            "useofvalidationdataiscommoninneuralnetworks. WhenIrefertothe“MNISTtraining\n",
            "data”fromnowon,I’llbereferringtoour50,000imagedataset,nottheoriginal60,000\n",
            "imagedataset5. ApartfromtheMNISTdatawealsoneedaPythonlibrarycalledNumpy,fordoingfast\n",
            "linearalgebra. Ifyoudon’talreadyhaveNumpyinstalled,youcangetithere.\n",
            "Letmeexplainthecorefeaturesoftheneuralnetworkscode,beforegivingafulllisting,\n",
            "below.\n",
            "ThecenterpieceisaNetworkclass, whichweusetorepresentaneuralnetwork. Here’sthecodeweusetoinitializeaNetworkobject:\n",
            "class Network(object):\n",
            "def __init__(self, sizes):\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
            "Inthiscode,thelistsizescontainsthenumberofneuronsintherespectivelayers. So,for\n",
            "example,ifwewanttocreateaNetworkobjectwith2neuronsinthefirstlayer,3neurons\n",
            "inthesecondlayer,and1neuroninthefinallayer,we’ddothiswiththecode:\n",
            "5Asnotedearlier,theMNISTdatasetisbasedontwodatasetscollectedbyNIST,theUnitedStates’\n",
            "NationalInstituteofStandardsandTechnology.ToconstructMNISTtheNISTdatasetswerestripped\n",
            "downandputintoamoreconvenientformatbyYannLeCun,CorinnaCortes,andChristopherJ.C. Burges.Seethislinkformoredetails.Thedatasetinmyrepositoryisinaformthatmakesiteasyto\n",
            "loadandmanipulatetheMNISTdatainPython. Iobtainedthisparticularformofthedatafromthe\n",
            "LISAmachinelearninglaboratoryattheUniversityofMontreal(link). \n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 25\n",
            "(cid:12)\n",
            "1\n",
            "net = Network([2, 3, 1])\n",
            "ThebiasesandweightsintheNetworkobjectareallinitializedrandomly,usingtheNumpy\n",
            "np.random.randnfunctiontogenerateGaussiandistributionswithmean0andstandard\n",
            "deviation1. Thisrandominitializationgivesourstochasticgradientdescentalgorithma\n",
            "placetostartfrom.\n",
            "Inlaterchapterswe’llfindbetterwaysofinitializingtheweightsand\n",
            "biases,butthiswilldofornow. NotethattheNetworkinitializationcodeassumesthatthe\n",
            "firstlayerofneuronsisaninputlayer,andomitstosetanybiasesforthoseneurons,since\n",
            "biasesareonlyeverusedincomputingtheoutputsfromlaterlayers. NotealsothatthebiasesandweightsarestoredaslistsofNumpymatrices. So, for\n",
            "examplenet.weights[1]isaNumpymatrixstoringtheweightsconnectingthesecondand\n",
            "thirdlayersofneurons. (It’snotthefirstandsecondlayers,sincePython’slistindexingstarts\n",
            "at0.) Sincenet.weights[1]isratherverbose,let’sjustdenotethatmatrixw. It’samatrix\n",
            "suchthatw istheweightfortheconnectionbetweenthek-thneuroninthesecondlayer,\n",
            "jk\n",
            "andthe j-thneuroninthethirdlayer. Thisorderingofthe jandkindicesmayseemstrange–\n",
            "surelyit’dmakemoresensetoswapthe jandkindicesaround? Thebigadvantageofusing\n",
            "thisorderingisthatitmeansthatthevectorofactivationsofthethirdlayerofneuronsis:\n",
            "a (cid:48)= σ (wa+b). (1.22)\n",
            "There’squiteabitgoingoninthisequation,solet’sunpackitpiecebypiece. aisthevector\n",
            "ofactivationsofthesecondlayerofneurons.\n",
            "Toobtaina wemultiplyabytheweightmatrix\n",
            "(cid:48)\n",
            "w,andaddthevector bofbiases. Wethenapplythefunctionσelementwisetoeveryentry\n",
            "inthevector wa+b6. It’seasytoverifythatEquation1.22givesthesameresultasour\n",
            "earlierrule,Equation1.4,forcomputingtheoutputofasigmoidneuron. Exercise\n",
            "WriteoutEquation1.22incomponentform,andverifythatitgivesthesameresult\n",
            "• astherule1.4forcomputingtheoutputofasigmoidneuron. Withallthisinmind,it’seasytowritecodecomputingtheoutputfromaNetworkinstance. Webeginbydefiningthesigmoidfunction:\n",
            "def sigmoid(z):\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "NotethatwhentheinputzisavectororNumpyarray,Numpyautomaticallyappliesthe\n",
            "functionsigmoidelementwise,thatis,invectorizedform. WethenaddafeedforwardmethodtotheNetworkclass,which,givenaninputafor\n",
            "thenetwork,returnsthecorrespondingoutput7. AllthemethoddoesisappliesEquation\n",
            "1.22foreachlayer:\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "6Thisiscalledvectorizingthefunctionσ. 7Itisassumedthattheinputaisan(n, 1)Numpyndarray,nota(n,)vector.Here,nisthenumber\n",
            "ofinputstothenetwork.Ifyoutrytousean(n,)vectorasinputyou’llgetstrangeresults.Although\n",
            "usingan(n,)vectorappearsthemorenaturalchoice,usingan(n, 1)ndarraymakesitparticularly\n",
            "easytomodifythecodetofeedforwardmultipleinputsatonce,andthatissometimesconvenient.\n",
            "\n",
            "(cid:12)\n",
            "26 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 return a\n",
            "Ofcourse,themainthingwewantourNetworkobjectstodoistolearn. Tothatend\n",
            "we’llgivethemanSGDmethodwhichimplementsstochasticgradientdescent.\n",
            "Here’sthe\n",
            "code.\n",
            "It’salittlemysteriousinafewplaces,butI’llbreakitdownbelow,afterthelisting. def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
            "\"\"\"Train the neural network using mini-batch stochastic gradient descent. The\n",
            "\"training_data\" is a list of tuples \"(x, y)\" representing the training\n",
            "inputs and the desired outputs. The other non-optional parameters are self-\n",
            "explanatory. If \"test_data\" is provided then the network will be evaluated\n",
            "against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially. \"\"\"\n",
            "if test_data:\n",
            "n_test = len(test_data)\n",
            "n = len(training_data)\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [training_data[k:k+mini_batch_size] for k in xrange(0, n,\n",
            "mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(mini_batch, eta)\n",
            "if test_data:\n",
            "print \"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test)\n",
            "else:\n",
            "print \"Epoch {0} complete\".format(j)\n",
            "Thetraining_dataisalistoftuples(x, y)representingthetraininginputsandcorre-\n",
            "sponding desiredoutputs. The variablesepochsandmini_batch_sizeare whatyou’d\n",
            "expect–thenumberofepochstotrainfor,andthesizeofthemini-batchestousewhen\n",
            "sampling. etaisthelearningrate,η. Iftheoptionalargumenttest_dataissupplied,then\n",
            "theprogramwillevaluatethenetworkaftereachepochoftraining,andprintoutpartial\n",
            "progress. Thisisusefulfortrackingprogress,butslowsthingsdownsubstantially. Thecodeworksasfollows. Ineachepoch,itstartsbyrandomlyshufflingthetraining\n",
            "data,andthenpartitionsitintomini-batchesoftheappropriatesize. Thisisaneasywayof\n",
            "samplingrandomlyfromthetrainingdata. Thenforeachmini_batchweapplyasingle\n",
            "stepofgradientdescent. Thisisdonebythecodeself.update_mini_batch(mini_batch\n",
            ", eta), which updates the network weights and biases according to a single iteration\n",
            "ofgradientdescent,usingjustthetrainingdatainmini_batch. Here’sthecodeforthe\n",
            "update_mini_batchmethod:\n",
            "def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the network’s weights and biases by applying gradient descent using\n",
            "backpropagation to a single mini batch. The \"mini_batch\" is a list of tuples\n",
            "\"(x, y)\", and \"eta\" is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights,\n",
            "nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b\n",
            ")]\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 27\n",
            "(cid:12)\n",
            "1\n",
            "Mostoftheworkisdonebytheline\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "Thisinvokessomethingcalledthebackpropagationalgorithm,whichisafastwayofcomput-\n",
            "ingthegradientofthecostfunction. Soupdate_mini_batchworkssimplybycomputing\n",
            "these gradients for every training example in the mini_batch, and then updating self\n",
            ".weightsandself.biasesappropriately. I’mnotgoingtoshowthecodeforself.backproprightnow. We’llstudyhowback-\n",
            "propagationworksinthenextchapter,includingthecodeforself.backprop. Fornow,just\n",
            "assumethatitbehavesasclaimed,returningtheappropriategradientforthecostassociated\n",
            "tothetrainingexamplex. Let’slookatthefullprogram, includingthedocumentationstrings, whichIomitted\n",
            "above. Apart from self.backprop the program is self-explanatory – all the heavy lift-\n",
            "ingisdoneinself.SGDandself.update_mini_batch,whichwe’vealreadydiscussed. Theself.backpropmethodmakesuseofafewextrafunctionstohelpincomputingthe\n",
            "gradient,namelysigmoid_prime,whichcomputesthederivativeoftheσfunction,and\n",
            "self.cost_derivative,whichIwon’tdescribehere. Youcangetthegistofthese(and\n",
            "perhapsthedetails)justbylookingatthecodeanddocumentationstrings.\n",
            "We’lllookat\n",
            "themindetailinthenextchapter. Notethatwhiletheprogramappearslengthy,muchofthe\n",
            "codeisdocumentationstringsintendedtomakethecodeeasytounderstand. Infact,the\n",
            "programcontainsjust74linesofnon-whitespace,non-commentcode. Allthecodemaybe\n",
            "foundonGitHubhere. \"\"\"\n",
            "network.py\n",
            "~~~~~~~~~~\n",
            "A module to implement the stochastic gradient descent learning\n",
            "algorithm for a feedforward neural network. Gradients are calculated\n",
            "using backpropagation. Note that I have focused on making the code\n",
            "simple, easily readable, and easily modifiable.\n",
            "It is not optimized,\n",
            "and omits many desirable features. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import random\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "class Network(object):\n",
            "def __init__(self, sizes):\n",
            "\"\"\"The list ‘‘sizes‘‘ contains the number of neurons in the\n",
            "respective layers of the network. For example, if the list\n",
            "was [2, 3, 1] then it would be a three-layer network, with the\n",
            "first layer containing 2 neurons, the second layer 3 neurons,\n",
            "and the third layer 1 neuron. The biases and weights for the\n",
            "network are initialized randomly, using a Gaussian\n",
            "distribution with mean 0, and variance 1. Note that the first\n",
            "layer is assumed to be an input layer, and by convention we\n",
            "won’t set any biases for those neurons, since biases are only\n",
            "ever used in computing the outputs from later layers.\"\"\"\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
            "\n",
            "(cid:12)\n",
            "28 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "return a\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
            "\"\"\"Train the neural network using mini-batch stochastic\n",
            "gradient descent. The ‘‘training_data‘‘ is a list of tuples\n",
            "‘‘(x, y)‘‘ representing the training inputs and the desired\n",
            "outputs. The other non-optional parameters are\n",
            "self-explanatory. If ‘‘test_data‘‘ is provided then the\n",
            "network will be evaluated against the test data after each\n",
            "epoch, and partial progress printed out. This is useful for\n",
            "tracking progress, but slows things down substantially.\"\"\"\n",
            "if test_data:\n",
            "n_test = len(test_data)\n",
            "n = len(training_data)\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [\n",
            "training_data[k:k+mini_batch_size]\n",
            "for k in xrange(0, n, mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(mini_batch, eta)\n",
            "if test_data:\n",
            "print \"Epoch {0}: {1} / {2}\".format(\n",
            "j, self.evaluate(test_data), n_test)\n",
            "else:\n",
            "print \"Epoch {0} complete\".format(j)\n",
            "def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the network’s weights and biases by applying\n",
            "gradient descent using backpropagation to a single mini batch. The ‘‘mini_batch‘‘ is a list of tuples ‘‘(x, y)‘‘, and ‘‘eta‘‘\n",
            "is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "def backprop(self, x, y):\n",
            "\"\"\"Return a tuple ‘‘(nabla_b, nabla_w)‘‘ representing the\n",
            "gradient for the cost function C_x.\n",
            "‘‘nabla_b‘‘ and\n",
            "‘‘nabla_w‘‘ are layer-by-layer lists of numpy arrays, similar\n",
            "to ‘‘self.biases‘‘ and ‘‘self.weights‘‘.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 29\n",
            "(cid:12)\n",
            "zs.append(z) 1\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. It’s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "def evaluate(self, test_data):\n",
            "\"\"\"Return the number of test inputs for which the neural\n",
            "network outputs the correct result. Note that the neural\n",
            "network’s output is assumed to be the index of whichever\n",
            "neuron in the final layer has the highest activation.\"\"\"\n",
            "test_results = [(np.argmax(self.feedforward(x)), y)\n",
            "for (x, y) in test_data]\n",
            "return sum(int(x == y) for (x, y) in test_results)\n",
            "def cost_derivative(self, output_activations, y):\n",
            "\"\"\"Return the vector of partial derivatives \\partial C_x /\n",
            "\\partial a for the output activations.\"\"\"\n",
            "return (output_activations-y)\n",
            "#### Miscellaneous functions\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "Howwelldoestheprogramrecognizehandwrittendigits? Well,let’sstartbyloadinginthe\n",
            "MNISTdata.\n",
            "I’lldothisusingalittlehelperprogram,mnist_loader.py,tobedescribed\n",
            "below. WeexecutethefollowingcommandsinaPythonshell,\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            "Ofcourse,thiscouldalsobedoneinaseparatePythonprogram,butifyou’refollowing\n",
            "alongit’sprobablyeasiesttodoinaPythonshell. AfterloadingtheMNISTdata,we’llsetupaNetworkwith30hiddenneurons. Wedo\n",
            "thisafterimportingthePythonprogramlistedabove,whichisnamednetwork,\n",
            ">>> import network\n",
            ">>> net = network.Network([784, 30, 10])\n",
            "\n",
            "(cid:12)\n",
            "30 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "Finally,we’llusestochasticgradientdescenttolearnfromtheMNISTtraining_dataover\n",
            "30epochs,withamini-batchsizeof10,andalearningrateofη =3.0,\n",
            ">>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
            "Notethatifyou’rerunningthecodeasyoureadalong,itwilltakesometimetoexecute–\n",
            "foratypicalmachine(asof2015)itwilllikelytakeafewminutestorun. Isuggestyouset\n",
            "thingsrunning,continuetoread,andperiodicallychecktheoutputfromthecode.\n",
            "Ifyou’re\n",
            "inarushyoucanspeedthingsupbydecreasingthenumberofepochs,bydecreasingthe\n",
            "numberofhiddenneurons,orbyusingonlypartofthetrainingdata. Notethatproduction\n",
            "codewouldbemuch,muchfaster: thesePythonscriptsareintendedtohelpyouunderstand\n",
            "howneuralnetswork,nottobehigh-performancecode! And,ofcourse,oncewe’vetrained\n",
            "anetworkitcanberunveryquicklyindeed,onalmostanycomputingplatform. Forexample,\n",
            "oncewe’velearnedagoodsetofweightsandbiasesforanetwork,itcaneasilybeported\n",
            "toruninJavascriptinawebbrowser,orasanativeapponamobiledevice. Inanycase,\n",
            "hereisapartialtranscriptoftheoutputofonetrainingrunoftheneuralnetwork. The\n",
            "transcriptshowsthenumberoftestimagescorrectlyrecognizedbytheneuralnetworkafter\n",
            "eachepochoftraining. Asyoucansee,afterjustasingleepochthishasreached9,129out\n",
            "of10,000,andthenumbercontinuestogrow,\n",
            "Epoch 0: 9129 / 10000\n",
            "Epoch 1: 9295 / 10000\n",
            "Epoch 2: 9348 / 10000\n",
            "... Epoch 27: 9528 / 10000\n",
            "Epoch 28: 9542 / 10000\n",
            "Epoch 29: 9534 / 10000\n",
            "Thatis,thetrainednetworkgivesusaclassificationrateofabout95percent–95.42percent\n",
            "atitspeak(“Epoch28”)! That’squiteencouragingasafirstattempt. Ishouldwarnyou,\n",
            "however,thatifyourunthecodethenyourresultsarenotnecessarilygoingtobequitethe\n",
            "sameasmine,sincewe’llbeinitializingournetworkusing(different)randomweightsand\n",
            "biases. TogenerateresultsinthischapterI’vetakenbest-of-threeruns. Let’sreruntheaboveexperiment,changingthenumberofhiddenneuronsto100.\n",
            "As\n",
            "wasthecaseearlier,ifyou’rerunningthecodeasyoureadalong,youshouldbewarnedthat\n",
            "ittakesquiteawhiletoexecute(onmymachinethisexperimenttakestensofsecondsfor\n",
            "eachtrainingepoch),soit’swisetocontinuereadinginparallelwhilethecodeexecutes. >>> net = network.Network([784, 100, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
            "Sureenough,thisimprovestheresultsto96.59percent. Atleastinthiscase,usingmore\n",
            "hiddenneuronshelpsusgetbetterresults8\n",
            "Ofcourse,toobtaintheseaccuraciesIhadtomakespecificchoicesforthenumberof\n",
            "epochsoftraining,themini-batchsize,andthelearningrate,η. AsImentionedabove,these\n",
            "areknownashyper-parametersforourneuralnetwork,inordertodistinguishthemfrom\n",
            "theparameters(weightsandbiases)learntbyourlearningalgorithm. Ifwechooseour\n",
            "8Readerfeedbackindicatesquitesomevariationinresultsforthisexperiment,andsometraining\n",
            "runsgiveresultsquiteabitworse.Usingthetechniquesintroducedinchapter3willgreatlyreducethe\n",
            "variationinperformanceacrossdifferenttrainingrunsforournetworks. \n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 31\n",
            "(cid:12)\n",
            "hyper-parameterspoorly,wecangetbadresults. Suppose,forexample,thatwe’dchosen 1\n",
            "thelearningratetobeη =0.001,\n",
            ">>> net = network.Network([784, 100, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.001, test_data=test_data)\n",
            "Theresultsaremuchlessencouraging,\n",
            "Epoch 0: 1139 / 10000\n",
            "Epoch 1: 1136 / 10000\n",
            "Epoch 2: 1135 / 10000\n",
            "... Epoch 27: 2101 / 10000\n",
            "Epoch 28: 2123 / 10000\n",
            "Epoch 29: 2142 / 10000\n",
            "However,youcanseethattheperformanceofthenetworkisgettingslowlybetterovertime. Thatsuggestsincreasingthelearningrate,saytoη =0.01.\n",
            "Ifwedothat,wegetbetter\n",
            "results,whichsuggestsincreasingthelearningrateagain. (Ifmakingachangeimproves\n",
            "things, try doing more!) If we do that several times over, we’ll end up with a learning\n",
            "rateofsomethinglikeη =1.0(andperhapsfinetuneto3.0),whichisclosetoourearlier\n",
            "experiments. Soeventhoughweinitiallymadeapoorchoiceofhyper-parameters,weat\n",
            "leastgotenoughinformationtohelpusimproveourchoiceofhyper-parameters. Ingeneral,\n",
            "debugginganeuralnetworkcanbechallenging.\n",
            "Thisisespeciallytruewhentheinitial\n",
            "choiceofhyper-parametersproducesresultsnobetterthanrandomnoise. Supposewetry\n",
            "thesuccessful30hiddenneuronnetworkarchitecturefromearlier,butwiththelearning\n",
            "ratechangedtoη =100.0:\n",
            ">>> net = network.Network([784, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 100.0, test_data=test_data)\n",
            "Atthispointwe’veactuallygonetoofar,andthelearningrateistoohigh:\n",
            "Epoch 0: 1009 / 10000\n",
            "Epoch 1: 1009 / 10000\n",
            "Epoch 2: 1009 / 10000\n",
            "Epoch 3: 1009 / 10000\n",
            "... Epoch 27: 982 / 10000\n",
            "Epoch 28: 982 / 10000\n",
            "Epoch 29: 982 / 10000\n",
            "Nowimaginethatwewerecomingtothisproblemforthefirsttime.\n",
            "Ofcourse,weknow\n",
            "fromourearlierexperimentsthattherightthingtodoistodecreasethelearningrate. But\n",
            "ifwewerecomingtothisproblemforthefirsttimethentherewouldn’tbemuchinthe\n",
            "outputtoguideusonwhattodo. Wemightworrynotonlyaboutthelearningrate,but\n",
            "abouteveryotheraspectofourneuralnetwork. Wemightwonderifwe’veinitializedthe\n",
            "weightsandbiasesinawaythatmakesithardforthenetworktolearn? Ormaybewedon’t\n",
            "haveenoughtrainingdatatogetmeaningfullearning? Perhapswehaven’trunforenough\n",
            "epochs? Ormaybeit’simpossibleforaneuralnetworkwiththisarchitecturetolearnto\n",
            "recognizehandwrittendigits? Maybethelearningrateistoolow? Or,maybe,thelearning\n",
            "rateistoohigh? Whenyou’recomingtoaproblemforthefirsttime,you’renotalwayssure. Thelessontotakeawayfromthisisthatdebugginganeuralnetworkisnottrivial,and,\n",
            "justasforordinaryprogramming,thereisanarttoit. Youneedtolearnthatartofdebugging\n",
            "\n",
            "(cid:12)\n",
            "32 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 inordertogetgood resultsfromneuralnetworks. More generally, weneedto develop\n",
            "heuristicsforchoosinggoodhyper-parametersandagoodarchitecture.\n",
            "We’lldiscussall\n",
            "theseatlengththroughthebook,includinghowIchosethehyper-parametersabove. Exercise\n",
            "Trycreatinganetworkwithjusttwolayers–aninputandanoutputlayer,nohidden\n",
            "• layer–with784and10neurons,respectively. Trainthenetworkusingstochastic\n",
            "gradientdescent. Whatclassificationaccuracycanyouachieve?\n",
            "Earlier,IskippedoverthedetailsofhowtheMNISTdataisloaded. It’sprettystraightforward.\n",
            "Forcompleteness,here’sthecode. ThedatastructuresusedtostoretheMNISTdataare\n",
            "describedinthedocumentationstrings–it’sstraightforwardstuff,tuplesandlistsofNumpy\n",
            "ndarrayobjects(thinkofthemasvectorsifyou’renotfamiliarwithndarrays):\n",
            "\"\"\"\n",
            "mnist_loader\n",
            "~~~~~~~~~~~~\n",
            "A library to load the MNIST image data. For details of the data\n",
            "structures that are returned, see the doc strings for ‘‘load_data‘‘\n",
            "and ‘‘load_data_wrapper‘‘. In practice, ‘‘load_data_wrapper‘‘ is the\n",
            "function usually called by our neural network code. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import cPickle\n",
            "import gzip\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "def load_data():\n",
            "\"\"\"Return the MNIST data as a tuple containing the training data, the\n",
            "validation data, and the test data. The ‘‘training_data‘‘ is returned as a tuple with two entries. The first entry\n",
            "contains the actual training images. This is a\n",
            "numpy ndarray with 50,000 entries. Each entry is, in turn, a numpy ndarray\n",
            "with 784 values, representing the 28 * 28 = 784\n",
            "pixels in a single MNIST image. The second entry in the ‘‘training_data‘‘ tuple is a numpy ndarray containing\n",
            "50,000 entries. Those entries are just the digit\n",
            "values (0...9) for the corresponding images contained in the first entry of\n",
            "the tuple. The ‘‘validation_data‘‘ and ‘‘test_data‘‘ are similar, except each contains\n",
            "only 10,000 images. This is a nice data format, but for use in neural networks it’s helpful to\n",
            "modify the format of the ‘‘training_data‘‘ a little. That’s done in the wrapper function ‘‘load_data_wrapper()‘‘, see below. \"\"\"\n",
            "f = gzip.open(’../data/mnist.pkl.gz’, ’rb’)\n",
            "training_data, validation_data, test_data = cPickle.load(f)\n",
            "f.close()\n",
            "return (training_data, validation_data, test_data)\n",
            "\n",
            "(cid:12)\n",
            "1.6. Implementingournetworktoclassifydigits (cid:12) 33\n",
            "(cid:12)\n",
            "def load_data_wrapper(): 1\n",
            "\"\"\"Return a tuple containing ‘‘(training_data, validation_data,\n",
            "test_data)‘‘. Based on ‘‘load_data‘‘, but the format is more\n",
            "convenient for use in our implementation of neural networks. In particular, ‘‘training_data‘‘ is a list containing 50,000\n",
            "2-tuples ‘‘(x, y)‘‘. ‘‘x‘‘ is a 784-dimensional numpy.ndarray\n",
            "containing the input image. ‘‘y‘‘ is a 10-dimensional\n",
            "numpy.ndarray representing the unit vector corresponding to the\n",
            "correct digit for ‘‘x‘‘. ‘‘validation_data‘‘ and ‘‘test_data‘‘ are lists containing 10,000\n",
            "2-tuples ‘‘(x, y)‘‘. In each case, ‘‘x‘‘ is a 784-dimensional\n",
            "numpy.ndarry containing the input image, and ‘‘y‘‘ is the\n",
            "corresponding classification, i.e., the digit values (integers)\n",
            "corresponding to ‘‘x‘‘. Obviously, this means we’re using slightly different formats for\n",
            "the training data and the validation / test data. These formats\n",
            "turn out to be the most convenient for use in our neural network\n",
            "code.\"\"\"\n",
            "tr_d, va_d, te_d = load_data()\n",
            "training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
            "training_results = [vectorized_result(y) for y in tr_d[1]]\n",
            "training_data = zip(training_inputs, training_results)\n",
            "validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
            "validation_data = zip(validation_inputs, va_d[1])\n",
            "test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
            "test_data = zip(test_inputs, te_d[1])\n",
            "return (training_data, validation_data, test_data)\n",
            "def vectorized_result(j):\n",
            "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
            "position and zeroes elsewhere. This is used to convert a digit\n",
            "(0...9) into a corresponding desired output from the neural\n",
            "network.\"\"\"\n",
            "e = np.zeros((10, 1))\n",
            "e[j] = 1.0\n",
            "return e\n",
            "Isaidabovethatourprogramgetsprettygoodresults.Whatdoesthatmean?Goodcompared\n",
            "towhat?\n",
            "It’sinformativetohavesomesimple(non-neural-network)baselineteststocompare\n",
            "against,tounderstandwhatitmeanstoperformwell. Thesimplestbaselineofall,ofcourse,\n",
            "istorandomlyguessthedigit.\n",
            "That’llberightabouttenpercentofthetime. We’redoing\n",
            "muchbetterthanthat! Whataboutalesstrivialbaseline? Let’stryanextremelysimpleidea: we’lllookathow\n",
            "darkanimageis. Forinstance,animageofa2willtypicallybequiteabitdarkerthanan\n",
            "imageofa1,justbecausemorepixelsareblackenedout,asthefollowingexamplesillustrate:\n",
            "Thissuggestsusingthetrainingdatatocomputeaveragedarknessesforeachdigit,0,1,2,...,9. Whenpresentedwithanewimage,wecomputehowdarktheimageis,andthenguessthat\n",
            "it’swhicheverdigithastheclosestaveragedarkness. Thisisasimpleprocedure,andiseasy\n",
            "tocodeup,soIwon’texplicitlywriteoutthecode–ifyou’reinterestedit’sintheGitHub\n",
            "\n",
            "(cid:12)\n",
            "34 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 repository. Butit’sabigimprovementoverrandomguessing,getting2,225ofthe10,000\n",
            "testimagescorrect,i.e.,22.25percentaccuracy. It’snotdifficulttofindotherideaswhichachieveaccuraciesinthe20to50percent\n",
            "range. Ifyouworkabitharderyoucangetupover50percent. Buttogetmuchhigher\n",
            "accuraciesithelpstouseestablishedmachinelearningalgorithms. Let’stryusingoneofthe\n",
            "bestknownalgorithms,thesupportvectormachineorSVM.Ifyou’renotfamiliarwithSVMs,\n",
            "nottoworry,we’renotgoingtoneedtounderstandthedetailsofhowSVMswork. Instead,\n",
            "we’lluseaPythonlibrarycalledscikit-learn,whichprovidesasimplePythoninterfacetoa\n",
            "fastC-basedlibraryforSVMsknownasLIBSVM. Ifwerunscikit-learn’sSVMclassifierusingthedefaultsettings,thenitgets9,435of\n",
            "10,000testimagescorrect. (Thecodeisavailablehere.) That’sabigimprovementover\n",
            "ournaiveapproachofclassifyinganimagebasedonhowdarkitis. Indeed,itmeansthat\n",
            "theSVMisperformingroughlyaswellasourneuralnetworks,justalittleworse. Inlater\n",
            "chapterswe’llintroducenewtechniquesthatenableustoimproveourneuralnetworksso\n",
            "thattheyperformmuchbetterthantheSVM. That’snottheendofthestory,however. The9,435of10,000resultisforscikit-learn’s\n",
            "defaultsettingsforSVMs. SVMshaveanumberoftunableparameters,andit’spossibleto\n",
            "searchforparameterswhichimprovethisout-of-the-boxperformance. Iwon’texplicitlydo\n",
            "thissearch,butinsteadreferyoutothisblogpostbyAndreasMüllerifyou’dliketoknow\n",
            "more. MuellershowsthatwithsomeworkoptimizingtheSVM’sparametersit’spossibleto\n",
            "gettheperformanceupabove98.5percentaccuracy. Inotherwords,awell-tunedSVMonly\n",
            "makesanerroronaboutonedigitin70. That’sprettygood! Canneuralnetworksdobetter?\n",
            "Infact,theycan. Atpresent,well-designedneuralnetworksoutperformeveryother\n",
            "techniqueforsolvingMNIST,includingSVMs. Thecurrent(2013)recordisclassifying9,979\n",
            "of10,000imagescorrectly.\n",
            "ThiswasdonebyLiWan,MatthewZeiler,SixinZhang,Yann\n",
            "LeCun,andRobFergus.\n",
            "We’llseemostofthetechniquestheyusedlaterinthebook. At\n",
            "thatleveltheperformanceisclosetohuman-equivalent,andisarguablybetter,sincequitea\n",
            "fewoftheMNISTimagesaredifficultevenforhumanstorecognizewithconfidence,for\n",
            "example:\n",
            "Itrustyou’llagreethatthosearetoughtoclassify! WithimagesliketheseintheMNIST\n",
            "datasetit’sremarkablethatneuralnetworkscanaccuratelyclassifyallbut21ofthe10,000\n",
            "testimages. Usually,whenprogrammingwebelievethatsolvingacomplicatedproblem\n",
            "likerecognizingtheMNISTdigitsrequiresasophisticatedalgorithm. Buteventheneural\n",
            "networksintheWanetalpaperjustmentionedinvolvequitesimplealgorithms,variations\n",
            "onthealgorithmwe’veseeninthischapter. Allthecomplexityislearned,automatically,\n",
            "fromthetrainingdata. Insomesense, themoralofbothourresultsandthoseinmore\n",
            "sophisticatedpapers,isthatforsomeproblems:\n",
            "sophisticatedalgorithm simplelearningalgorithm+goodtrainingdata. ≤\n",
            "\n",
            "(cid:12)\n",
            "1.7.\n",
            "Towarddeeplearning (cid:12) 35\n",
            "(cid:12)\n",
            "1\n",
            "Figure1.1:Credits:1.EsterInbar.2.Unknown.3.NASA,ESA,G.Illingworth,D.Magee,andP.Oesch(University\n",
            "ofCalifornia,SantaCruz),R.Bouwens(LeidenUniversity),andtheHUDF09Team. 1.7 Toward deep learning\n",
            "Whileourneuralnetworkgivesimpressiveperformance, thatperformanceissomewhat\n",
            "mysterious. Theweightsandbiasesinthenetworkwerediscoveredautomatically. Andthat\n",
            "meanswedon’timmediatelyhaveanexplanationofhowthenetworkdoeswhatitdoes. Canwefindsomewaytounderstandtheprinciplesbywhichournetworkisclassifying\n",
            "handwrittendigits? And,givensuchprinciples,canwedobetter? Toputthesequestionsmorestarkly,supposethatafewdecadeshenceneuralnetworks\n",
            "leadtoartificialintelligence(AI).Willweunderstandhowsuchintelligentnetworkswork? Perhapsthenetworkswillbeopaquetous,withweightsandbiaseswedon’tunderstand,\n",
            "becausethey’vebeenlearnedautomatically. IntheearlydaysofAIresearchpeoplehoped\n",
            "thattheefforttobuildanAIwouldalsohelpusunderstandtheprinciplesbehindintelligence\n",
            "and,maybe,thefunctioningofthehumanbrain. Butperhapstheoutcomewillbethatwe\n",
            "endupunderstandingneitherthebrainnorhowartificialintelligenceworks! Toaddressthesequestions,let’sthinkbacktotheinterpretationofartificialneuronsthat\n",
            "Igaveatthestartofthechapter,asameansofweighingevidence. Supposewewantto\n",
            "determinewhetheranimageshowsahumanfaceornot:\n",
            "Wecouldattackthisproblemthesamewayweattackedhandwritingrecognition–by\n",
            "usingthepixelsintheimageasinputtoaneuralnetwork,withtheoutputfromthenetwork\n",
            "asingleneuronindicatingeither“Yes,it’saface”or“No,it’snotaface”. Let’ssupposewedothis,butthatwe’renotusingalearningalgorithm. Instead,we’re\n",
            "goingtotrytodesignanetworkbyhand,choosingappropriateweightsandbiases. How\n",
            "mightwegoaboutit? Forgettingneuralnetworksentirelyforthemoment,aheuristicwe\n",
            "coulduseistodecomposetheproblemintosub-problems: doestheimagehaveaneyein\n",
            "thetopleft? Doesithaveaneyeinthetopright?\n",
            "Doesithaveanoseinthemiddle? Doesit\n",
            "haveamouthinthebottommiddle? Istherehairontop?\n",
            "Andsoon. Iftheanswerstoseveralofthesequestionsare“yes”,orevenjust“probablyyes”,then\n",
            "we’dconcludethattheimageislikelytobeaface. Conversely,iftheanswerstomostofthe\n",
            "questionsare“no”,thentheimageprobablyisn’taface. Ofcourse,thisisjustaroughheuristic,anditsuffersfrommanydeficiencies. Maybe\n",
            "thepersonisbald,sotheyhavenohair. Maybewecanonlyseepartoftheface,orthe\n",
            "faceisatanangle,sosomeofthefacialfeaturesareobscured. Still,theheuristicsuggests\n",
            "thatifwecansolvethesub-problemsusingneuralnetworks,thenperhapswecanbuilda\n",
            "neuralnetworkforface-detection,bycombiningthenetworksforthesub-problems. Here’s\n",
            "\n",
            "(cid:12)\n",
            "36 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1 a possible architecture, with rectangles denoting the sub-networks. Note that this isn’t\n",
            "intendedasarealisticapproachtosolvingtheface-detectionproblem;rather,it’stohelpus\n",
            "buildintuitionabouthownetworksfunction.\n",
            "Here’sthearchitecture:\n",
            "It’salsoplausiblethatthesub-networkscanbedecomposed. Supposewe’reconsideringthe\n",
            "question: “Isthereaneyeinthetopleft?” Thiscanbedecomposedintoquestionssuchas:\n",
            "“Isthereaneyebrow?”;“Arethereeyelashes?”;“Isthereaniris?”;andsoon. Ofcourse,these\n",
            "questionsshouldreallyincludepositionalinformation,aswell–“Istheeyebrowinthetop\n",
            "left,andabovetheiris?”,thatkindofthing–butlet’skeepitsimple. Thenetworktoanswer\n",
            "thequestion“Isthereaneyeinthetopleft?” cannowbedecomposed:\n",
            "Those questions too can be broken down, further and further through multiple layers. Ultimately,we’llbeworkingwithsub-networksthatanswerquestionssosimpletheycan\n",
            "easilybeansweredatthelevelofsinglepixels. Thosequestionsmight, forexample, be\n",
            "aboutthepresenceorabsenceofverysimpleshapesatparticularpointsintheimage. Such\n",
            "questionscanbeansweredbysingleneuronsconnectedtotherawpixelsintheimage. \n",
            "(cid:12)\n",
            "1.7. Towarddeeplearning (cid:12) 37\n",
            "(cid:12)\n",
            "Theendresultisanetworkwhichbreaksdownaverycomplicatedquestion–doesthis 1\n",
            "imageshowafaceornot–intoverysimplequestionsanswerableatthelevelofsinglepixels. Itdoesthisthroughaseriesofmanylayers,withearlylayersansweringverysimpleand\n",
            "specificquestionsabouttheinputimage,andlaterlayersbuildingupahierarchyofever\n",
            "morecomplexandabstractconcepts. Networkswiththiskindofmany-layerstructure–two\n",
            "ormorehiddenlayers–arecalleddeepneuralnetworks. Ofcourse,Ihaven’tsaidhowtodothisrecursivedecompositionintosub-networks. It\n",
            "certainlyisn’tpracticaltohand-designtheweightsandbiasesinthenetwork. Instead,we’d\n",
            "like to use learning algorithms so that the network can automatically learn the weights\n",
            "andbiases–andthus,thehierarchyofconcepts–fromtrainingdata. Researchersinthe\n",
            "1980sand1990striedusingstochasticgradientdescentandbackpropagationtotraindeep\n",
            "networks. Unfortunately,exceptforafewspecialarchitectures,theydidn’thavemuchluck.\n",
            "Thenetworkswouldlearn,butveryslowly,andinpracticeoftentooslowlytobeuseful. Since2006,asetoftechniqueshasbeendevelopedthatenablelearningindeepneural\n",
            "nets. Thesedeeplearningtechniquesarebasedonstochasticgradientdescentandback-\n",
            "propagation,butalsointroducenewideas. Thesetechniqueshaveenabledmuchdeeper\n",
            "(andlarger)networkstobetrained–peoplenowroutinelytrainnetworkswith5to10\n",
            "hiddenlayers. And,itturnsoutthattheseperformfarbetteronmanyproblemsthanshallow\n",
            "neuralnetworks,i.e.,networkswithjustasinglehiddenlayer. Thereason,ofcourse,is\n",
            "theabilityofdeepnetstobuildupacomplexhierarchyofconcepts. It’sabitliketheway\n",
            "conventionalprogramminglanguagesusemodulardesignandideasaboutabstractionto\n",
            "enablethecreationofcomplexcomputerprograms. Comparingadeepnetworktoashallow\n",
            "networkisabitlikecomparingaprogramminglanguagewiththeabilitytomakefunction\n",
            "callstoastrippeddownlanguagewithnoabilitytomakesuchcalls. Abstractiontakesa\n",
            "differentforminneuralnetworksthanitdoesinconventionalprogramming,butit’sjustas\n",
            "important.\n",
            "\n",
            "(cid:12)\n",
            "38 (cid:12) Usingneuralnetstorecognizehandwrittendigits\n",
            "(cid:12)\n",
            "1\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 39\n",
            "(cid:12)\n",
            "2222\n",
            "How the backpropagation\n",
            "algorithm works\n",
            "2\n",
            "Inthelastchapterwesawhowneuralnetworkscanlearntheirweightsandbiasesusing\n",
            "thegradientdescentalgorithm. Therewas,however,agapinourexplanation: wedidn’t\n",
            "discuss how to compute the gradient of the cost function. That’s quite a gap!\n",
            "In this\n",
            "chapterI’llexplainafastalgorithmforcomputingsuchgradients,analgorithmknownas\n",
            "backpropagation. Thebackpropagationalgorithmwasoriginallyintroducedinthe1970s,butitsimportance\n",
            "wasn’tfullyappreciateduntilafamous1986paperbyDavidRumelhart,GeoffreyHinton,\n",
            "andRonaldWilliams. Thatpaperdescribesseveralneuralnetworkswherebackpropagation\n",
            "worksfarfasterthanearlierapproachestolearning,makingitpossibletouseneuralnetsto\n",
            "solveproblemswhichhadpreviouslybeeninsoluble. Today,thebackpropagationalgorithm\n",
            "istheworkhorseoflearninginneuralnetworks. Thischapterismoremathematicallyinvolvedthantherestofthebook. Ifyou’renotcrazy\n",
            "aboutmathematicsyoumaybetemptedtoskipthechapter,andtotreatbackpropagationas\n",
            "ablackboxwhosedetailsyou’rewillingtoignore. Whytakethetimetostudythosedetails?\n",
            "Thereason,ofcourse,isunderstanding. Attheheartofbackpropagationisanexpression\n",
            "forthepartialderivative∂C/∂wofthecostfunctionC withrespecttoanyweightw(orbias\n",
            "b)inthenetwork. Theexpressiontellsushowquicklythecostchangeswhenwechangethe\n",
            "weightsandbiases. Andwhiletheexpressionissomewhatcomplex,italsohasabeautytoit,\n",
            "witheachelementhavinganatural,intuitiveinterpretation. Andsobackpropagationisn’t\n",
            "justafastalgorithmforlearning. Itactuallygivesusdetailedinsightsintohowchangingthe\n",
            "weightsandbiaseschangestheoverallbehaviourofthenetwork. That’swellworthstudying\n",
            "indetail. Withthatsaid,ifyouwanttoskimthechapter,orjumpstraighttothenextchapter,that’s\n",
            "fine. I’vewrittentherestofthebooktobeaccessibleevenifyoutreatbackpropagationasa\n",
            "blackbox. Thereare,ofcourse,pointslaterinthebookwhereIreferbacktoresultsfrom\n",
            "thischapter. Butatthosepointsyoushouldstillbeabletounderstandthemainconclusions,\n",
            "\n",
            "(cid:12)\n",
            "40 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "evenifyoudon’tfollowallthereasoning. 2.1 Warm up: a fast matrix-based approach to computing the\n",
            "2\n",
            "output from a neural network\n",
            "Beforediscussingbackpropagation, let’swarm upwithafastmatrix-basedalgorithm to\n",
            "computetheoutputfromaneuralnetwork. Weactuallyalreadybrieflysawthisalgorithm\n",
            "neartheendofthelastchapter(section1.6),butIdescribeditquickly,soit’sworthrevisiting\n",
            "indetail.\n",
            "Inparticular,thisisagoodwayofgettingcomfortablewiththenotationusedin\n",
            "backpropagation,inafamiliarcontext.\n",
            "Let’sbeginwithanotationwhichletsusrefertoweightsinthenetworkinanunam-\n",
            "biguousway. We’llusewl todenotetheweightfortheconnectionfromthek-thneuronin\n",
            "jk\n",
            "the(l 1)-thlayertothe j-thneuroninthel-thlayer. So,forexample,thediagrambelow\n",
            "shows−theweightonaconnectionfromthefourthneuroninthesecondlayertothesecond\n",
            "neuroninthethirdlayerofanetwork:\n",
            "Thisnotationiscumbersomeatfirst,anditdoestakesomeworktomaster. Butwithalittle\n",
            "effortyou’llfindthenotationbecomeseasyandnatural. Onequirkofthenotationisthe\n",
            "orderingofthejandkindices. Youmightthinkthatitmakesmoresensetousejtorefer\n",
            "totheinputneuron, andktotheoutputneuron, notviceversa, asisactuallydone. I’ll\n",
            "explainthereasonforthisquirkbelow. Weuseasimilarnotationforthenetwork’sbiases\n",
            "andactivations. Explicitly,weuse bl forthebiasofthe j-thneuroninthel-thlayer. Andwe\n",
            "j\n",
            "useal fortheactivationofthe j-thneuroninthel-thlayer. Thefollowingdiagramshows\n",
            "j\n",
            "examplesofthesenotationsinuse:\n",
            "Withthesenotations,theactivational ofthe j-thneuroninthel-thlayerisrelatedtothe\n",
            "j\n",
            "activationsinthe(l 1)-thlayerbytheequation(compareEquation1.4andsurrounding\n",
            "discussioninthelast−chapter)\n",
            "\n",
            "(cid:12)\n",
            "2.1. Warmup: afastmatrix-basedapproachtocomputingtheoutputfromaneuralnetwork (cid:12) 41\n",
            "(cid:12)\n",
            "(cid:130) (cid:140)\n",
            "(cid:88)\n",
            "al j= σ wl jk a k l − 1 +bl j , (2.1)\n",
            "k\n",
            "2\n",
            "wherethesumisoverallneuronskinthe(l 1)-thlayer. Torewritethisexpressionina\n",
            "matrixformwedefineaweightmatrixwl fore−achlayer,l. Theentriesoftheweightmatrix\n",
            "wl arejusttheweightsconnectingtothel-thlayerofneurons,thatis,theentryinthe j-th\n",
            "rowandk-thcolumniswl . Similarly,foreachlayerl wedefineabiasvector, bl. Youcan\n",
            "jk\n",
            "probablyguesshowthisworks–thecomponentsofthebiasvectorarejustthevalues bl,\n",
            "j\n",
            "onecomponentforeachneuroninthel-thlayer. Andfinally,wedefineanactivationvector\n",
            "alwhosecomponentsaretheactivationsal. Thelastingredientweneedtorewrite2.1ina\n",
            "j\n",
            "matrixformistheideaofvectorizingafunctionsuchasσ. Wemetvectorizationbrieflyin\n",
            "thelastchapter,buttorecap,theideaisthatwewanttoapplyafunctionsuchasσtoevery\n",
            "elementinavectorv. Weusetheobviousnotationσ (v)todenotethiskindofelementwise\n",
            "applicationofafunction.\n",
            "Thatis,thecomponentsofσ (v)arejustσ (v)j = σ (v j). Asan\n",
            "example,ifwehavethefunction f(x)=x2thenthevectorizedformof f hastheeffect\n",
            "(cid:130)(cid:150) (cid:153)(cid:140) (cid:150) (cid:153) (cid:150) (cid:153)\n",
            "2 f(2) 4\n",
            "f = = , (2.2)\n",
            "3 f(3) 9\n",
            "thatis,thevectorized f justsquareseveryelementofthevector. Withthesenotationsinmind,Equation2.1canberewritteninthebeautifulandcompact\n",
            "vectorizedform\n",
            "al = σ (wlal − 1 +bl ).\n",
            "(2.3)\n",
            "Thisexpressiongivesusamuchmoreglobalwayofthinkingabouthowtheactivationsin\n",
            "onelayerrelatetoactivationsinthepreviouslayer: wejustapplytheweightmatrixtothe\n",
            "activations,thenaddthebiasvector,andfinallyapplytheσfunction1. Thatglobalviewis\n",
            "ofteneasierandmoresuccinct(andinvolvesfewerindices!) thantheneuron-by-neuron\n",
            "viewwe’vetakentonow. Thinkofitasawayofescapingindexhell,whileremainingprecise\n",
            "aboutwhat’sgoingon. Theexpressionisalsousefulinpractice,becausemostmatrixlibraries\n",
            "providefastwaysofimplementingmatrixmultiplication,vectoraddition,andvectorization. Indeed,thecode(see1.6)inthelastchaptermadeimplicituseofthisexpressiontocompute\n",
            "thebehaviourofthenetwork.\n",
            "WhenusingEquation2.3tocomputeal,wecomputetheintermediatequantityzl\n",
            "wlal\n",
            "−\n",
            "1 +bl alongtheway. Thisquantityturnsouttobeusefulenoughtobeworthnamin≡g:\n",
            "we call zl the weighted input to the neurons in layer l. We’ll make considerable use of\n",
            "the weighted input zl later in the chapter. Equation 2.3 is sometimes written in terms\n",
            "oftheweightedinput,asal = σ (zl ). It’salsoworthnotingthatzl hascomponentszl j =\n",
            "(cid:80)\n",
            "k\n",
            "wl\n",
            "jk\n",
            "a\n",
            "k\n",
            "l\n",
            "−\n",
            "1 +bl\n",
            "j\n",
            ",thatis,zl\n",
            "j\n",
            "isjusttheweightedinputtotheactivationfunctionforneuron j\n",
            "inlayerl. 1Bytheway,it’sthisexpressionthatmotivatesthequirkinthewl notationmentionedearlier.Ifwe\n",
            "jk\n",
            "usedjtoindextheinputneuron,andktoindextheoutputneuron,thenwe’dneedtoreplacetheweight\n",
            "matrixinEquation2.3bythetransposeoftheweightmatrix.That’sasmallchange,butannoying,and\n",
            "we’dlosetheeasysimplicityofsaying(andthinking)“applytheweightmatrixtotheactivations”. \n",
            "(cid:12)\n",
            "42 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2.2 The two assumptions we need about the cost function\n",
            "Thegoalofbackpropagationistocomputethepartialderivatives∂C/∂wand∂C/∂bofthe\n",
            "2 costfunctionCwithrespecttoanyweightworbiasbinthenetwork. Forbackpropagation\n",
            "toworkweneedtomaketwomainassumptionsabouttheformofthecostfunction.\n",
            "Before\n",
            "statingthoseassumptions,though,it’susefultohaveanexamplecostfunctioninmind. We’ll\n",
            "usethequadraticcostfunctionfromlastchapter(c.f.\n",
            "Equation1.6). Inthenotationofthe\n",
            "lastsection,thequadraticcosthastheform\n",
            "C= 1 (cid:88)(cid:13) (cid:13)y(x) aL (x) (cid:13) (cid:13) 2 , (2.4)\n",
            "2n x −\n",
            "where: n is the total number of training examples; the sum is over individual training\n",
            "examples, x; y= y(x)isthecorrespondingdesiredoutput; Ldenotesthenumberoflayers\n",
            "inthenetwork;andaL =aL (x)isthevectorofactivationsoutputfromthenetworkwhen x\n",
            "isinput. Okay,sowhatassumptionsdoweneedtomakeaboutourcostfunction, C,inorder\n",
            "thatbackpropagationcanbeapplied? Thefirstassumptionweneedisthatthecostfunction\n",
            "canbewrittenasanaverage C = 1 n (cid:80) x C x overcostfunctions C x forindividualtraining\n",
            "examples, x. Thisisthecaseforthequadraticcostfunction,wherethecostforasingle\n",
            "trainingexampleisC x = 1 2 y aL 2. Thisassumptionwillalsoholdtrueforalltheother\n",
            "costfunctionswe’llmeetin(cid:107)thi−sboo(cid:107)k. Thereasonweneedthisassumptionisbecausewhatbackpropagationactuallyletsus\n",
            "doiscomputethepartialderivatives∂C /∂wand∂C /∂bforasingletrainingexample. x x\n",
            "Wethenrecover∂C/∂wand∂C/∂bbyaveragingovertrainingexamples. Infact,withthis\n",
            "assumptioninmind,we’llsupposethetrainingexample x hasbeenfixed,anddropthe x\n",
            "subscript,writingthecostC asC. We’lleventuallyputthe x backin,butfornowit’sa\n",
            "x\n",
            "notationalnuisancethatisbetterleftimplicit. Thesecondassumptionwemakeaboutthecostisthatitcanbewrittenasafunctionof\n",
            "theoutputsfromtheneuralnetwork:\n",
            "Forexample,thequadraticcostfunctionsatisfiesthisrequirement,sincethequadraticcost\n",
            "forasingletrainingexample x maybewrittenas\n",
            "1 1(cid:88)\n",
            "C= 2(cid:107) y − aL (cid:107) 2 = 2 j (y j − aL j) 2, (2.5)\n",
            "andthusisafunctionoftheoutputactivations. Ofcourse,thiscostfunctionalsodepends\n",
            "\n",
            "(cid:12)\n",
            "2.3. TheHadamardproduct,s t (cid:12) 43\n",
            "(cid:12)\n",
            "(cid:12)\n",
            "onthedesiredoutput y,andyoumaywonderwhywe’renotregardingthecostalsoas\n",
            "a function of y. Remember, though, that the input training example x is fixed, and so\n",
            "theoutput y isalsoafixedparameter. Inparticular,it’snotsomethingwecanmodifyby\n",
            "changingtheweightsandbiasesinanyway,i.e.,it’snotsomethingwhichtheneuralnetwork 2\n",
            "learns. AndsoitmakessensetoregardC asafunctionoftheoutputactivationsaL alone,\n",
            "with y merelyaparameterthathelpsdefinethatfunction. 2.3 The Hadamard product, s t\n",
            "(cid:12)\n",
            "Thebackpropagationalgorithmisbasedoncommonlinearalgebraicoperations–things\n",
            "likevectoraddition,multiplyingavectorbyamatrix,andsoon.\n",
            "Butoneoftheoperations\n",
            "isalittlelesscommonlyused.\n",
            "Inparticular,supposesand t aretwovectorsofthesame\n",
            "dimension. Thenweuses t todenotetheelementwiseproductofthetwovectors. Thus\n",
            "thecomponentsofs t are(cid:12)just(s t)j=s\n",
            "j\n",
            "t\n",
            "j\n",
            ". Asanexample,\n",
            "(cid:12) (cid:12)\n",
            "(cid:150) (cid:153) (cid:150) (cid:153) (cid:150) (cid:153) (cid:150) (cid:153)\n",
            "1 3 1 3 3\n",
            "= ∗ = . (2.6)\n",
            "2 (cid:12) 4 2 4 8\n",
            "∗\n",
            "ThiskindofelementwisemultiplicationissometimescalledtheHadamardproductorSchur\n",
            "product.\n",
            "We’llrefertoitastheHadamardproduct. Goodmatrixlibrariesusuallyprovidefast\n",
            "implementationsoftheHadamardproduct,andthatcomesinhandywhenimplementing\n",
            "backpropagation. 2.4 The four fundamental equations behind backpropagation\n",
            "Backpropagationisaboutunderstandinghowchangingtheweightsandbiasesinanetwork\n",
            "changesthecostfunction. Ultimately,thismeanscomputingthepartialderivatives∂C/∂wl\n",
            "jk\n",
            "and∂C/∂bl. Buttocomputethose,wefirstintroduceanintermediatequantity,δl,which\n",
            "j j\n",
            "wecalltheerrorinthe j-thneuroninthel-thlayer. Backpropagationwillgiveusaprocedure\n",
            "tocomputetheerrorδl,andthenwillrelateδl to∂C/∂wl and∂C/∂bl. j j jk j\n",
            "Tounderstandhowtheerrorisdefined,imaginethereisademoninourneuralnetwork:\n",
            "Thedemonsitsatthe j-thneuroninlayerl. Astheinputtotheneuroncomesin,thedemon\n",
            "messeswiththeneuron’soperation. Itaddsalittlechange∆zl totheneuron’sweighted\n",
            "j\n",
            "input,sothatinsteadofoutputtingσ (zl j),theneuroninsteadoutputsσ (zl j+ ∆zl j). This\n",
            "changepropagatesthroughlaterlayersinthenetwork,finallycausingtheoverallcostto\n",
            "changebyanamount\n",
            "∂C∆zl. ∂zl j\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "44 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "Now,thisdemonisagooddemon,andistryingtohelpyouimprovethecost,i.e.,they’re\n",
            "tryingtofindaδzl whichmakesthecostsmaller. Suppose∂C/∂zl hasalargevalue(either\n",
            "j j\n",
            "positiveornegative).\n",
            "Thenthedemoncanlowerthecostquiteabitbychoosing∆zl tohave\n",
            "j\n",
            "2 theoppositesignto∂C/∂zl. Bycontrast,if∂C/∂zl isclosetozero,thenthedemoncan’t\n",
            "j j\n",
            "improvethecostmuchatallbyperturbingtheweightedinputzl. Sofarasthedemoncan\n",
            "j\n",
            "tell,theneuronisalreadyprettynearoptimal2. Andsothere’saheuristicsenseinwhich\n",
            "∂C/∂zl isameasureoftheerrorintheneuron. j\n",
            "Motivatedbythisstory,wedefinetheerrorδl ofneuron jinlayerl by\n",
            "j\n",
            "∂C\n",
            "δl . (2.7)\n",
            "j\n",
            "≡\n",
            "∂zl\n",
            "j\n",
            "Asperourusualconventions,weuseδl todenotethevectoroferrorsassociatedwithlayerl. Backpropagationwillgiveusawayofcomputingδl foreverylayer,andthenrelatingthose\n",
            "errorstothequantitiesofrealinterest,∂C/∂wl and∂C/∂bl. jk j\n",
            "Youmightwonderwhythedemonischangingtheweightedinputzl. Surelyit’dbemore\n",
            "j\n",
            "naturaltoimaginethedemonchangingtheoutputactivational,withtheresultthatwe’dbe\n",
            "j\n",
            "using\n",
            "∂C\n",
            "asourmeasureoferror. Infact,ifyoudothisthingsworkoutquitesimilarlyto\n",
            "∂al\n",
            "j\n",
            "thediscussionbelow. Butitturnsouttomakethepresentationofbackpropagationalittle\n",
            "morealgebraicallycomplicated. Sowe’llstickwithδl j= ∂ ∂ z C l asourmeasureoferror3. j\n",
            "Planofattack: Backpropagationisbasedaroundfourfundamentalequations. Together,\n",
            "thoseequationsgiveusawayofcomputingboththeerrorδl andthegradientofthecost\n",
            "function.\n",
            "Istatethefourequationsbelow. Bewarned, though: youshouldn’texpectto\n",
            "instantaneouslyassimilatetheequations. Suchanexpectationwillleadtodisappointment. Infact,thebackpropagationequationsaresorichthatunderstandingthemwellrequires\n",
            "considerabletimeandpatienceasyougraduallydelvedeeperintotheequations. Thegood\n",
            "newsisthatsuchpatienceisrepaidmanytimesover. Andsothediscussioninthissectionis\n",
            "merelyabeginning,helpingyouonthewaytoathoroughunderstandingoftheequations. Here’sapreviewofthewayswe’lldelvemoredeeplyintotheequationslaterinthe\n",
            "chapter: I’ll give a short proof of the equations, which helps explain why they are true;\n",
            "we’llrestatetheequationsinalgorithmicformaspseudocode,andseehowthepseudocode\n",
            "canbeimplementedasreal,runningPythoncode;and,inthefinalsectionofthechapter,\n",
            "we’lldevelopanintuitivepictureofwhatthebackpropagationequationsmean,andhow\n",
            "someonemightdiscoverthemfromscratch. Alongthewaywe’llreturnrepeatedlytothe\n",
            "fourfundamentalequations,andasyoudeepenyourunderstandingthoseequationswill\n",
            "cometoseemcomfortableand,perhaps,evenbeautifulandnatural. Anequationfortheerrorintheoutputlayer,δL: ThecomponentsofδL aregivenby\n",
            "∂C\n",
            "δL j = ∂aL σ (cid:48)(z j L ). (BP1)\n",
            "j\n",
            "2Thisisonlythecaseforsmallchanges∆zl,ofcourse.We’llassumethatthedemonisconstrained\n",
            "j\n",
            "tomakesuchsmallchanges. 3InclassificationproblemslikeMNISTtheterm“error”issometimesusedtomeantheclassification\n",
            "failurerate. E.g.,iftheneuralnetcorrectlyclassifies96.0percentofthedigits,thentheerroris4.0\n",
            "percent. Obviously,thishasquiteadifferentmeaningfromourδvectors.\n",
            "Inpractice,youshouldn’t\n",
            "havetroubletellingwhichmeaningisintendedinanygivenusage. \n",
            "(cid:12)\n",
            "2.4. Thefourfundamentalequationsbehindbackpropagation (cid:12) 45\n",
            "(cid:12)\n",
            "Thisisaverynaturalexpression. Thefirsttermontheright,∂C/∂aL,justmeasureshow\n",
            "j\n",
            "fastthecostischangingasafunctionofthe j-thoutputactivation. If,forexample,C doesn’t\n",
            "dependmuchonaparticularoutputneuron, j,thenδL willbesmall,whichiswhatwe’d\n",
            "j\n",
            "expect. Thesecondtermontheright,σ (cid:48)(z\n",
            "j\n",
            "L ),measureshowfasttheactivationfunctionσis 2\n",
            "changingatzL. j\n",
            "NoticethateverythinginEq.BP1iseasilycomputed. Inparticular,wecomputezL while\n",
            "j\n",
            "computingthebehaviourofthenetwork,andit’sonlyasmalladditionaloverheadtocompute\n",
            "σ (cid:48)(z\n",
            "j\n",
            "L ). Theexactformof∂C/∂aL\n",
            "j\n",
            "will,ofcourse,dependontheformofthecostfunction. However, provided the cost function is known there should be little trouble computing\n",
            "∂C/∂aL j . Forexample,ifwe’reusingthequadraticcostfunctionthenC= 1 2 (cid:80) j(y j aL j) 2,\n",
            "andso∂C/∂aL\n",
            "j\n",
            "=(aL\n",
            "j\n",
            "y j),whichobviouslyiseasilycomputable. −\n",
            "EquationBP1isac − omponentwiseexpressionforδL. It’saperfectlygoodexpression,but\n",
            "notthematrix-basedformwewantforbackpropagation. However,it’seasytorewritethe\n",
            "equationinamatrix-basedform,as\n",
            "δL = a C σ (cid:48)(zL ). (BP1a)\n",
            "∇ (cid:12)\n",
            "Here, C isdefinedtobeavectorwhosecomponentsarethepartialderivatives∂C/∂aL. a j\n",
            "You ca∇n think of C as expressing the rate of change of C with respect to the output\n",
            "a\n",
            "activations. It’seas∇ytoseethatEquationsBP1aandBP1areequivalent,andforthatreason\n",
            "fromnowonwe’lluseBP1interchangeablytorefertobothequations.\n",
            "Asanexample,inthe\n",
            "caseofthequadraticcostwehave\n",
            "a\n",
            "C=(aL y),andsothefullymatrix-basedformof\n",
            "BP1becomes ∇ −\n",
            "δL =(aL y) σ (cid:48)(zL ). (2.8)\n",
            "− (cid:12)\n",
            "Asyoucansee,everythinginthisexpressionhasanicevectorform,andiseasilycomputed\n",
            "usingalibrarysuchasNumpy. Anequationfortheerrorδlintermsoftheerrorinthenextlayer,δl+1:Inparticular\n",
            "δl =((wl+1 ) Tδl+1 ) σ (cid:48)(zl ), (BP2)\n",
            "(cid:12)\n",
            "where(wl+1 ) T isthetransposeoftheweightmatrixwl+1forthe(l+1)-thlayer. Thisequation\n",
            "appearscomplicated,buteachelementhasaniceinterpretation. Supposeweknowthe\n",
            "errorδl+1atthe(l+1)-thlayer. Whenweapplythetransposeweightmatrix,(wl+1\n",
            ")\n",
            "T,we\n",
            "canthinkintuitivelyofthisasmovingtheerrorbackwardthroughthenetwork,givingus\n",
            "somesortofmeasureoftheerrorattheoutputofthel-thlayer. WethentaketheHadamard\n",
            "product σ (cid:48)(zl ).\n",
            "Thismovestheerrorbackwardthroughtheactivationfunctioninlayerl,\n",
            "givingus(cid:12)theerrorδl intheweightedinputtolayerl.\n",
            "Bycombining(BP2)with(BP1)wecancomputetheerrorδlforanylayerinthenetwork. Westartbyusing(BP1)tocomputeδL,thenapplyEquation(BP2)tocomputeδL 1,then\n",
            "−\n",
            "Equation(BP2)againtocomputeδL 2,andsoon,allthewaybackthroughthenetwork. −\n",
            "Anequationfortherateofchangeofthecostwithrespecttoanybiasinthenet-\n",
            "work: Inparticular:\n",
            "∂C\n",
            "∂bl = δl j . (BP3)\n",
            "j\n",
            "Thatis,theerrorδl isexactlyequaltotherateofchange∂C/∂bl. Thisisgreatnews,since\n",
            "j j\n",
            "\n",
            "(cid:12)\n",
            "46 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "(BP1)and(BP2)havealreadytoldushowtocomputeδl. Wecanrewrite(BP3)inshorthand\n",
            "j\n",
            "as\n",
            "∂C\n",
            "∂b = δ, (2.9)\n",
            "2\n",
            "whereitisunderstoodthatδisbeingevaluatedatthesameneuronasthebias b. An equation for the rate of change of the cost with respect to any weight in the\n",
            "network: Inparticular:\n",
            "∂C\n",
            "∂wl\n",
            "=a\n",
            "k\n",
            "l\n",
            "−\n",
            "1δl\n",
            "j\n",
            ". (BP4)\n",
            "jk\n",
            "Thistellsushowtocomputethepartialderivatives∂C/∂wl intermsofthequantitiesδl\n",
            "jk\n",
            "andal 1,whichwealreadyknowhowtocompute. Theequationcanberewritteninaless\n",
            "−\n",
            "index-heavynotationas\n",
            "∂C\n",
            "∂w\n",
            "=a\n",
            "in\n",
            "δ\n",
            "out\n",
            ", (2.10)\n",
            "whereit’sunderstoodthata istheactivationoftheneuroninputtotheweightw,andδ\n",
            "in out\n",
            "istheerroroftheneuronoutputfromtheweightw. Zoomingintolookatjusttheweightw,\n",
            "andthetwoneuronsconnectedbythatweight,wecandepictthisas:\n",
            "AniceconsequenceofEquation2.10isthatwhentheactivationa issmall,a 0,the\n",
            "in in\n",
            "gradientterm∂C/∂wwillalsotendtobesmall. Inthiscase,we’llsaytheweigh≈tlearns\n",
            "slowly,meaningthatit’snotchangingmuchduringgradientdescent. Inotherwords,one\n",
            "consequenceof(BP4)isthatweightsoutputfromlow-activationneuronslearnslowly. Thereareotherinsightsalongtheselineswhichcanbeobtainedfrom(BP1)–(BP4). Let’s\n",
            "startbylookingattheoutputlayer.\n",
            "Considerthetermσ (cid:48)(z\n",
            "j\n",
            "L )in(BP1). Recallfromthegraph\n",
            "ofthesigmoidfunctioninthelastchapterthattheσfunctionbecomesveryflatwhenσ (z j L )\n",
            "isapproximately0or1. Whenthisoccurswewillhaveσ (cid:48)(z j L ) 0. Andsothelessonisthat\n",
            "aweightinthefinallayerwilllearnslowlyiftheoutputneuron≈iseitherlowactivation( 0)\n",
            "orhighactivation( 1). Inthiscaseit’scommontosaytheoutputneuronhassatur≈ated\n",
            "and,asaresult,the≈weighthasstoppedlearning(orislearningslowly). Similarremarks\n",
            "holdalsoforthebiasesofoutputneuron.\n",
            "Wecanobtainsimilarinsightsforearlierlayers. Inparticular,notetheσ (cid:48)(zl )termin\n",
            "(BP2). Thismeansthatδl islikelytogetsmalliftheneuronisnearsaturation.\n",
            "Andthis,in\n",
            "j\n",
            "turn,meansthatanyweightsinputtoasaturatedneuronwilllearnslowly4. Summingup,we’velearntthataweightwilllearnslowlyifeithertheinputneuronis\n",
            "low-activation,oriftheoutputneuronhassaturated,i.e.,iseitherhigh-orlow-activation. Noneoftheseobservationsistoogreatlysurprising. Still,theyhelpimproveourmental\n",
            "modelofwhat’sgoingonasaneuralnetworklearns. Furthermore,wecanturnthistype\n",
            "ofreasoningaround. Thefourfundamentalequationsturnouttoholdforanyactivation\n",
            "function,notjustthestandardsigmoidfunction(that’sbecause,aswe’llseeinamoment,\n",
            "4Thisreasoningwon’tholdif(wl+1\n",
            ")\n",
            "Tδl+1haslargeenoughentriestocompensateforthesmallness\n",
            "ofσ (cid:48)(zl j).ButI’mspeakingofthegeneraltendency. \n",
            "(cid:12)\n",
            "2.4. Thefourfundamentalequationsbehindbackpropagation (cid:12) 47\n",
            "(cid:12)\n",
            "theproofsdon’tuseanyspecialpropertiesofσ). Andsowecanusetheseequationsto\n",
            "designactivationfunctionswhichhaveparticulardesiredlearningproperties. Asanexample\n",
            "togiveyoutheidea,supposeweweretochoosea(non-sigmoid)activationfunctionσso\n",
            "thatσ isalwayspositive,andnevergetsclosetozero. Thatwouldpreventtheslow-down 2\n",
            "(cid:48)\n",
            "oflearningthatoccurswhenordinarysigmoidneuronssaturate. Laterinthebookwe’llsee\n",
            "exampleswherethiskindofmodificationismadetotheactivationfunction. Keepingthe\n",
            "fourequationsBP1–BP4inmindcanhelpexplainwhysuchmodificationsaretried,and\n",
            "whatimpacttheycanhave. Problem\n",
            "Alternatepresentationoftheequationsofbackpropagation: I’vestatedtheequa-\n",
            "• tionsofbackpropagation(notablyBP1andBP2)usingtheHadamardproduct. This\n",
            "presentationmaybedisconcertingifyou’reunusedtotheHadamardproduct.\n",
            "There’s\n",
            "analternativeapproach,basedonconventionalmatrixmultiplication,whichsome\n",
            "readersmayfindenlightening. (1) Showthat(BP1)mayberewrittenas\n",
            "δL = Σ (cid:48)(zL ) a C, (2.11)\n",
            "∇\n",
            "whereΣ (cid:48)(zL )isasquarematrixwhosediagonalentriesarethevaluesσ (cid:48)(z\n",
            "j\n",
            "L ),\n",
            "andwhoseoff-diagonalentriesarezero. Notethatthismatrixactson C by\n",
            "a\n",
            "conventionalmatrixmultiplication. ∇\n",
            "(2) Showthat(BP2)mayberewrittenas\n",
            "δl = Σ (cid:48)(zl )(wl+1 ) Tδl+1. (2.12)\n",
            "(3) Bycombiningobservations(1)and(2)showthat\n",
            "δl = Σ (cid:48)(zl )(wl+1 ) T...Σ (cid:48)(zL − 1 )(wL ) TΣ (cid:48)(zL ) a C (2.13)\n",
            "∇\n",
            "Forreaderscomfortablewithmatrixmultiplicationthisequationmaybeeasier\n",
            "tounderstandthan(BP1)and(BP2). ThereasonI’vefocusedon(BP1)and\n",
            "(BP2)isbecausethatapproachturnsouttobefastertoimplementnumerically.\n",
            "\n",
            "(cid:12)\n",
            "48 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2.5 Proof of the four fundamental equations (optional)\n",
            "2 We’llnowprovethefourfundamentalequations(BP1)–(BP4). Allfourareconsequencesof\n",
            "thechainrulefrommultivariablecalculus.\n",
            "Ifyou’recomfortablewiththechainrule,thenI\n",
            "stronglyencourageyoutoattemptthederivationyourselfbeforereadingon. Let’sbeginwithEquation(BP1),whichgivesanexpressionfortheoutputerror,δl. To\n",
            "provethisequation,recallthatbydefinition\n",
            "∂C\n",
            "δL j = ∂zL . (2.14)\n",
            "j\n",
            "Applyingthechainrule,wecanre-expressthepartialderivativeaboveintermsofpartial\n",
            "derivativeswithrespecttotheoutputactivations,\n",
            "(cid:88) ∂C ∂aL\n",
            "δL j = ∂aL ∂z k L , (2.15)\n",
            "k k j\n",
            "wherethesumisoverallneuronskintheoutputlayer. Ofcourse,theoutputactivationaL\n",
            "k\n",
            "ofthek-thneurondependsonlyontheweightedinputz\n",
            "j\n",
            "L forthe j-thneuronwhenk= j. Andso∂a\n",
            "k\n",
            "L/∂z\n",
            "j\n",
            "L vanisheswhenk= j.\n",
            "Asaresultwecansimplifythepreviousequationto\n",
            "(cid:54)\n",
            "δL j = ∂ ∂ a C L ∂ ∂ a zL L j . (2.16)\n",
            "j j\n",
            "RecallingthataL j = σ (z j L )thesecondtermontherightcanbewrittenasσ (cid:48)(z j L ),andthe\n",
            "equationbecomes\n",
            "∂C\n",
            "δL j = ∂aL σ (cid:48)(z j L ), (2.17)\n",
            "j\n",
            "whichisjust(BP1),incomponentform. Next,we’llprove(BP2),whichgivesanequation\n",
            "fortheerrorδl intermsoftheerrorinthenextlayer,δl+1. Todothis,wewanttorewrite\n",
            "δl j= ∂C/∂zl j intermsofδ k l+1 = ∂C/∂z k l+1. Wecandothisusingthechainrule,\n",
            "∂C (cid:88) ∂C ∂zl+1 (cid:88) ∂zl+1\n",
            "δl j= ∂zl = ∂zl+1 ∂z k l = ∂z k l δ k l+1, (2.18)\n",
            "j k k j k j\n",
            "where in the last line we have interchanged the two terms on the right-hand side, and\n",
            "substitutedthedefinitionofδl+1. Toevaluatethefirsttermonthelastline,notethat\n",
            "k\n",
            "(cid:88) (cid:88)\n",
            "z k l+1 = wl k + j 1al j+b k l+1 = wl k + j 1σ (zl j)+b k l+1. (2.19)\n",
            "j j\n",
            "Differentiating,weobtain\n",
            "∂zl+1\n",
            "∂z\n",
            "k\n",
            "l\n",
            "=wl\n",
            "k\n",
            "+\n",
            "j\n",
            "1σ (cid:48)(zl j). (2.20)\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "2.6. Thebackpropagationalgorithm (cid:12) 49\n",
            "(cid:12)\n",
            "Substitutingbackinto(2.18)weobtain\n",
            "(cid:88)\n",
            "δl j= wl k + j 1δ k l+1σ (cid:48)(zl j). (2.21)\n",
            "k 2\n",
            "Thisisjust(BP2)writtenincomponentform.\n",
            "Thefinaltwoequationswewanttoproveare(BP3)and(BP4). Thesealsofollowfrom\n",
            "thechainrule,inamannersimilartotheproofsofthetwoequationsabove. Ileavethemto\n",
            "youasanexercise. Exercise\n",
            "ProveEquations(BP3)and(BP4). •\n",
            "Thatcompletestheproofofthefourfundamentalequationsofbackpropagation. Theproof\n",
            "mayseemcomplicated. Butit’sreallyjusttheoutcomeofcarefullyapplyingthechainrule. Alittlelesssuccinctly,wecanthinkofbackpropagationasawayofcomputingthegradient\n",
            "ofthecostfunctionbysystematicallyapplyingthechainrulefrommulti-variablecalculus. That’salltherereallyistobackpropagation–therestisdetails. 2.6 The backpropagation algorithm\n",
            "Thebackpropagationequationsprovideuswithawayofcomputingthegradientofthecost\n",
            "function. Let’sexplicitlywritethisoutintheformofanalgorithm:\n",
            "1.\n",
            "Input x: Setthecorrespondingactivationa1fortheinputlayer.\n",
            "2.\n",
            "Feedforward: Foreachl=2,3,...,Lcomputezl =wlal − 1 +bl andal = σ (z l). 3. OutputerrorδL: ComputethevectorδL = a C σ (cid:48)(zL ). 4. Backpropagatetheerror:Foreachl=L 1,∇L 2,(cid:12)...,2computeδl =((wl+1 ) Tδl+1 )\n",
            "σ (cid:48)(zl ). − − (cid:12)\n",
            "5. Output: Thegradientofthecostfunctionisgivenby ∂ ∂ w C l =a k l − 1δl j and ∂ ∂ b C l = δl j . jk j\n",
            "Examiningthealgorithmyoucanseewhyit’scalledbackpropagation. Wecomputetheerror\n",
            "vectorsδl backward,startingfromthefinallayer. Itmayseempeculiarthatwe’regoing\n",
            "throughthenetworkbackward. Butifyouthinkabouttheproofofbackpropagation,the\n",
            "backwardmovementisaconsequenceofthefactthatthecostisafunctionofoutputsfrom\n",
            "thenetwork. Tounderstandhowthecostvarieswithearlierweightsandbiasesweneed\n",
            "torepeatedlyapplythechainrule,workingbackwardthroughthelayerstoobtainusable\n",
            "expressions. Exercises\n",
            "BackpropagationwithasinglemodifiedneuronSupposewemodifyasingleneuron\n",
            "• inafeedforwardnetworksothattheoutputfromtheneuronisgivenby f( (cid:80) j w j x j+\n",
            "b),where f issomefunctionotherthanthesigmoid. Howshouldwemodifythe\n",
            "backpropagationalgorithminthiscase? Backpropagation with linear neurons Suppose we replace the usual non-linear\n",
            "• σ function with σ (z) = z throughout the network. Rewrite the backpropagation\n",
            "algorithmforthiscase. As I’ve described it above, the backpropagation algorithm computes the gradient of the\n",
            "costfunctionforasingletrainingexample,C =C\n",
            "x\n",
            ". Inpractice,it’scommontocombine\n",
            "backpropagationwithalearningalgorithmsuchasstochasticgradientdescent,inwhichwe\n",
            "computethegradientformanytrainingexamples. Inparticular,givenamini-batchofm\n",
            "\n",
            "(cid:12)\n",
            "50 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "trainingexamples,thefollowingalgorithmappliesagradientdescentlearningstepbasedon\n",
            "thatmini-batch:\n",
            "1. Inputasetoftrainingexamples\n",
            "2 2. Foreachtrainingexamplex: Setthecorrespondinginputactivationax,1,andperform\n",
            "thefollowingsteps:\n",
            "Feedforward: Foreachl=2,3,...,L computezx,l = wlax,l − 1 +bl and ax,l =\n",
            "• σ (zx,l ). Outputerrorδx,L: Computethevectorδx,L = a C x σ (cid:48)(zx,L ). • Backpropagate the error: For each l = L ∇1,L (cid:12)2,...,2 compute δx,l =\n",
            "• ((wl+1 ) Tδx,l+1 ) σ (cid:48)(zx,l ). − −\n",
            "3.\n",
            "Gradientdescent: For(cid:12)eachl=L,L 1,...,2updatetheweightsaccordingtotherule\n",
            "wl wl\n",
            "m\n",
            "η(cid:80)\n",
            "x\n",
            "δx,l (ax,l\n",
            "−\n",
            "1\n",
            ")\n",
            "T,andth−ebiasesaccordingtotherulebl bl\n",
            "m\n",
            "η(cid:80)\n",
            "x\n",
            "δx,l. → − → −\n",
            "Ofcourse,toimplementstochasticgradientdescentinpracticeyoualsoneedanouterloop\n",
            "generatingmini-batchesoftrainingexamples,andanouterloopsteppingthroughmultiple\n",
            "epochsoftraining. I’veomittedthoseforsimplicity. 2.7 The code for backpropagation\n",
            "Havingunderstoodbackpropagationintheabstract,wecannowunderstandthecodeused\n",
            "inthelastchaptertoimplementbackpropagation. Recallfromthatchapterthatthecodewas\n",
            "containedintheupdate_mini_batchandbackpropmethodsoftheNetworkclass. The\n",
            "codeforthesemethodsisadirecttranslationofthealgorithmdescribedabove. Inparticular,\n",
            "theupdate_mini_batchmethodupdatestheNetwork’sweightsandbiasesbycomputing\n",
            "thegradientforthecurrentmini_batchoftrainingexamples:\n",
            "class Network(object):\n",
            "... def update_mini_batch(self, mini_batch, eta):\n",
            "\"\"\"Update the network’s weights and biases by applying\n",
            "gradient descent using backpropagation to a single mini batch. The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
            "is the learning rate.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "Mostoftheworkisdonebythelinedelta_nabla_b,delta_nabla_w = self.backprop\n",
            "(x, y)whichusesthebackpropmethodtofigureoutthepartialderivatives∂C /∂bl and\n",
            "x j\n",
            "∂C /∂wl . Thebackpropmethodfollowsthealgorithminthelastsectionclosely.\n",
            "Thereis\n",
            "x jk\n",
            "onesmallchange–weuseaslightlydifferentapproachtoindexingthelayers. Thischange\n",
            "ismadetotakeadvantageofafeatureofPython,namelytheuseofnegativelistindices\n",
            "tocountbackwardfromtheendofalist,so,e.g.,l[-3]isthethirdlastentryinalistl. Thecodeforbackpropisbelow,togetherwithafewhelperfunctions,whichareusedto\n",
            "computetheσfunction,thederivativeσ,andthederivativeofthecostfunction. Withthese\n",
            "(cid:48)\n",
            "\n",
            "(cid:12)\n",
            "2.7. Thecodeforbackpropagation (cid:12) 51\n",
            "(cid:12)\n",
            "inclusionsyoushouldbeabletounderstandthecodeinaself-containedway. Ifsomething’s\n",
            "trippingyouup,youmayfindithelpfultoconsulttheoriginaldescription(andcomplete\n",
            "listing)ofthecode. 2\n",
            "class Network(object):\n",
            "... def backprop(self, x, y):\n",
            "\"\"\"Return a tuple \"(nabla_b, nabla_w)\" representing the\n",
            "gradient for the cost function C_x. \"nabla_b\" and\n",
            "\"nabla_w\" are layer-by-layer lists of numpy arrays, similar\n",
            "to \"self.biases\" and \"self.weights\".\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "zs.append(z)\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. It’s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "... def cost_derivative(self, output_activations, y):\n",
            "\"\"\"Return the vector of partial derivatives \\partial{} C_x /\n",
            "\\partial{} a for the output activations.\"\"\"\n",
            "return (output_activations-y)\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "Problem\n",
            "Fully matrix-based approach to backpropagation over a mini-batch Our imple-\n",
            "• mentationofstochasticgradientdescentloopsovertrainingexamplesinamini-batch. It’spossibletomodifythebackpropagationalgorithmsothatitcomputesthegradients\n",
            "foralltrainingexamplesinamini-batchsimultaneously.\n",
            "Theideaisthatinsteadof\n",
            "beginningwithasingleinputvector, x,wecanbeginwithamatrixX =[x 1 x 2 ...x m]\n",
            "\n",
            "(cid:12)\n",
            "52 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "whosecolumnsarethevectorsinthemini-batch. Weforward-propagatebymultiply-\n",
            "ingbytheweightmatrices,addingasuitablematrixforthebiasterms,andapplying\n",
            "thesigmoidfunctioneverywhere. Webackpropagatealongsimilarlines. Explicitly\n",
            "2 writeoutpseudocodeforthisapproachtothebackpropagationalgorithm. Modify\n",
            "network.pysothatitusesthisfullymatrix-basedapproach. Theadvantageofthis\n",
            "approachisthatittakesfulladvantageofmodernlibrariesforlinearalgebra.\n",
            "Asa\n",
            "resultitcanbequiteabitfasterthanloopingoverthemini-batch. (Onmylaptop,\n",
            "forexample,thespeedupisaboutafactoroftwowhenrunonMNISTclassification\n",
            "problemslikethoseweconsideredinthelastchapter.) Inpractice,allseriouslibraries\n",
            "forbackpropagationusethisfullymatrix-basedapproachorsomevariant. 2.8 In what sense is backpropagation a fast algorithm? Inwhatsenseisbackpropagationafastalgorithm?\n",
            "Toanswerthisquestion,let’sconsider\n",
            "anotherapproachtocomputingthegradient. Imagineit’stheearlydaysofneuralnetworks\n",
            "research. Maybeit’sthe1950sor1960s,andyou’rethefirstpersonintheworldtothinkof\n",
            "usinggradientdescenttolearn! Buttomaketheideaworkyouneedawayofcomputing\n",
            "thegradientofthecostfunction. Youthinkbacktoyourknowledgeofcalculus,anddecide\n",
            "toseeifyoucanusethechainruletocomputethegradient. Butafterplayingaroundabit,\n",
            "thealgebralookscomplicated,andyougetdiscouraged.\n",
            "Soyoutrytofindanotherapproach. YoudecidetoregardthecostasafunctionoftheweightsC=C(w)alone(we’llgetbackto\n",
            "thebiasesinamoment). Younumbertheweightsw ,w ,...,andwanttocompute∂C/∂w\n",
            "1 2 j\n",
            "forsomeparticularweightw . Anobviouswayofdoingthatistousetheapproximation\n",
            "j\n",
            "∂C C(w+ εe j) C(w)\n",
            "∂w\n",
            "j ≈\n",
            "ε − , (2.22)\n",
            "whereε>0isasmallpositivenumber,ande istheunitvectorinthe j-thdirection. In\n",
            "j\n",
            "otherwords,wecanestimate∂C/∂w bycomputingthecostC fortwoslightlydifferent\n",
            "j\n",
            "valuesofw ,andthenapplyingEquation2.22. Thesameideawillletuscomputethepartial\n",
            "j\n",
            "derivatives∂C/∂bwithrespecttothebiases. Thisapproachlooksverypromising. It’ssimpleconceptually, andextremelyeasyto\n",
            "implement,usingjustafewlinesofcode. Certainly,itlooksmuchmorepromisingthanthe\n",
            "ideaofusingthechainruletocomputethegradient! Unfortunately,whilethisapproachappearspromising,whenyouimplementthecodeit\n",
            "turnsouttobeextremelyslow. Tounderstandwhy,imaginewehaveamillionweightsin\n",
            "ournetwork. Thenforeachdistinctweightw\n",
            "j\n",
            "weneedtocomputeC(w+ εe j)inorderto\n",
            "compute∂C/∂w . Thatmeansthattocomputethegradientweneedtocomputethecost\n",
            "j\n",
            "functionamilliondifferenttimes,requiringamillionforwardpassesthroughthenetwork\n",
            "(pertrainingexample). WeneedtocomputeC(w)aswell,sothat’satotalofamillionand\n",
            "onepassesthroughthenetwork. What’scleveraboutbackpropagationisthatitenablesustosimultaneouslycomputeall\n",
            "thepartialderivatives∂C/∂w usingjustoneforwardpassthroughthenetwork,followed\n",
            "j\n",
            "by one backward pass through the network. Roughly speaking, the computational cost\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 53\n",
            "(cid:12)\n",
            "of the backward pass is about the same as the forward pass5. And so the total cost of\n",
            "backpropagationisroughlythesameasmakingjusttwoforwardpassesthroughthenetwork. Comparethattothemillionandoneforwardpassesweneededfortheapproachbasedon\n",
            "(2.22)! Andsoeventhoughbackpropagationappearssuperficiallymorecomplexthanthe 2\n",
            "approachbasedon(2.22),it’sactuallymuch,muchfaster. Thisspeedupwasfirstfullyappreciatedin1986,anditgreatlyexpandedtherangeof\n",
            "problemsthatneuralnetworkscouldsolve. That,inturn,causedarushofpeopleusing\n",
            "neuralnetworks. Ofcourse,backpropagationisnotapanacea. Eveninthelate1980speople\n",
            "ranupagainstlimits,especiallywhenattemptingtousebackpropagationtotraindeepneural\n",
            "networks,i.e.,networkswithmanyhiddenlayers. Laterinthebookwe’llseehowmodern\n",
            "computersandsomeclevernewideasnowmakeitpossibletousebackpropagationtotrain\n",
            "suchdeepneuralnetworks. 2.9 Backpropagation: the big picture\n",
            "AsI’veexplainedit,backpropagationpresentstwomysteries. First,what’sthealgorithm\n",
            "reallydoing?\n",
            "We’vedevelopedapictureoftheerrorbeingbackpropagatedfromtheoutput. Butcanwegoanydeeper,andbuildupmoreintuitionaboutwhatisgoingonwhenwe\n",
            "doallthesematrixandvectormultiplications? Thesecondmysteryishowsomeonecould\n",
            "everhavediscoveredbackpropagationinthefirstplace? It’sonethingtofollowthestepsin\n",
            "analgorithm,oreventofollowtheproofthatthealgorithmworks. Butthatdoesn’tmean\n",
            "youunderstandtheproblemsowellthatyoucouldhavediscoveredthealgorithminthe\n",
            "firstplace. Isthereaplausiblelineofreasoningthatcouldhaveledyoutodiscoverthe\n",
            "backpropagationalgorithm?\n",
            "InthissectionI’lladdressboththesemysteries. Toimproveourintuitionaboutwhatthealgorithmisdoing,let’simaginethatwe’ve\n",
            "madeasmallchange∆wl tosomeweightinthenetwork,wl :\n",
            "jk jk\n",
            "Thatchangeinweightwillcauseachangeintheoutputactivationfromthecorresponding\n",
            "neuron:\n",
            "5Thisshouldbeplausible,butitrequiressomeanalysistomakeacarefulstatement.It’splausible\n",
            "becausethedominantcomputationalcostintheforwardpassismultiplyingbytheweightmatrices,\n",
            "whileinthebackwardpassit’smultiplyingbythetransposesoftheweightmatrices.Theseoperations\n",
            "obviouslyhavesimilarcomputationalcost. \n",
            "(cid:12)\n",
            "54 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2\n",
            "That,inturn,willcauseachangeinalltheactivationsinthenextlayer:\n",
            "Thosechangeswillinturncausechangesinthenextlayer,andthenthenext,andsoonall\n",
            "thewaythroughtocausingachangeinthefinallayer,andtheninthecostfunction:\n",
            "Thechange∆C inthecostisrelatedtothechange∆wl intheweightbytheequation\n",
            "jk\n",
            "∂C\n",
            "∆C ∆wl . (2.23)\n",
            "≈\n",
            "∂wl\n",
            "jk\n",
            "jk\n",
            "Thissuggeststhatapossibleapproachtocomputing∂C/∂wl istocarefullytrackhowa\n",
            "jk\n",
            "smallchangeinwl propagatestocauseasmallchangeinC.\n",
            "Ifwecandothat,beingcareful\n",
            "jk\n",
            "toexpresseverythingalongthewayintermsofeasilycomputablequantities,thenweshould\n",
            "beabletocompute∂C/∂wl . jk\n",
            "Let’strytocarrythisout. Thechange∆wl causesasmallchange∆al intheactivation\n",
            "jk j\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 55\n",
            "(cid:12)\n",
            "ofthe j-thneuroninthel-thlayer. Thischangeisgivenby\n",
            "∂al\n",
            "∆al j\n",
            "≈\n",
            "∂wl\n",
            "j\n",
            "j\n",
            "k\n",
            "∆wl jk .\n",
            "(2.24) 2\n",
            "Thechangeinactivation∆al willcausechangesinalltheactivationsinthenextlayer,i.e.,\n",
            "j\n",
            "the(l+1)-thlayer. We’llconcentrateonthewayjustasingleoneofthoseactivationsis\n",
            "affected,sayal+1,\n",
            "q\n",
            "Infact,it’llcausethefollowingchange:\n",
            "∂al+1\n",
            "∆al+1 q ∆al. (2.25)\n",
            "q\n",
            "≈\n",
            "∂al\n",
            "j\n",
            "j\n",
            "SubstitutingintheexpressionfromEquation2.24,weget:\n",
            "∂al+1 ∂al\n",
            "∆al+1 q j ∆wl .\n",
            "(2.26)\n",
            "q\n",
            "≈\n",
            "∂al\n",
            "j\n",
            "∂wl\n",
            "jk\n",
            "jk\n",
            "Ofcourse,thechange∆al+1will,inturn,causechangesintheactivationsinthenextlayer. q\n",
            "Infact,wecanimagineapathallthewaythroughthenetworkfromwl toC,witheach\n",
            "jk\n",
            "changeinactivationcausingachangeinthenextactivation,and,finally,achangeinthecost\n",
            "attheoutput. Ifthepathgoesthroughactivationsal,al+1, ,aL 1,aL thentheresulting\n",
            "j q n− m\n",
            "expressionis ···\n",
            "∆C\n",
            "∂C ∂a\n",
            "m\n",
            "L ∂a\n",
            "n\n",
            "L\n",
            "−\n",
            "1\n",
            "... ∂a\n",
            "q\n",
            "l+1 ∂al\n",
            "j ∆wl , (2.27)\n",
            "≈\n",
            "∂a\n",
            "m\n",
            "L ∂a\n",
            "n\n",
            "L\n",
            "−\n",
            "1∂a\n",
            "p\n",
            "L\n",
            "−\n",
            "2 ∂al\n",
            "j\n",
            "∂wl\n",
            "jk\n",
            "jk\n",
            "thatis,we’vepickedupa∂a/∂atypetermforeachadditionalneuronwe’vepassedthrough,\n",
            "aswellasthe∂C/∂aL termattheend. ThisrepresentsthechangeinC duetochangesin\n",
            "m\n",
            "theactivationsalongthisparticularpaththroughthenetwork. Ofcourse,there’smanypaths\n",
            "bywhichachangeinwl canpropagatetoaffectthecost,andwe’vebeenconsideringjusta\n",
            "jk\n",
            "singlepath. TocomputethetotalchangeinC itisplausiblethatweshouldsumoverallthe\n",
            "possiblepathsbetweentheweightandthefinalcost,i.e.,\n",
            "∆C (cid:88) ∂C ∂a m L ∂a n L − 1 ... ∂a q l+1 ∂al j ∆wl , (2.28)\n",
            "≈mnp...q\n",
            "∂a\n",
            "m\n",
            "L ∂a\n",
            "n\n",
            "L\n",
            "−\n",
            "1∂a\n",
            "p\n",
            "L\n",
            "−\n",
            "2 ∂al\n",
            "j\n",
            "∂wl\n",
            "jk\n",
            "jk\n",
            "\n",
            "(cid:12)\n",
            "56 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "wherewe’vesummedoverallpossiblechoicesfortheintermediateneuronsalongthepath. Comparingwith(2.23)weseethat\n",
            "2 ∂ ∂ w C l = (cid:88) ∂ ∂ a C L ∂ ∂ a a L m L 1 ∂ ∂ a a n L L − 1 2 ...\n",
            "∂ ∂ a a q l+ l 1 ∂ ∂ w a l l j . (2.29)\n",
            "jk mnp...q m n− p− j jk\n",
            "Now,Equation2.29lookscomplicated.\n",
            "However,ithasaniceintuitiveinterpretation.\n",
            "We’re\n",
            "computingtherateofchangeofCwithrespecttoaweightinthenetwork. Whattheequation\n",
            "tellsusisthateveryedgebetweentwoneuronsinthenetworkisassociatedwitharate\n",
            "factorwhichisjustthepartialderivativeofoneneuron’sactivationwithrespecttotheother\n",
            "neuron’sactivation. Theedge fromthefirst weightto thefirstneuron hasa ratefactor\n",
            "∂al/∂wl . Theratefactorforapathisjusttheproductoftheratefactorsalongthepath. j jk\n",
            "Andthetotalrateofchange∂C/∂wl isjustthesumoftheratefactorsofallpathsfromthe\n",
            "jk\n",
            "initialweighttothefinalcost. Thisprocedureisillustratedhere,forasinglepath:\n",
            "WhatI’vebeenprovidinguptonowisaheuristicargument,awayofthinkingaboutwhat’s\n",
            "goingonwhenyouperturbaweightinanetwork. Letmesketchoutalineofthinkingyou\n",
            "couldusetofurtherdevelopthisargument. First,youcouldderiveexplicitexpressionsfor\n",
            "alltheindividualpartialderivativesinEquation2.29. That’seasytodowithabitofcalculus. Havingdonethat,youcouldthentrytofigureouthowtowriteallthesumsoverindicesas\n",
            "matrixmultiplications. Thisturnsouttobetedious,andrequiressomepersistence,butnot\n",
            "extraordinaryinsight. Afterdoingallthis,andthensimplifyingasmuchaspossible,what\n",
            "youdiscoveristhatyouendupwithexactlythebackpropagationalgorithm! Andsoyoucan\n",
            "thinkofthebackpropagationalgorithmasprovidingawayofcomputingthesumoverthe\n",
            "ratefactorforallthesepaths. Or,toputitslightlydifferently,thebackpropagationalgorithm\n",
            "isacleverwayofkeepingtrackofsmallperturbationstotheweights(andbiases)asthey\n",
            "propagatethroughthenetwork,reachtheoutput,andthenaffectthecost. Now,I’mnotgoingtoworkthroughallthishere. It’smessyandrequiresconsiderable\n",
            "caretoworkthroughallthedetails. Ifyou’reupforachallenge,youmayenjoyattemptingit. Andevenifnot,Ihopethislineofthinkinggivesyousomeinsightintowhatbackpropagation\n",
            "isaccomplishing. Whatabouttheothermystery–howbackpropagationcouldhavebeendiscoveredin\n",
            "thefirstplace? Infact,ifyoufollowtheapproachIjustsketchedyouwilldiscoveraproof\n",
            "ofbackpropagation. Unfortunately,theproofisquiteabitlongerandmorecomplicated\n",
            "thantheoneIdescribedearlierinthischapter. Sohowwasthatshort(butmoremysterious)\n",
            "proofdiscovered? Whatyoufindwhenyouwriteoutallthedetailsofthelongproofis\n",
            "\n",
            "(cid:12)\n",
            "2.9. Backpropagation: thebigpicture (cid:12) 57\n",
            "(cid:12)\n",
            "that,afterthefact,thereareseveralobvioussimplificationsstaringyouintheface. You\n",
            "makethosesimplifications,getashorterproof,andwritethatout.\n",
            "Andthenseveralmore\n",
            "obvioussimplificationsjumpoutatyou. Soyourepeatagain. Theresultafterafewiterations\n",
            "istheproofwesawearlier6 –short,butsomewhatobscure,becauseallthesignpoststo 2\n",
            "itsconstructionhavebeenremoved! Iam,ofcourse,askingyoutotrustmeonthis,but\n",
            "therereallyisnogreatmysterytotheoriginoftheearlierproof. It’sjustalotofhardwork\n",
            "simplifyingtheproofI’vesketchedinthissection. 6Thereisonecleversteprequired.InEquation2.29theintermediatevariablesareactivationslike\n",
            "al+1.Thecleverideaistoswitchtousingweightedinputs,likezl+1,astheintermediatevariables.If\n",
            "q q\n",
            "youdon’thavethisidea,andinsteadcontinueusingtheactivationsal+1,theproofyouobtainturnsout\n",
            "q\n",
            "tobeslightlymorecomplexthantheproofgivenearlierinthechapter. \n",
            "(cid:12)\n",
            "58 (cid:12) Howthebackpropagationalgorithmworks\n",
            "(cid:12)\n",
            "2\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 59\n",
            "(cid:12)\n",
            "3333\n",
            "Improving the way neural\n",
            "networks learn\n",
            "3\n",
            "When a golf player is first learning to play golf, they usually spend most of their time\n",
            "developingabasicswing. Onlygraduallydotheydevelopothershots,learningtochip,draw\n",
            "andfadetheball,buildingonandmodifyingtheirbasicswing.\n",
            "Inasimilarway,uptonow\n",
            "we’vefocusedonunderstandingthebackpropagationalgorithm. It’sour“basicswing”,the\n",
            "foundationforlearninginmostworkonneuralnetworks. InthischapterIexplainasuiteof\n",
            "techniqueswhichcanbeusedtoimproveonourvanillaimplementationofbackpropagation,\n",
            "andsoimprovethewayournetworkslearn. Thetechniqueswe’lldevelopinthischapterinclude: abetterchoiceofcostfunction,\n",
            "knownasthecross-entropycostfunction;fourso-called“regularization”methods(L1and\n",
            "L2regularization,dropout,andartificialexpansionofthetrainingdata),whichmakeour\n",
            "networksbetteratgeneralizingbeyondthetrainingdata;abettermethodforinitializing\n",
            "theweightsinthenetwork;andasetofheuristicstohelpchoosegoodhyper-parameters\n",
            "forthenetwork. I’llalsooverviewseveralothertechniquesinlessdepth. Thediscussions\n",
            "arelargelyindependentofoneanother,andsoyoumayjumpaheadifyouwish. We’llalso\n",
            "implementmanyofthetechniquesinrunningcode,andusethemtoimprovetheresults\n",
            "obtainedonthehandwritingclassificationproblemstudiedinChapter1. Ofcourse,we’reonlycoveringafewofthemany,manytechniqueswhichhavebeen\n",
            "developedforuseinneuralnets. Thephilosophyisthatthebestentreetotheplethoraof\n",
            "availabletechniquesisin-depthstudyofafewofthemostimportant. Masteringthoseimpor-\n",
            "tanttechniquesisnotjustusefulinitsownright,butwillalsodeepenyourunderstandingof\n",
            "whatproblemscanarisewhenyouuseneuralnetworks. Thatwillleaveyouwellprepared\n",
            "toquicklypickupothertechniques,asyouneedthem.\n",
            "\n",
            "(cid:12)\n",
            "60 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "3.1 The cross-entropy cost function\n",
            "Mostofusfinditunpleasanttobewrong. SoonafterbeginningtolearnthepianoIgavemy\n",
            "firstperformancebeforeanaudience.\n",
            "Iwasnervous,andbeganplayingthepieceanoctave\n",
            "toolow. Igotconfused,andcouldn’tcontinueuntilsomeonepointedoutmyerror. Iwas\n",
            "veryembarrassed. Yetwhileunpleasant,wealsolearnquicklywhenwe’redecisivelywrong. 3 YoucanbetthatthenexttimeIplayedbeforeanaudienceIplayedinthecorrectoctave! By\n",
            "contrast,welearnmoreslowlywhenourerrorsarelesswell-defined. Ideally,wehopeandexpectthatourneuralnetworkswilllearnfastfromtheirerrors.\n",
            "Isthiswhathappensinpractice? Toanswerthisquestion,let’slookatatoyexample. The\n",
            "exampleinvolvesaneuronwithjustoneinput:\n",
            "We’lltrainthisneurontodosomethingridiculouslyeasy: taketheinput1totheoutput0. Ofcourse,thisissuchatrivialtaskthatwecouldeasilyfigureoutanappropriateweightand\n",
            "biasbyhand,withoutusingalearningalgorithm. However,itturnsouttobeilluminatingto\n",
            "usegradientdescenttoattempttolearnaweightandbias. Solet’stakealookathowthe\n",
            "neuronlearns. Tomakethingsdefinite,I’llpicktheinitialweighttobe0.6andtheinitialbiastobe\n",
            "0.9. Thesearegenericchoicesusedasaplacetobeginlearning,Iwasn’tpickingthemto\n",
            "bespecialinanyway. Theinitialoutputfromtheneuronis0.82,soquiteabitoflearning\n",
            "willbeneededbeforeourneurongetsnearthedesiredoutput,0.0. Thelearningrateis\n",
            "η=0.15,whichturnsouttobeslowenoughthatwecanfollowwhat’shappening,butfast\n",
            "enoughthatwecangetsubstantiallearninginjustafewseconds. Thecostisthequadratic\n",
            "costfunction,C,introducedbackinChapter1. I’llremindyouoftheexactformofthecost\n",
            "functionshortly,sothere’snoneedtogoanddigupthedefinition.\n",
            "Asyoucansee,theneuronrapidlylearnsaweightandbiasthatdrivesdownthecost,and\n",
            "givesanoutputfromtheneuronofabout0.09. That’snotquitethedesiredoutput,0.0,but\n",
            "itisprettygood. Suppose,however,thatweinsteadchooseboththestartingweightandthe\n",
            "startingbiastobe2.0. Inthiscasetheinitialoutputis0.98,whichisverybadlywrong. Let’s\n",
            "lookathowtheneuronlearnstooutput0inthiscase. \n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 61\n",
            "(cid:12)\n",
            "3\n",
            "Althoughthisexampleusesthesamelearningrate(η =0.15),wecanseethatlearning\n",
            "startsoutmuchmoreslowly. Indeed,forthefirst150orsolearningepochs,theweights\n",
            "andbiasesdon’tchangemuchatall. Thenthelearningkicksinand,muchasinourfirst\n",
            "example,theneuron’soutputrapidlymovescloserto0.0. Thisbehaviorisstrangewhencontrastedtohumanlearning. AsIsaidatthebeginning\n",
            "ofthissection,weoftenlearnfastestwhenwe’rebadlywrongaboutsomething. Butwe’ve\n",
            "justseenthatourartificialneuronhasalotofdifficultylearningwhenit’sbadlywrong–far\n",
            "moredifficultythanwhenit’sjustalittlewrong. What’smore,itturnsoutthatthisbehavior\n",
            "occursnotjustinthistoymodel,butinmoregeneralnetworks. Whyislearningsoslow?\n",
            "Andcanwefindawayofavoidingthisslowdown? Tounderstandtheoriginoftheproblem,considerthatourneuronlearnsbychangingthe\n",
            "weightandbiasataratedeterminedbythepartialderivativesofthecostfunction,∂C/∂w\n",
            "and ∂C/∂b. So saying “learning is slow” is really the same as saying that those partial\n",
            "derivativesaresmall. Thechallengeistounderstandwhytheyaresmall.\n",
            "Tounderstand\n",
            "that,let’scomputethepartialderivatives. Recallthatwe’reusingthequadraticcostfunction,\n",
            "which,fromEquation1.6,isgivenby\n",
            "(y a) 2\n",
            "C=\n",
            "−\n",
            ", (3.1)\n",
            "2\n",
            "where a istheneuron’soutputwhenthetraininginput x =1isused, and y =0isthe\n",
            "correspondingdesiredoutput. Towritethismoreexplicitlyintermsoftheweightandbias,\n",
            "recallthata= σ (z),wherez=wx+b. Usingthechainruletodifferentiatewithrespectto\n",
            "theweightandbiasweget\n",
            "∂C\n",
            "∂w = (a\n",
            "−\n",
            "y) σ (cid:48)(z)x=aσ (cid:48)(z) (3.2)\n",
            "∂C\n",
            "∂b = (a\n",
            "−\n",
            "y) σ (cid:48)(z)=aσ (cid:48)(z), (3.3)\n",
            "whereIhavesubstituted x=1and y=0. Tounderstandthebehavioroftheseexpressions,\n",
            "let’slookmorecloselyattheσ (cid:48)(z)termontheright-handside. Recalltheshapeoftheσ\n",
            "function:\n",
            "\n",
            "(cid:12)\n",
            "62 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Sigmoidfunction\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "3\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "6 4 2 0 2 4 6\n",
            "− − −\n",
            "Wecanseefromthisgraphthatwhentheneuron’soutputiscloseto1,thecurvegetsvery\n",
            "flat,andsoσ (cid:48)(z)getsverysmall. Equations3.2and3.3thentellusthat∂C/∂wand∂C/∂b\n",
            "getverysmall.\n",
            "Thisistheoriginofthelearningslowdown. What’smore,asweshallseea\n",
            "littlelater,thelearningslowdownoccursforessentiallythesamereasoninmoregeneral\n",
            "neuralnetworks,notjustthetoyexamplewe’vebeenplayingwith. 3.1.1 Introducingthecross-entropycostfunction\n",
            "Howcanweaddressthelearningslowdown? Itturnsoutthatwecansolvetheproblemby\n",
            "replacingthequadraticcostwithadifferentcostfunction,knownasthecross-entropy. To\n",
            "understandthecross-entropy,let’smovealittleawayfromoursuper-simpletoymodel. We’ll\n",
            "supposeinsteadthatwe’retryingtotrainaneuronwithseveralinputvariables, x ,x ,...,\n",
            "1 2\n",
            "correspondingweightsw ,w ,...,andabias, b:\n",
            "1 2\n",
            "Theoutputfromtheneuronis,ofcourse,a= σ (z),wherez= (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "b j+bistheweighted\n",
            "sumoftheinputs. Wedefinethecross-entropycostfunctionforthisneuronby\n",
            "1(cid:88)\n",
            "C= [ylna+(1 y)ln(1 a)], (3.4)\n",
            "−n x − −\n",
            "wherenisthetotalnumberofitemsoftrainingdata,thesumisoveralltraininginputs, x,\n",
            "and y isthecorrespondingdesiredoutput. It’snotobviousthattheexpression(3.4)fixesthelearningslowdownproblem. Infact,\n",
            "frankly,it’snotevenobviousthatitmakessensetocallthisacostfunction! Beforeaddressing\n",
            "thelearningslowdown,let’sseeinwhatsensethecross-entropycanbeinterpretedasacost\n",
            "function. Twopropertiesinparticularmakeitreasonabletointerpretthecross-entropyasacost\n",
            "function. First,it’snon-negative,thatis,C>0.\n",
            "Toseethis,noticethat: (a)alltheindividual\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 63\n",
            "(cid:12)\n",
            "termsinthesumin(3.4)arenegative,sincebothlogarithmsareofnumbersintherange0\n",
            "to1;and(b)thereisaminussignoutthefrontofthesum. Second,iftheneuron’sactualoutputisclosetothedesiredoutputforalltraininginputs,\n",
            "x,thenthecross-entropywillbeclosetozero1. Toseethis,supposeforexamplethat y=0\n",
            "anda 0forsomeinputx.\n",
            "Thisisacasewhentheneuronisdoingagoodjobonthatinput. Wesee≈thatthefirsttermintheexpression(57)forthecostvanishes,since y=0,whilethe\n",
            "secondtermisjust ln(1 a) 0. Asimilaranalysisholdswhen y=1anda 1. Andso 3\n",
            "thecontributionto−theco−stwi≈llbelowprovidedtheactualoutputiscloseto≈thedesired\n",
            "output. Summingup,thecross-entropyispositive,andtendstowardzeroastheneurongets\n",
            "betteratcomputingthedesiredoutput, y,foralltraininginputs,x. Thesearebothproperties\n",
            "we’dintuitivelyexpectforacostfunction. Indeed,bothpropertiesarealsosatisfiedbythe\n",
            "quadraticcost. Sothat’sgoodnewsforthecross-entropy. Butthecross-entropycostfunction\n",
            "hasthebenefitthat,unlikethequadraticcost,itavoidstheproblemoflearningslowing\n",
            "down.\n",
            "Toseethis,let’scomputethepartialderivativeofthecross-entropycostwithrespect\n",
            "totheweights. Wesubstitutea= σ (z)into(3.4),andapplythechainruletwice,obtaining:\n",
            "∂C 1(cid:88)(cid:129) y 1 y (cid:139) ∂σ 1(cid:88)(cid:129) y 1 y (cid:139)\n",
            "∂w\n",
            "j\n",
            "= −n\n",
            "x\n",
            "σ (z)−1 −σ (z) ∂w\n",
            "j\n",
            "= −n\n",
            "x\n",
            "σ (z)−1 −σ (z) σ (cid:48)(z)x j . (3.5)\n",
            "− −\n",
            "Puttingeverythingoveracommondenominatorandsimplifyingthisbecomes:\n",
            "∂ ∂ w C j = 1 n (cid:88) x σ (z σ )( (cid:48) 1 (z)x σ j (z)) ( σ (z) − y). (3.6)\n",
            "−\n",
            "Usingthedefinitionofthesigmoidfunction,σ (z)=1/ (1+e\n",
            "−\n",
            "z ),andalittlealgebrawecan\n",
            "showthatσ (cid:48)(z)= σ (z)(1 σ (z)). I’llaskyoutoverifythisinanexercisebelow,butfor\n",
            "nowlet’sacceptitasgiven−. Weseethattheσ (cid:48)(z)andσ (z)(1 σ (z))termscancelinthe\n",
            "equationjustabove,anditsimplifiestobecome: −\n",
            "∂C 1(cid:88)\n",
            "∂w j = n x x j( σ (z) − y). (3.7)\n",
            "Thisisabeautifulexpression.\n",
            "Ittellsusthattherateatwhichtheweightlearnsiscontrolled\n",
            "byσ (z) y,i.e.,bytheerrorintheoutput. Thelargertheerror,thefastertheneuronwill\n",
            "learn. Th−isisjustwhatwe’dintuitivelyexpect. Inparticular,itavoidsthelearningslowdown\n",
            "causedbytheσ (cid:48)(z)termintheanalogousequationforthequadraticcost,Equation(3.2). Whenweusethecross-entropy,theσ (cid:48)(z)termgetscanceledout,andwenolongerneed\n",
            "worryaboutitbeingsmall. Thiscancellationisthespecialmiracleensuredbythecross-\n",
            "entropycostfunction. Actually,it’snotreallyamiracle. Aswe’llseelater,thecross-entropy\n",
            "wasspeciallychosentohavejustthisproperty. Inasimilarway,wecancomputethepartialderivativeforthebias. Iwon’tgothrough\n",
            "1ToprovethisIwillneedtoassumethatthedesiredoutputs yarealleither0or1.Thisisusually\n",
            "thecasewhensolvingclassificationproblems,forexample,orwhencomputingBooleanfunctions.To\n",
            "understandwhathappenswhenwedon’tmakethisassumption,seetheexercisesattheendofthis\n",
            "section. \n",
            "(cid:12)\n",
            "64 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "allthedetailsagain,butyoucaneasilyverifythat\n",
            "∂C 1(cid:88)\n",
            "∂b = n x ( σ (z) − y). (3.8)\n",
            "Again,thisavoidsthelearningslowdowncausedbytheσ (cid:48)(z)termintheanalogousequation\n",
            "3 forthequadraticcost,Equation(3.3). Exercise\n",
            "Verifythatσ (cid:48)(z)= σ (z)(1 σ (z))\n",
            "• −\n",
            "Let’sreturntothetoyexampleweplayedwithearlier,andexplorewhathappenswhenwe\n",
            "usethecross-entropyinsteadofthequadraticcost. Tore-orientourselves,we’llbeginwith\n",
            "thecasewherethequadraticcostdidjustfine,withstartingweight0.6andstartingbias0.9:\n",
            "Unsurprisingly,theneuronlearnsperfectlywellinthisinstance,justasitdidearlier.\n",
            "And\n",
            "nowlet’slookatthecasewhereourneurongotstuckbefore,withtheweightandbiasboth\n",
            "startingat2.0:\n",
            "Success! Thistimetheneuronlearnedquickly,justaswehoped. Ifyouobservecloselyyou\n",
            "canseethattheslopeofthecostcurvewasmuchsteeperinitiallythantheinitialflatregion\n",
            "onthecorrespondingcurveforthequadraticcost. It’sthatsteepnesswhichthecross-entropy\n",
            "buysus,preventingusfromgettingstuckjustwhenwe’dexpectourneurontolearnfastest,\n",
            "i.e.,whentheneuronstartsoutbadlywrong. \n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 65\n",
            "(cid:12)\n",
            "Ididn’tsaywhatlearningratewasusedintheexamplesjustillustrated. Earlier,withthe\n",
            "quadraticcost,weusedη =0.15.\n",
            "Shouldwehaveusedthesamelearningrateinthenew\n",
            "examples? Infact,withthechangeincostfunctionit’snotpossibletosaypreciselywhatit\n",
            "meanstousethe“same”learningrate;it’sanapplesandorangescomparison. Forbothcost\n",
            "functionsIsimplyexperimentedtofindalearningratethatmadeitpossibletoseewhatis\n",
            "goingon. Ifyou’restillcurious,despitemydisavowal,here’sthelowdown: Iusedη =0.005\n",
            "intheexamplesjustgiven. 3\n",
            "Youmightobjectthatthechangeinlearningratemakesthegraphsabovemeaningless.\n",
            "Whocareshowfasttheneuronlearns,whenourchoiceoflearningratewasarbitraryto\n",
            "beginwith?! Thatobjectionmissesthepoint. Thepointofthegraphsisn’tabouttheabsolute\n",
            "speedoflearning. It’sabouthowthespeedoflearningchanges. Inparticular,whenwe\n",
            "usethequadraticcostlearningisslowerwhentheneuronisunambiguouslywrongthan\n",
            "itislateron,astheneurongetsclosertothecorrectoutput;whilewiththecross-entropy\n",
            "learningisfasterwhentheneuronisunambiguouslywrong. Thosestatementsdon’tdepend\n",
            "onhowthelearningrateisset. We’vebeenstudyingthecross-entropyforasingleneuron.However,it’seasytogeneralize\n",
            "thecross-entropytomany-neuronmulti-layernetworks. Inparticular,suppose y= y\n",
            "1\n",
            ",y\n",
            "2\n",
            ",... are the desired values at the output neurons, i.e., the neurons in the final layer, while\n",
            "aL,aL,...aretheactualoutputvalues. Thenwedefinethecross-entropyby\n",
            "1 2\n",
            "(cid:88)(cid:148) (cid:151)\n",
            "y j lnaL j +(1 y j)ln(1 aL j) . (3.9)\n",
            "j − −\n",
            "(cid:80)\n",
            "Thisisthesameasourearlierexpression,Equation(3.4),exceptnowwe’vegotthe\n",
            "j\n",
            "summingoveralltheoutputneurons. Iwon’texplicitlyworkthroughaderivation,butit\n",
            "shouldbeplausiblethatusingtheexpression(3.9)avoidsalearningslowdowninmany-\n",
            "neuronnetworks.\n",
            "Ifyou’reinterested,youcanworkthroughthederivationintheproblem\n",
            "below.\n",
            "Incidentally,I’musingtheterm“cross-entropy”inawaythathasconfusedsomeearly\n",
            "readers,sinceitsuperficiallyappearstoconflictwithothersources. Inparticular,it’scommon\n",
            "(cid:80)\n",
            "todefinethecross-entropyfortwoprobabilitydistributions,p andq ,as p lnq . This\n",
            "j j j j j\n",
            "definitionmaybeconnectedto(3.4),ifwetreatasinglesigmoidneuronasoutputtinga\n",
            "probabilitydistributionconsistingoftheneuron’sactivationaanditscomplement1 a. However,whenwehavemanysigmoidneuronsinthefinallayer,thevectoraL of−activa-\n",
            "(cid:80)j\n",
            "tionsdon’tusuallyformaprobabilitydistribution. Asaresult,adefinitionlike p lnq\n",
            "j j j\n",
            "doesn’tevenmakesense,sincewe’renotworkingwithprobabilitydistributions. Instead,\n",
            "youcanthinkof(3.9)asasummedsetofper-neuroncross-entropies,withtheactivation\n",
            "ofeachneuronbeinginterpretedaspartofatwo-elementprobabilitydistribution2. Inthis\n",
            "sense,(3.9)isageneralizationofthecross-entropyforprobabilitydistributions. Whenshouldweusethecross-entropyinsteadofthequadraticcost? Infact,thecross-\n",
            "entropyisnearlyalwaysthebetterchoice,providedtheoutputneuronsaresigmoidneurons. Toseewhy,considerthatwhenwe’resettingupthenetworkweusuallyinitializetheweights\n",
            "andbiasesusingsomesortofrandomization. Itmayhappenthatthoseinitialchoicesresult\n",
            "inthenetworkbeingdecisivelywrongforsometraininginput–thatis,anoutputneuron\n",
            "willhavesaturatednear1,whenitshouldbe0,orviceversa. Ifwe’reusingthequadratic\n",
            "costthatwillslowdownlearning. Itwon’tstoplearningcompletely,sincetheweightswill\n",
            "2Ofcourse,inournetworkstherearenoprobabilisticelements,sothey’renotreallyprobabilities. \n",
            "(cid:12)\n",
            "66 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "continuelearningfromothertraininginputs,butit’sobviouslyundesirable. Exercises\n",
            "Onegotchawiththecross-entropyisthatitcanbedifficultatfirsttorememberthe\n",
            "• respectiverolesoftheysandtheas.\n",
            "It’seasytogetconfusedaboutwhethertheright\n",
            "formis\n",
            "[ylna+(1 y)ln(1 a)]. 3 − − −\n",
            "Whathappenstothesecondoftheseexpressionswhen y=0or1? Doesthisproblem\n",
            "afflictthefirstexpression?\n",
            "Whyorwhynot? Inthesingle-neurondiscussionatthestartofthissection,Iarguedthatthecross-\n",
            "• entropy is small if σ (z) y for all training inputs. The argument relied on y\n",
            "beingequaltoeither0or≈1. Thisisusuallytrueinclassificationproblems,butfor\n",
            "otherproblems(e.g.,regressionproblems) y cansometimestakevaluesintermediate\n",
            "between0and1. Showthatthecross-entropyisstillminimizedwhenσ (z)= y for\n",
            "alltraininginputs. Whenthisisthecasethecross-entropyhasthevalue:\n",
            "1(cid:88)\n",
            "C= [ylny+(1 y)ln(1 y)]. (3.10)\n",
            "−n x − −\n",
            "Thequantity [ylny+(1 y)ln(1 y)]issometimesknownasthebinaryentropy. − − −\n",
            "Problems\n",
            "Many-layermulti-neuronnetworksInthenotationintroducedinthelastchapter,\n",
            "• showthatforthequadraticcostthepartialderivativewithrespecttoweightsinthe\n",
            "outputlayeris\n",
            "∂C 1(cid:88)\n",
            "∂wL jk = n x a k L − 1 (aL j − y j) σ (cid:48)(z j L ). (3.11)\n",
            "Thetermσ (cid:48)(z\n",
            "j\n",
            "L )causesalearningslowdownwheneveranoutputneuronsaturates\n",
            "onthewrongvalue. Showthatforthecross-entropycosttheoutputerrorδL fora\n",
            "singletrainingexample x isgivenby\n",
            "δL =aL y. (3.12)\n",
            "−\n",
            "Usethisexpressiontoshowthatthepartialderivativewithrespecttotheweightsin\n",
            "theoutputlayerisgivenby\n",
            "∂C 1(cid:88)\n",
            "∂wL jk = n x a k L − 1 (aL j − y j). (3.13)\n",
            "Theσ (cid:48)(z\n",
            "j\n",
            "L )termhasvanished,andsothecross-entropyavoidstheproblemoflearning\n",
            "slowdown,notjustwhenusedwithasingleneuron,aswesawearlier,butalsoin\n",
            "many-layermulti-neuronnetworks. Asimplevariationonthisanalysisholdsalsofor\n",
            "thebiases. Ifthisisnotobvioustoyou,thenyoushouldworkthroughthatanalysis\n",
            "aswell. UsingthequadraticcostwhenwehavelinearneuronsintheoutputlayerSup-\n",
            "• posethatwehaveamany-layermulti-neuronnetwork. Supposealltheneuronsin\n",
            "thefinallayerarelinearneurons,meaningthatthesigmoidactivationfunctionisnot\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 67\n",
            "(cid:12)\n",
            "applied,andtheoutputsaresimplyaL\n",
            "j\n",
            "=z\n",
            "j\n",
            "L. Showthatifweusethequadraticcost\n",
            "functionthentheoutputerrorδL forasingletrainingexample x isgivenby\n",
            "δL =aL y. (3.14)\n",
            "−\n",
            "Similarlytothepreviousproblem,usethisexpressiontoshowthatthepartialderiva-\n",
            "tiveswithrespecttotheweightsandbiasesintheoutputlayeraregivenby 3\n",
            "∂C 1(cid:88)\n",
            "∂wL jk = n x a k L − 1 (aL j − y j) (3.15)\n",
            "∂C 1(cid:88)\n",
            "∂bL j = n x (aL j − y j). (3.16)\n",
            "Thisshowsthatiftheoutputneuronsarelinearneuronsthenthequadraticcostwill\n",
            "notgiverisetoanyproblemswithalearningslowdown. Inthiscasethequadratic\n",
            "costis,infact,anappropriatecostfunctiontouse. 3.1.2 Usingthecross-entropytoclassifyMNISTdigits\n",
            "Thecross-entropyiseasytoimplementaspartofaprogramwhichlearnsusinggradient\n",
            "descentandbackpropagation. We’lldothatlaterinthechapter,developinganimproved\n",
            "versionofourearlierprogramforclassifyingtheMNISThandwrittendigits,network.py. Thenewprogramiscallednetwork2.py,andincorporatesnotjustthecross-entropy,but\n",
            "alsoseveralothertechniquesdevelopedinthischapter3. Fornow,let’slookathowwellour\n",
            "newprogramclassifiesMNISTdigits. AswasthecaseinChapter1,we’lluseanetworkwith\n",
            "30hiddenneurons,andwe’lluseamini-batchsizeof10. Wesetthelearningratetoη =0.54\n",
            "andwetrainfor30epochs. Theinterfacetonetwork2.pyisslightlydifferentthannetwork.\n",
            "py,butitshouldstillbeclearwhatisgoingon.\n",
            "Youcan,bytheway,getdocumentationabout\n",
            "network2.py’sinterfacebyusingcommandssuchashelp(network2.Network.SGD)ina\n",
            "Pythonshell. >>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data,\n",
            "monitor_evaluation_accuracy=True)\n",
            "3ThecodeisavailableonGitHub. 4InChapter1weusedthequadraticcostandalearningrateofη =3.0.Asdiscussedabove,it’snot\n",
            "possibletosaypreciselywhatitmeanstousethe“same”learningratewhenthecostfunctionischanged.\n",
            "ForbothcostfunctionsIexperimentedtofindalearningratethatprovidesnear-optimalperformance,\n",
            "giventheotherhyper-parameterchoices. Thereis,incidentally,averyroughgeneralheuristicforrelatingthelearningrateforthecross-entropy\n",
            "andthequadraticcost. Aswesawearlier,thegradienttermsforthequadraticcosthaveanextra\n",
            "σ (cid:48)= σ (1 σ )terminthem. Supposeweaveragethisovervaluesforσ, (cid:82) 0 1 dσσ (1 σ )=1/6. We\n",
            "seethat(v−eryroughly)thequadraticcostlearnsanaverageof6timesslower,forth−esamelearning\n",
            "rate.Thissuggeststhatareasonablestartingpointistodividethelearningrateforthequadraticcost\n",
            "by6.Ofcourse,thisargumentisfarfromrigorous,andshouldn’tbetakentooseriously.Still,itcan\n",
            "sometimesbeausefulstartingpoint. \n",
            "(cid:12)\n",
            "68 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Note,bytheway,thatthenet.large_weight_initializer()commandisusedtoinitial-\n",
            "izetheweightsandbiasesinthesamewayasdescribedinChapter1. Weneedtorunthis\n",
            "commandbecauselaterinthischapterwe’llchangethedefaultweightinitializationinour\n",
            "networks. Theresultfromrunningtheabovesequenceofcommandsisanetworkwith95.49\n",
            "percentaccuracy.\n",
            "ThisisprettyclosetotheresultweobtainedinChapter1,95.42percent,\n",
            "usingthequadraticcost. 3 Let’slookalsoatthecasewhereweuse100hiddenneurons,thecross-entropy,and\n",
            "otherwisekeeptheparametersthesame. Inthiscaseweobtainanaccuracyof96.82percent. That’sasubstantialimprovementovertheresultsfromChapter1, whereweobtaineda\n",
            "classificationaccuracyof96.59percent,usingthequadraticcost. Thatmaylooklikeasmall\n",
            "change, butconsiderthattheerrorratehasdroppedfrom3.41percentto3.18percent. Thatis,we’veeliminatedaboutoneinfourteenoftheoriginalerrors. That’squiteahandy\n",
            "improvement. It’sencouragingthatthecross-entropycostgivesussimilarorbetterresultsthanthe\n",
            "quadraticcost. However,theseresultsdon’tconclusivelyprovethatthecross-entropyisa\n",
            "betterchoice. ThereasonisthatI’veputonlyalittleeffortintochoosinghyper-parameters\n",
            "suchaslearningrate,mini-batchsize,andsoon. Fortheimprovementtobereallyconvincing\n",
            "we’dneedtodoathoroughjoboptimizingsuchhyper-parameters. Still, theresultsare\n",
            "encouraging,andreinforceourearliertheoreticalargumentthatthecross-entropyisabetter\n",
            "choicethanthequadraticcost. This,bytheway,ispartofageneralpatternthatwe’llseethroughthischapterand,\n",
            "indeed,throughmuchoftherestofthebook.\n",
            "We’lldevelopanewtechnique,we’lltryitout,\n",
            "andwe’llget“improved”results. Itis,ofcourse,nicethatweseesuchimprovements.\n",
            "But\n",
            "theinterpretationofsuchimprovementsisalwaysproblematic. They’reonlytrulyconvincing\n",
            "if we see an improvement after putting tremendous effort into optimizing all the other\n",
            "hyper-parameters. That’sagreatdealofwork,requiringlotsofcomputingpower,andwe’re\n",
            "notusuallygoingtodosuchanexhaustiveinvestigation.\n",
            "Instead,we’llproceedonthebasis\n",
            "ofinformaltestslikethosedoneabove. Still,youshouldkeepinmindthatsuchtestsfall\n",
            "shortofdefinitiveproof,andremainalerttosignsthattheargumentsarebreakingdown. Bynow,we’vediscussedthecross-entropyatgreatlength. Whygotosomucheffort\n",
            "whenitgivesonlyasmallimprovementtoourMNISTresults? Laterinthechapterwe’ll\n",
            "see other techniques – notably, regularization – which give much bigger improvements. Sowhysomuchfocusoncross-entropy? Partofthereasonisthatthecross-entropyisa\n",
            "widely-usedcostfunction, andsoisworthunderstandingwell. Butthemoreimportant\n",
            "reasonisthatneuronsaturationisanimportantprobleminneuralnets,aproblemwe’ll\n",
            "returntorepeatedlythroughoutthebook. AndsoI’vediscussedthecross-entropyatlength\n",
            "becauseit’sagoodlaboratorytobeginunderstandingneuronsaturationandhowitmaybe\n",
            "addressed. 3.1.3 Whatdoesthecross-entropymean? Wheredoesitcomefrom? Ourdiscussionofthecross-entropyhasfocusedonalgebraicanalysisandpracticalimplemen-\n",
            "tation. That’suseful,butitleavesunansweredbroaderconceptualquestions,like: whatdoes\n",
            "thecross-entropymean? Istheresomeintuitivewayofthinkingaboutthecross-entropy? Andhowcouldwehavedreamedupthecross-entropyinthefirstplace? Let’sbeginwiththelastofthesequestions: whatcouldhavemotivatedustothinkupthe\n",
            "cross-entropyinthefirstplace? Supposewe’ddiscoveredthelearningslowdowndescribed\n",
            "\n",
            "(cid:12)\n",
            "3.1. Thecross-entropycostfunction (cid:12) 69\n",
            "(cid:12)\n",
            "earlier,andunderstoodthattheoriginwastheσ (cid:48)(z)termsinEquations(3.2)and(3.3). Afterstaringatthoseequationsforabit,wemightwonderifit’spossibletochooseacost\n",
            "functionsothattheσ (cid:48)(z)termdisappeared. Inthatcase,thecostC=C\n",
            "x\n",
            "forasingletraining\n",
            "example x wouldsatisfy\n",
            "∂C\n",
            "∂w\n",
            "j\n",
            "= x j(a\n",
            "−\n",
            "y) (3.17)\n",
            "3\n",
            "∂C\n",
            "∂b = (a\n",
            "−\n",
            "y). (3.18)\n",
            "Ifwecouldchoosethecostfunctiontomaketheseequationstrue,thentheywouldcapture\n",
            "inasimplewaytheintuitionthatthegreatertheinitialerror,thefastertheneuronlearns.\n",
            "They’d also eliminate the problem of a learning slowdown. In fact, starting from these\n",
            "equationswe’llnowshowthatit’spossibletoderivetheformofthecross-entropy,simplyby\n",
            "followingourmathematicalnoses. Toseethis,notethatfromthechainrulewehave\n",
            "∂C ∂C\n",
            "∂b = ∂a σ (cid:48)(z).\n",
            "(3.19)\n",
            "Usingσ (cid:48)(z)= σ (z)(1 σ (z))=a(1 a)thelastequationbecomes\n",
            "− −\n",
            "∂C ∂C\n",
            "∂b = ∂a a(1\n",
            "−\n",
            "a). (3.20)\n",
            "ComparingtoEquation3.18weobtain\n",
            "∂C a y\n",
            "∂a = a(1 − a) . (3.21)\n",
            "−\n",
            "Integratingthisexpressionwithrespecttoagives\n",
            "C= [ylna+(1 y)ln(1 a)]+constant, (3.22)\n",
            "− − −\n",
            "forsomeconstantofintegration. Thisisthecontributiontothecostfromasingletraining\n",
            "example, x. Togetthefullcostfunctionwemustaverageovertrainingexamples,obtaining\n",
            "1(cid:88)\n",
            "C= [ylna+(1 y)ln(1 a)]+constant, (3.23)\n",
            "−n x − −\n",
            "wheretheconstanthereistheaverageoftheindividualconstantsforeachtrainingexam-\n",
            "ple. AndsoweseethatEquations(3.17)and(3.18)uniquelydeterminetheformofthe\n",
            "cross-entropy,uptoanoverallconstantterm. Thecross-entropyisn’tsomethingthatwas\n",
            "miraculouslypulledoutofthinair. Rather,it’ssomethingthatwecouldhavediscoveredina\n",
            "simpleandnaturalway. Whatabouttheintuitivemeaningofthecross-entropy? Howshouldwethinkaboutit? ExplainingthisindepthwouldtakeusfurtherafieldthanIwanttogo. However,itisworth\n",
            "mentioningthatthereisastandardwayofinterpretingthecross-entropythatcomesfromthe\n",
            "fieldofinformationtheory. Roughlyspeaking,theideaisthatthecross-entropyisameasure\n",
            "\n",
            "(cid:12)\n",
            "70 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "ofsurprise. Inparticular,ourneuronistryingtocomputethefunction x y= y(x). But\n",
            "insteaditcomputesthefunction x a = a(x). Supposewethinkofa→asourneuron’s\n",
            "estimatedprobabilitythat yis1,and→1 aistheestimatedprobabilitythattherightvaluefor\n",
            "y is0. Thenthecross-entropymeasure−show“surprised”weare,onaverage,whenwelearn\n",
            "thetruevaluefor y. Wegetlowsurpriseiftheoutputiswhatweexpect,andhighsurprise\n",
            "iftheoutputisunexpected. Ofcourse,Ihaven’tsaidexactlywhat“surprise”means,andso\n",
            "3 thisperhapsseemslikeemptyverbiage. Butinfactthereisapreciseinformation-theoretic\n",
            "way of saying what is meant by surprise. Unfortunately, I don’t know of a good, short,\n",
            "self-containeddiscussionofthissubjectthat’savailableonline.\n",
            "Butifyouwanttodigdeeper,\n",
            "thenWikipediacontainsabriefsummarythatwillgetyoustarteddowntherighttrack.\n",
            "And\n",
            "thedetailscanbefilledinbyworkingthroughthematerialsabouttheKraftinequalityin\n",
            "chapter5ofthebookaboutinformationtheorybyCoverandThomas. Problem\n",
            "We’vediscussedatlengththelearningslowdownthatcanoccurwhenoutputneurons\n",
            "• saturate,innetworksusingthequadraticcosttotrain. Anotherfactorthatmayinhibit\n",
            "learningisthepresenceofthe x terminEquation(3.7). Becauseofthisterm,when\n",
            "j\n",
            "aninput x isneartozero,thecorrespondingweightw willlearnslowly. Explain\n",
            "j j\n",
            "whyitisnotpossibletoeliminatethex termthroughacleverchoiceofcostfunction. j\n",
            "3.1.4 Softmax\n",
            "Inthischapterwe’llmostlyusethecross-entropycosttoaddresstheproblemoflearning\n",
            "slowdown. However,Iwanttobrieflydescribeanotherapproachtotheproblem,basedon\n",
            "whatarecalledsoftmaxlayersofneurons. We’renotactuallygoingtousesoftmaxlayersin\n",
            "theremainderofthechapter,soifyou’reinagreathurry,youcanskiptothenextsection. However,softmaxisstillworthunderstanding,inpartbecauseit’sintrinsicallyinteresting,\n",
            "andinpartbecausewe’llusesoftmaxlayersinChapter6,inourdiscussionofdeepneural\n",
            "networks. Theideaofsoftmaxistodefineanewtypeofoutputlayerforourneuralnetworks. I (cid:80) tb k e w g L j i k n a s k L − in 1 + th b e L j .\n",
            "sa H m o e we w v a e y r, a w s e w d i o th n’t a a s p i p g l m y o th id es la ig y m er o , i b d y fu fo n r c m tio in n g to th g e et w th ei e g o h u te tp d u i t n . p In u s t t s e 5 a z d j L ,i = n\n",
            "asoftmaxlayerweapplytheso-calledsoftmaxfunctiontothezL. Accordingtothisfunction,\n",
            "j\n",
            "theactivationaL ofthe j-thoutputneuronis\n",
            "j\n",
            "ex\n",
            "j\n",
            "L\n",
            "aL j = (cid:80) ez\n",
            "k\n",
            "L (3.24)\n",
            "k\n",
            "whereinthedenominatorwesumoveralltheoutputneurons. Ifyou’renotfamiliarwiththesoftmaxfunction,Equation(3.24)maylookprettyopaque. It’scertainlynotobviouswhywe’dwanttousethisfunction.Andit’salsonotobviousthatthis\n",
            "willhelpusaddressthelearningslowdownproblem. TobetterunderstandEquation(3.24),\n",
            "supposewehaveanetworkwithfouroutputneurons,andfourcorrespondingweighted\n",
            "inputs,whichwe’lldenotezL,zL,zL,andzL. Figure3.1showsagraphofthecorresponding\n",
            "1 2 3 4\n",
            "outputactivationsfordifferentinputs6. AsyouincreasezL,you’llseeanincreaseinthe\n",
            "4\n",
            "5Indescribingthesoftmaxwe’llmakefrequentuseofnotationintroducedinthelastchapter.You\n",
            "maywishtorevisitthatchapterifyouneedtorefreshyourmemoryaboutthemeaningofthenotation. 6Thisparagraphisanadaptationofananimationfromonlineversionofthebook. \n",
            "(cid:12)\n",
            "3.1.\n",
            "Thecross-entropycostfunction (cid:12) 71\n",
            "(cid:12)\n",
            "1\n",
            "0.8\n",
            "0.6\n",
            "0.4\n",
            "0.2\n",
            "0\n",
            "4 2 0 2 4\n",
            "− − z\n",
            "4\n",
            ")4...1(\n",
            "j,\n",
            "j a\n",
            "∈\n",
            "a 1(z 1= 1) a 2(z 2=0)\n",
            "a 3(z 3= −1) a 4\n",
            "3\n",
            "Figure3.1:Equation3.24fordifferentfixedvaluesofzL andvariablezL.Lindexavoidedforclarity. 1,2,3 4\n",
            "correspondingoutputactivation,aL,andadecreaseintheotheroutputactivations.\n",
            "Similarly,\n",
            "4\n",
            "ifyoudecreasezL thenaL willdecrease,andalltheotheroutputactivationswillincrease. In\n",
            "4 4\n",
            "fact,ifyoulookclosely,you’llseethatinbothcasesthetotalchangeintheotheractivations\n",
            "exactlycompensatesforthechangein aL. Thereasonisthattheoutputactivationsare\n",
            "4\n",
            "guaranteedtoalwayssumupto1,aswecanproveusingEquation(3.24)andalittlealgebra:\n",
            "(cid:88) j aL j = (cid:80) (cid:80) k j e e z z k j L L =1. (3.25)\n",
            "Asaresult,ifaL increases,thentheotheroutputactivationsmustdecreasebythesametotal\n",
            "4\n",
            "amount,toensurethesumoverallactivationsremains1. And,ofcourse,similarstatements\n",
            "holdforalltheotheractivations. Equation(3.24)alsoimpliesthattheoutputactivationsareallpositive,sincetheex-\n",
            "ponentialfunctionispositive. Combiningthiswiththeobservationinthelastparagraph,\n",
            "weseethattheoutputfromthesoftmaxlayerisasetofpositivenumberswhichsumup\n",
            "to1. Inotherwords,theoutputfromthesoftmaxlayercanbethoughtofasaprobability\n",
            "distribution. Thefactthatasoftmaxlayeroutputsaprobabilitydistributionisratherpleasing. Inmany\n",
            "problemsit’sconvenienttobeabletointerprettheoutputactivationaL asthenetwork’s\n",
            "j\n",
            "estimate of the probability that the correct output is j. So, for instance, in the MNIST\n",
            "classificationproblem,wecaninterpretaL asthenetwork’sestimatedprobabilitythatthe\n",
            "j\n",
            "correctdigitclassificationis j. Bycontrast,iftheoutputlayerwasasigmoidlayer,thenwecertainlycouldn’tassume\n",
            "thattheactivationsformedaprobabilitydistribution. Iwon’texplicitlyproveit,butitshould\n",
            "beplausiblethattheactivationsfromasigmoidlayerwon’tingeneralformaprobability\n",
            "distribution. Andsowithasigmoidoutputlayerwedon’thavesuchasimpleinterpretation\n",
            "oftheoutputactivations. Exercise\n",
            "Constructanexampleshowingexplicitlythatinanetworkwithasigmoidoutputlayer,\n",
            "• theoutputactivationsaL won’talwayssumto1. j\n",
            "We’restartingtobuildupsomefeelforthesoftmaxfunctionandthewaysoftmaxlayers\n",
            "\n",
            "(cid:12)\n",
            "72 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "behave. Justtoreviewwherewe’reat: theexponentialsinEquation(3.24)ensurethat\n",
            "alltheoutputactivationsarepositive.\n",
            "AndthesuminthedenominatorofEquation(3.24)\n",
            "ensuresthatthesoftmaxoutputssumto1. Sothatparticularformnolongerappearsso\n",
            "mysterious: rather,itisanaturalwaytoensurethattheoutputactivationsformaprobability\n",
            "distribution. YoucanthinkofsoftmaxasawayofrescalingthezL,andthensquishingthem\n",
            "j\n",
            "togethertoformaprobabilitydistribution. 3 Exercises\n",
            "MonotonicityofsoftmaxShowthat∂aL\n",
            "j\n",
            "/∂z\n",
            "k\n",
            "Lispositiveif j=kandnegativeif j=k. • Asaconsequence,increasingzL isguaranteedtoincreasethecorrespondingout(cid:54)put\n",
            "j\n",
            "activation,aL,andwilldecreasealltheotheroutputactivations. Wealreadysawthis\n",
            "j\n",
            "empiricallywiththesliders,butthisisarigorousproof. Non-localityofsoftmaxAnicethingaboutsigmoidlayersisthattheoutputaL isa\n",
            "j\n",
            "• functionofthecorrespondingweightedinput,aL j = σ (z j L ). Explainwhythisisnot\n",
            "thecaseforasoftmaxlayer: anyparticularoutputactivationaL dependsonallthe\n",
            "j\n",
            "weightedinputs. Problem\n",
            "InvertingthesoftmaxlayerSupposewehaveaneuralnetworkwithasoftmaxoutput\n",
            "• layer,andtheactivationsaLareknown. Showthatthecorrespondingweightedinputs\n",
            "j\n",
            "havetheformz\n",
            "j\n",
            "L =lnaL\n",
            "j\n",
            "+C,forsomeconstantC thatisindependentof j. Thelearningslowdownproblem:We’venowbuiltupconsiderablefamiliaritywithsoftmax\n",
            "layersofneurons. Butwehaven’tyetseenhowasoftmaxlayerletsusaddressthelearning\n",
            "slowdownproblem. Tounderstandthat,let’sdefinethelog-likelihoodcostfunction.\n",
            "We’ll\n",
            "use x todenoteatraininginputtothenetwork,and y todenotethecorrespondingdesired\n",
            "output. Thenthelog-likelihoodcostassociatedtothistraininginputis\n",
            "C lnaL (3.26)\n",
            "j\n",
            "≡−\n",
            "So, for instance, if we’re training with MNIST images, and input an image of a 7, then\n",
            "thelog-likelihoodcostis lnaL. Toseethatthismakesintuitivesense,considerthecase\n",
            "7\n",
            "whenthenetworkisdoin−gagoodjob,thatis,itisconfidenttheinputisa7. Inthatcase\n",
            "itwillestimateavalueforthecorrespondingprobabilityaL whichiscloseto1,andsothe\n",
            "7\n",
            "cost lnaL willbesmall. Bycontrast,whenthenetworkisn’tdoingsuchagoodjob,the\n",
            "7\n",
            "proba−bilityaL willbesmaller,andthecost lnaL willbelarger. Sothelog-likelihoodcost\n",
            "7 7\n",
            "behavesaswe’dexpectacostfunctiontobe−have. Whataboutthelearningslowdownproblem?\n",
            "Toanalyzethat,recallthatthekeytothe\n",
            "learningslowdownisthebehaviourofthequantities∂C/∂wL and∂C/∂bL. Iwon’tgo\n",
            "jk j\n",
            "throughthederivationexplicitly–I’llaskyoutodointheproblems,below–butwithalittle\n",
            "algebrayoucanshowthat7\n",
            "∂C\n",
            "∂bL\n",
            "j\n",
            "= aL j\n",
            "−\n",
            "y j (3.27)\n",
            "∂C\n",
            "∂wL\n",
            "jk\n",
            "= a k L − 1 (aL j\n",
            "−\n",
            "y j) (3.28)\n",
            "7NotethatI’mabusingnotationhere,using yinaslightlydifferentwaytolastparagraph.Inthelast\n",
            "paragraphweusedytodenotethedesiredoutputfromthenetwork–e.g.,outputa“7”ifanimageofa\n",
            "7wasinput.ButintheequationswhichfollowI’musing ytodenotethevectorofoutputactivations\n",
            "whichcorrespondsto7,thatis,avectorwhichisall0s,exceptfora1inthe7thlocation. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 73\n",
            "(cid:12)\n",
            "Theseequationsarethesameastheanalogousexpressionsobtainedinourearlieranalysis\n",
            "ofthecross-entropy. Compare,forexample,Equation(3.28)toEquation(3.13).\n",
            "It’sthe\n",
            "sameequation,albeitinthelatterI’veaveragedovertraininginstances.\n",
            "And,justasinthe\n",
            "earlieranalysis,theseexpressionsensurethatwewillnotencounteralearningslowdown. Infact,it’susefultothinkofasoftmaxoutputlayerwithlog-likelihoodcostasbeingquite\n",
            "similartoasigmoidoutputlayerwithcross-entropycost. Given this similarity, should you use a sigmoid output layer and cross-entropy, or a 3\n",
            "softmaxoutputlayerandlog-likelihood? Infact,inmanysituationsbothapproacheswork\n",
            "well. Through the remainder of this chapter we’ll use a sigmoid output layer, with the\n",
            "cross-entropycost. Later,inChapter6,we’llsometimesuseasoftmaxoutputlayer,with\n",
            "log-likelihoodcost. Thereasonfortheswitchistomakesomeofourlaternetworksmore\n",
            "similartonetworksfoundincertaininfluentialacademicpapers. Asamoregeneralpoint\n",
            "ofprinciple,softmaxpluslog-likelihoodisworthusingwheneveryouwanttointerpretthe\n",
            "output activations as probabilities. That’s not always a concern, but can be useful with\n",
            "classificationproblems(likeMNIST)involvingdisjointclasses. Problems\n",
            "DeriveEquations(3.27)and(3.28). • Where does the “softmax” name come from? Suppose we change the softmax\n",
            "• functionsotheoutputactivationsaregivenby\n",
            "ecz\n",
            "j\n",
            "L\n",
            "aL j = (cid:80) ecz\n",
            "k\n",
            "L , (3.29)\n",
            "k\n",
            "wherecisapositiveconstant. Notethatc=1correspondstothestandardsoftmax\n",
            "function. But if we use a different value of c we get a different function, which\n",
            "isnonethelessqualitativelyrathersimilartothesoftmax. Inparticular,showthat\n",
            "theoutputactivationsformaprobabilitydistribution,justasfortheusualsoftmax. Supposeweallowctobecomelarge,i.e.,c . Whatisthelimitingvalueforthe\n",
            "outputactivationsaL? Aftersolvingthispr→obl∞emitshouldbecleartoyouwhywe\n",
            "j\n",
            "thinkofthec=1functionasa“softened”versionofthemaximumfunction. Thisis\n",
            "theoriginoftheterm“softmax”. Backpropagationwithsoftmaxandthelog-likelihoodcostInthelastchapterwe\n",
            "• derivedthebackpropagationalgorithmforanetworkcontainingsigmoidlayers. To\n",
            "apply the algorithm to a network with a softmax layer we need to figure out an\n",
            "expressionfortheerrorδL ∂C/∂zLinthefinallayer.Showthatasuitableexpression\n",
            "j j\n",
            "is: ≡\n",
            "δL\n",
            "j\n",
            "=aL\n",
            "j\n",
            "y\n",
            "j\n",
            ". (3.30)\n",
            "−\n",
            "Usingthisexpressionwecanapplythebackpropagationalgorithmtoanetworkusing\n",
            "asoftmaxoutputlayerandthelog-likelihoodcost. 3.2 Overfitting and regularization\n",
            "TheNobelprizewinningphysicistEnricoFermiwasonceaskedhisopinionofamathematical\n",
            "model some colleagues had proposed as the solution to an important unsolved physics\n",
            "problem. Themodelgaveexcellentagreementwithexperiment,butFermiwasskeptical.\n",
            "He\n",
            "askedhowmanyfreeparameterscouldbesetinthemodel. “Four”wastheanswer. Fermi\n",
            "\n",
            "(cid:12)\n",
            "74 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "replied8: “IremembermyfriendJohnnyvonNeumannusedtosay,withfourparametersI\n",
            "canfitanelephant,andwithfiveIcanmakehimwigglehistrunk.”. Thepoint,ofcourse,isthatmodelswithalargenumberoffreeparameterscandescribe\n",
            "anamazinglywiderangeofphenomena. Evenifsuchamodelagreeswellwiththeavailable\n",
            "data,thatdoesn’tmakeitagoodmodel. Itmayjustmeanthere’senoughfreedominthe\n",
            "3 model that it can describe almost any data set of the given size, without capturing any\n",
            "genuineinsightsintotheunderlyingphenomenon. Whenthathappensthemodelwillwork\n",
            "wellfortheexistingdata,butwillfailtogeneralizetonewsituations.\n",
            "Thetruetestofa\n",
            "modelisitsabilitytomakepredictionsinsituationsithasn’tbeenexposedtobefore. Fermi and von Neumann were suspicious of models with four parameters. Our 30\n",
            "hiddenneuronnetworkforclassifyingMNISTdigitshasnearly24,000parameters! That’s\n",
            "alotofparameters. Our100hiddenneuronnetworkhasnearly80,000parameters,and\n",
            "state-of-the-artdeepneuralnetssometimescontainmillionsorevenbillionsofparameters. Shouldwetrusttheresults? Let’ssharpenthisproblemupbyconstructingasituationwhereournetworkdoesa\n",
            "badjobgeneralizingtonewsituations. We’lluseour30hiddenneuronnetwork,withits\n",
            "23,860parameters. Butwewon’ttrainthenetworkusingall50,000MNISTtrainingimages. Instead,we’llusejustthefirst1,000trainingimages. Usingthatrestrictedsetwillmake\n",
            "theproblemwithgeneralizationmuchmoreevident. We’lltraininasimilarwaytobefore,\n",
            "usingthecross-entropycostfunction,withalearningrateofη =0.5andamini-batchsize\n",
            "of10. However,we’lltrainfor400epochs,asomewhatlargernumberthanbefore,because\n",
            "we’renotusingasmanytrainingexamples. Let’susenetwork2tolookatthewaythecost\n",
            "functionchanges:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data,\n",
            "monitor_evaluation_accuracy=True, monitor_training_cost=True)\n",
            "Usingtheresultswecanplotthewaythecostchangesasthenetworklearns9:\n",
            "8ThequotecomesfromacharmingarticlebyFreemanDyson,whoisoneofthepeoplewhoproposed\n",
            "theflawedmodel.Afour-parameterelephantmaybefoundhere. 9Thisandthenextfourgraphsweregeneratedbytheprogramoverfitting.py.\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 75\n",
            "(cid:12)\n",
            "3\n",
            "Thislooksencouraging,showingasmoothdecreaseinthecost,justasweexpect. Notethat\n",
            "I’veonlyshowntrainingepochs200through399.\n",
            "Thisgivesusaniceup-closeviewofthe\n",
            "laterstagesoflearning,which,aswe’llsee,turnsouttobewheretheinterestingactionis. Let’snowlookathowtheclassificationaccuracyonthetestdatachangesovertime:\n",
            "Again,I’vezoomedinquiteabit. Inthefirst200epochs(notshown)theaccuracyrisesto\n",
            "justunder82percent. Thelearningthengraduallyslowsdown. Finally,ataroundepoch\n",
            "280theclassificationaccuracyprettymuchstopsimproving. Laterepochsmerelyseesmall\n",
            "stochasticfluctuationsnearthevalueoftheaccuracyatepoch280. Contrastthiswiththe\n",
            "earliergraph,wherethecostassociatedtothetrainingdatacontinuestosmoothlydrop. Ifwejustlookatthatcost,itappearsthatourmodelisstillgetting“better”.\n",
            "Butthetest\n",
            "accuracyresultsshowtheimprovementisanillusion. JustlikethemodelthatFermidisliked,\n",
            "whatournetworklearnsafterepoch280nolongergeneralizestothetestdata. Andsoit’s\n",
            "notusefullearning. Wesaythenetworkisoverfittingorovertrainingbeyondepoch280. YoumightwonderiftheproblemhereisthatI’mlookingatthecostonthetraining\n",
            "data,asopposedtotheclassificationaccuracyonthetestdata. Inotherwords,maybethe\n",
            "problemisthatwe’remakinganapplesandorangescomparison. Whatwouldhappenifwe\n",
            "comparedthecostonthetrainingdatawiththecostonthetestdata,sowe’recomparing\n",
            "similarmeasures? Orperhapswecouldcomparetheclassificationaccuracyonboththe\n",
            "trainingdataandthetestdata? Infact, essentiallythesamephenomenonshowsupno\n",
            "\n",
            "(cid:12)\n",
            "76 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "matterhowwedothecomparison. Thedetailsdochange,however.\n",
            "Forinstance,let’slook\n",
            "atthecostonthetestdata:\n",
            "3\n",
            "Wecanseethatthecostonthetestdataimprovesuntilaroundepoch15,butafterthatit\n",
            "actuallystartstogetworse,eventhoughthecostonthetrainingdataiscontinuingtoget\n",
            "better. Thisisanothersignthatourmodelisoverfitting. Itposesapuzzle,though,whichis\n",
            "whetherweshouldregardepoch15orepoch280asthepointatwhichoverfittingiscoming\n",
            "todominatelearning? Fromapracticalpointofview,whatwereallycareaboutisimproving\n",
            "classificationaccuracyonthetestdata,whilethecostonthetestdataisnomorethana\n",
            "proxyforclassificationaccuracy. Andsoitmakesmostsensetoregardepoch280asthe\n",
            "pointbeyondwhichoverfittingisdominatinglearninginourneuralnetwork. Anothersignofoverfittingmaybeseenintheclassificationaccuracyonthetraining\n",
            "data:\n",
            "Theaccuracyrisesallthewayupto100percent. Thatis,ournetworkcorrectlyclassifiesall\n",
            "1,000trainingimages! Meanwhile,ourtestaccuracytopsoutatjust82.27percent. Soour\n",
            "networkreallyislearningaboutpeculiaritiesofthetrainingset,notjustrecognizingdigits\n",
            "ingeneral. It’salmostasthoughournetworkismerelymemorizingthetrainingset,without\n",
            "understandingdigitswellenoughtogeneralizetothetestset.\n",
            "Overfittingisamajorprobleminneuralnetworks. Thisisespeciallytrueinmodern\n",
            "networks,whichoftenhaveverylargenumbersofweightsandbiases. Totraineffectively,\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 77\n",
            "(cid:12)\n",
            "weneedawayofdetectingwhenoverfittingisgoingon,sowedon’tovertrain. Andwe’d\n",
            "liketohavetechniquesforreducingtheeffectsofoverfitting. Theobviouswaytodetectoverfittingistousetheapproachabove,keepingtrackof\n",
            "accuracyonthetestdataasournetworktrains. Ifweseethattheaccuracyonthetest\n",
            "dataisnolongerimproving,thenweshouldstoptraining. Ofcourse,strictlyspeaking,this\n",
            "isnotnecessarilyasignofoverfitting. Itmightbethataccuracyonthetestdataandthe\n",
            "trainingdatabothstopimprovingatthesametime. Still,adoptingthisstrategywillprevent 3\n",
            "overfitting. Infact,we’lluseavariationonthisstrategy. RecallthatwhenweloadintheMNIST\n",
            "dataweloadinthreedatasets:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            "Uptonowwe’vebeenusingthetraining_dataandtest_data, andignoringtheval-\n",
            "idation_data. The validation_data contains 10,000 images of digits, images which\n",
            "aredifferentfromthe50,000imagesintheMNISTtrainingset,andthe10,000imagesin\n",
            "theMNISTtestset. Insteadofusingthetest_datatopreventoverfitting,wewillusethe\n",
            "validation_data.\n",
            "Todothis,we’llusemuchthesamestrategyaswasdescribedabovefor\n",
            "thetest_data. Thatis,we’llcomputetheclassificationaccuracyonthevalidation_data\n",
            "attheendofeachepoch. Oncetheclassificationaccuracyonthevalidation_datahas\n",
            "saturated,westoptraining. Thisstrategyiscalledearlystopping.\n",
            "Ofcourse,inpracticewe\n",
            "won’timmediatelyknowwhentheaccuracyhassaturated. Instead,wecontinuetraining\n",
            "untilwe’reconfidentthattheaccuracyhassaturated10. Whyusethevalidation_datatopreventoverfitting,ratherthanthetest_data? In\n",
            "fact,thisispartofamoregeneralstrategy,whichistousethevalidation_datatoevaluate\n",
            "differenttrialchoicesofhyper-parameterssuchasthenumberofepochstotrainfor,the\n",
            "learningrate,thebestnetworkarchitecture,andsoon. Weusesuchevaluationstofind\n",
            "andsetgoodvaluesforthehyper-parameters. Indeed,althoughIhaven’tmentionedituntil\n",
            "now,thatis,inpart,howIarrivedatthehyper-parameterchoicesmadeearlierinthisbook. (Moreonthislater.)\n",
            "Ofcourse,thatdoesn’tinanywayanswerthequestionofwhywe’reusingthevalida-\n",
            "tion_datatopreventoverfitting,ratherthanthetest_data. Instead,itreplacesitwith\n",
            "amoregeneralquestion,whichiswhywe’reusingthevalidation_dataratherthanthe\n",
            "test_datatosetgoodhyper-parameters? Tounderstandwhy,considerthatwhensetting\n",
            "hyper-parameterswe’relikelytotrymanydifferentchoicesforthehyper-parameters. Ifwe\n",
            "setthehyper-parametersbasedonevaluationsofthetest_datait’spossiblewe’llendup\n",
            "overfittingourhyper-parameterstothetest_data. Thatis,wemayendupfindinghyper-\n",
            "parameterswhichfitparticularpeculiaritiesofthetest_data,butwheretheperformance\n",
            "ofthenetworkwon’tgeneralizetootherdatasets. Weguardagainstthatbyfiguringoutthe\n",
            "hyper-parametersusingthevalidation_data. Then,oncewe’vegotthehyper-parameters\n",
            "wewant,wedoafinalevaluationofaccuracyusingthetest_data.\n",
            "Thatgivesusconfidence\n",
            "thatourresultsonthetest_dataareatruemeasureofhowwellourneuralnetworkgener-\n",
            "alizes. Toputitanotherway,youcanthinkofthevalidationdataasatypeoftrainingdata\n",
            "10Itrequiressomejudgmenttodeterminewhentostop.InmyearliergraphsIidentifiedepoch280as\n",
            "theplaceatwhichaccuracysaturated.It’spossiblethatwastoopessimistic.Neuralnetworkssometimes\n",
            "plateauforawhileintraining,beforecontinuingtoimprove.Iwouldn’tbesurprisedifmorelearning\n",
            "couldhaveoccurredevenafterepoch400,althoughthemagnitudeofanyfurtherimprovementwould\n",
            "likelybesmall.Soit’spossibletoadoptmoreorlessaggressivestrategiesforearlystopping. \n",
            "(cid:12)\n",
            "78 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thathelpsuslearngoodhyper-parameters. Thisapproachtofindinggoodhyper-parameters\n",
            "issometimesknownastheholdoutmethod,sincethevalidation_dataiskeptapartor\n",
            "“heldout”fromthetraining_data. Now,inpractice,evenafterevaluatingperformanceonthetest_datawemaychange\n",
            "ourmindsandwanttotryanotherapproach–perhapsadifferentnetworkarchitecture–\n",
            "whichwillinvolvefindinganewsetofhyper-parameters. Ifwedothis,isn’tthereadanger\n",
            "3 we’llendupoverfittingtothetest_dataaswell? Doweneedapotentiallyinfiniteregress\n",
            "ofdatasets,sowecanbeconfidentourresultswillgeneralize? Addressingthisconcernfully\n",
            "isadeepanddifficultproblem.\n",
            "Butforourpracticalpurposes,we’renotgoingtoworrytoo\n",
            "muchaboutthisquestion. Instead,we’llplungeahead,usingthebasicholdoutmethod,\n",
            "basedonthetraining_data,validation_data,andtest_data,asdescribedabove. We’vebeenlookingsofaratoverfittingwhenwe’rejustusing1,000trainingimages. Whathappenswhenweusethefulltrainingsetof50,000images? We’llkeepalltheother\n",
            "parametersthesame(30hiddenneurons, learningrate0.5, mini-batchsizeof10), but\n",
            "trainusingall50,000imagesfor30epochs. Here’sagraphshowingtheresultsforthe\n",
            "classificationaccuracyonboththetrainingdataandthetestdata. NotethatI’veusedthe\n",
            "testdatahere,ratherthanthevalidationdata,inordertomaketheresultsmoredirectly\n",
            "comparablewiththeearliergraphs. Asyoucansee,theaccuracyonthetestandtrainingdataremainmuchclosertogetherthan\n",
            "whenwewereusing1,000trainingexamples. Inparticular,thebestclassificationaccuracy\n",
            "of97.86percentonthetrainingdataisonly2.53percenthigherthanthe95.33percenton\n",
            "thetestdata. That’scomparedtothe17.73percentgapwehadearlier!\n",
            "Overfittingisstill\n",
            "goingon,butit’sbeengreatlyreduced. Ournetworkisgeneralizingmuchbetterfromthe\n",
            "trainingdatatothetestdata. Ingeneral,oneofthebestwaysofreducingoverfittingisto\n",
            "increasethesizeofthetrainingdata. Withenoughtrainingdataitisdifficultforevenavery\n",
            "largenetworktooverfit. Unfortunately,trainingdatacanbeexpensiveordifficulttoacquire,\n",
            "sothisisnotalwaysapracticaloption. 3.2.1 Regularization\n",
            "Increasingtheamountoftrainingdataisonewayofreducingoverfitting. Arethereother\n",
            "wayswecanreducetheextenttowhichoverfittingoccurs?\n",
            "Onepossibleapproachisto\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 79\n",
            "(cid:12)\n",
            "reducethesizeofournetwork. However,largenetworkshavethepotentialtobemore\n",
            "powerfulthansmallnetworks,andsothisisanoptionwe’donlyadoptreluctantly. Fortunately,thereareothertechniqueswhichcanreduceoverfitting,evenwhenwehave\n",
            "afixednetworkandfixedtrainingdata. Theseareknownasregularizationtechniques. In\n",
            "thissectionIdescribeoneofthemostcommonlyusedregularizationtechniques,atechnique\n",
            "sometimesknownasweightdecayorL2regularization.\n",
            "TheideaofL2regularizationisto\n",
            "3\n",
            "addanextratermtothecostfunction,atermcalledtheregularizationterm. Here’sthe\n",
            "regularizedcross-entropy:\n",
            "1(cid:88)(cid:148) (cid:151) λ (cid:88)\n",
            "C= −n xj y j lnaL j +(1 − y j)ln(1 − aL j) + 2n w w2. (3.31)\n",
            "Thefirsttermisjusttheusualexpressionforthecross-entropy. Butwe’veaddedasecond\n",
            "term,namelythesumofthesquaresofalltheweightsinthenetwork. Thisisscaledbya\n",
            "factorλ/2n,whereλ>0isknownastheregularizationparameter,andnis,asusual,the\n",
            "sizeofourtrainingset. I’lldiscusslaterhowλischosen. It’salsoworthnotingthatthe\n",
            "regularizationtermdoesn’tincludethebiases. I’llalsocomebacktothatbelow.\n",
            "Ofcourse,it’spossibletoregularizeothercostfunctions,suchasthequadraticcost. This\n",
            "canbedoneinasimilarway:\n",
            "1 (cid:88) λ (cid:88)\n",
            "C= y aL 2 + w2. (3.32)\n",
            "2n x (cid:107) − (cid:107) 2n w\n",
            "Inbothcaseswecanwritetheregularizedcostfunctionas\n",
            "λ (cid:88)\n",
            "C=C 0+\n",
            "2n\n",
            "w2, (3.33)\n",
            "w\n",
            "whereC istheoriginal,unregularizedcostfunction. 0\n",
            "Intuitively,theeffectofregularizationistomakeitsothenetworkpreferstolearnsmall\n",
            "weights,allotherthingsbeingequal. Largeweightswillonlybeallowediftheyconsiderably\n",
            "improvethefirstpartofthecostfunction. Putanotherway,regularizationcanbeviewed\n",
            "asawayofcompromisingbetweenfindingsmallweightsandminimizingtheoriginalcost\n",
            "function. Therelativeimportanceofthetwoelementsofthecompromisedependsonthe\n",
            "valueofλ: whenλissmallweprefertominimizetheoriginalcostfunction,butwhenλis\n",
            "largeweprefersmallweights. Now,it’sreallynotatallobviouswhymakingthiskindofcompromiseshouldhelpreduce\n",
            "overfitting!\n",
            "Butitturnsoutthatitdoes. We’lladdressthequestionofwhyithelpsinthe\n",
            "nextsection. Butfirst,let’sworkthroughanexampleshowingthatregularizationreallydoes\n",
            "reduceoverfitting. Toconstructsuchanexample,wefirstneedtofigureouthowtoapplyourstochastic\n",
            "gradientdescentlearningalgorithminaregularizedneuralnetwork. Inparticular,weneed\n",
            "toknowhowtocomputethepartialderivatives∂C/∂wand∂C/∂bforalltheweightsand\n",
            "\n",
            "(cid:12)\n",
            "80 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "biasesinthenetwork. TakingthepartialderivativesofEquation(3.33)gives\n",
            "∂C ∂C λ\n",
            "∂w = ∂w 0 + n w (3.34)\n",
            "∂C ∂C\n",
            "∂b = ∂b 0. (3.35)\n",
            "3\n",
            "The∂C /∂wand∂C /∂btermscanbecomputedusingbackpropagation,asdescribedin\n",
            "0 0\n",
            "thelastchapter. Andsoweseethatit’seasytocomputethegradientoftheregularizedcost\n",
            "λ\n",
            "function: justusebackpropagation,asusual,andthenadd wtothepartialderivativeofall\n",
            "n\n",
            "theweightterms. Thepartialderivativeswithrespecttothebiasesareunchanged,andso\n",
            "thegradientdescentlearningruleforthebiasesdoesn’tchangefromtheusualrule:\n",
            "∂C\n",
            "b b η 0. (3.36)\n",
            "→ −\n",
            "∂b\n",
            "Thelearningrulefortheweightsbecomes:\n",
            "∂C ηλ (cid:129) ηλ(cid:139) ∂C\n",
            "w\n",
            "→\n",
            "w\n",
            "−\n",
            "η\n",
            "∂w\n",
            "0\n",
            "− n\n",
            "w= 1\n",
            "− n\n",
            "w\n",
            "−\n",
            "η\n",
            "∂w\n",
            "0. (3.37)\n",
            "Thisisexactlythesameastheusualgradientdescentlearningrule,exceptwefirstrescale\n",
            "theweightwbyafactor1\n",
            "ηλ\n",
            ". Thisrescalingissometimesreferredtoasweightdecay,\n",
            "n\n",
            "sinceitmakestheweightss−maller. Atfirstglanceitlooksasthoughthismeanstheweights\n",
            "arebeingdrivenunstoppablytowardzero. Butthat’snotright,sincetheothertermmay\n",
            "leadtheweightstoincrease,ifsodoingcausesadecreaseintheunregularizedcostfunction. Okay,that’showgradientdescentworks.\n",
            "Whataboutstochasticgradientdescent? Well,\n",
            "justasinunregularizedstochasticgradientdescent,wecanestimate∂C /∂wbyaveraging\n",
            "0\n",
            "overamini-batchofmtrainingexamples. Thustheregularizedlearningruleforstochastic\n",
            "gradientdescentbecomes(c.f. Equation(1.20))\n",
            "(cid:129) ηλ(cid:139) η (cid:88)∂C\n",
            "w 1 w x, (3.38)\n",
            "→ − n − m x ∂w\n",
            "wherethesumisovertrainingexamples x inthemini-batch,andC isthe(unregularized)\n",
            "x\n",
            "costforeachtrainingexample. Thisisexactlythesameastheusualruleforstochastic\n",
            "gradientdescent,exceptforthe1 ηλ/nweightdecayfactor. Finally,andforcompleteness,\n",
            "letmestatetheregularizedlearni−ngruleforthebiases. Thisis,ofcourse,exactlythesame\n",
            "asintheunregularizedcase(c.f. Equation1.21),\n",
            "η (cid:88)∂C\n",
            "b b x, (3.39)\n",
            "→ − m x ∂b\n",
            "wherethesumisovertrainingexamples x inthemini-batch.\n",
            "Let’sseehowregularizationchangestheperformanceofourneuralnetwork. We’lluse\n",
            "a network with 30 hidden neurons, a mini-batch size of 10, a learning rate of 0.5, and\n",
            "thecross-entropycostfunction. However,thistimewe’llusearegularizationparameter\n",
            "ofλ =0.1.\n",
            "Notethatinthecode,weusethevariablenamelmbda,becauselambdaisa\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 81\n",
            "(cid:12)\n",
            "reservedwordinPython,withanunrelatedmeaning. I’vealsousedthetest_dataagain,\n",
            "notthevalidation_data. Strictlyspeaking,weshouldusethevalidation_data,forall\n",
            "the reasons we discussed earlier. But I decided to use the test_data because it makes\n",
            "theresultsmoredirectlycomparablewithourearlier,unregularizedresults.\n",
            "Youcaneasily\n",
            "changethecodetousethevalidation_datainstead,andyou’llfindthatitgivessimilar\n",
            "results. 3\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data[:1000], 400, 10, 0.5, evaluation_data=test_data, lmbda\n",
            "= 0.1, monitor_evaluation_cost=True, monitor_evaluation_accuracy=True,\n",
            "monitor_training_cost=True, monitor_training_accuracy=True)\n",
            "Thecostonthetrainingdatadecreasesoverthewholetime,muchasitdidintheearlier,\n",
            "unregularizedcase11:\n",
            "Butthistimetheaccuracyonthetest_datacontinuestoincreasefortheentire400epochs:\n",
            "11Thisandthenexttwographswereproducedwiththeprogramoverfitting.py. \n",
            "(cid:12)\n",
            "82 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Clearly,theuseofregularizationhassuppressedoverfitting. What’smore,theaccuracyis\n",
            "considerablyhigher,withapeakclassificationaccuracyof87.1percent,comparedtothepeak\n",
            "of82.27percentobtainedintheunregularizedcase. Indeed,wecouldalmostcertainlyget\n",
            "considerablybetterresultsbycontinuingtotrainpast400epochs. Itseemsthat,empirically,\n",
            "regularizationiscausingournetworktogeneralizebetter,andconsiderablyreducingthe\n",
            "effectsofoverfitting. 3 Whathappensifwemoveoutoftheartificialenvironmentofjusthaving1,000training\n",
            "images,andreturntothefull50,000imagetrainingset? Ofcourse,we’veseenalreadythat\n",
            "overfittingismuchlessofaproblemwiththefull50,000images. Doesregularizationhelp\n",
            "anyfurther? Let’skeepthehyper-parametersthesameasbefore–30epochs,learningrate\n",
            "0.5,mini-batchsizeof10. However,weneedtomodifytheregularizationparameter. The\n",
            "reasonisbecausethesizenofthetrainingsethaschangedfromn=1,000ton=50,000,and\n",
            "thischangestheweightdecayfactor1 ηλ/n. Ifwecontinuedtouseλ =0.1thatwould\n",
            "meanmuchlessweightdecay,andthus−muchlessofaregularizationeffect. Wecompensate\n",
            "bychangingtoλ =5.0. Okay,let’strainournetwork,stoppingfirsttore-initializetheweights:\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, evaluation_data=test_data, lmbda = 5.0,\n",
            "... monitor_evaluation_accuracy=True, monitor_training_accuracy=True)\n",
            "Weobtaintheresults:\n",
            "There’slotsofgoodnewshere.\n",
            "First,ourclassificationaccuracyonthetestdataisup,from\n",
            "95.49percentwhenrunningunregularized,to96.49percent.\n",
            "That’sabigimprovement. Second, we can see that the gap between results on the training and test data is much\n",
            "narrowerthanbefore,runningatunderapercent. That’sstillasignificantgap,butwe’ve\n",
            "obviouslymadesubstantialprogressreducingoverfitting. Finally,let’sseewhattestclassificationaccuracywegetwhenweuse100hiddenneurons\n",
            "andaregularizationparameterofλ =5.0.Iwon’tgothroughadetailedanalysisofoverfitting\n",
            "here,thisispurelyforfun,justtoseehowhighanaccuracywecangetwhenweuseour\n",
            "newtricks: thecross-entropycostfunctionandL2regularization. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 83\n",
            "(cid:12)\n",
            ">>> net = network2.Network([784, 100, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, lmbda=5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Thefinalresultisaclassificationaccuracyof97.92percentonthevalidationdata. That’sa\n",
            "bigjumpfromthe30hiddenneuroncase. Infact,tuningjustalittlemore,torunfor60\n",
            "epochsatη =0.1andλ =5.0webreakthe98percentbarrier,achieving98.04percent\n",
            "classificationaccuracyonthevalidationdata. Notbadforwhatturnsouttobe152linesof\n",
            "code! I’vedescribedregularizationasawaytoreduceoverfittingandtoincreaseclassification\n",
            "accuracies.\n",
            "Infact,that’snottheonlybenefit. Empirically,whendoingmultiplerunsof\n",
            "ourMNISTnetworks,butwithdifferent(random)weightinitializations,I’vefoundthatthe\n",
            "unregularizedrunswilloccasionallyget“stuck”,apparentlycaughtinlocalminimaofthe\n",
            "costfunction. Theresultisthatdifferentrunssometimesprovidequitedifferentresults.\n",
            "By\n",
            "contrast,theregularizedrunshaveprovidedmuchmoreeasilyreplicableresults. Whyisthisgoingon? Heuristically,ifthecostfunctionisunregularized,thenthelength\n",
            "oftheweightvectorislikelytogrow,allotherthingsbeingequal. Overtimethiscanlead\n",
            "totheweightvectorbeingverylargeindeed. Thiscancausetheweightvectortogetstuck\n",
            "pointinginmoreorlessthesamedirection,sincechangesduetogradientdescentonlymake\n",
            "tinychangestothedirection,whenthelengthislong. Ibelievethisphenomenonismaking\n",
            "ithardforourlearningalgorithmtoproperlyexploretheweightspace,andconsequently\n",
            "hardertofindgoodminimaofthecostfunction. 3.2.2 Whydoesregularizationhelpreduceoverfitting? We’veseenempiricallythatregularizationhelpsreduceoverfitting. That’sencouragingbut,\n",
            "unfortunately, it’snotobviouswhyregularizationhelps! Astandardstorypeopletellto\n",
            "explainwhat’sgoingonisalongthefollowinglines: smallerweightsare,insomesense,\n",
            "lowercomplexity,andsoprovideasimplerandmorepowerfulexplanationforthedata,and\n",
            "shouldthusbepreferred. That’saprettytersestory,though,andcontainsseveralelements\n",
            "thatperhapsseemdubiousormystifying.\n",
            "Let’sunpackthestoryandexamineitcritically. To\n",
            "dothat,let’ssupposewehaveasimpledatasetforwhichwewishtobuildamodel:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "3\n",
            "\n",
            "(cid:12)\n",
            "84 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Implicitly,we’restudyingsomereal-worldphenomenonhere,with x and y representing\n",
            "real-worlddata. Ourgoalistobuildamodelwhichletsuspredict y asafunctionof x. We\n",
            "couldtryusingneuralnetworkstobuildsuchamodel,butI’mgoingtodosomethingeven\n",
            "simpler: I’lltrytomodel y asapolynomialin x. I’mdoingthisinsteadofusingneuralnets\n",
            "becauseusingpolynomialswillmakethingsparticularlytransparent.\n",
            "Oncewe’veunderstood\n",
            "thepolynomialcase,we’lltranslatetoneuralnetworks. Now,therearetenpointsinthegraph\n",
            "above,whichmeanswecanfindaunique9-th-orderpolynomial y=a 0 x9 +a 1 x8 +...+a 9\n",
            "whichfitsthedataexactly. Here’sthegraphofthatpolynomial12:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "Thatprovidesanexactfit. Butwecanalsogetagoodfitusingthelinearmodel y=2x:\n",
            "10\n",
            "5\n",
            "0\n",
            "0 1 2 3 4 5\n",
            "x\n",
            "y\n",
            "3\n",
            "Whichoftheseisthebettermodel? Whichismorelikelytobetrue? Andwhichmodelismore\n",
            "likelytogeneralizewelltootherexamplesofthesameunderlyingreal-worldphenomenon? Thesearedifficultquestions. Infact,wecan’tdeterminewithcertaintytheanswerto\n",
            "anyoftheabovequestions,withoutmuchmoreinformationabouttheunderlyingreal-world\n",
            "phenomenon. Butlet’sconsidertwopossibilities: (1)the9thorderpolynomialis,infact,\n",
            "themodelwhichtrulydescribesthereal-worldphenomenon,andthemodelwilltherefore\n",
            "generalizeperfectly;(2)thecorrectmodelis y=2x,butthere’salittleadditionalnoisedue\n",
            "to,say,measurementerror,andthat’swhythemodelisn’tanexactfit. 12Iwon’tshowthecoefficientsexplicitly,althoughtheyareeasytofindusingaroutinesuchasNumpy’s\n",
            "polyfit.Youcanviewtheexactformofthepolynomialinthesourcecodeforthegraphifyou’recurious.\n",
            "It’sthefunctionp(x)definedstartingonline14oftheprogramwhichproducesthegraph. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 85\n",
            "(cid:12)\n",
            "It’snotaprioripossibletosaywhichofthesetwopossibilitiesiscorrect. (Or,indeed,if\n",
            "somethirdpossibilityholds).\n",
            "Logically,eithercouldbetrue. Andit’snotatrivialdifference. It’struethatonthedataprovidedthere’sonlyasmalldifferencebetweenthetwomodels. Butsupposewewanttopredictthevalueof ycorrespondingtosomelargevalueofx,much\n",
            "largerthananyshownonthegraphabove. Ifwetrytodothattherewillbeadramatic\n",
            "differencebetweenthepredictionsofthetwomodels,asthe9thorderpolynomialmodel\n",
            "comestobedominatedbythe x9term,whilethelinearmodelremains,well,linear. 3\n",
            "Onepointofviewistosaythatinscienceweshouldgowiththesimplerexplanation,\n",
            "unlesscompellednotto. Whenwefindasimplemodelthatseemstoexplainmanydata\n",
            "pointswearetemptedtoshout“Eureka!” Afterall,itseemsunlikelythatasimpleexplanation\n",
            "shouldoccurmerelybycoincidence. Rather,wesuspectthatthemodelmustbeexpressing\n",
            "someunderlyingtruthaboutthephenomenon. Inthecaseathand,themodel y=2x+noise\n",
            "seemsmuchsimplerthan y=a\n",
            "0\n",
            "x9 +a\n",
            "1\n",
            "x8 +.... Itwouldbesurprisingifthatsimplicityhad\n",
            "occurredbychance,andsowesuspectthat y=2x+noiseexpressessomeunderlyingtruth. Inthispointofview,the9thordermodelisreallyjustlearningtheeffectsoflocalnoise. And\n",
            "sowhilethe9thordermodelworksperfectlyfortheseparticulardatapoints,themodelwill\n",
            "failtogeneralizetootherdatapoints,andthenoisylinearmodelwillhavegreaterpredictive\n",
            "power. Let’sseewhatthispointofviewmeansforneuralnetworks. Supposeournetworkmostly\n",
            "hassmallweights,aswilltendtohappeninaregularizednetwork. Thesmallnessofthe\n",
            "weightsmeansthatthebehaviourofthenetworkwon’tchangetoomuchifwechangeafew\n",
            "randominputshereandthere. Thatmakesitdifficultforaregularizednetworktolearnthe\n",
            "effectsoflocalnoiseinthedata. Thinkofitasawayofmakingitsosinglepiecesofevidence\n",
            "don’tmattertoomuchtotheoutputofthenetwork. Instead,aregularizednetworklearns\n",
            "torespondtotypesofevidencewhichareseenoftenacrossthetrainingset. Bycontrast,a\n",
            "networkwithlargeweightsmaychangeitsbehaviourquiteabitinresponsetosmallchanges\n",
            "intheinput. Andsoanunregularizednetworkcanuselargeweightstolearnacomplex\n",
            "modelthatcarriesalotofinformationaboutthenoiseinthetrainingdata. Inanutshell,\n",
            "regularizednetworksareconstrainedtobuildrelativelysimplemodelsbasedonpatterns\n",
            "seenofteninthetrainingdata,andareresistanttolearningpeculiaritiesofthenoiseinthe\n",
            "trainingdata. Thehopeisthatthiswillforceournetworkstodoreallearningaboutthe\n",
            "phenomenonathand,andtogeneralizebetterfromwhattheylearn.\n",
            "Withthatsaid,thisideaofpreferringsimplerexplanationshouldmakeyounervous. Peoplesometimesrefertothisideaas“Occam’sRazor”,andwillzealouslyapplyitasthough\n",
            "ithasthestatusofsomegeneralscientificprinciple. But,ofcourse,it’snotageneralscientific\n",
            "principle. Thereisnoapriorilogicalreasontoprefersimpleexplanationsovermorecomplex\n",
            "explanations. Indeed,sometimesthemorecomplexexplanationturnsouttobecorrect. Letmedescribetwoexampleswheremorecomplexexplanationshaveturnedouttobe\n",
            "correct.\n",
            "Inthe1940sthephysicistMarcelScheinannouncedthediscoveryofanewparticle\n",
            "ofnature. Thecompanyheworkedfor,GeneralElectric,wasecstatic,andpublicizedthe\n",
            "discoverywidely. ButthephysicistHansBethewasskeptical.\n",
            "BethevisitedSchein, and\n",
            "lookedattheplatesshowingthetracksofSchein’snewparticle. ScheinshowedBetheplate\n",
            "afterplate,butoneachplateBetheidentifiedsomeproblemthatsuggestedthedatashould\n",
            "bediscarded. Finally,ScheinshowedBetheaplatethatlookedgood. Bethesaiditmight\n",
            "justbeastatisticalfluke. Schein: “Yes,butthechancethatthiswouldbestatistics,even\n",
            "accordingtoyourownformula,isoneinfive.” Bethe: “Butwehavealreadylookedatfive\n",
            "plates.” Finally,Scheinsaid: “Butonmyplates,eachoneofthegoodplates,eachoneof\n",
            "\n",
            "(cid:12)\n",
            "86 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thegoodpictures,youexplainbyadifferenttheory,whereasIhaveonehypothesisthat\n",
            "explainsalltheplates,thattheyare[thenewparticle].” Bethereplied: “Thesoledifference\n",
            "betweenyourandmyexplanationsisthatyoursiswrongandallofmineareright. Your\n",
            "singleexplanationiswrong,andallofmymultipleexplanationsareright.” Subsequentwork\n",
            "confirmedthatNatureagreedwithBethe,andSchein’sparticleisnomore13. Asasecondexample,in1859theastronomerUrbainLeVerrierobservedthattheorbit\n",
            "3 oftheplanetMercurydoesn’thavequitetheshapethatNewton’stheoryofgravitationsaysit\n",
            "shouldhave.Itwasatiny,tinydeviationfromNewton’stheory,andseveraloftheexplanations\n",
            "proferredatthetimeboileddowntosayingthatNewton’stheorywasmoreorlessright,but\n",
            "neededatinyalteration. In1916,Einsteinshowedthatthedeviationcouldbeexplainedvery\n",
            "wellusinghisgeneraltheoryofrelativity,atheoryradicallydifferenttoNewtoniangravitation,\n",
            "andbasedonmuchmorecomplexmathematics. Despitethatadditionalcomplexity,todayit’s\n",
            "acceptedthatEinstein’sexplanationiscorrect,andNewtoniangravity,eveninitsmodified\n",
            "forms,iswrong. ThisisinpartbecausewenowknowthatEinstein’stheoryexplainsmany\n",
            "otherphenomenawhichNewton’stheoryhasdifficultywith. Furthermore,andevenmore\n",
            "impressively,Einstein’stheoryaccuratelypredictsseveralphenomenawhicharen’tpredicted\n",
            "byNewtoniangravityatall. Buttheseimpressivequalitiesweren’tentirelyobviousinthe\n",
            "earlydays. Ifonehadjudgedmerelyonthegroundsofsimplicity,thensomemodifiedform\n",
            "ofNewton’stheorywouldarguablyhavebeenmoreattractive. Therearethreemoralstodrawfromthesestories.\n",
            "First,itcanbequiteasubtlebusiness\n",
            "decidingwhichoftwoexplanationsistruly“simpler”. Second,evenifwecanmakesucha\n",
            "judgment,simplicityisaguidethatmustbeusedwithgreatcaution! Third,thetruetestof\n",
            "amodelisnotsimplicity,butratherhowwellitdoesinpredictingnewphenomena,innew\n",
            "regimesofbehaviour. Withthatsaid, andkeepingtheneedforcautioninmind, it’sanempiricalfactthat\n",
            "regularizedneuralnetworksusuallygeneralizebetterthanunregularizednetworks. Andso\n",
            "throughtheremainderofthebookwewillmakefrequentuseofregularization. I’veincluded\n",
            "thestoriesabovemerelytohelpconveywhyno-onehasyetdevelopedanentirelyconvincing\n",
            "theoreticalexplanationforwhyregularizationhelpsnetworksgeneralize. Indeed,researchers\n",
            "continuetowritepaperswheretheytrydifferentapproachestoregularization,compare\n",
            "themtoseewhichworksbetter,andattempttounderstandwhydifferentapproacheswork\n",
            "betterorworse. Andsoyoucanviewregularizationassomethingofakludge. Whileitoften\n",
            "helps,wedon’thaveanentirelysatisfactorysystematicunderstandingofwhat’sgoingon,\n",
            "merelyincompleteheuristicsandrulesofthumb. There’s a deeper set of issues here, issues which go to the heart of science.\n",
            "It’s the\n",
            "questionofhowwegeneralize. Regularizationmaygiveusacomputationalmagicwand\n",
            "thathelpsournetworksgeneralizebetter,butitdoesn’tgiveusaprincipledunderstanding\n",
            "ofhowgeneralizationworks,norofwhatthebestapproachis14. Thisisparticularlygallingbecauseineverydaylife,wehumansgeneralizephenomenally\n",
            "well. Shownjustafewimagesofanelephantachildwillquicklylearntorecognizeother\n",
            "elephants. Ofcourse,theymayoccasionallymakemistakes,perhapsconfusingarhinoceros\n",
            "foranelephant, butingeneralthisprocessworksremarkablyaccurately.\n",
            "Sowehavea\n",
            "13ThestoryisrelatedbythephysicistRichardFeynmaninaninterviewwiththehistorianCharles\n",
            "Weiner. 14Theseissuesgobacktotheproblemofinduction,famouslydiscussedbytheScottishphilosopher\n",
            "DavidHumein“AnEnquiryConcerningHumanUnderstanding”(1748).Theproblemofinductionhas\n",
            "beengivenamodernmachinelearningformintheno-freelunchtheoremofDavidWolpertandWilliam\n",
            "Macready(1997). \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 87\n",
            "(cid:12)\n",
            "system–thehumanbrain–withahugenumberoffreeparameters. Andafterbeingshown\n",
            "justoneorafewtrainingimagesthatsystemlearnstogeneralizetootherimages. Our\n",
            "brainsare,insomesense,regularizingamazinglywell! Howdowedoit?\n",
            "Atthispointwe\n",
            "don’tknow. Iexpectthatinyearstocomewewilldevelopmorepowerfultechniquesfor\n",
            "regularizationinartificialneuralnetworks,techniquesthatwillultimatelyenableneural\n",
            "netstogeneralizewellevenfromsmalldatasets. Infact,ournetworksalreadygeneralizebetterthanonemightaprioriexpect. Anetwork 3\n",
            "with100hiddenneuronshasnearly80,000parameters. Wehaveonly50,000imagesinour\n",
            "trainingdata. It’sliketryingtofitan80,000thdegreepolynomialto50,000datapoints. By\n",
            "allrights,ournetworkshouldoverfitterribly.\n",
            "Andyet,aswesawearlier,suchanetwork\n",
            "actuallydoesaprettygoodjobgeneralizing. Whyisthatthecase?\n",
            "It’snotwellunderstood. Ithasbeenconjectured15that“thedynamicsofgradientdescentlearninginmultilayernets\n",
            "hasa‘self-regularization’effect”. Thisisexceptionallyfortunate, butit’salsosomewhat\n",
            "disquietingthatwedon’tunderstandwhyit’sthecase. Inthemeantime,wewilladoptthe\n",
            "pragmaticapproachanduseregularizationwheneverwecan. Ourneuralnetworkswillbe\n",
            "thebetterforit. LetmeconcludethissectionbyreturningtoadetailwhichIleftunexplainedearlier:\n",
            "thefactthatL2regularizationdoesn’tconstrainthebiases. Ofcourse,itwouldbeeasyto\n",
            "modifytheregularizationproceduretoregularizethebiases. Empirically,doingthisoften\n",
            "doesn’tchangetheresultsverymuch,sotosomeextentit’smerelyaconventionwhetherto\n",
            "regularizethebiasesornot. However,it’sworthnotingthathavingalargebiasdoesn’tmake\n",
            "aneuronsensitivetoitsinputsinthesamewayashavinglargeweights. Andsowedon’t\n",
            "needtoworryaboutlargebiasesenablingournetworktolearnthenoiseinourtrainingdata. Atthesametime,allowinglargebiasesgivesournetworksmoreflexibilityinbehaviour–in\n",
            "particular,largebiasesmakeiteasierforneuronstosaturate,whichissometimesdesirable. Forthesereasonswedon’tusuallyincludebiastermswhenregularizing. 3.2.3 Othertechniquesforregularization\n",
            "TherearemanyregularizationtechniquesotherthanL2regularization. Infact,somany\n",
            "techniqueshavebeendevelopedthatIcan’tpossiblysummarizethemall. InthissectionI\n",
            "brieflydescribethreeotherapproachestoreducingoverfitting: L1regularization,dropout,\n",
            "and artificially increasing the training set size. We won’t go into nearly as much depth\n",
            "studyingthesetechniquesaswedidearlier.\n",
            "Instead, thepurposeistogetfamiliarwith\n",
            "themainideas,andtoappreciatesomethingofthediversityofregularizationtechniques\n",
            "available. L1regularization:Inthisapproachwemodifytheunregularizedcostfunctionbyadding\n",
            "thesumoftheabsolutevaluesoftheweights:\n",
            "λ(cid:88)\n",
            "C=C 0+\n",
            "n w |\n",
            "w\n",
            "|\n",
            ". (3.40)\n",
            "Intuitively,thisissimilartoL2regularization,penalizinglargeweights,andtendingtomake\n",
            "thenetworkprefersmallweights. Ofcourse,theL1regularizationtermisn’tthesameasthe\n",
            "L2regularizationterm,andsoweshouldn’texpecttogetexactlythesamebehaviour. Let’s\n",
            "15InGradient-BasedLearningAppliedtoDocumentRecognition,byYannLeCun,LéonBottou,Yoshua\n",
            "Bengio,andPatrickHaffner(1998). \n",
            "(cid:12)\n",
            "88 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "trytounderstandhowthebehaviourofanetworktrainedusingL1regularizationdiffers\n",
            "fromanetworktrainedusingL2regularization. Todothat,we’lllookatthepartialderivativesofthecostfunction. Differentiating(95)\n",
            "weobtain:\n",
            "3 ∂C ∂C λ\n",
            "∂w = ∂w 0 + n sgn(w), (3.41)\n",
            "wheresgn(w)isthesignofw,thatis,+1ifwispositive,and 1ifwisnegative. Usingthis\n",
            "expression,wecaneasilymodifybackpropagationtodostocha−sticgradientdescentusingL1\n",
            "regularization. TheresultingupdateruleforanL1regularizednetworkis\n",
            "ηλ ∂C\n",
            "w\n",
            "→\n",
            "w (cid:48)=w\n",
            "− n\n",
            "sgn(w)\n",
            "−\n",
            "η\n",
            "∂w\n",
            "0, (3.42)\n",
            "where, as per usual, we can estimate ∂C /∂w using a mini-batch average, if we wish. 0\n",
            "ComparethattotheupdateruleforL2regularization(c.f. Equation(3.38)),\n",
            "(cid:129) ηλ(cid:139) ∂C\n",
            "w\n",
            "→\n",
            "w (cid:48)=w 1\n",
            "− n −\n",
            "η\n",
            "∂w\n",
            "0. (3.43)\n",
            "Inbothexpressionstheeffectofregularizationistoshrinktheweights.\n",
            "Thisaccordswithour\n",
            "intuitionthatbothkindsofregularizationpenalizelargeweights. Butthewaytheweights\n",
            "shrinkisdifferent. InL1regularization,theweightsshrinkbyaconstantamounttoward0. In\n",
            "L2regularization,theweightsshrinkbyanamountwhichisproportionaltow. Andsowhen\n",
            "aparticularweighthasalargemagnitude, w,L1regularizationshrinkstheweightmuch\n",
            "lessthanL2regularizationdoes. Bycontras|t,|when w issmall,L1regularizationshrinks\n",
            "theweightmuchmorethanL2regularization. Thene|tr|esultisthatL1regularizationtends\n",
            "toconcentratetheweightofthenetworkinarelativelysmallnumberofhigh-importance\n",
            "connections,whiletheotherweightsaredriventowardzero. I’veglossedoveranissueintheabovediscussion,whichisthatthepartialderivative\n",
            "∂C/∂wisn’tdefinedwhenw=0. Thereasonisthatthefunction w hasasharp“corner”\n",
            "at w=0,andsoisn’tdifferentiableatthatpoint. That’sokay,th|ou|gh.\n",
            "Whatwe’lldois\n",
            "justapplytheusual(unregularized)ruleforstochasticgradientdescentwhenw=0. That\n",
            "shouldbeokay–intuitively,theeffectofregularizationistoshrinkweights,andobviouslyit\n",
            "can’tshrinkaweightwhichisalready0. Toputitmoreprecisely,we’lluseEquations(3.41)\n",
            "and(3.42)withtheconventionthatsgn(0)=0. Thatgivesanice,compactrulefordoing\n",
            "stochasticgradientdescentwithL1regularization. Dropout: Dropoutisaradicallydifferenttechniqueforregularization. UnlikeL1and\n",
            "L2regularization,dropoutdoesn’trelyonmodifyingthecostfunction. Instead,indropout\n",
            "wemodifythenetworkitself.\n",
            "Letmedescribethebasicmechanicsofhowdropoutworks,\n",
            "beforegettingintowhyitworks,andwhattheresultsare. Supposewe’retryingtotrainanetwork:\n",
            "\n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 89\n",
            "(cid:12)\n",
            "3\n",
            "Inparticular,supposewehaveatraininginput x andcorrespondingdesiredoutput y. Ordi-\n",
            "narily,we’dtrainbyforward-propagating x throughthenetwork,andthenbackpropagating\n",
            "todeterminethecontributiontothegradient. Withdropout,thisprocessismodified. We\n",
            "startbyrandomly(andtemporarily)deletinghalfthehiddenneuronsinthenetwork,while\n",
            "leaving the input and output neurons untouched. After doing this, we’ll end up with a\n",
            "networkalongthefollowinglines. Notethatthedropoutneurons,i.e.,theneuronswhich\n",
            "havebeentemporarilydeleted,arestillghostedin:\n",
            "Weforward-propagatetheinput x throughthemodifiednetwork,andthenbackpropagate\n",
            "theresult,alsothroughthemodifiednetwork.\n",
            "Afterdoingthisoveramini-batchofexamples,\n",
            "weupdatetheappropriateweightsandbiases. Wethenrepeattheprocess,firstrestoringthe\n",
            "dropoutneurons,thenchoosinganewrandomsubsetofhiddenneuronstodelete,estimating\n",
            "thegradientforadifferentmini-batch,andupdatingtheweightsandbiasesinthenetwork. Byrepeatingthisprocessoverandover,ournetworkwilllearnasetofweightsand\n",
            "biases.\n",
            "Ofcourse,thoseweightsandbiaseswillhavebeenlearntunderconditionsinwhich\n",
            "halfthehiddenneuronsweredroppedout. Whenweactuallyrunthefullnetworkthat\n",
            "meansthattwiceasmanyhiddenneuronswillbeactive. Tocompensateforthat,wehalve\n",
            "theweightsoutgoingfromthehiddenneurons. \n",
            "(cid:12)\n",
            "90 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Thisdropoutproceduremayseemstrangeandadhoc. Whywouldweexpectittohelp\n",
            "withregularization? Toexplainwhat’sgoingon,I’dlikeyoutobrieflystopthinkingabout\n",
            "dropout,andinsteadimaginetrainingneuralnetworksinthestandardway(nodropout). In\n",
            "particular,imaginewetrainseveraldifferentneuralnetworks,allusingthesametraining\n",
            "data. Ofcourse, thenetworksmaynotstartoutidentical, andasaresultaftertraining\n",
            "theymaysometimesgivedifferentresults. Whenthathappenswecouldusesomekind\n",
            "3 ofaveragingorvotingschemetodecidewhichoutputtoaccept. Forinstance,ifwehave\n",
            "trainedfivenetworks,andthreeofthemareclassifyingadigitasa“3”,thenitprobablyreally\n",
            "isa“3”. Theothertwonetworksareprobablyjustmakingamistake. Thiskindofaveraging\n",
            "schemeisoftenfoundtobeapowerful(thoughexpensive)wayofreducingoverfitting.\n",
            "The\n",
            "reasonisthatthedifferentnetworksmayoverfitindifferentways,andaveragingmayhelp\n",
            "eliminatethatkindofoverfitting. What’s this got to do with dropout? Heuristically, when we dropout different sets\n",
            "ofneurons, it’sratherlikewe’retrainingdifferentneuralnetworks. Andsothedropout\n",
            "procedureislikeaveragingtheeffectsofaverylargenumberofdifferentnetworks. The\n",
            "differentnetworkswilloverfitindifferentways,andso,hopefully,theneteffectofdropout\n",
            "willbetoreduceoverfitting. Arelatedheuristicexplanationfordropoutisgiveninoneoftheearliestpaperstouse\n",
            "thetechnique16: “Thistechniquereducescomplexco-adaptationsofneurons,sinceaneuron\n",
            "cannotrelyonthepresenceofparticularotherneurons. Itis,therefore,forcedtolearnmore\n",
            "robustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsetsofthe\n",
            "otherneurons.” Inotherwords,ifwethinkofournetworkasamodelwhichismaking\n",
            "predictions,thenwecanthinkofdropoutasawayofmakingsurethatthemodelisrobust\n",
            "tothelossofanyindividualpieceofevidence. Inthis,it’ssomewhatsimilartoL1andL2\n",
            "regularization,whichtendtoreduceweights,andthusmakethenetworkmorerobustto\n",
            "losinganyindividualconnectioninthenetwork. Ofcourse,thetruemeasureofdropoutisthatithasbeenverysuccessfulinimproving\n",
            "theperformanceofneuralnetworks. Theoriginalpaper17introducingthetechniqueapplied\n",
            "ittomanydifferenttasks. Forus, it’sofparticularinterestthattheyapplieddropoutto\n",
            "MNISTdigitclassification,usingavanillafeedforwardneuralnetworkalonglinessimilarto\n",
            "thosewe’vebeenconsidering. Thepapernotedthatthebestresultanyonehadachieved\n",
            "uptothatpointusingsuchanarchitecturewas98.4percentclassificationaccuracyonthe\n",
            "testset. Theyimprovedthatto98.7percentaccuracyusingacombinationofdropoutand\n",
            "amodifiedformofL2regularization. Similarlyimpressiveresultshavebeenobtainedfor\n",
            "manyothertasks,includingproblemsinimageandspeechrecognition,andnaturallanguage\n",
            "processing. Dropouthasbeenespeciallyusefulintraininglarge,deepnetworks,wherethe\n",
            "problemofoverfittingisoftenacute. Artificiallyexpandingthetrainingdata: WesawearlierthatourMNISTclassification\n",
            "accuracydroppeddowntopercentagesinthemid-80swhenweusedonly1,000training\n",
            "images.\n",
            "It’snotsurprisingthatthisisthecase,sincelesstrainingdatameansournetwork\n",
            "willbeexposedtofewervariationsinthewayhumanbeingswritedigits. Let’strytraining\n",
            "our 30 hidden neuron network with a variety of different training data set sizes, to see\n",
            "howperformancevaries. Wetrainusingamini-batchsizeof10,alearningrateη =0.5,\n",
            "16ImageNetClassificationwithDeepConvolutionalNeuralNetworks,byAlexKrizhevsky,IlyaSutskever,\n",
            "andGeoffreyHinton(2012).\n",
            "17Improvingneuralnetworksbypreventingco-adaptationoffeaturedetectorsbyGeoffreyHinton,\n",
            "NitishSrivastava,AlexKrizhevsky,IlyaSutskever,andRuslanSalakhutdinov(2012).Notethatthepaper\n",
            "discussesanumberofsubtletiesthatIhaveglossedoverinthisbriefintroduction. \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 91\n",
            "(cid:12)\n",
            "a regularization parameter λ = 5.0, and the cross-entropy cost function. We will train\n",
            "for30epochswhenthefulltrainingdatasetisused,andscaleupthenumberofepochs\n",
            "proportionallywhensmallertrainingsetsareused. Toensuretheweightdecayfactorremains\n",
            "thesameacrosstrainingsets,wewillusearegularizationparameterofλ =5.0whenthe\n",
            "fulltrainingdatasetisused,andscaledownλproportionallywhensmallertrainingsetsare\n",
            "used18. 3\n",
            "Asyoucansee,theclassificationaccuraciesimproveconsiderablyasweusemoretraining\n",
            "data.\n",
            "Presumablythisimprovementwouldcontinuestillfurtherifmoredatawasavailable. Of course, looking at the graph above it does appear that we’re getting near saturation. Suppose,however,thatweredothegraphwiththetrainingsetsizeplottedlogarithmically:\n",
            "Itseemsclearthatthegraphisstillgoinguptowardtheend. Thissuggeststhatifweused\n",
            "vastlymoretrainingdata–say,millionsorevenbillionsofhandwritingsamples,insteadof\n",
            "just50,000–thenwe’dlikelygetconsiderablybetterperformance,evenfromthisverysmall\n",
            "network. Obtainingmoretrainingdataisagreatidea.\n",
            "Unfortunately,itcanbeexpensive,andso\n",
            "isnotalwayspossibleinpractice. However,there’sanotherideawhichcanworknearlyas\n",
            "well,andthat’stoartificiallyexpandthetrainingdata. Suppose,forexample,thatwetake\n",
            "anMNISTtrainingimageofafive,\n",
            "18Thisandthenexttwographareproducedwiththeprogrammore_data.py. \n",
            "(cid:12)\n",
            "92 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "androtateitbyasmallamount,let’ssay15degrees:\n",
            "3\n",
            "It’sstillrecognizablythesamedigit. Andyetatthepixellevelit’squitedifferenttoanyimage\n",
            "currentlyintheMNISTtrainingdata. It’sconceivablethataddingthisimagetothetraining\n",
            "datamighthelpournetworklearnmoreabouthowtoclassifydigits. What’smore,obviously\n",
            "we’renotlimitedtoaddingjustthisoneimage. Wecanexpandourtrainingdatabymaking\n",
            "manysmallrotationsofalltheMNISTtrainingimages,andthenusingtheexpandedtraining\n",
            "datatoimproveournetwork’sperformance.\n",
            "Thisideaisverypowerfulandhasbeenwidelyused. Let’slookatsomeoftheresults\n",
            "fromapaper19 whichappliedseveralvariationsoftheideatoMNIST.Oneoftheneural\n",
            "networkarchitecturestheyconsideredwasalongsimilarlinestowhatwe’vebeenusing,a\n",
            "feedforwardnetworkwith800hiddenneuronsandusingthecross-entropycostfunction. RunningthenetworkwiththestandardMNISTtrainingdatatheyachievedaclassification\n",
            "accuracyof98.4percentontheirtestset. Butthentheyexpandedthetrainingdata,using\n",
            "notjustrotations,asIdescribedabove,butalsotranslatingandskewingtheimages.\n",
            "By\n",
            "trainingontheexpandeddatasettheyincreasedtheirnetwork’saccuracyto98.9percent. Theyalsoexperimentedwithwhattheycalled“elasticdistortions”,aspecialtypeofimage\n",
            "distortionintendedtoemulatetherandomoscillationsfoundinhandmuscles. Byusingthe\n",
            "elasticdistortionstoexpandthedatatheyachievedanevenhigheraccuracy,99.3percent. Effectively,theywerebroadeningtheexperienceoftheirnetworkbyexposingittothesort\n",
            "ofvariationsthatarefoundinrealhandwriting. Variationsonthisideacanbeusedtoimproveperformanceonmanylearningtasks,not\n",
            "justhandwritingrecognition. Thegeneralprincipleistoexpandthetrainingdatabyapplying\n",
            "operationsthatreflectreal-worldvariation. It’snotdifficulttothinkofwaysofdoingthis. Suppose,forexample,thatyou’rebuildinganeuralnetworktodospeechrecognition. We\n",
            "humanscanrecognizespeecheveninthepresenceofdistortionssuchasbackgroundnoise. Andsoyoucanexpandyourdatabyaddingbackgroundnoise.\n",
            "Wecanalsorecognizespeech\n",
            "ifit’sspeduporsloweddown. Sothat’sanotherwaywecanexpandthetrainingdata. These\n",
            "techniquesarenotalwaysused–forinstance,insteadofexpandingthetrainingdataby\n",
            "addingnoise,itmaywellbemoreefficienttocleanuptheinputtothenetworkbyfirst\n",
            "applyinganoisereductionfilter. Still,it’sworthkeepingtheideaofexpandingthetraining\n",
            "datainmind,andlookingforopportunitiestoapplyit. Exercise\n",
            "Asdiscussedabove,onewayofexpandingtheMNISTtrainingdataistousesmall\n",
            "• rotationsoftrainingimages. What’saproblemthatmightoccurifweallowarbitrarily\n",
            "largerotationsoftrainingimages? Anasideonbigdataandwhatitmeanstocompareclassificationaccuracies: Let’slook\n",
            "againathowourneuralnetwork’saccuracyvarieswithtrainingsetsize:\n",
            "19BestPracticesforConvolutionalNeuralNetworksAppliedtoVisualDocumentAnalysisbyPatrice\n",
            "Simard,DaveSteinkraus,andJohnPlatt(2003). \n",
            "(cid:12)\n",
            "3.2. Overfittingandregularization (cid:12) 93\n",
            "(cid:12)\n",
            "3\n",
            "Supposethatinsteadofusinganeuralnetworkweusesomeothermachinelearningtechnique\n",
            "toclassifydigits. Forinstance,let’stryusingthesupportvectormachines(SVM)whichwe\n",
            "metbrieflybackinChapter1. AswasthecaseinChapter1,don’tworryifyou’renotfamiliar\n",
            "withSVMs,wedon’tneedtounderstandtheirdetails.\n",
            "Instead,we’llusetheSVMsupplied\n",
            "bythescikit-learnlibrary. Here’showSVMperformancevariesasafunctionoftrainingset\n",
            "size. I’veplottedtheneuralnetresultsaswell,tomakecomparisoneasy20:\n",
            "Probablythefirstthingthatstrikesyouaboutthisgraphisthatourneuralnetworkoutper-\n",
            "formstheSVMforeverytrainingsetsize. That’snice,althoughyoushouldn’treadtoomuch\n",
            "intoit,sinceIjustusedtheout-of-the-boxsettingsfromscikit-learn’sSVM,whilewe’vedone\n",
            "afairbitofworkimprovingourneuralnetwork. Amoresubtlebutmoreinterestingfact\n",
            "aboutthegraphisthatifwetrainourSVMusing50,000imagesthenitactuallyhasbetter\n",
            "performance(94.48percentaccuracy)thanourneuralnetworkdoeswhentrainedusing\n",
            "5,000images(93.24percentaccuracy). Inotherwords,moretrainingdatacansometimes\n",
            "compensatefordifferencesinthemachinelearningalgorithmused.\n",
            "Somethingevenmoreinterestingcanoccur. Supposewe’retryingtosolveaproblem\n",
            "usingtwomachinelearningalgorithms,algorithmAandalgorithmB.Itsometimeshappens\n",
            "thatalgorithmAwilloutperformalgorithmBwithonesetoftrainingdata,whilealgorithm\n",
            "20Thisgraphwasproducedwiththeprogrammore_data.py(aswerethelastfewgraphs). \n",
            "(cid:12)\n",
            "94 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "BwilloutperformalgorithmAwithadifferentsetoftrainingdata. Wedon’tseethatabove–\n",
            "itwouldrequirethetwographstocross–butitdoeshappen21. Thecorrectresponsetothe\n",
            "question“IsalgorithmAbetterthanalgorithmB?”isreally: “Whattrainingdatasetareyou\n",
            "using?”\n",
            "Allthisisacautiontokeepinmind,bothwhendoingdevelopment,andwhenreading\n",
            "researchpapers. Manypapersfocusonfindingnewtrickstowringoutimprovedperformance\n",
            "3 onstandardbenchmarkdatasets. “Ourwhiz-bangtechniquegaveusanimprovementofX\n",
            "percentonstandardbenchmarkY”isacanonicalformofresearchclaim. Suchclaimsare\n",
            "oftengenuinelyinteresting,buttheymustbeunderstoodasapplyingonlyinthecontextof\n",
            "thespecifictrainingdatasetused. Imagineanalternatehistoryinwhichthepeoplewho\n",
            "originallycreatedthebenchmarkdatasethadalargerresearchgrant. Theymighthaveused\n",
            "theextramoneytocollectmoretrainingdata. It’sentirelypossiblethatthe“improvement”\n",
            "duetothewhiz-bangtechniquewoulddisappearonalargerdataset. Inotherwords,the\n",
            "purportedimprovementmightbejustanaccidentofhistory. Themessagetotakeaway,\n",
            "especiallyinpracticalapplications,isthatwhatwewantisbothbetteralgorithmsandbetter\n",
            "trainingdata. It’sfinetolookforbetteralgorithms,butmakesureyou’renotfocusingon\n",
            "betteralgorithmstotheexclusionofeasywinsgettingmoreorbettertrainingdata. Problem\n",
            "(Researchproblem)Howdoourmachinelearningalgorithmsperforminthelimitof\n",
            "• verylargedatasets? Foranygivenalgorithmit’snaturaltoattempttodefineanotion\n",
            "ofasymptoticperformanceinthelimitoftrulybigdata. Aquick-and-dirtyapproach\n",
            "tothisproblemistosimplytryfittingcurvestographslikethoseshownabove,and\n",
            "thentoextrapolatethefittedcurvesouttoinfinity. Anobjectiontothisapproach\n",
            "isthatdifferentapproachestocurvefittingwillgivedifferentnotionsofasymptotic\n",
            "performance. Canyoufindaprincipledjustificationforfittingtosomeparticularclass\n",
            "ofcurves? Ifso,comparetheasymptoticperformanceofseveraldifferentmachine\n",
            "learningalgorithms. Summingup: We’venowcompletedourdiveintooverfittingandregularization.\n",
            "Ofcourse,\n",
            "we’llreturnagaintotheissue. AsI’vementionedseveraltimes,overfittingisamajorproblem\n",
            "inneuralnetworks,especiallyascomputersgetmorepowerful,andwehavetheabilityto\n",
            "trainlargernetworks. Asaresultthere’sapressingneedtodeveloppowerfulregularization\n",
            "techniquestoreduceoverfitting,andthisisanextremelyactiveareaofcurrentwork. 3.3 Weight initialization\n",
            "Whenwecreateourneuralnetworks,wehavetomakechoicesfortheinitialweightsand\n",
            "biases. Uptonow,we’vebeenchoosingthemaccordingtoaprescriptionwhichIdiscussed\n",
            "onlybrieflybackinChapter1. Justtoremindyou,thatprescriptionwastochooseboththe\n",
            "weightsandbiasesusingindependentGaussianrandomvariables,normalizedtohavemean\n",
            "0andstandarddeviation1. Whilethisapproachhasworkedwell,itwasquiteadhoc,and\n",
            "it’sworthrevisitingtoseeifwecanfindabetterwayofsettingourinitialweightsandbiases,\n",
            "andperhapshelpourneuralnetworkslearnfaster.\n",
            "ItturnsoutthatwecandoquiteabitbetterthaninitializingwithnormalizedGaussians. Toseewhy,supposewe’reworkingwithanetworkwithalargenumber–say1,000–of\n",
            "inputneurons. Andlet’ssupposewe’veusednormalizedGaussianstoinitializetheweights\n",
            "21StrikingexamplesmaybefoundinScalingtoveryverylargecorporafornaturallanguagedisam-\n",
            "biguation,byMicheleBankoandEricBrill(2001).\n",
            "\n",
            "(cid:12)\n",
            "3.3.\n",
            "Weightinitialization (cid:12) 95\n",
            "(cid:12)\n",
            "connectingtothefirsthiddenlayer. FornowI’mgoingtoconcentratespecificallyonthe\n",
            "weightsconnectingtheinputneuronstothefirstneuroninthehiddenlayer,andignorethe\n",
            "restofthenetwork:\n",
            "3\n",
            "We’llsupposeforsimplicitythatwe’retryingtotrainusingatraininginput x inwhichhalf\n",
            "theinputneuronsareon,i.e.,setto1,andhalftheinputneuronsareoff,i.e.,setto0. The\n",
            "argumentwhichfollowsappliesmoregenerally,butyou’llgetthegistfromthisspecialcase. (cid:80)\n",
            "Let’sconsidertheweightedsumz=\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bofinputstoourhiddenneuron. 500terms\n",
            "inthissumvanish,becausethecorrespondinginputx iszero.\n",
            "Andsozisasumoveratotal\n",
            "j\n",
            "of501normalizedGaussianrandomvariables,accountingforthe500weighttermsandthe\n",
            "1extrabiasterm. Thusz isitselfdistributedasaGaussianwithmeanzeroandstandard\n",
            "deviation(cid:112)501 22.4. Thatis,zhasaverybroadGaussiandistribution,notsharplypeaked\n",
            "atall: ≈\n",
            "0.02\n",
            "0.01\n",
            "30 20 10 10 20 30\n",
            "− − −\n",
            "Inparticular,wecanseefromthisgraphthatit’squitelikelythat z willbeprettylarge,\n",
            "i.e.,eitherz 1orz 1. Ifthat’sthecasethentheoutputσ (z)fr|o|mthehiddenneuron\n",
            "will be very(cid:29)close to(cid:28)eit−her 1 or 0. That means our hidden neuron will have saturated. Andwhenthathappens,asweknow,makingsmallchangesintheweightswillmakeonly\n",
            "absolutelyminisculechangesintheactivationofourhiddenneuron. Thatminisculechange\n",
            "intheactivationofthehiddenneuronwill,inturn,barelyaffecttherestoftheneuronsin\n",
            "thenetworkatall,andwe’llseeacorrespondinglyminisculechangeinthecostfunction. Asaresult, thoseweightswillonlylearnveryslowlywhenweusethegradientdescent\n",
            "algorithm22. It’ssimilartotheproblemwediscussedearlierinthischapter,inwhichoutput\n",
            "neuronswhichsaturatedonthewrongvaluecausedlearningtoslowdown. Weaddressed\n",
            "22WediscussedthisinmoredetailinChapter2,whereweusedtheequationsofbackpropagationto\n",
            "showthatweightsinputtosaturatedneuronslearnedslowly. \n",
            "(cid:12)\n",
            "96 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "thatearlierproblemwithacleverchoiceofcostfunction. Unfortunately,whilethathelped\n",
            "withsaturatedoutputneurons,itdoesnothingatallfortheproblemwithsaturatedhidden\n",
            "neurons. I’vebeentalkingabouttheweightsinputtothefirsthiddenlayer. Ofcourse,similar\n",
            "argumentsapplyalsotolaterhiddenlayers:iftheweightsinlaterhiddenlayersareinitialized\n",
            "usingnormalizedGaussians,thenactivationswilloftenbeverycloseto0or1,andlearning\n",
            "3 willproceedveryslowly. Istheresomewaywecanchoosebetterinitializationsfortheweightsandbiases,so\n",
            "thatwedon’tgetthiskindofsaturation,andsoavoidalearningslowdown? Supposewe\n",
            "haveaneuronwithn inputweights. ThenweshallinitializethoseweightsasGaussian\n",
            "in\n",
            "randomvariableswithmean0andstandarddeviation1/ n . Thatis,we’llsquashthe\n",
            "(cid:112) in\n",
            "Gaussiansdown,makingitlesslikelythatourneuronwillsaturate. We’llcontinuetochoose\n",
            "thebiasasaGaussianwithmean0andstandarddeviation1,forreasonsI’llreturntoina\n",
            "(cid:80)\n",
            "moment. Withthesechoices,theweightedsumz=\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bwillagainbeaGaussian\n",
            "randomvariablewithmean0,butit’llbemuchmoresharplypeakedthanitwasbefore. Suppose,aswedidearlier,that500oftheinputsarezeroand500are1.\n",
            "Thenit’seasyto\n",
            "show(seetheexercisebelow)thatzhasaGaussiandistributionwithmean0andstandard\n",
            "deviation (cid:112) 3/2=1.22.... Thisismuchmoresharplypeakedthanbefore,somuchsothat\n",
            "eventhegraphbelowunderstatesthesituation,sinceI’vehadtorescaletheverticalaxis,\n",
            "whencomparedtotheearliergraph:\n",
            "0.4\n",
            "30 20 10 10 20 30\n",
            "− − −\n",
            "Suchaneuronismuchlesslikelytosaturate,andcorrespondinglymuchlesslikelytohave\n",
            "problemswithalearningslowdown. Exercise\n",
            "Verifythatthestandarddeviationofz= (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+bintheparagraphaboveis (cid:112) 3/2.\n",
            "• Itmayhelptoknowthat: (a)thevarianceofasumofindependentrandomvariables\n",
            "isthesumofthevariancesoftheindividualrandomvariables;and(b)thevarianceis\n",
            "thesquareofthestandarddeviation. I stated above that we’ll continue to initialize the biases as before, as Gaussian random\n",
            "variableswithameanof0andastandarddeviationof1. Thisisokay,becauseitdoesn’t\n",
            "makeittoomuchmorelikelythatourneuronswillsaturate.\n",
            "Infact,itdoesn’tmuchmatter\n",
            "howweinitializethebiases,providedweavoidtheproblemwithsaturation. Somepeoplego\n",
            "sofarastoinitializeallthebiasesto0,andrelyongradientdescenttolearnappropriatebiases. Butsinceit’sunlikelytomakemuchdifference,we’llcontinuewiththesameinitialization\n",
            "procedureasbefore. Let’scomparetheresultsforbothouroldandnewapproachestoweightinitialization,\n",
            "usingtheMNISTdigitclassificationtask. Asbefore,we’lluse30hiddenneurons,amini-batch\n",
            "sizeof10,aregularizationparameterλ =5.0,andthecross-entropycostfunction. Wewill\n",
            "decreasethelearningrateslightlyfromη =0.5to0.1,sincethatmakestheresultsalittle\n",
            "moreeasilyvisibleinthegraphs. Wecantrainusingtheoldmethodofweightinitialization:\n",
            "\n",
            "(cid:12)\n",
            "3.3. Weightinitialization (cid:12) 97\n",
            "(cid:12)\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.large_weight_initializer()\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True) 3\n",
            "Wecanalsotrainusingthenewapproachtoweightinitialization. Thisisactuallyeveneasier,\n",
            "sincenetwork2’sdefaultwayofinitializingtheweightsisusingthisnewapproach. That\n",
            "meanswecanomitthenet.large_weight_initializer()callabove:\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Plottingtheresults23,weobtain:\n",
            "Inbothcases,weendupwithaclassificationaccuracysomewhatover96percent.\n",
            "Thefinal\n",
            "classificationaccuracyisalmostexactlythesameinthetwocases. Butthenewinitialization\n",
            "techniquebringsustheremuch,muchfaster. Attheendofthefirstepochoftrainingthe\n",
            "oldapproachtoweightinitializationhasaclassificationaccuracyunder87percent,while\n",
            "thenewapproachisalreadyalmost93percent. Whatappearstobegoingonisthatour\n",
            "newapproachtoweightinitializationstartsusoffinamuchbetterregime,whichletsusget\n",
            "goodresultsmuchmorequickly. Thesamephenomenonisalsoseenifweplotresultswith\n",
            "100hiddenneurons:\n",
            "23Theprogramusedtogeneratethisandthenextgraphisweight_initialization.py. \n",
            "(cid:12)\n",
            "98 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "3\n",
            "Inthiscase,thetwocurvesdon’tquitemeet. However,myexperimentssuggestthatwithjust\n",
            "afewmoreepochsoftraining(notshown)theaccuraciesbecomealmostexactlythesame. Soonthebasisoftheseexperimentsitlooksasthoughtheimprovedweightinitialization\n",
            "onlyspeedsuplearning,itdoesn’tchangethefinalperformanceofournetworks. However,in\n",
            "Chapter4we’llseeexamplesofneuralnetworkswherethelong-runbehaviourissignificantly\n",
            "betterwiththe1/ n weightinitialization. Thusit’snotonlythespeedoflearningwhichis\n",
            "(cid:112) in\n",
            "improved,it’ssometimesalsothefinalperformance. The1/ n approachtoweightinitializationhelpsimprovethewayourneuralnets\n",
            "(cid:112) in\n",
            "learn. Othertechniquesforweightinitializationhavealsobeenproposed,manybuildingon\n",
            "thisbasicidea. Iwon’treviewtheotherapproacheshere,since1/ n workswellenoughfor\n",
            "(cid:112) in\n",
            "ourpurposes.\n",
            "Ifyou’reinterestedinlookingfurther,Irecommendlookingatthediscussion\n",
            "onpages14and15ofa2012paperbyYoshuaBengio24,aswellasthereferencestherein. Problem\n",
            "ConnectingregularizationandtheimprovedmethodofweightinitializationL2\n",
            "• regularizationsometimesautomaticallygivesussomethingsimilartothenewap-\n",
            "proachtoweightinitialization. Supposeweareusingtheoldapproachtoweight\n",
            "initialization. Sketchaheuristicargumentthat: (1)supposingλisnottoosmall,\n",
            "thefirstepochsoftrainingwillbedominatedalmostentirelybyweightdecay;(2)\n",
            "providedηλ ntheweightswilldecaybyafactorofexp( ηλ/m)perepoch;and\n",
            "(3)supposing(cid:28)λisnottoolarge,theweightdecaywilltailo−ffwhentheweightsare\n",
            "downtoasizearound1/ n ,wherenisthetotalnumberofweightsinthenetwork. (cid:112) in\n",
            "Arguethattheseconditionsareallsatisfiedintheexamplesgraphedinthissection. 3.4 Handwriting recognition revisited: the code\n",
            "Let’simplementtheideaswe’vediscussedinthischapter. We’lldevelopanewprogram,\n",
            "network2.py,whichisanimprovedversionoftheprogramnetwork.pywedevelopedin\n",
            "Chapter1.\n",
            "Ifyouhaven’tlookedatnetwork.pyinawhilethenyoumayfindithelpfulto\n",
            "spendafewminutesquicklyreadingovertheearlierdiscussion. It’sonly74linesofcode,\n",
            "andiseasilyunderstood. 24PracticalRecommendationsforGradient-BasedTrainingofDeepArchitectures,byYoshuaBengio\n",
            "(2012). \n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 99\n",
            "(cid:12)\n",
            "Aswasthecaseinnetwork.py,thestarofnetwork2.pyistheNetworkclass,which\n",
            "weusetorepresentourneuralnetworks. WeinitializeaninstanceofNetworkwithalistof\n",
            "sizesfortherespectivelayersinthenetwork,andachoiceforthecosttouse,defaultingto\n",
            "thecross-entropy:\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "self.num_layers = len(sizes) 3\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "Thefirstcoupleoflinesofthe__init__methodarethesameasinnetwork.py,andare\n",
            "prettyself-explanatory. Butthenexttwolinesarenew,andweneedtounderstandwhat\n",
            "they’redoingindetail.\n",
            "Let’sstartbyexaminingthedefault_weight_initializermethod. Thismakesuse\n",
            "ofournewandimprovedapproachtoweightinitialization. Aswe’veseen,inthatapproach\n",
            "theweightsinputtoaneuronareinitializedasGaussianrandomvariableswithmean0and\n",
            "standarddeviation1dividedbythesquarerootofthenumberofconnectionsinputtothe\n",
            "neuron. Alsointhismethodwe’llinitializethebiases,usingGaussianrandomvariableswith\n",
            "mean0andstandarddeviation1. Here’sthecode:\n",
            "def default_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "Tounderstandthecode,itmayhelptorecallthatnpistheNumpylibraryfordoinglinear\n",
            "algebra. We’llimportNumpyatthebeginningofourprogram. Also,noticethatwedon’t\n",
            "initialize any biases for the first layer of neurons. We avoid doing this because the first\n",
            "layerisaninputlayer,andsoanybiaseswouldnotbeused. Wedidexactlythesamething\n",
            "innetwork.py. Complementingthedefault_weight_initializerwe’llalsoincludea\n",
            "large_weight_initializermethod. Thismethodinitializestheweightsandbiasesusing\n",
            "the old approach from Chapter 1, with both weights and biases initialized as Gaussian\n",
            "randomvariableswithmean0andstandarddeviation1. Thecodeis,ofcourse,onlyatiny\n",
            "bitdifferentfromthedefault_weight_initializer:\n",
            "def large_weight_initializer(self):\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x) for x, y in zip(self.sizes[:-1], self. sizes[1:])]\n",
            "I’veincludedthelarge_weight_initializermethodmostlyasaconveniencetomake\n",
            "iteasiertocomparetheresultsinthischaptertothoseinChapter1. Ican’tthinkofmany\n",
            "practicalsituationswhereIwouldrecommendusingit!\n",
            "ThesecondnewthinginNetwork’s__init__methodisthatwenowinitializeacost\n",
            "attribute. Tounderstandhowthatworks, let’slookattheclassweusetorepresentthe\n",
            "cross-entropycost25:\n",
            "class CrossEntropyCost(object):\n",
            "25Ifyou’renotfamiliarwithPython’sstaticmethodsyoucanignorethe@staticmethoddecorators,\n",
            "andjusttreatfnanddeltaasordinarymethods.Ifyou’recuriousaboutdetails,all@staticmethod\n",
            "doesistellthePythoninterpreterthatthemethodwhichfollowsdoesn’tdependontheobjectinany\n",
            "way.That’swhyselfisn’tpassedasaparametertothefnanddeltamethods. \n",
            "(cid:12)\n",
            "100 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y)\n",
            "Let’sbreakthisdown. Thefirstthingtoobserveisthateventhoughthecross-entropyis,\n",
            "3\n",
            "mathematicallyspeaking,afunction,we’veimplementeditasaPythonclass,notaPython\n",
            "function. WhyhaveImadethatchoice? Thereasonisthatthecostplaystwodifferentroles\n",
            "inournetwork. Theobviousroleisthatit’sameasureofhowwellanoutputactivation,a,\n",
            "matchesthedesiredoutput,y. ThisroleiscapturedbytheCrossEntropyCost.fnmethod. (Note,bytheway,thatthenp.nan_to_numcallinsideCrossEntropyCost.fnensuresthat\n",
            "Numpydealscorrectlywiththelogofnumbersveryclosetozero.) Butthere’salsoasecond\n",
            "waythecostfunctionentersournetwork. RecallfromChapter2thatwhenrunningthe\n",
            "backpropagationalgorithmweneedtocomputethenetwork’soutputerror,δL. Theformof\n",
            "theoutputerrordependsonthechoiceofcostfunction: differentcostfunction,different\n",
            "formfortheoutputerror. Forthecross-entropytheoutputerroris,aswesawinEquation\n",
            "(3.12),\n",
            "δL =aL y. (3.44)\n",
            "−\n",
            "Forthisreasonwedefineasecondmethod,CrossEntropyCost.delta,whosepurposeisto\n",
            "tellournetworkhowtocomputetheoutputerror. Andthenwebundlethesetwomethodsup\n",
            "intoasingleclasscontainingeverythingournetworksneedtoknowaboutthecostfunction. In a similar way, network2.py also contains a class to represent the quadratic cost\n",
            "function. ThisisincludedforcomparisonwiththeresultsofChapter1,sincegoingforward\n",
            "we’llmostlyusethecrossentropy.\n",
            "Thecodeisjustbelow. TheQuadraticCost.fnmethod\n",
            "isastraightforwardcomputationofthequadraticcostassociatedtotheactualoutput,a,\n",
            "andthedesiredoutput,y. ThevaluereturnedbyQuadraticCost.deltaisbasedonthe\n",
            "expression(2.8)fortheoutputerrorforthequadraticcost,whichwederivedbackinChapter\n",
            "2. class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "We’venowunderstoodthemaindifferencesbetweennetwork2.pyandnetwork.py. It’s\n",
            "all pretty simple stuff.\n",
            "There are a number of smaller changes, which I’ll discuss below,\n",
            "includingtheimplementationofL2regularization. Beforegettingtothat,let’slookatthe\n",
            "completecodefornetwork2.py. Youdon’tneedtoreadallthecodeindetail,butitisworth\n",
            "understandingthebroadstructure,andinparticularreadingthedocumentationstrings,so\n",
            "youunderstandwhateachpieceoftheprogramisdoing. Ofcourse,you’realsowelcometo\n",
            "delveasdeeplyasyouwish!\n",
            "Ifyougetlost,youmaywishtocontinuereadingtheprose\n",
            "below,andreturntothecodelater. Anyway,here’sthecode:\n",
            "\"\"\"network2.py\n",
            "~~~~~~~~~~~~~~\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 101\n",
            "(cid:12)\n",
            "An improved version of network.py, implementing the stochastic\n",
            "gradient descent learning algorithm for a feedforward neural network. Improvements include the addition of the cross-entropy cost function,\n",
            "regularization, and better initialization of network weights.\n",
            "Note\n",
            "that I have focused on making the code simple, easily readable, and\n",
            "easily modifiable.\n",
            "It is not optimized, and omits many desirable\n",
            "features. \"\"\"\n",
            "#### Libraries 3\n",
            "# Standard library\n",
            "import json\n",
            "import random\n",
            "import sys\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "#### Define the quadratic and cross-entropy cost functions\n",
            "class QuadraticCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output ‘‘y‘‘. \"\"\"\n",
            "return 0.5*np.linalg.norm(a-y)**2\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer.\"\"\"\n",
            "return (a-y) * sigmoid_prime(z)\n",
            "class CrossEntropyCost(object):\n",
            "@staticmethod\n",
            "def fn(a, y):\n",
            "\"\"\"Return the cost associated with an output ‘‘a‘‘ and desired output\n",
            "‘‘y‘‘. Note that np.nan_to_num is used to ensure numerical\n",
            "stability. In particular, if both ‘‘a‘‘ and ‘‘y‘‘ have a 1.0\n",
            "in the same slot, then the expression (1-y)*np.log(1-a)\n",
            "returns nan. The np.nan_to_num ensures that that is converted\n",
            "to the correct value (0.0). \"\"\"\n",
            "return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
            "@staticmethod\n",
            "def delta(z, a, y):\n",
            "\"\"\"Return the error delta from the output layer. Note that the\n",
            "parameter ‘‘z‘‘ is not used by the method. It is included in\n",
            "the method’s parameters in order to make the interface\n",
            "consistent with the delta method for other cost classes. \"\"\"\n",
            "return (a-y)\n",
            "#### Main Network class\n",
            "class Network(object):\n",
            "def __init__(self, sizes, cost=CrossEntropyCost):\n",
            "\"\"\"The list ‘‘sizes‘‘ contains the number of neurons in the respective\n",
            "\n",
            "(cid:12)\n",
            "102 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "layers of the network. For example, if the list was [2, 3, 1]\n",
            "then it would be a three-layer network, with the first layer\n",
            "containing 2 neurons, the second layer 3 neurons, and the\n",
            "third layer 1 neuron. The biases and weights for the network\n",
            "are initialized randomly, using\n",
            "‘‘self.default_weight_initializer‘‘ (see docstring for that\n",
            "method). 3 \"\"\"\n",
            "self.num_layers = len(sizes)\n",
            "self.sizes = sizes\n",
            "self.default_weight_initializer()\n",
            "self.cost=cost\n",
            "def default_weight_initializer(self):\n",
            "\"\"\"Initialize each weight using a Gaussian distribution with mean 0\n",
            "and standard deviation 1 over the square root of the number of\n",
            "weights connecting to the same neuron. Initialize the biases\n",
            "using a Gaussian distribution with mean 0 and standard\n",
            "deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we won’t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)/np.sqrt(x) for x, y in zip(self.sizes\n",
            "[:-1], self.sizes[1:])]\n",
            "def large_weight_initializer(self):\n",
            "\"\"\"Initialize the weights using a Gaussian distribution with mean 0\n",
            "and standard deviation 1. Initialize the biases using a\n",
            "Gaussian distribution with mean 0 and standard deviation 1. Note that the first layer is assumed to be an input layer, and\n",
            "by convention we won’t set any biases for those neurons, since\n",
            "biases are only ever used in computing the outputs from later\n",
            "layers. This weight and bias initializer uses the same approach as in\n",
            "Chapter 1, and is included for purposes of comparison.\n",
            "It\n",
            "will usually be better to use the default weight initializer\n",
            "instead. \"\"\"\n",
            "self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
            "self.weights = [np.random.randn(y, x)\n",
            "for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
            "def feedforward(self, a):\n",
            "\"\"\"Return the output of the network if ‘‘a‘‘ is input.\"\"\"\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "a = sigmoid(np.dot(w, a)+b)\n",
            "return a\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta, lmbda = 0.0,\n",
            "evaluation_data=None, monitor_evaluation_cost=False,\n",
            "monitor_evaluation_accuracy=False, monitor_training_cost=False,\n",
            "monitor_training_accuracy=False):\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 103\n",
            "(cid:12)\n",
            "\"\"\"Train the neural network using mini-batch stochastic gradient\n",
            "descent. The ‘‘training_data‘‘ is a list of tuples ‘‘(x, y)‘‘\n",
            "representing the training inputs and the desired outputs.\n",
            "The\n",
            "other non-optional parameters are self-explanatory, as is the\n",
            "regularization parameter ‘‘lmbda‘‘. The method also accepts\n",
            "‘‘evaluation_data‘‘, usually either the validation or test\n",
            "data. We can monitor the cost and accuracy on either the\n",
            "evaluation data or the training data, by setting the\n",
            "appropriate flags. The method returns a tuple containing four 3\n",
            "lists: the (per-epoch) costs on the evaluation data, the\n",
            "accuracies on the evaluation data, the costs on the training\n",
            "data, and the accuracies on the training data. All values are\n",
            "evaluated at the end of each training epoch. So, for example,\n",
            "if we train for 30 epochs, then the first element of the tuple\n",
            "will be a 30-element list containing the cost on the\n",
            "evaluation data at the end of each epoch. Note that the lists\n",
            "are empty if the corresponding flag is not set. \"\"\"\n",
            "if evaluation_data:\n",
            "n_data = len(evaluation_data)\n",
            "n = len(training_data)\n",
            "evaluation_cost, evaluation_accuracy = [], []\n",
            "training_cost, training_accuracy = [], []\n",
            "for j in xrange(epochs):\n",
            "random.shuffle(training_data)\n",
            "mini_batches = [\n",
            "training_data[k:k+mini_batch_size]\n",
            "for k in xrange(0, n, mini_batch_size)]\n",
            "for mini_batch in mini_batches:\n",
            "self.update_mini_batch(\n",
            "mini_batch, eta, lmbda, len(training_data))\n",
            "print \"Epoch %s training complete\" % j\n",
            "if monitor_training_cost:\n",
            "cost = self.total_cost(training_data, lmbda)\n",
            "training_cost.append(cost)\n",
            "print \"Cost on training data: {}\".format(cost)\n",
            "if monitor_training_accuracy:\n",
            "accuracy = self.accuracy(training_data, convert=True)\n",
            "training_accuracy.append(accuracy)\n",
            "print \"Accuracy on training data: {} / {}\".format(accuracy, n)\n",
            "if monitor_evaluation_cost:\n",
            "cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
            "evaluation_cost.append(cost)\n",
            "print \"Cost on evaluation data: {}\".format(cost)\n",
            "if monitor_evaluation_accuracy:\n",
            "accuracy = self.accuracy(evaluation_data)\n",
            "evaluation_accuracy.append(accuracy)\n",
            "print \"Accuracy on evaluation data: {} / {}\".format(self.accuracy(\n",
            "evaluation_data), n_data)\n",
            "print\n",
            "return evaluation_cost, evaluation_accuracy, training_cost, training_accuracy\n",
            "def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
            "\"\"\"Update the network’s weights and biases by applying gradient\n",
            "descent using backpropagation to a single mini batch. The\n",
            "‘‘mini_batch‘‘ is a list of tuples ‘‘(x, y)‘‘, ‘‘eta‘‘ is the\n",
            "learning rate, ‘‘lmbda‘‘ is the regularization parameter, and\n",
            "‘‘n‘‘ is the total size of the training data set.\n",
            "\"\"\"\n",
            "\n",
            "(cid:12)\n",
            "104 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "for x, y in mini_batch:\n",
            "delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
            "nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
            "nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
            "self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
            "for w, nw in zip(self.weights, nabla_w)]\n",
            "3 self.biases = [b-(eta/len(mini_batch))*nb\n",
            "for b, nb in zip(self.biases, nabla_b)]\n",
            "def backprop(self, x, y):\n",
            "\"\"\"Return a tuple ‘‘(nabla_b, nabla_w)‘‘ representing the\n",
            "gradient for the cost function C_x. ‘‘nabla_b‘‘ and\n",
            "‘‘nabla_w‘‘ are layer-by-layer lists of numpy arrays, similar\n",
            "to ‘‘self.biases‘‘ and ‘‘self.weights‘‘.\"\"\"\n",
            "nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
            "nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
            "# feedforward\n",
            "activation = x\n",
            "activations = [x] # list to store all the activations, layer by layer\n",
            "zs = [] # list to store all the z vectors, layer by layer\n",
            "for b, w in zip(self.biases, self.weights):\n",
            "z = np.dot(w, activation)+b\n",
            "zs.append(z)\n",
            "activation = sigmoid(z)\n",
            "activations.append(activation)\n",
            "# backward pass\n",
            "delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
            "nabla_b[-1] = delta\n",
            "nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
            "# Note that the variable l in the loop below is used a little\n",
            "# differently to the notation in Chapter 2 of the book. Here,\n",
            "# l = 1 means the last layer of neurons, l = 2 is the\n",
            "# second-last layer, and so on. It’s a renumbering of the\n",
            "# scheme in the book, used here to take advantage of the fact\n",
            "# that Python can use negative indices in lists. for l in xrange(2, self.num_layers):\n",
            "z = zs[-l]\n",
            "sp = sigmoid_prime(z)\n",
            "delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
            "nabla_b[-l] = delta\n",
            "nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
            "return (nabla_b, nabla_w)\n",
            "def accuracy(self, data, convert=False):\n",
            "\"\"\"Return the number of inputs in ‘‘data‘‘ for which the neural\n",
            "network outputs the correct result. The neural network’s\n",
            "output is assumed to be the index of whichever neuron in the\n",
            "final layer has the highest activation.\n",
            "The flag ‘‘convert‘‘ should be set to False if the data set is\n",
            "validation or test data (the usual case), and to True if the\n",
            "data set is the training data. The need for this flag arises\n",
            "due to differences in the way the results ‘‘y‘‘ are\n",
            "represented in the different data sets. In particular, it\n",
            "flags whether we need to convert between the different\n",
            "representations. It may seem strange to use different\n",
            "representations for the different data sets. Why not use the\n",
            "same representation for all three data sets? It’s done for\n",
            "efficiency reasons -- the program usually evaluates the cost\n",
            "\n",
            "(cid:12)\n",
            "3.4. Handwritingrecognitionrevisited: thecode (cid:12) 105\n",
            "(cid:12)\n",
            "on the training data and the accuracy on other data sets. These are different types of computations, and using different\n",
            "representations speeds things up. More details on the\n",
            "representations can be found in\n",
            "mnist_loader.load_data_wrapper. \"\"\"\n",
            "if convert:\n",
            "results = [(np.argmax(self.feedforward(x)), np.argmax(y)) for (x, y) in 3\n",
            "data]\n",
            "else:\n",
            "results = [(np.argmax(self.feedforward(x)), y) for (x, y) in data]\n",
            "return sum(int(x == y) for (x, y) in results)\n",
            "def total_cost(self, data, lmbda, convert=False):\n",
            "\"\"\"Return the total cost for the data set ‘‘data‘‘. The flag\n",
            "‘‘convert‘‘ should be set to False if the data set is the\n",
            "training data (the usual case), and to True if the data set is\n",
            "the validation or test data. See comments on the similar (but\n",
            "reversed) convention for the ‘‘accuracy‘‘ method, above. \"\"\"\n",
            "cost = 0.0\n",
            "for x, y in data:\n",
            "a = self.feedforward(x)\n",
            "if convert:\n",
            "y = vectorized_result(y)\n",
            "cost += self.cost.fn(a, y)/len(data)\n",
            "cost += 0.5*(lmbda/len(data))*sum(np.linalg.norm(w)**2 for w in self.weights)\n",
            "return cost\n",
            "def save(self, filename):\n",
            "\"\"\"Save the neural network to the file ‘‘filename‘‘.\"\"\"\n",
            "data = {\"sizes\": self.sizes,\n",
            "\"weights\": [w.tolist() for w in self.weights],\n",
            "\"biases\": [b.tolist() for b in self.biases],\n",
            "\"cost\": str(self.cost.__name__)}\n",
            "f = open(filename, \"w\")\n",
            "json.dump(data, f)\n",
            "f.close()\n",
            "#### Loading a Network\n",
            "def load(filename):\n",
            "\"\"\"Load a neural network from the file ‘‘filename‘‘. Returns an\n",
            "instance of Network. \"\"\"\n",
            "f = open(filename, \"r\")\n",
            "data = json.load(f)\n",
            "f.close()\n",
            "cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
            "net = Network(data[\"sizes\"], cost=cost)\n",
            "net.weights = [np.array(w) for w in data[\"weights\"]]\n",
            "net.biases = [np.array(b) for b in data[\"biases\"]]\n",
            "return net\n",
            "#### Miscellaneous functions\n",
            "def vectorized_result(j):\n",
            "\"\"\"Return a 10-dimensional unit vector with a 1.0 in the j’th position\n",
            "and zeroes elsewhere. This is used to convert a digit (0...9)\n",
            "into a corresponding desired output from the neural network.\n",
            "\n",
            "(cid:12)\n",
            "106 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "\"\"\"\n",
            "e = np.zeros((10, 1))\n",
            "e[j] = 1.0\n",
            "return e\n",
            "def sigmoid(z):\n",
            "\"\"\"The sigmoid function.\"\"\"\n",
            "return 1.0/(1.0+np.exp(-z))\n",
            "3\n",
            "def sigmoid_prime(z):\n",
            "\"\"\"Derivative of the sigmoid function.\"\"\"\n",
            "return sigmoid(z)*(1-sigmoid(z))\n",
            "OneofthemoreinterestingchangesinthecodeistoincludeL2regularization. Although\n",
            "thisisamajorconceptualchange,it’ssotrivialtoimplementthatit’seasytomissinthe\n",
            "code.\n",
            "Forthemostpartitjustinvolvespassingtheparameterlmbdatovariousmethods,\n",
            "notablytheNetwork.SGDmethod. Therealworkisdoneinasinglelineoftheprogram,the\n",
            "fourth-lastlineoftheNetwork.update_mini_batchmethod. That’swherewemodifythe\n",
            "gradientdescentupdateruletoincludeweightdecay. Butalthoughthemodificationistiny,\n",
            "ithasabigimpactonresults!\n",
            "Thisis,bytheway,commonwhenimplementingnewtechniquesinneuralnetworks. We’vespentthousandsofwordsdiscussingregularization. It’sconceptuallyquitesubtleand\n",
            "difficulttounderstand. Andyetitwastrivialtoaddtoourprogram! Itoccurssurprisingly\n",
            "oftenthatsophisticatedtechniquescanbeimplementedwithsmallchangestocode. Anothersmallbutimportantchangetoourcodeistheadditionofseveraloptionalflags\n",
            "tothestochasticgradientdescentmethod,Network.SGD.Theseflagsmakeitpossibleto\n",
            "monitorthecostandaccuracyeitheronthetraining_dataoronasetofevaluation_data\n",
            "whichcanbepassedtoNetwork.SGD. We’veusedtheseflagsoftenearlierinthechapter,\n",
            "butletmegiveanexampleofhowitworks,justtoremindyou:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10], cost=network2.CrossEntropyCost)\n",
            ">>> net.SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data,\n",
            "... monitor_evaluation_accuracy=True, monitor_evaluation_cost=True,\n",
            "monitor_training_accuracy=True,\n",
            "... monitor_training_cost=True)\n",
            "Here,we’resettingtheevaluation_datatobethevalidation_data. Butwecouldalso\n",
            "havemonitoredperformanceonthetest_dataoranyotherdataset. Wealsohavefour\n",
            "flagstellingustomonitorthecostandaccuracyonboththeevaluation_dataandthe\n",
            "training_data. ThoseflagsareFalsebydefault,butthey’vebeenturnedonhereinorder\n",
            "tomonitorourNetwork’sperformance. Furthermore,network2.py’sNetwork.SGDmethod\n",
            "returnsafour-elementtuplerepresentingtheresultsofthemonitoring. Wecanusethisas\n",
            "follows:\n",
            ">>> evaluation_cost, evaluation_accuracy, training_cost, training_accuracy = net\n",
            ".SGD(training_data, 30, 10, 0.5, lmbda = 5.0, evaluation_data=\n",
            "validation_data, monitor_evaluation_accuracy=True, monitor_evaluation_cost=\n",
            "True, monitor_training_accuracy=True, monitor_training_cost=True)\n",
            "So,forexample,evaluation_costwillbea30-elementlistcontainingthecostonthe\n",
            "evaluationdataattheendofeachepoch. Thissortofinformationisextremelyusefulin\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 107\n",
            "(cid:12)\n",
            "understandinganetwork’sbehaviour.\n",
            "Itcan,forexample,beusedtodrawgraphsshowing\n",
            "howthenetworklearnsovertime. Indeed,that’sexactlyhowIconstructedallthegraphs\n",
            "earlierinthechapter. Note,however,thatifanyofthemonitoringflagsarenotset,thenthe\n",
            "correspondingelementinthetuplewillbetheemptylist. OtheradditionstothecodeincludeaNetwork.savemethod,tosaveNetworkobjects\n",
            "todisk,andafunctiontoloadthembackinagainlater. Notethatthesavingandloading\n",
            "isdoneusingJSON,notPython’spickleorcPicklemodules,whicharetheusualwaywe 3\n",
            "saveandloadobjectstoandfromdiskinPython. UsingJSONrequiresmorecodethan\n",
            "pickleorcPicklewould. TounderstandwhyI’veusedJSON,imaginethatatsometimein\n",
            "thefuturewedecidedtochangeourNetworkclasstoallowneuronsotherthansigmoid\n",
            "neurons. Toimplementthatchangewe’dmostlikelychangetheattributesdefinedinthe\n",
            "Network.__init__method. Ifwe’vesimplypickledtheobjectsthatwouldcauseourload\n",
            "functiontofail. UsingJSONtodotheserializationexplicitlymakesiteasytoensurethatold\n",
            "Networkswillstillload. Therearemanyotherminorchangesinthecodefornetwork2.py,butthey’reallsimple\n",
            "variationsonnetwork.py. Thenetresultistoexpandour74-lineprogramtoafarmore\n",
            "capable152lines. Problems\n",
            "ModifythecodeabovetoimplementL1regularization,anduseL1regularizationto\n",
            "• classifyMNISTdigitsusinga30hiddenneuronnetwork. Canyoufindaregularization\n",
            "parameterthatenablesyoutodobetterthanrunningunregularized? TakealookattheNetwork.cost_derivativemethodinnetwork.py.\n",
            "Thatmethod\n",
            "• waswrittenforthequadraticcost. Howwouldyourewritethemethodforthecross-\n",
            "entropycost? Canyouthinkofaproblemthatmightariseinthecross-entropyversion? Innetwork2.pywe’veeliminatedtheNetwork.cost_derivativemethodentirely,\n",
            "insteadincorporatingitsfunctionalityintotheCrossEntropyCost.deltamethod.\n",
            "Howdoesthissolvetheproblemyou’vejustidentified? 3.5 How to choose a neural network’s hyper-parameters? UpuntilnowIhaven’texplainedhowI’vebeenchoosingvaluesforhyper-parameterssuch\n",
            "asthelearningrate,η,theregularizationparameter,λ,andsoon. I’vejustbeensupplying\n",
            "valueswhichworkprettywell. Inpractice,whenyou’reusingneuralnetstoattackaproblem,\n",
            "it can be difficult to find good hyper-parameters. Imagine, for example, that we’ve just\n",
            "beenintroducedtotheMNISTproblem,andhavebegunworkingonit,knowingnothing\n",
            "atallaboutwhathyper-parameterstouse. Let’ssupposethatbygoodfortuneinourfirst\n",
            "experimentswechoosemanyofthehyper-parametersinthesamewayaswasdoneearlier\n",
            "this chapter: 30 hidden neurons, a mini-batch size of 10, training for 30 epochs using\n",
            "thecross-entropy. Butwechoosealearningrateη =10.0andregularizationparameter\n",
            "λ =1000.0. Here’swhatIsawononesuchrun:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 10.0, lmbda = 1000.0,\n",
            "\n",
            "(cid:12)\n",
            "108 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 1030 / 10000\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 990 / 10000\n",
            "3\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "... Epoch 27 training complete\n",
            "Accuracy on evaluation data: 1009 / 10000\n",
            "Epoch 28 training complete\n",
            "Accuracy on evaluation data: 983 / 10000\n",
            "Epoch 29 training complete\n",
            "Accuracy on evaluation data: 967 / 10000\n",
            "Ourclassificationaccuraciesarenobetterthanchance! Ournetworkisactingasarandom\n",
            "noisegenerator!\n",
            "“Well,that’seasytofix,”youmightsay,“justdecreasethelearningrateandregularization\n",
            "hyper-parameters”. Unfortunately,youdon’taprioriknowthosearethehyper-parameters\n",
            "youneedtoadjust. Maybetherealproblemisthatour30hiddenneuronnetworkwillnever\n",
            "workwell,nomatterhowtheotherhyper-parametersarechosen? Maybewereallyneedat\n",
            "least100hiddenneurons? Or300hiddenneurons? Ormultiplehiddenlayers? Oradifferent\n",
            "approachtoencodingtheoutput? Maybeournetworkislearning,butweneedtotrain\n",
            "formoreepochs? Maybethemini-batchesaretoosmall? Maybewe’ddobetterswitching\n",
            "backtothequadraticcostfunction? Maybeweneedtotryadifferentapproachtoweight\n",
            "initialization? Andsoon,onandonandon. It’seasytofeellostinhyper-parameterspace. Thiscanbeparticularlyfrustratingifyournetworkisverylarge,orusesalotoftraining\n",
            "data,sinceyoumaytrainforhoursordaysorweeks,onlytogetnoresult. Ifthesituation\n",
            "persists,itdamagesyourconfidence. Maybeneuralnetworksarethewrongapproachto\n",
            "yourproblem?\n",
            "Maybeyoushouldquityourjobandtakeupbeekeeping? InthissectionIexplainsomeheuristicswhichcanbeusedtosetthehyper-parameters\n",
            "inaneuralnetwork. Thegoalistohelpyoudevelopaworkflowthatenablesyoutodoa\n",
            "prettygoodjobsettinghyper-parameters. Ofcourse,Iwon’tcovereverythingabouthyper-\n",
            "parameteroptimization. That’sahugesubject, andit’snot, inanycase, aproblemthat\n",
            "isevercompletelysolved,noristhereuniversalagreementamongstpractitionersonthe\n",
            "rightstrategiestouse. There’salwaysonemoretrickyoucantrytoekeoutabitmore\n",
            "performancefromyournetwork. Buttheheuristicsinthissectionshouldgetyoustarted. Broadstrategy:Whenusingneuralnetworkstoattackanewproblemthefirstchallenge\n",
            "istogetanynon-triviallearning,i.e.,forthenetworktoachieveresultsbetterthanchance. Thiscanbesurprisinglydifficult,especiallywhenconfrontinganewclassofproblem.\n",
            "Let’s\n",
            "lookatsomestrategiesyoucanuseifyou’rehavingthiskindoftrouble. Suppose, for example, that you’re attacking MNIST for the first time. You start out\n",
            "enthusiastic,butarealittlediscouragedwhenyourfirstnetworkfailscompletely,asinthe\n",
            "exampleabove.\n",
            "Thewaytogoistostriptheproblemdown. Getridofallthetraining\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 109\n",
            "(cid:12)\n",
            "andvalidationimagesexceptimageswhichare0sor1s. Thentrytotrainanetworkto\n",
            "distinguish0sfrom1s. Notonlyisthataninherentlyeasierproblemthandistinguishingall\n",
            "tendigits,italsoreducestheamountoftrainingdataby80percent,speedinguptrainingby\n",
            "afactorof5. Thatenablesmuchmorerapidexperimentation,andsogivesyoumorerapid\n",
            "insightintohowtobuildagoodnetwork. Youcanfurtherspeedupexperimentationbystrippingyournetworkdowntothesimplest\n",
            "networklikelytodomeaningfullearning. Ifyoubelievea[784, 10]networkcanlikely 3\n",
            "dobetter-than-chanceclassificationofMNISTdigits,thenbeginyourexperimentationwith\n",
            "suchanetwork. It’llbemuchfasterthantraininga[784, 30, 10]network,andyoucan\n",
            "buildbackuptothelatter. Youcangetanotherspeedupinexperimentationbyincreasingthefrequencyofmoni-\n",
            "toring. Innetwork2.pywemonitorperformanceattheendofeachtrainingepoch. With\n",
            "50,000imagesperepoch,thatmeanswaitingalittlewhile–abouttensecondsperepoch,\n",
            "onmylaptop,whentraininga[784,30,10]network–beforegettingfeedbackonhowwell\n",
            "thenetworkislearning. Ofcourse,tensecondsisn’tverylong,butifyouwanttotrialdozens\n",
            "ofhyper-parameterchoicesit’sannoying,andifyouwanttotrialhundredsorthousands\n",
            "ofchoicesitstartstogetdebilitating. Wecangetfeedbackmorequicklybymonitoringthe\n",
            "validationaccuracymoreoften,say,afterevery1,000trainingimages. Furthermore,instead\n",
            "ofusingthefull10,000imagevalidationsettomonitorperformance,wecangetamuch\n",
            "fasterestimateusingjust100validationimages. Allthatmattersisthatthenetworksees\n",
            "enoughimagestodoreallearning,andtogetaprettygoodroughestimateofperformance. Ofcourse,ourprogramnetwork2.pydoesn’tcurrentlydothiskindofmonitoring. Butas\n",
            "akludgetoachieveasimilareffectforthepurposesofillustration, we’llstripdownour\n",
            "trainingdatatojustthefirst1,000MNISTtrainingimages. Let’stryitandseewhathappens.\n",
            "(TokeepthecodebelowsimpleIhaven’timplementedtheideaofusingonly0and1images. Ofcourse,thatcanbedonewithjustalittlemorework.)\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 1000.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "...\n",
            "monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... We’restillgettingpurenoise!\n",
            "Butthere’sabigwin: we’renowgettingfeedbackinafraction\n",
            "ofasecond,ratherthanonceeverytensecondsorso. Thatmeansyoucanmorequickly\n",
            "experimentwithotherchoicesofhyper-parameter,orevenconductexperimentstrialling\n",
            "manydifferentchoicesofhyper-parameternearlysimultaneously. IntheaboveexampleIleftλasλ =1000.0,asweusedearlier. Butsincewechanged\n",
            "thenumberoftrainingexamplesweshouldreallychangeλtokeeptheweightdecaythe\n",
            "same. Thatmeanschangingλto20.0. Ifwedothatthenthisiswhathappens:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 10.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "\n",
            "(cid:12)\n",
            "110 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 12 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 14 / 100\n",
            "Epoch 2 training complete\n",
            "3 Accuracy on evaluation data: 25 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 18 / 100\n",
            "... Ahah!\n",
            "Wehaveasignal. Notaterriblygoodsignal,butasignalnonetheless. That’ssomething\n",
            "wecanbuildon,modifyingthehyper-parameterstotrytogetfurtherimprovement. Maybe\n",
            "weguessthatourlearningrateneedstobehigher. (Asyouperhapsrealize,that’sasilly\n",
            "guess,forreasonswe’lldiscussshortly,butpleasebearwithme.) Sototestourguesswetry\n",
            "dialingηupto100.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 100.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 10 / 100\n",
            "... That’snogood!\n",
            "Itsuggeststhatourguesswaswrong, andtheproblemwasn’t thatthe\n",
            "learningratewastoolow. Soinsteadwetrydialingηdowntoη =1.0:\n",
            ">>> net = network2.Network([784, 10])\n",
            ">>> net.SGD(training_data[:1000], 30, 10, 1.0, lmbda = 20.0, \\\n",
            "... evaluation_data=validation_data[:100], \\\n",
            "... monitor_evaluation_accuracy=True)\n",
            "Epoch 0 training complete\n",
            "Accuracy on evaluation data: 62 / 100\n",
            "Epoch 1 training complete\n",
            "Accuracy on evaluation data: 42 / 100\n",
            "Epoch 2 training complete\n",
            "Accuracy on evaluation data: 43 / 100\n",
            "Epoch 3 training complete\n",
            "Accuracy on evaluation data: 61 / 100\n",
            "... \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 111\n",
            "(cid:12)\n",
            "That’sbetter!\n",
            "Andsowecancontinue,individuallyadjustingeachhyper-parameter,gradually\n",
            "improvingperformance. Oncewe’veexploredtofindanimprovedvalueforη,thenwe\n",
            "moveontofindagoodvalueforλ. Thenexperimentwithamorecomplexarchitecture,\n",
            "sayanetworkwith10hiddenneurons. Thenadjustthevaluesforηandλagain. Then\n",
            "increaseto20hiddenneurons. Andthenadjustotherhyper-parameterssomemore.\n",
            "And\n",
            "soon,ateachstageevaluatingperformanceusingourheld-outvalidationdata,andusing\n",
            "thoseevaluationstofindbetterandbetterhyper-parameters. Aswedoso,ittypicallytakes 3\n",
            "longertowitnesstheimpactduetomodificationsofthehyper-parameters,andsowecan\n",
            "graduallydecreasethefrequencyofmonitoring. Thisalllooksverypromisingasabroadstrategy.\n",
            "However,Iwanttoreturntothatinitial\n",
            "stageoffindinghyper-parametersthatenableanetworktolearnanythingatall. Infact,\n",
            "eventheabovediscussionconveystoopositiveanoutlook. Itcanbeimmenselyfrustrating\n",
            "toworkwithanetworkthat’slearningnothing. Youcantweakhyper-parametersfordays,\n",
            "andstillgetnomeaningfulresponse. AndsoI’dliketore-emphasizethatduringtheearly\n",
            "stagesyoushouldmakesureyoucangetquickfeedbackfromexperiments. Intuitively,it\n",
            "mayseemasthoughsimplifyingtheproblemandthearchitecturewillmerelyslowyoudown. Infact,itspeedsthingsup,sinceyoumuchmorequicklyfindanetworkwithameaningful\n",
            "signal. Onceyou’vegotsuchasignal,youcanoftengetrapidimprovementsbytweakingthe\n",
            "hyper-parameters. Aswithmanythingsinlife,gettingstartedcanbethehardestthingtodo. Okay,that’sthebroadstrategy. Let’snowlookatsomespecificrecommendationsfor\n",
            "settinghyper-parameters. Iwillfocusonthelearningrate,η,theL2regularizationparameter,\n",
            "λ, and the mini-batch size. However, many of the remarks apply also to other hyper-\n",
            "parameters,includingthoseassociatedtonetworkarchitecture,otherformsofregularization,\n",
            "andsomehyper-parameterswe’llmeetlaterinthebook,suchasthemomentumco-efficient. Learningrate: SupposewerunthreeMNISTnetworkswiththreedifferentlearningrates,\n",
            "η =0.025,η =0.25andη =2.5,respectively. We’llsettheotherhyper-parametersasfor\n",
            "theexperimentsinearliersections,runningover30epochs,withamini-batchsizeof10,\n",
            "andwithλ =5.0. We’llalsoreturntousingthefull50,000trainingimages. Here’sagraph\n",
            "showingthebehaviourofthetrainingcostaswetrain26:\n",
            "26Thegraphwasgeneratedbymultiple_eta.py. \n",
            "(cid:12)\n",
            "112 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "Withη =0.025thecostdecreasessmoothlyuntilthefinalepoch. Withη =0.25thecost\n",
            "initiallydecreases,butafterabout20epochsitisnearsaturation,andthereaftermostofthe\n",
            "changesaremerelysmallandapparentlyrandomoscillations. Finally,withη =2.5thecost\n",
            "makeslargeoscillationsrightfromthestart. Tounderstandthereasonfortheoscillations,\n",
            "recallthatstochasticgradientdescentissupposedtostepusgraduallydownintoavalleyof\n",
            "thecostfunction,\n",
            "3\n",
            "However,ifηistoolargethenthestepswillbesolargethattheymayactuallyovershootthe\n",
            "minimum,causingthealgorithmtoclimbupoutofthevalleyinstead. That’slikely27what’s\n",
            "causingthecosttooscillatewhenη =2.5. Whenwechooseη=0.25theinitialstepsdotake\n",
            "ustowardaminimumofthecostfunction,andit’sonlyoncewegetnearthatminimumthat\n",
            "westarttosufferfromtheovershootingproblem. Andwhenwechooseη =0.025wedon’t\n",
            "sufferfromthisproblematallduringthefirst30epochs. Ofcourse,choosingηsosmall\n",
            "createsanotherproblem,namely,thatitslowsdownstochasticgradientdescent. Aneven\n",
            "betterapproachwouldbetostartwithη =0.25,trainfor20epochs,andthenswitchto\n",
            "η =0.025. We’lldiscusssuchvariablelearningratescheduleslater. Fornow,though,let’s\n",
            "sticktofiguringouthowtofindasinglegoodvalueforthelearningrate,η. Withthispictureinmind, wecansetηasfollows. First, weestimatethethreshold\n",
            "valueforηatwhichthecostonthetrainingdataimmediatelybeginsdecreasing,insteadof\n",
            "oscillatingorincreasing. Thisestimatedoesn’tneedtobetooaccurate.\n",
            "Youcanestimate\n",
            "theorderofmagnitudebystartingwithη=0.01. Ifthecostdecreasesduringthefirstfew\n",
            "epochs,thenyoushouldsuccessivelytryη =0.1,1.0,...untilyoufindavalueforηwhere\n",
            "thecostoscillatesorincreasesduringthefirstfewepochs. Alternately,ifthecostoscillates\n",
            "orincreasesduringthefirstfewepochswhenη=0.01,thentryη=0.001,0.0001,...until\n",
            "youfindavalueforηwherethecostdecreasesduringthefirstfewepochs. Followingthis\n",
            "procedurewillgiveusanorderofmagnitudeestimateforthethresholdvalueofη. You\n",
            "mayoptionallyrefineyourestimate,topickoutthelargestvalueofηatwhichthecost\n",
            "decreasesduringthefirstfewepochs,sayη=0.5orη=0.2(there’snoneedforthistobe\n",
            "super-accurate). Thisgivesusanestimateforthethresholdvalueofη.\n",
            "Obviously,theactualvalueofηthatyouuseshouldbenolargerthanthethreshold\n",
            "value.\n",
            "Infact,ifthevalueofηistoremainusableovermanyepochsthenyoulikelywantto\n",
            "27Thispictureishelpful,butit’sintendedasanintuition-buildingillustrationofwhatmaygoon,not\n",
            "asacomplete,exhaustiveexplanation. Briefly,amorecompleteexplanationisasfollows: gradient\n",
            "descentusesafirst-orderapproximationtothecostfunctionasaguidetohowtodecreasethecost. Forlargeη,higher-ordertermsinthecostfunctionbecomemoreimportant,andmaydominatethe\n",
            "behaviour,causinggradientdescenttobreakdown.Thisisespeciallylikelyasweapproachminimaand\n",
            "quasi-minimaofthecostfunction,sincenearsuchpointsthegradientbecomessmall,makingiteasier\n",
            "forhigher-ordertermstodominatebehaviour. \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 113\n",
            "(cid:12)\n",
            "useavalueforηthatissmaller,say,afactoroftwobelowthethreshold.\n",
            "Suchachoicewill\n",
            "typicallyallowyoutotrainformanyepochs,withoutcausingtoomuchofaslowdownin\n",
            "learning. InthecaseoftheMNISTdata,followingthisstrategyleadstoanestimateof0.1forthe\n",
            "orderofmagnitudeofthethresholdvalueofη. Aftersomemorerefinement,weobtaina\n",
            "thresholdvalueη=0.5. Followingtheprescriptionabove,thissuggestsusingη=0.25asour\n",
            "valueforthelearningrate. Infact,Ifoundthatusingη=0.5workedwellenoughover30 3\n",
            "epochsthatforthemostpartIdidn’tworryaboutusingalowervalueofη. Thisallseemsquitestraightforward. However,usingthetrainingcosttopickηappears\n",
            "tocontradictwhatIsaidearlierinthissection,namely,thatwe’dpickhyper-parameters\n",
            "byevaluatingperformanceusingourheld-outvalidationdata. Infact,we’llusevalidation\n",
            "accuracy to pick the regularization hyper-parameter, the mini-batch size, and network\n",
            "parameterssuchasthenumberoflayersandhiddenneurons,andsoon. Whydothings\n",
            "differentlyforthelearningrate?\n",
            "Frankly,thischoiceismypersonalaestheticpreference,\n",
            "andisperhapssomewhatidiosyncratic. Thereasoningisthattheotherhyper-parameters\n",
            "areintendedtoimprovethefinalclassificationaccuracyonthetestset,andsoitmakes\n",
            "sensetoselectthemonthebasisofvalidationaccuracy. However,thelearningrateisonly\n",
            "incidentallymeanttoimpactthefinalclassificationaccuracy. Itsprimarypurposeisreallyto\n",
            "controlthestepsizeingradientdescent,andmonitoringthetrainingcostisthebestway\n",
            "todetectifthestepsizeistoobig. Withthatsaid,thisisapersonalaestheticpreference. Earlyonduringlearningthetrainingcostusuallyonlydecreasesifthevalidationaccuracy\n",
            "improves,andsoinpracticeit’sunlikelytomakemuchdifferencewhichcriterionyouuse. Useearly stoppingto determinethenumber oftraining epochs: Aswediscussed\n",
            "earlierinthechapter,earlystoppingmeansthatattheendofeachepochweshouldcompute\n",
            "theclassificationaccuracyonthevalidationdata. Whenthatstopsimproving,terminate.\n",
            "Thismakessettingthenumberofepochsverysimple. Inparticular,itmeansthatwedon’t\n",
            "needtoworryaboutexplicitlyfiguringouthowthenumberofepochsdependsontheother\n",
            "hyper-parameters. Instead,that’stakencareofautomatically. Furthermore,earlystopping\n",
            "alsoautomaticallypreventsusfromoverfitting. Thisis,ofcourse,agoodthing,althoughin\n",
            "theearlystagesofexperimentationitcanbehelpfultoturnoffearlystopping,soyoucan\n",
            "seeanysignsofoverfitting,anduseittoinformyourapproachtoregularization. To implement early stopping we need to say more precisely what it means that the\n",
            "classificationaccuracyhasstoppedimproving. Aswe’veseen,theaccuracycanjumparound\n",
            "quiteabit,evenwhentheoveralltrendistoimprove.\n",
            "Ifwestopthefirsttimetheaccuracy\n",
            "decreasesthenwe’llalmostcertainlystopwhentherearemoreimprovementstobehad. A\n",
            "betterruleistoterminateifthebestclassificationaccuracydoesn’timproveforquitesome\n",
            "time. Suppose,forexample,thatwe’redoingMNIST.Thenwemightelecttoterminateifthe\n",
            "classificationaccuracyhasn’timprovedduringthelasttenepochs. Thisensuresthatwedon’t\n",
            "stoptoosoon,inresponsetobadluckintraining,butalsothatwe’renotwaitingaround\n",
            "foreverforanimprovementthatnevercomes. This no-improvement-in-ten rule is good for initial exploration of MNIST. However,\n",
            "networkscansometimesplateaunearaparticularclassificationaccuracyforquitesometime,\n",
            "onlytothenbeginimprovingagain. Ifyou’retryingtogetreallygoodperformance,theno-\n",
            "improvement-in-tenrulemaybetooaggressiveaboutstopping. Inthatcase,Isuggestusing\n",
            "theno-improvement-in-tenruleforinitialexperimentation,andgraduallyadoptingmore\n",
            "lenientrules,asyoubetterunderstandthewayyournetworktrains: no-improvement-in-\n",
            "twenty,no-improvement-in-fifty,andsoon. Ofcourse,thisintroducesanewhyper-parameter\n",
            "\n",
            "(cid:12)\n",
            "114 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "tooptimize! Inpractice,however,it’susuallyeasytosetthishyper-parametertogetpretty\n",
            "goodresults. Similarly,forproblemsotherthanMNIST,theno-improvement-in-tenrule\n",
            "maybemuchtooaggressiveornotnearlyaggressiveenough,dependingonthedetailsof\n",
            "theproblem. However,withalittleexperimentationit’susuallyeasytofindaprettygood\n",
            "strategyforearlystopping. Wehaven’tusedearlystoppinginourMNISTexperimentstodate. Thereasonisthat\n",
            "3 we’vebeendoingalotofcomparisonsbetweendifferentapproachestolearning. Forsuch\n",
            "comparisonsit’shelpfultousethesamenumberofepochsineachcase. However,it’swell\n",
            "worthmodifyingnetwork2.pytoimplementearlystopping:\n",
            "Problem\n",
            "Modifynetwork2.pysothatitimplementsearlystoppingusingano-improvement-\n",
            "• in-nepochsstrategy,wherenisaparameterthatcanbeset. Canyouthinkofaruleforearlystoppingotherthanno-improvement-in-n? Ideally,the\n",
            "• ruleshouldcompromisebetweengettinghighvalidationaccuraciesandnottraining\n",
            "toolong. Addyourruletonetwork2.py,andrunthreeexperimentscomparingthe\n",
            "validationaccuraciesandnumberofepochsoftrainingtono-improvement-in-10. Learningrateschedule: We’vebeenholdingthelearningrateηconstant. However,it’s\n",
            "oftenadvantageoustovarythelearningrate. Earlyonduringthelearningprocessit’slikely\n",
            "thattheweightsarebadlywrong. Andsoit’sbesttousealargelearningratethatcauses\n",
            "theweightstochangequickly. Later, wecanreducethelearningrateaswemakemore\n",
            "fine-tunedadjustmentstoourweights. Howshouldwesetourlearningrateschedule?\n",
            "Manyapproachesarepossible. One\n",
            "naturalapproachistousethesamebasicideaasearlystopping. Theideaistoholdthe\n",
            "learningrateconstantuntilthevalidationaccuracystartstogetworse. Thendecreasethe\n",
            "learningratebysomeamount,sayafactoroftwoorten. Werepeatthismanytimes,until,\n",
            "say,thelearningrateisafactorof1,024(or1,000)timeslowerthantheinitialvalue. Then\n",
            "weterminate. Avariablelearningschedulecanimproveperformance,butitalsoopensupaworldof\n",
            "possiblechoicesforthelearningschedule. Thosechoicescanbeaheadache–youcanspend\n",
            "forevertryingtooptimizeyourlearningschedule. Forfirstexperimentsmysuggestionisto\n",
            "useasingle,constantvalueforthelearningrate.\n",
            "That’llgetyouagoodfirstapproximation.\n",
            "Later,ifyouwanttoobtainthebestperformancefromyournetwork,it’sworthexperimenting\n",
            "withalearningschedule,alongthelinesI’vedescribed28. Exercise\n",
            "Modify network2.py so that it implements a learning schedule that: halves the\n",
            "• learningrateeachtimethevalidationaccuracysatisfiestheno-improvement-in-10\n",
            "rule;andterminateswhenthelearningratehasdroppedto1/128ofitsoriginalvalue. Theregularizationparameter,λ: Isuggeststartinginitiallywithnoregularization(λ\n",
            "=\n",
            "0.0),anddeterminingavalueforη,asabove.\n",
            "Usingthatchoiceofη,wecanthenusethe\n",
            "validationdatatoselectagoodvalueforλ. Startbytriallingλ =1.029,andthenincreaseor\n",
            "decreasebyfactorsof10,asneededtoimproveperformanceonthevalidationdata. Once\n",
            "you’vefoundagoodorderofmagnitude,youcanfinetuneyourvalueofλ. Thatdone,you\n",
            "shouldreturnandre-optimizeηagain. 28Areadablerecentpaperwhichdemonstratesthebenefitsofvariablelearningratesinattacking\n",
            "MNISTisDeep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCire¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJürgenSchmidhuber(2010). 29Idon’thaveagoodprincipledjustificationforusingthisasastartingvalue.Ifanyoneknowsofa\n",
            "goodprincipleddiscussionofwheretostartwithλ,I’dappreciatehearingit(mn@michaelnielsen.org). \n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 115\n",
            "(cid:12)\n",
            "Exercise\n",
            "It’stemptingtousegradientdescenttotrytolearngoodvaluesforhyper-parameters\n",
            "• suchasλandη. Canyouthinkofanobstacletousinggradientdescenttodetermine\n",
            "λ? Canyouthinkofanobstacletousinggradientdescenttodetermineη? HowIselectedhyper-parametersearlierinthisbook: Ifyouusetherecommendations\n",
            "inthissectionyou’llfindthatyougetvaluesforηandλwhichdon’talwaysexactlymatch 3\n",
            "thevaluesI’veusedearlierinthebook. Thereasonisthatthebookhasnarrativeconstraints\n",
            "thathavesometimesmadeitimpracticaltooptimizethehyper-parameters. Thinkofallthe\n",
            "comparisonswe’vemadeofdifferentapproachestolearning,e.g.,comparingthequadratic\n",
            "andcross-entropycostfunctions,comparingtheoldandnewmethodsofweightinitialization,\n",
            "runningwithandwithoutregularization,andsoon. Tomakesuchcomparisonsmeaningful,\n",
            "I’veusuallytriedtokeephyper-parametersconstantacrosstheapproachesbeingcompared\n",
            "(ortoscaletheminanappropriateway). Ofcourse,there’snoreasonforthesamehyper-\n",
            "parameterstobeoptimalforallthedifferentapproachestolearning,sothehyper-parameters\n",
            "I’veusedaresomethingofacompromise. Asanalternativetothiscompromise,Icouldhavetriedtooptimizetheheckoutofthe\n",
            "hyper-parametersforeverysingleapproachtolearning. Inprinciplethat’dbeabetter,fairer\n",
            "approach,sincethenwe’dseethebestfromeveryapproachtolearning. However,we’ve\n",
            "madedozensofcomparisonsalongtheselines,andinpracticeIfoundittoocomputationally\n",
            "expensive. That’swhyI’veadoptedthecompromiseofusingprettygood(butnotnecessarily\n",
            "optimal)choicesforthehyper-parameters. Mini-batchsize: Howshouldwesetthemini-batchsize? Toanswerthisquestion,let’s\n",
            "firstsupposethatwe’redoingonlinelearning,i.e.,thatwe’reusingamini-batchsizeof1. Theobviousworryaboutonlinelearningisthatusingmini-batcheswhichcontainjust\n",
            "asingletrainingexamplewillcausesignificanterrorsinourestimateofthegradient. In\n",
            "fact,though,theerrorsturnouttonotbesuchaproblem. Thereasonisthattheindividual\n",
            "gradientestimatesdon’tneedtobesuper-accurate. Allweneedisanestimateaccurate\n",
            "enoughthatourcostfunctiontendstokeepdecreasing. It’sasthoughyouaretryingtoget\n",
            "totheNorthMagneticPole,buthaveawonkycompassthat’s10–20degreesoffeachtime\n",
            "youlookatit. Providedyoustoptocheckthecompassfrequently,andthecompassgetsthe\n",
            "directionrightonaverage,you’llendupattheNorthMagneticPolejustfine. Basedonthisargument,itsoundsasthoughweshoulduseonlinelearning.\n",
            "Infact,the\n",
            "situationturnsouttobemorecomplicatedthanthat.\n",
            "InaprobleminthelastchapterI\n",
            "pointedoutthatit’spossibletousematrixtechniquestocomputethegradientupdatefor\n",
            "allexamplesinamini-batchsimultaneously,ratherthanloopingoverthem. Dependingon\n",
            "thedetailsofyourhardwareandlinearalgebralibrarythiscanmakeitquiteabitfaster\n",
            "tocomputethegradientestimateforamini-batchof(forexample)size100,ratherthan\n",
            "computing the mini-batch gradient estimate by looping over the 100 training examples\n",
            "separately. Itmighttake(say)only50timesaslong,ratherthan100timesaslong.\n",
            "Now,atfirstitseemsasthoughthisdoesn’thelpusthatmuch. Withourmini-batchof\n",
            "size100thelearningrulefortheweightslookslike:\n",
            "1 (cid:88)\n",
            "w\n",
            "→\n",
            "w (cid:48)=w\n",
            "−\n",
            "η\n",
            "100 x ∇\n",
            "C\n",
            "x\n",
            ", (3.45)\n",
            "\n",
            "(cid:12)\n",
            "116 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "wherethesumisovertrainingexamplesinthemini-batch. Thisisversus\n",
            "w w (cid:48)=w η C\n",
            "x\n",
            "(3.46)\n",
            "→ − ∇\n",
            "foronlinelearning. Evenifitonlytakes50timesaslongtodothemini-batchupdate,it\n",
            "stillseemslikelytobebettertodoonlinelearning,becausewe’dbeupdatingsomuchmore\n",
            "3 frequently. Suppose,however,thatinthemini-batchcaseweincreasethelearningratebya\n",
            "factor100,sotheupdaterulebecomes\n",
            "(cid:88)\n",
            "w w (cid:48)=w η C\n",
            "x\n",
            ". (3.47)\n",
            "→ − x ∇\n",
            "That’salotlikedoing100separateinstancesofonlinelearningwithalearningrateofη. Butitonlytakes50timesaslongasdoingasingleinstanceofonlinelearning.\n",
            "Ofcourse,it’s\n",
            "nottrulythesameas100instancesofonlinelearning,sinceinthemini-batchthe C ’sare\n",
            "x\n",
            "allevaluatedforthesamesetofweights,asopposedtothecumulativelearningth∇atoccurs\n",
            "intheonlinecase. Still,itseemsdistinctlypossiblethatusingthelargermini-batchwould\n",
            "speedthingsup. Withthesefactorsinmind,choosingthebestmini-batchsizeisacompromise. Toosmall,\n",
            "andyoudon’tgettotakefulladvantageofthebenefitsofgoodmatrixlibrariesoptimizedfor\n",
            "fasthardware.\n",
            "Toolargeandyou’resimplynotupdatingyourweightsoftenenough. What\n",
            "youneedistochooseacompromisevaluewhichmaximizesthespeedoflearning.Fortunately,\n",
            "thechoiceofmini-batchsizeatwhichthespeedismaximizedisrelativelyindependent\n",
            "oftheotherhyper-parameters(apartfromtheoverallarchitecture),soyoudon’tneedto\n",
            "haveoptimizedthosehyper-parametersinordertofindagoodmini-batchsize. Theway\n",
            "togoisthereforetousesomeacceptable(butnotnecessarilyoptimal)valuesfortheother\n",
            "hyper-parameters,andthentrialanumberofdifferentmini-batchsizes,scalingηasabove. Plotthevalidationaccuracyversustime(asin,realelapsedtime,notepoch!),andchoose\n",
            "whichevermini-batchsizegivesyouthemostrapidimprovementinperformance. Withthe\n",
            "mini-batchsizechosenyoucanthenproceedtooptimizetheotherhyper-parameters. Ofcourse,asyou’venodoubtrealized,Ihaven’tdonethisoptimizationinourwork. Indeed,ourimplementationdoesn’tusethefasterapproachtomini-batchupdatesatall. I’ve\n",
            "simplyusedamini-batchsizeof10withoutcommentorexplanationinnearlyallexamples. Becauseofthis,wecouldhavespeduplearningbyreducingthemini-batchsize. Ihaven’t\n",
            "donethis,inpartbecauseIwantedtoillustratetheuseofmini-batchesbeyondsize1,andin\n",
            "partbecausemypreliminaryexperimentssuggestedthespeedupwouldberathermodest. In\n",
            "practicalimplementations,however,wewouldmostcertainlyimplementthefasterapproach\n",
            "tomini-batchupdates,andthenmakeanefforttooptimizethemini-batchsize,inorderto\n",
            "maximizeouroverallspeed. Automatedtechniques: I’vebeendescribingtheseheuristicsasthoughyou’reoptimiz-\n",
            "ingyourhyper-parametersbyhand. Hand-optimizationisagoodwaytobuildupafeelfor\n",
            "howneuralnetworksbehave. However,andunsurprisingly,agreatdealofworkhasbeen\n",
            "doneonautomatingtheprocess. Acommontechniqueisgridsearch,whichsystematically\n",
            "searchesthroughagridinhyper-parameterspace. Areviewofboththeachievementsand\n",
            "thelimitationsofgridsearch(withsuggestionsforeasily-implementedalternatives)maybe\n",
            "foundina2012paper30byJamesBergstraandYoshuaBengio. Manymoresophisticated\n",
            "30Randomsearchforhyper-parameteroptimization,byJamesBergstraandYoshuaBengio(2012).\n",
            "\n",
            "(cid:12)\n",
            "3.5. Howtochooseaneuralnetwork’shyper-parameters? (cid:12) 117\n",
            "(cid:12)\n",
            "approacheshavealsobeenproposed. Iwon’treviewallthatworkhere,butdowanttomen-\n",
            "tionaparticularlypromising2012paperwhichusedaBayesianapproachtoautomatically\n",
            "optimizehyper-parameters31. Thecodefromthepaperispubliclyavailable,andhasbeen\n",
            "usedwithsomesuccessbyotherresearchers. Summingup: Followingtherules-of-thumbI’vedescribedwon’tgiveyoutheabsolute\n",
            "bestpossibleresultsfromyourneuralnetwork. Butitwilllikelygiveyouagoodstartanda\n",
            "basisforfurtherimprovements. Inparticular,I’vediscussedthehyper-parameterslargely 3\n",
            "independently. Inpractice,therearerelationshipsbetweenthehyper-parameters. Youmay\n",
            "experimentwithη,feelthatyou’vegotitjustright,thenstarttooptimizeforλ,onlytofind\n",
            "thatit’smessingupyouroptimizationforη. Inpractice,ithelpstobouncebackwardand\n",
            "forward,graduallyclosingingoodvalues. Aboveall,keepinmindthattheheuristicsI’ve\n",
            "describedarerulesofthumb,notrulescastinstone. Youshouldbeonthelookoutforsigns\n",
            "thatthingsaren’tworking,andbewillingtoexperiment. Inparticular,thismeanscarefully\n",
            "monitoringyournetwork’sbehaviour,especiallythevalidationaccuracy. The difficulty of choosing hyper-parameters is exacerbated by the fact that the lore\n",
            "abouthowtochoosehyper-parametersiswidelyspread,acrossmanyresearchpapersand\n",
            "softwareprograms,andoftenisonlyavailableinsidetheheadsofindividualpractitioners. Therearemany,manypaperssettingout(sometimescontradictory)recommendationsfor\n",
            "howtoproceed.\n",
            "However,thereareafewparticularlyusefulpapersthatsynthesizeand\n",
            "distilloutmuchofthislore. YoshuaBengiohasa2012paper32 thatgivessomepractical\n",
            "recommendationsforusingbackpropagationandgradientdescenttotrainneuralnetworks,\n",
            "includingdeepneuralnets. BengiodiscussesmanyissuesinmuchmoredetailthanIhave,\n",
            "includinghowtodomoresystematichyper-parametersearches.\n",
            "Anothergoodpaperisa\n",
            "1998paper33 byYannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller.\n",
            "Both\n",
            "thesepapersappearinanextremelyuseful2012bookthatcollectsmanytrickscommonly\n",
            "usedinneuralnets34. Thebookisexpensive,butmanyofthearticleshavebeenplaced\n",
            "onlinebytheirrespectiveauthorswith,onepresumes,theblessingofthepublisher,andmay\n",
            "belocatedusingasearchengine. Onethingthatbecomesclearasyoureadthesearticlesand,especially,asyouengagein\n",
            "yourownexperiments,isthathyper-parameteroptimizationisnotaproblemthatisever\n",
            "completelysolved. There’salwaysanothertrickyoucantrytoimproveperformance. There\n",
            "isasayingcommonamongwritersthatbooksareneverfinished,onlyabandoned. Thesame\n",
            "isalsotrueofneuralnetworkoptimization: thespaceofhyper-parametersissolargethat\n",
            "oneneverreallyfinishesoptimizing,oneonlyabandonsthenetworktoposterity. Soyour\n",
            "goalshouldbetodevelopaworkflowthatenablesyoutoquicklydoaprettygoodjobon\n",
            "theoptimization,whileleavingyoutheflexibilitytotrymoredetailedoptimizations,ifthat’s\n",
            "important. Thechallengeofsettinghyper-parametershasledsomepeopletocomplainthatneural\n",
            "networksrequirealotofworkwhencomparedwithothermachinelearningtechniques. I’ve\n",
            "heardmanyvariationsonthefollowingcomplaint: “Yes,awell-tunedneuralnetworkmay\n",
            "getthebestperformanceontheproblem. Ontheotherhand,Icantryarandomforest[or\n",
            "31PracticalBayesianoptimizationofmachinelearningalgorithms,byJasperSnoek,HugoLarochelle,\n",
            "andRyanAdams. 32Practicalrecommendationsforgradient-basedtrainingofdeeparchitectures,byYoshuaBengio\n",
            "(2012). 33EfficientBackProp,byYannLeCun,LÃl’onBottou,GenevieveOrrandKlaus-RobertMüller(1998)\n",
            "34NeuralNetworks:TricksoftheTrade,editedbyGrégoireMontavon,GenevièveOrr,andKlaus-Robert\n",
            "Müller.\n",
            "\n",
            "(cid:12)\n",
            "118 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "SVMor... insertyourownfavoritetechnique]anditjustworks.\n",
            "Idon’thavetimetofigure\n",
            "outjusttherightneuralnetwork.” Ofcourse,fromapracticalpointofviewit’sgoodto\n",
            "haveeasy-to-applytechniques. Thisisparticularlytruewhenyou’rejustgettingstartedona\n",
            "problem,anditmaynotbeobviouswhethermachinelearningcanhelpsolvetheproblemat\n",
            "all. Ontheotherhand,ifgettingoptimalperformanceisimportant,thenyoumayneedto\n",
            "tryapproachesthatrequiremorespecialistknowledge. Whileitwouldbeniceifmachine\n",
            "3 learningwerealwayseasy,thereisnoapriorireasonitshouldbetriviallysimple. 3.6 Other techniques\n",
            "Eachtechniquedevelopedinthischapterisvaluabletoknowinitsownright,butthat’s\n",
            "nottheonlyreasonI’veexplainedthem. Thelargerpointistofamiliarizeyouwithsomeof\n",
            "theproblemswhichcanoccurinneuralnetworks,andwithastyleofanalysiswhichcan\n",
            "helpovercomethoseproblems. Inasense,we’vebeenlearninghowtothinkaboutneural\n",
            "nets. OvertheremainderofthischapterIbrieflysketchahandfulofothertechniques.\n",
            "These\n",
            "sketchesarelessin-depththantheearlierdiscussions,butshouldconveysomefeelingfor\n",
            "thediversityoftechniquesavailableforuseinneuralnetworks. 3.6.1 Variationsonstochasticgradientdescent\n",
            "StochasticgradientdescentbybackpropagationhasserveduswellinattackingtheMNIST\n",
            "digitclassificationproblem. However,therearemanyotherapproachestooptimizingthecost\n",
            "function,andsometimesthoseotherapproachesofferperformancesuperiortomini-batch\n",
            "stochasticgradientdescent. InthissectionIsketchtwosuchapproaches,theHessianand\n",
            "momentumtechniques. Hessiantechnique: Tobeginourdiscussionithelpstoputneuralnetworksasidefora\n",
            "bit. Instead,we’rejustgoingtoconsidertheabstractproblemofminimizingacostfunction\n",
            "C whichisafunctionofmanyvariables,w=w\n",
            "1\n",
            ",w\n",
            "2\n",
            ",...,soC=C(w). ByTaylor’stheorem,\n",
            "thecostfunctioncanbeapproximatednearapointwby\n",
            "(cid:88) ∂C 1(cid:88) ∂2C\n",
            "C(w+ ∆w)=C(w)+ ∂w ∆w j+ 2 ∆w j∂w ∂w ∆w k+... (3.48)\n",
            "j j jk j k\n",
            "Wecanrewritethismorecompactlyas\n",
            "1\n",
            "C(w+ ∆w)=C(w)+ C ∆w+ ∆wTH∆w+..., (3.49)\n",
            "∇ · 2\n",
            "where Cistheusualgradientvector,andHisamatrixknownastheHessianmatrix,whose\n",
            "jk-the∇ntryis∂2C/∂w ∂w . Supposeweapproximate C bydiscardingthehigher-order\n",
            "j k\n",
            "termsrepresentedby...above,\n",
            "1\n",
            "C(w+ ∆w) C(w)+ C ∆w+ ∆wTH∆w. (3.50)\n",
            "≈ ∇ · 2\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 119\n",
            "(cid:12)\n",
            "Usingcalculuswecanshowthattheexpressionontheright-handsidecanbeminimized35\n",
            "bychoosing\n",
            "∆w= H\n",
            "−\n",
            "1 C. (3.51)\n",
            "− ∇\n",
            "Provided(3.50)isagoodapproximateexpressionforthecostfunction,thenwe’dexpect\n",
            "thatmovingfromthepointwtow+ ∆w=w H\n",
            "−\n",
            "1 C shouldsignificantlydecreasethecost 3\n",
            "function. Thatsuggestsapossiblealgorithm−formi∇nimizingthecost:\n",
            "Chooseastartingpoint,w. • Updatewtoanewpointw (cid:48)=w H\n",
            "−\n",
            "1 C,wheretheHessianHand Carecomputed\n",
            "• atw. − ∇ ∇\n",
            "Update w\n",
            "(cid:48)\n",
            "toanewpoint w\n",
            "(cid:48)(cid:48)\n",
            "=w\n",
            "(cid:48)\n",
            "H\n",
            "(cid:48)−\n",
            "1\n",
            "(cid:48)\n",
            "C,wheretheHessian H\n",
            "(cid:48)\n",
            "and\n",
            "(cid:48)\n",
            "C are\n",
            "• computedatw. − ∇ ∇\n",
            "(cid:48)\n",
            "... •\n",
            "Inpractice,(3.50)isonlyanapproximation,andit’sbettertotakesmallersteps. Wedothis\n",
            "byrepeatedlychangingwbyanamount∆w= ηH\n",
            "−\n",
            "1 C,whereηisknownasthelearning\n",
            "rate. − ∇\n",
            "ThisapproachtominimizingacostfunctionisknownastheHessiantechniqueorHessian\n",
            "optimization. TherearetheoreticalandempiricalresultsshowingthatHessianmethods\n",
            "convergeonaminimuminfewerstepsthanstandardgradientdescent. Inparticular,by\n",
            "incorporating information about second-order changes in the cost function it’s possible\n",
            "for the Hessian approach to avoid many pathologies that can occur in gradient descent. Furthermore,thereareversionsofthebackpropagationalgorithmwhichcanbeusedto\n",
            "computetheHessian. If Hessian optimization is so great, why aren’t we using it in our neural networks? Unfortunately,whileithasmanydesirableproperties,ithasoneveryundesirableproperty:\n",
            "it’sverydifficulttoapplyinpractice.PartoftheproblemisthesheersizeoftheHessianmatrix. Supposeyouhaveaneuralnetworkwith107weightsandbiases. Thenthecorresponding\n",
            "Hessianmatrixwillcontain107 107 =1014entries. That’salotofentries!\n",
            "Andthatmakes\n",
            "computingH 1 C extremelyd×ifficultinpractice.\n",
            "However,thatdoesn’tmeanthatit’snot\n",
            "−\n",
            "useful to under∇stand. In fact, there are many variations on gradient descent which are\n",
            "inspiredbyHessianoptimization,butwhichavoidtheproblemwithoverly-largematrices. Let’stakealookatonesuchtechnique,momentum-basedgradientdescent. Momentum-basedgradientdescent: Intuitively,theadvantageHessianoptimization\n",
            "hasisthatitincorporatesnotjustinformationaboutthegradient,butalsoinformationabout\n",
            "how the gradient is changing. Momentum-based gradient descent is based on a similar\n",
            "intuition,butavoidslargematricesofsecondderivatives. Tounderstandthemomentum\n",
            "technique,thinkbacktoouroriginalpictureofgradientdescent1.5,inwhichweconsidered\n",
            "aballrollingdownintoavalley. Atthetime,weobservedthatgradientdescentis,despite\n",
            "itsname,onlylooselysimilartoaballfallingtothebottomofavalley. Themomentum\n",
            "techniquemodifiesgradientdescentintwowaysthatmakeitmoresimilartothephysical\n",
            "picture. First,itintroducesanotionof“velocity”fortheparameterswe’retryingtooptimize. Thegradientactstochangethevelocity,not(directly)the“position”,inmuchthesame\n",
            "wayasphysicalforceschangethevelocity,andonlyindirectlyaffectposition. Second,the\n",
            "35Strictlyspeaking,forthistobeaminimum,andnotmerelyanextremum,weneedtoassumethat\n",
            "theHessianmatrixispositivedefinite. Intuitively,thismeansthatthefunctionC lookslikeavalley\n",
            "locally,notamountainorasaddle.\n",
            "\n",
            "(cid:12)\n",
            "120 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "momentummethodintroducesakindoffrictionterm,whichtendstograduallyreducethe\n",
            "velocity. Let’s give a more precise mathematical description. We introduce velocity variables\n",
            "v = v 1 ,v 2 ,..., one for each corresponding w j variable36. Then we replace the gradient\n",
            "descentupdaterulew w (cid:48)=w η C by\n",
            "→ − ∇\n",
            "3 v v (cid:48)= µv η C (3.52)\n",
            "→ − ∇\n",
            "w w (cid:48)=w+v\n",
            "(cid:48)\n",
            ". (3.53)\n",
            "→\n",
            "Intheseequations,µisahyper-parameterwhichcontrolstheamountofdampingorfriction\n",
            "inthesystem.\n",
            "Tounderstandthemeaningoftheequationsit’shelpfultofirstconsiderthe\n",
            "casewhereµ =1,whichcorrespondstonofriction. Whenthat’sthecase,inspectionof\n",
            "theequationsshowsthatthe“force” C isnowmodifyingthevelocity,v,andthevelocity\n",
            "iscontrollingtherateofchangeofw∇. Intuitively,webuildupthevelocitybyrepeatedly\n",
            "addinggradienttermstoit. Thatmeansthatifthegradientisin(roughly)thesamedirection\n",
            "throughseveralroundsoflearning,wecanbuildupquiteabitofsteammovinginthat\n",
            "direction. Think,forexample,ofwhathappensifwe’removingstraightdownaslope:\n",
            "Witheachstepthevelocitygetslargerdowntheslope,sowemovemoreandmorequickly\n",
            "tothebottomofthevalley. Thiscanenablethemomentumtechniquetoworkmuchfaster\n",
            "thanstandardgradientdescent. Ofcourse,aproblemisthatoncewereachthebottomof\n",
            "thevalleywewillovershoot. Or,ifthegradientshouldchangerapidly,thenwecouldfind\n",
            "ourselvesmovinginthewrongdirection. That’sthereasonfortheµhyper-parameterin\n",
            "(3.52). Isaidearlierthatµcontrolstheamountoffrictioninthesystem;tobealittlemore\n",
            "precise,youshouldthinkof1 µastheamountoffrictioninthesystem. Whenµ =1,as\n",
            "we’veseen,thereisnofriction−,andthevelocityiscompletelydrivenbythegradient C. Bycontrast,whenµ =0there’salotoffriction,thevelocitycan’tbuildup,andEquati∇ons\n",
            "(3.52)and(3.53)reducetotheusualequationforgradientdescent,w w (cid:48)=w η C. In\n",
            "practice,usingavalueofµintermediatebetween0and1cangiveusm→uchofthe−be∇nefitof\n",
            "beingabletobuildupspeed,butwithoutcausingovershooting. Wecanchoosesuchavalue\n",
            "forµusingtheheld-outvalidationdata,inmuchthesamewayasweselectηandλ.\n",
            "I’veavoidednamingthehyper-parameterµuptonow. Thereasonisthatthestandard\n",
            "name for µ is badly chosen: it’s called the momentum co-efficient. This is potentially\n",
            "36Inaneuralnetthew variableswould,ofcourse,includeallweightsandbiases.\n",
            "j\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 121\n",
            "(cid:12)\n",
            "confusing,sinceµisnotatallthesameasthenotionofmomentumfromphysics. Rather,it\n",
            "ismuchmorecloselyrelatedtofriction.\n",
            "However,thetermmomentumco-efficientiswidely\n",
            "used,sowewillcontinuetouseit. Anicethingaboutthemomentumtechniqueisthatittakesalmostnoworktomod-\n",
            "ify an implementation of gradient descent to incorporate momentum. We can still use\n",
            "backpropagationtocomputethegradients,justasbefore,anduseideassuchassampling\n",
            "stochasticallychosenmini-batches. Inthisway,wecangetsomeoftheadvantagesofthe 3\n",
            "Hessiantechnique,usinginformationabouthowthegradientischanging. Butit’sdone\n",
            "withoutthedisadvantages,andwithonlyminormodificationstoourcode. Inpractice,the\n",
            "momentumtechniqueiscommonlyused,andoftenspeedsuplearning. Exercise\n",
            "Whatwouldgowrongifweusedµ>1inthemomentumtechnique?\n",
            "• Whatwouldgowrongifweusedµ<0inthemomentumtechnique? •\n",
            "Problem\n",
            "Addmomentum-basedstochasticgradientdescenttonetwork2.py. •\n",
            "Otherapproachestominimizingthecostfunction: Manyotherapproachestominimizingthe\n",
            "costfunctionhavebeendeveloped,andthereisn’tuniversalagreementonwhichisthebest\n",
            "approach. Asyougodeeperintoneuralnetworksit’sworthdiggingintotheothertechniques,\n",
            "understandinghowtheywork,theirstrengthsandweaknesses,andhowtoapplythemin\n",
            "practice. ApaperImentionedearlier37introducesandcomparesseveralofthesetechniques,\n",
            "includingconjugategradientdescentandtheBFGSmethod(seealsothecloselyrelated\n",
            "limited-memoryBFGSmethod,knownasL-BFGS).Anothertechniquewhichhasrecently\n",
            "shownpromisingresults38isNesterov’sacceleratedgradienttechnique,whichimproveson\n",
            "themomentumtechnique. However,formanyproblems,plainstochasticgradientdescent\n",
            "workswell,especiallyifmomentumisused,andsowe’llsticktostochasticgradientdescent\n",
            "throughtheremainderofthisbook. Othermodelsofartificialneuron\n",
            "Uptonowwe’vebuiltourneuralnetworksusingsigmoidneurons. Inprinciple,anetwork\n",
            "builtfromsigmoidneuronscancomputeanyfunction. Inpractice,however,networksbuilt\n",
            "usingothermodelneuronssometimesoutperformsigmoidnetworks. Dependingonthe\n",
            "application,networksbasedonsuchalternatemodelsmaylearnfaster,generalizebetterto\n",
            "testdata,orperhapsdoboth. Letmementionacoupleofalternatemodelneurons,togive\n",
            "youtheflavorofsomevariationsincommonuse. Perhapsthesimplestvariationisthetanh(pronounced“tanch”)neuron,whichreplaces\n",
            "thesigmoidfunctionbythehyperbolictangentfunction. Theoutputofatanhneuronwith\n",
            "input x,weightvectorw,andbias bisgivenby\n",
            "tanh(w x+b), (3.54)\n",
            "·\n",
            "wheretanhis,ofcourse,thehyperbolictangentfunction. Itturnsoutthatthisisveryclosely\n",
            "37EfficientBackProp,byYannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller(1998).\n",
            "38See,forexample,Ontheimportanceofinitializationandmomentumindeeplearning,byIlya\n",
            "Sutskever,JamesMartens,GeorgeDahl,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "122 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "relatedtothesigmoidneuron. Toseethis,recallthatthetanhfunctionisdefinedby\n",
            "ez e z\n",
            "tanh(z)\n",
            "−\n",
            "− . (3.55)\n",
            "≡\n",
            "ez+e z\n",
            "−\n",
            "Withalittlealgebraitcaneasilybeverifiedthat\n",
            "3\n",
            "σ (z)=\n",
            "1+tanh(z/2)\n",
            ", (3.56)\n",
            "2\n",
            "thatis,tanhisjustarescaledversionofthesigmoidfunction. Wecanalsoseegraphically\n",
            "thatthetanhfunctionhasthesameshapeasthesigmoidfunction,\n",
            "tanhfunction\n",
            "1\n",
            "0.5\n",
            "4 2 2 4\n",
            "− −\n",
            "0.5\n",
            "−\n",
            "1\n",
            "−\n",
            "Onedifferencebetweentanhneuronsandsigmoidneuronsisthattheoutputfromtanh\n",
            "neuronsrangesfrom 1to1,not0to1. Thismeansthatifyou’regoingtobuildanetwork\n",
            "basedontanhneuron−syoumayneedtonormalizeyouroutputs(and,dependingonthe\n",
            "detailsoftheapplication,possiblyyourinputs)alittledifferentlythaninsigmoidnetworks. Similartosigmoidneurons,anetworkoftanhneuronscan,inprinciple,computeany\n",
            "function39mappinginputstotherange 1to1. Furthermore,ideassuchasbackpropagation\n",
            "andstochasticgradientdescentareas−easilyappliedtoanetworkoftanhneuronsastoa\n",
            "networkofsigmoidneurons. Exercise\n",
            "ProvetheidentityinEquation(3.56). Whic•htypeofneuronshouldyouuseinyournetworks,thetanhorsigmoid?\n",
            "Apriorithe\n",
            "answerisnotobvious,toputitmildly! However,therearetheoreticalargumentsandsome\n",
            "empiricalevidencetosuggestthatthetanhsometimesperformsbetter40. Letmebriefly\n",
            "giveyoutheflavorofoneofthetheoreticalargumentsfortanhneurons. Supposewe’re\n",
            "usingsigmoidneurons, soallactivationsinournetworkarepositive. Let’sconsiderthe\n",
            "weightswl\n",
            "j\n",
            "+\n",
            "k\n",
            "1inputtothe j-thneuroninthe(l+1)-thlayer. Therulesforbackpropagation\n",
            "tellusthattheassociatedgradientwillbealδl+1. Becausetheactivationsarepositivethe\n",
            "k j\n",
            "39Therearesometechnicalcaveatstothisstatementforbothtanhandsigmoidneurons,aswellas\n",
            "fortherectifiedlinearneuronsdiscussedbelow.However,informallyit’susuallyfinetothinkofneural\n",
            "networksasbeingabletoapproximateanyfunctiontoarbitraryaccuracy. 40See,forexample,EfficientBackProp,byYannLeCun,LéonBottou,GenevieveOrrandKlaus-Robert\n",
            "Müller(1998),andUnderstandingthedifficultyoftrainingdeepfeedforwardnetworks,byXavierGlorot\n",
            "andYoshuaBengio(2010).\n",
            "\n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 123\n",
            "(cid:12)\n",
            "signofthisgradientwillbethesameasthesignofδl+1. Whatthismeansisthatifδl+1\n",
            "j j\n",
            "ispositivethenalltheweightswl+1willdecreaseduringgradientdescent,whileifδl+1is\n",
            "jk j\n",
            "negativethenalltheweightswl+1 willincreaseduringgradientdescent. Inotherwords,\n",
            "jk\n",
            "allweightstothesameneuronmusteitherincreasetogetherordecreasetogether. That’s\n",
            "aproblem,sincesomeoftheweightsmayneedtoincreasewhileothersneedtodecrease. Thatcanonlyhappenifsomeoftheinputactivationshavedifferentsigns. Thatsuggests\n",
            "replacingthesigmoidbyanactivationfunction,suchastanh,whichallowsbothpositiveand 3\n",
            "negativeactivations. Indeed,becausetanhissymmetricaboutzero,tanh( z)= tanh(z),\n",
            "wemightevenexpectthat, roughlyspeaking, theactivationsinhiddenl−ayers−wouldbe\n",
            "equallybalancedbetweenpositiveandnegative. Thatwouldhelpensurethatthereisno\n",
            "systematicbiasfortheweightupdatestobeonewayortheother.\n",
            "Howseriouslyshouldwetakethisargument? Whiletheargumentissuggestive,it’sa\n",
            "heuristic,notarigorousproofthattanhneuronsoutperformsigmoidneurons. Perhapsthere\n",
            "areotherpropertiesofthesigmoidneuronwhichcompensateforthisproblem? Indeed,\n",
            "formanytasksthetanhisfoundempiricallytoprovideonlyasmallornoimprovementin\n",
            "performanceoversigmoidneurons. Unfortunately,wedon’tyethavehard-and-fastrulesto\n",
            "knowwhichneurontypeswilllearnfastest,orgivethebestgeneralizationperformance,for\n",
            "anyparticularapplication. Anothervariationonthesigmoidneuronistherectifiedlinearneuronorrectifiedlinear\n",
            "unit. Theoutputofarectifiedlinearunitwithinput x,weightvectorw,andbias bisgiven\n",
            "by\n",
            "max(0,w x+b). (3.57)\n",
            "·\n",
            "Graphically,therectifyingfunctionmax(0,z)lookslikethis:\n",
            "max(0,z)\n",
            "4\n",
            "2\n",
            "z\n",
            "0\n",
            "4 2 0 2 4\n",
            "− −\n",
            "2\n",
            "−\n",
            "4\n",
            "−\n",
            "Obviouslysuchneuronsarequitedifferentfrombothsigmoidandtanhneurons. However,\n",
            "likethesigmoidandtanhneurons,rectifiedlinearunitscanbeusedtocomputeanyfunction,\n",
            "andtheycanbetrainedusingideassuchasbackpropagationandstochasticgradientdescent. Whenshouldyouuserectifiedlinearunitsinsteadofsigmoidortanhneurons?\n",
            "Some\n",
            "\n",
            "(cid:12)\n",
            "124 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "recentworkonimagerecognition41hasfoundconsiderablebenefitinusingrectifiedlinear\n",
            "unitsthroughmuchofthenetwork. However,aswithtanhneurons,wedonotyethavea\n",
            "reallydeepunderstandingofwhen,exactly,rectifiedlinearunitsarepreferable,norwhy. To\n",
            "giveyoutheflavorofsomeoftheissues,recallthatsigmoidneuronsstoplearningwhen\n",
            "theysaturate,i.e.,whentheiroutputisneareither0or1. Aswe’veseenrepeatedlyinthis\n",
            "chapter,theproblemisthatσ termsreducethegradient,andthatslowsdownlearning. (cid:48)\n",
            "3 Tanhneuronssufferfromasimilarproblemwhentheysaturate. Bycontrast,increasing\n",
            "theweightedinputtoarectifiedlinearunitwillnevercauseittosaturate,andsothere\n",
            "isnocorrespondinglearningslowdown. Ontheotherhand,whentheweightedinputto\n",
            "arectifiedlinearunitisnegative,thegradientvanishes,andsotheneuronstopslearning\n",
            "entirely. Thesearejusttwoofthemanyissuesthatmakeitnon-trivialtounderstandwhen\n",
            "andwhyrectifiedlinearunitsperformbetterthansigmoidortanhneurons. I’vepaintedapictureofuncertaintyhere,stressingthatwedonotyethaveasolidtheory\n",
            "ofhowactivationfunctionsshouldbechosen. Indeed,theproblemishardereventhanI\n",
            "havedescribed, forthereareinfinitelymanypossibleactivationfunctions. Whichisthe\n",
            "bestforanygivenproblem? Whichwillresultinanetworkwhichlearnsfastest? Which\n",
            "willgivethehighesttestaccuracies? Iamsurprisedhowlittlereallydeepandsystematic\n",
            "investigationhasbeendoneofthesequestions. Ideally,we’dhaveatheorywhichtellsus,in\n",
            "detail,howtochoose(andperhapsmodify-on-the-fly)ouractivationfunctions. Ontheother\n",
            "hand,weshouldn’tletthelackofafulltheorystopus!\n",
            "Wehavepowerfultoolsalreadyat\n",
            "hand,andcanmakealotofprogresswiththosetools. Throughtheremainderofthisbook\n",
            "I’llcontinuetousesigmoidneuronsasourgo-toneuron,sincethey’repowerfulandprovide\n",
            "concreteillustrationsofthecoreideasaboutneuralnets. Butkeepinthebackofyourmind\n",
            "thatthesesameideascanbeappliedtoothertypesofneuron,andthattherearesometimes\n",
            "advantagesindoingso. Onstoriesinneuralnetworks\n",
            "Question:Howdoyouapproachutilizingandresearchingmachinelearning\n",
            "techniques that are supported almost entirely empirically, as opposed to\n",
            "mathematically? Alsoinwhatsituationshaveyounoticedsomeofthese\n",
            "techniquesfail?\n",
            "Answer: You have to realize that our theoretical tools are very weak. Sometimes,wehavegoodmathematicalintuitionsforwhyaparticular\n",
            "techniqueshouldwork. Sometimesourintuitionendsupbeingwrong[...]\n",
            "Thequestionsbecome: howwelldoesmymethodworkonthisparticular\n",
            "problem,andhowlargeisthesetofproblemsonwhichitworkswell. —QuestionandanswerwithneuralnetworksresearcherYannLeCun\n",
            "Once,attendingaconferenceonthefoundationsofquantummechanics,Inoticedwhat\n",
            "seemedtomeamostcuriousverbalhabit: whentalksfinished,questionsfromtheaudience\n",
            "41See,forexample,WhatistheBestMulti-StageArchitectureforObjectRecognition?,byKevinJarrett,\n",
            "KorayKavukcuoglu,Marc’AurelioRanzatoandYannLeCun(2009),,byXavierGlorot,AntoineBordes,\n",
            "andYoshuaBengio(2011),andImageNetClassificationwithDeepConvolutionalNeuralNetworks,by\n",
            "AlexKrizhevsky,IlyaSutskever,andGeoffreyHinton(2012).Notethatthesepapersfillinimportant\n",
            "detailsabouthowtosetuptheoutputlayer,costfunction,andregularizationinnetworksusingrectified\n",
            "linearunits. I’veglossedoverallthesedetailsinthisbriefaccount.\n",
            "Thepapersalsodiscussinmore\n",
            "detailthebenefitsanddrawbacksofusingrectifiedlinearunits.AnotherinformativepaperisRectified\n",
            "LinearUnitsImproveRestrictedBoltzmannMachines,byVinodNairandGeoffreyHinton(2010),which\n",
            "demonstratesthebenefitsofusingrectifiedlinearunitsinasomewhatdifferentapproachtoneural\n",
            "networks. \n",
            "(cid:12)\n",
            "3.6. Othertechniques (cid:12) 125\n",
            "(cid:12)\n",
            "oftenbeganwith“I’mverysympathetictoyourpointofview,but[...]”. Quantumfoundations\n",
            "wasnotmyusualfield,andInoticedthisstyleofquestioningbecauseatotherscientific\n",
            "conferencesI’drarelyorneverheardaquestionerexpresstheirsympathyforthepointof\n",
            "viewofthespeaker. Atthetime,Ithoughttheprevalenceofthequestionsuggestedthat\n",
            "littlegenuineprogresswasbeingmadeinquantumfoundations,andpeopleweremerely\n",
            "spinningtheirwheels. Later,Irealizedthatassessmentwastooharsh. Thespeakerswere\n",
            "wrestlingwithsomeofthehardestproblemshumanmindshaveeverconfronted. Ofcourse 3\n",
            "progresswasslow! Buttherewasstillvalueinhearingupdatesonhowpeoplewerethinking,\n",
            "eveniftheydidn’talwayshaveunarguablenewprogresstoreport.\n",
            "Youmayhavenoticedaverbalticsimilarto“I’mverysympathetic[...]” inthecurrent\n",
            "book. Toexplainwhatwe’reseeingI’veoftenfallenbackonsaying“Heuristically,[...]”,or\n",
            "“Roughlyspeaking,[...]”,followingupwithastorytoexplainsomephenomenonorother. Thesestoriesareplausible,buttheempiricalevidenceI’vepresentedhasoftenbeenpretty\n",
            "thin. Ifyoulookthroughtheresearchliteratureyou’llseethatstoriesinasimilarstyleappear\n",
            "inmanyresearchpapersonneuralnets,oftenwiththinsupportingevidence. Whatshould\n",
            "wethinkaboutsuchstories? Inmanypartsofscience–especiallythosepartsthatdealwithsimplephenomena–it’s\n",
            "possibletoobtainverysolid,veryreliableevidenceforquitegeneralhypotheses. Butin\n",
            "neuralnetworkstherearelargenumbersofparametersandhyper-parameters,andextremely\n",
            "complexinteractionsbetweenthem. Insuchextraordinarilycomplexsystemsit’sexceedingly\n",
            "difficulttoestablishreliablegeneralstatements. Understandingneuralnetworksintheirfull\n",
            "generalityisaproblemthat,likequantumfoundations,teststhelimitsofthehumanmind. Instead,weoftenmakedowithevidencefororagainstafewspecificinstancesofageneral\n",
            "statement. Asaresultthosestatementssometimeslaterneedtobemodifiedorabandoned,\n",
            "whennewevidencecomestolight.\n",
            "Onewayofviewingthissituationisthatanyheuristicstoryaboutneuralnetworkscarries\n",
            "withitanimpliedchallenge. Forexample,considerthestatementIquotedearlier,explaining\n",
            "whydropoutworks42: “Thistechniquereducescomplexco-adaptationsofneurons,sincea\n",
            "neuroncannotrelyonthepresenceofparticularotherneurons. Itis,therefore,forcedto\n",
            "learnmorerobustfeaturesthatareusefulinconjunctionwithmanydifferentrandomsubsets\n",
            "oftheotherneurons.” Thisisarich,provocativestatement,andonecouldbuildafruitful\n",
            "researchprogramentirelyaroundunpackingthestatement,figuringoutwhatinitistrue,\n",
            "whatisfalse,whatneedsvariationandrefinement. Indeed,thereisnowasmallindustryof\n",
            "researcherswhoareinvestigatingdropout(andmanyvariations),tryingtounderstandhow\n",
            "itworks,andwhatitslimitsare. Andsoitgoeswithmanyoftheheuristicswe’vediscussed. Eachheuristicisnotjusta(potential)explanation,it’salsoachallengetoinvestigateand\n",
            "understandinmoredetail. Of course, there is not time for any single person to investigate all these heuristic\n",
            "explanationsindepth. It’sgoingtotakedecades(orlonger)forthecommunityofneural\n",
            "networksresearcherstodevelopareallypowerful,evidence-basedtheoryofhowneural\n",
            "networkslearn. Doesthismeanyoushouldrejectheuristicexplanationsasunrigorous,and\n",
            "notsufficientlyevidence-based?\n",
            "No! Infact,weneedsuchheuristicstoinspireandguide\n",
            "ourthinking. It’slikethegreatageofexploration: theearlyexplorerssometimesexplored\n",
            "(andmadenewdiscoveries)onthebasisofbeliefswhichwerewronginimportantways.\n",
            "Later,thosemistakeswerecorrectedaswefilledinourknowledgeofgeography. Whenyou\n",
            "42FromImageNetClassificationwithDeepConvolutionalNeuralNetworks,byAlexKrizhevsky,Ilya\n",
            "Sutskever,andGeoffreyHinton(2012). \n",
            "(cid:12)\n",
            "126 (cid:12) Improvingthewayneuralnetworkslearn\n",
            "(cid:12)\n",
            "understandsomethingpoorly–astheexplorersunderstoodgeography,andasweunderstand\n",
            "neuralnetstoday–it’smoreimportanttoexploreboldlythanitistoberigorouslycorrect\n",
            "ineverystepofyourthinking. Andsoyoushouldviewthesestoriesasausefulguideto\n",
            "howtothinkaboutneuralnets,whileretainingahealthyawarenessofthelimitationsof\n",
            "suchstories,andcarefullykeepingtrackofjusthowstrongtheevidenceisforanygivenline\n",
            "ofreasoning. Putanotherway,weneedgoodstoriestohelpmotivateandinspireus,and\n",
            "3 rigorousin-depthinvestigationinordertouncovertherealfactsofthematter.\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 127\n",
            "(cid:12)\n",
            "4444\n",
            "A visual proof that neural nets\n",
            "can compute any function\n",
            "4\n",
            "Oneofthemoststrikingfactsaboutneuralnetworksisthattheycancomputeanyfunction\n",
            "atall. Thatis,supposesomeonehandsyousomecomplicated,wigglyfunction, f(x):\n",
            "f(x)\n",
            "Nomatterwhatthefunction,thereisguaranteedtobeaneuralnetworksothatforevery\n",
            "possibleinput, x,thevalue f(x)(orsomecloseapproximation)isoutputfromthenetwork,\n",
            "e.g.:\n",
            "\n",
            "(cid:12)\n",
            "128 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4 Thisresultholdsevenifthefunctionhasmanyinputs, f = f(x 1 ,...,x m),andmanyoutputs. Forinstance,here’sanetworkcomputingafunctionwithm=3inputsandn=2outputs:\n",
            "Thisresulttellsusthatneuralnetworkshaveakindofuniversality.\n",
            "Nomatterwhatfunction\n",
            "wewanttocompute,weknowthatthereisaneuralnetworkwhichcandothejob. What’smore,thisuniversalitytheoremholdsevenifwerestrictournetworkstohave\n",
            "justasinglelayerintermediatebetweentheinputandtheoutputneurons–aso-calledsingle\n",
            "hiddenlayer. Soevenverysimplenetworkarchitecturescanbeextremelypowerful. Theuniversalitytheoremiswellknownbypeoplewhouseneuralnetworks. Butwhyit’s\n",
            "trueisnotsowidelyunderstood. Mostoftheexplanationsavailablearequitetechnical. For\n",
            "instance,oneoftheoriginalpapersprovingtheresult1didsousingtheHahn-Banachtheorem,\n",
            "theRieszRepresentationtheorem,andsomeFourieranalysis. Ifyou’reamathematicianthe\n",
            "argumentisnotdifficulttofollow,butit’snotsoeasyformostpeople. That’sapity,since\n",
            "theunderlyingreasonsforuniversalityaresimpleandbeautiful. InthischapterIgiveasimpleandmostlyvisualexplanationoftheuniversalitytheorem.\n",
            "We’llgostepbystepthroughtheunderlyingideas. You’llunderstandwhyit’struethatneural\n",
            "networkscancomputeanyfunction. You’llunderstandsomeofthelimitationsoftheresult.\n",
            "Andyou’llunderstandhowtheresultrelatestodeepneuralnetworks. Tofollowthematerialinthechapter, youdonotneedtohavereadearlierchapters\n",
            "inthisbook. Instead, thechapterisstructuredtobeenjoyableasaself-containedessay. Providedyouhavejustalittlebasicfamiliaritywithneuralnetworks,youshouldbeableto\n",
            "followtheexplanation. Iwill,however,provideoccasionallinkstoearliermaterial,tohelp\n",
            "fillinanygapsinyourknowledge. 1Approximationbysuperpositionsofasigmoidalfunction,byGeorgeCybenko(1989).Theresult\n",
            "wasverymuchintheairatthetime,andseveralgroupsprovedcloselyrelatedresults. Cybenko’s\n",
            "papercontainsausefuldiscussionofmuchofthatwork.AnotherimportantearlypaperisMultilayer\n",
            "feedforwardnetworksareuniversalapproximators,byKurtHornik,MaxwellStinchcombe,andHalbert\n",
            "White(1989).ThispaperusestheStone-Weierstrasstheoremtoarriveatsimilarresults. \n",
            "(cid:12)\n",
            "4.1. Twocaveats (cid:12) 129\n",
            "(cid:12)\n",
            "Universality theorems are a commonplace in computer science, so much so that we\n",
            "sometimesforgethowastonishingtheyare. Butit’sworthremindingourselves: theabilityto\n",
            "computeanarbitraryfunctionistrulyremarkable.\n",
            "Almostanyprocessyoucanimaginecan\n",
            "bethoughtofasfunctioncomputation. Considertheproblemofnamingapieceofmusic\n",
            "basedonashortsampleofthepiece. Thatcanbethoughtofascomputingafunction.\n",
            "Or\n",
            "considertheproblemoftranslatingaChinesetextintoEnglish. Again,thatcanbethought\n",
            "of as computing a function2. Or consider the problem of taking an mp4 movie file and\n",
            "generatingadescriptionoftheplotofthemovie,andadiscussionofthequalityoftheacting. Again,thatcanbethoughtofasakindoffunctioncomputation3Universalitymeansthat,in\n",
            "principle,neuralnetworkscandoallthesethingsandmanymore. 4\n",
            "Ofcourse,justbecauseweknowaneuralnetworkexiststhatcan(say)translateChinese\n",
            "text into English, that doesn’t mean we have good techniques for constructing or even\n",
            "recognizingsuchanetwork. Thislimitationappliesalsototraditionaluniversalitytheorems\n",
            "formodelssuchasBooleancircuits. But,aswe’veseenearlierinthebook,neuralnetworks\n",
            "havepowerfulalgorithmsforlearningfunctions. Thatcombinationoflearningalgorithms+\n",
            "universalityisanattractivemix. Uptonow,thebookhasfocusedonthelearningalgorithms.\n",
            "Inthischapter,wefocusonuniversality,andwhatitmeans. 4.1 Two caveats\n",
            "Beforeexplainingwhytheuniversalitytheoremistrue,Iwanttomentiontwocaveatstothe\n",
            "informalstatement“aneuralnetworkcancomputeanyfunction”. First,thisdoesn’tmeanthatanetworkcanbeusedtoexactlycomputeanyfunction. Rather,wecangetanapproximationthatisasgoodaswewant. Byincreasingthenumberof\n",
            "hiddenneuronswecanimprovetheapproximation. Forinstance,earlier(see4)Iillustrated\n",
            "anetworkcomputingsomefunction f(x)usingthreehiddenneurons. Formostfunctions\n",
            "onlyalow-qualityapproximationwillbepossibleusingthreehiddenneurons. Byincreasing\n",
            "thenumberofhiddenneurons(say,tofive)wecantypicallygetabetterapproximation:\n",
            "Andwecandostillbetterbyfurtherincreasingthenumberofhiddenneurons. 2Actually,computingoneofmanyfunctions,sincethereareoftenmanyacceptabletranslationsofa\n",
            "givenpieceoftext. 3Dittotheremarkabouttranslationandtherebeingmanypossiblefunctions.. \n",
            "(cid:12)\n",
            "130 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Tomakethisstatementmoreprecise,supposewe’regivenafunction f(x)whichwe’d\n",
            "liketocomputetowithinsomedesiredaccuracyε>0. Theguaranteeisthatbyusing\n",
            "enoughhiddenneuronswecanalwaysfindaneuralnetworkwhoseoutput g(x)satisfies\n",
            "g(x) f(x) <ε,forallinputsx. Inotherwords,theapproximationwillbegoodtowithin\n",
            "|thede−sireda|ccuracyforeverypossibleinput. Thesecondcaveatisthattheclassoffunctionswhichcanbeapproximatedintheway\n",
            "describedarethecontinuousfunctions. Ifafunctionisdiscontinuous,i.e.,makessudden,\n",
            "sharpjumps,thenitwon’tingeneralbepossibletoapproximateusinganeuralnet. This\n",
            "isnotsurprising,sinceourneuralnetworkscomputecontinuousfunctionsoftheirinput. 4 However,evenifthefunctionwe’dreallyliketocomputeisdiscontinuous,it’softenthe\n",
            "casethatacontinuousapproximationisgoodenough. Ifthat’sso,thenwecanuseaneural\n",
            "network.\n",
            "Inpractice,thisisnotusuallyanimportantlimitation. Summingup,amoreprecisestatementoftheuniversalitytheoremisthatneuralnetworks\n",
            "withasinglehiddenlayercanbeusedtoapproximateanycontinuousfunctiontoanydesired\n",
            "precision. Inthischapterwe’llactuallyproveaslightlyweakerversionofthisresult,using\n",
            "twohiddenlayersinsteadofone.\n",
            "IntheproblemsI’llbrieflyoutlinehowtheexplanation\n",
            "can,withafewtweaks,beadaptedtogiveaproofwhichusesonlyasinglehiddenlayer. 4.2 Universality with one input and one output\n",
            "Tounderstandwhytheuniversalitytheoremistrue, let’sstartbyunderstandinghowto\n",
            "construct a neural network which approximates a function with just one input and one\n",
            "output:\n",
            "f(x)\n",
            "It turns out that this is the core of the problem of universality. Once we’ve understood\n",
            "thisspecialcaseit’sactuallyprettyeasytoextendtofunctionswithmanyinputsandmany\n",
            "outputs. Tobuildinsightintohowtoconstructanetworktocompute f,let’sstartwithanetwork\n",
            "containingjustasinglehiddenlayer,withtwohiddenneurons,andanoutputlayercontaining\n",
            "asingleoutputneuron:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 131\n",
            "(cid:12)\n",
            "4\n",
            "Togetafeelforhowcomponentsinthenetworkwork,let’sfocusonthetophiddenneuron. Inthediagrambelow,clickontheweight,w,anddragthemousealittlewaystotherightto\n",
            "increasew. Youcanimmediatelyseehowthefunctioncomputedbythetophiddenneuron\n",
            "changes:\n",
            "Outputfromneuron\n",
            "1 w=7,b= 2\n",
            "w=7,b= −3\n",
            "w=7,b= −4\n",
            "w=7,b= −5\n",
            "w=7,b= −6\n",
            "−\n",
            "0 1\n",
            "Outputfromneuron\n",
            "1 w=9,b= 4\n",
            "w=8,b= −4\n",
            "w=7,b= −4\n",
            "w=6,b= −4\n",
            "w=5,b= −4\n",
            "−\n",
            "0 1\n",
            "Aswelearntearlierinthebook,what’sbeingcomputedbythehiddenneuronisσ (wx+b),\n",
            "whereσ (z) 1/ (1+e\n",
            "−\n",
            "z )isthesigmoidfunction. Uptonow,we’vemadefrequentuseofthis\n",
            "algebraicfor≡m.\n",
            "Butfortheproofofuniversalitywewillobtainmoreinsightbyignoringthe\n",
            "algebraentirely,andinsteadmanipulatingandobservingtheshapeshowninthegraph. This\n",
            "\n",
            "(cid:12)\n",
            "132 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "won’tjustgiveusabetterfeelforwhat’sgoingon,itwillalsogiveusaproof4ofuniversality\n",
            "thatappliestoactivationfunctionsotherthanthesigmoidfunction. Wecansimplifyour\n",
            "analysisquiteabitbyincreasingtheweightsomuchthattheoutputreallyisastepfunction,\n",
            "toaverygoodapproximation. BelowI’veplottedtheoutputfromthetophiddenneuron\n",
            "whentheweightisw=999. Outputfromtophiddenneuron\n",
            "4\n",
            "1\n",
            "x\n",
            "1\n",
            "It’sactuallyquiteabiteasiertoworkwithstepfunctionsthangeneralsigmoidfunctions. Thereasonisthatintheoutputlayerweaddupcontributionsfromallthehiddenneurons. It’seasytoanalyzethesumofabunchofstepfunctions,butrathermoredifficulttoreason\n",
            "aboutwhathappenswhenyouaddupabunchofsigmoidshapedcurves. Andsoitmakes\n",
            "thingsmucheasiertoassumethatourhiddenneuronsareoutputtingstepfunctions. More\n",
            "concretely,wedothisbyfixingtheweightwtobesomeverylargevalue,andthensettingthe\n",
            "positionofthestepbymodifyingthebias. Ofcourse,treatingtheoutputasastepfunction\n",
            "isanapproximation,butit’saverygoodapproximation,andfornowwe’lltreatitasexact. I’llcomebacklatertodiscusstheimpactofdeviationsfromthisapproximation. Atwhatvalueof x doesthestepoccur? Putanotherway,howdoesthepositionofthe\n",
            "stepdependupontheweightandbias? Toanswerthisquestion,trymodifyingtheweightandbiasinthediagramabove(you\n",
            "mayneedtoscrollbackabit). Canyoufigureouthowthepositionofthestepdependsonw\n",
            "and b?\n",
            "Withalittleworkyoushouldbeabletoconvinceyourselfthatthepositionofthe\n",
            "stepisproportionalto b,andinverselyproportionaltow. Infact,thestepisatpositions= b/w,asyoucanseebymodifyingtheweightand\n",
            "biasinthefollowingdiagram: −\n",
            "4Strictlyspeaking,thevisualapproachI’mtakingisn’twhat’straditionallythoughtofasaproof.But\n",
            "Ibelievethevisualapproachgivesmoreinsightintowhytheresultistruethanatraditionalproof.And,\n",
            "ofcourse,thatkindofinsightistherealpurposebehindaproof.Occasionally,therewillbesmallgapsin\n",
            "thereasoningIpresent:placeswhereImakeavisualargumentthatisplausible,butnotquiterigorous. Ifthisbothersyou,thenconsideritachallengetofillinthemissingsteps.Butdon’tlosesightofthe\n",
            "realpurpose:tounderstandwhytheuniversalitytheoremistrue.\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 133\n",
            "(cid:12)\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "b/w=0.4 1\n",
            "4\n",
            "−\n",
            "Itwillgreatlysimplifyourlivestodescribehiddenneuronsusingjustasingleparameter,s,\n",
            "whichisthestepposition,s= b/w. Trymodifyingsinthefollowingdiagram,inorderto\n",
            "getusedtothenewparameter−ization:\n",
            "Outputfromtophiddenneuron\n",
            "1\n",
            "x\n",
            "s 1\n",
            "Asnotedabove,we’veimplicitlysettheweightwontheinputtobesomelargevalue–big\n",
            "enoughthatthestepfunctionisaverygoodapproximation. Wecaneasilyconvertaneuron\n",
            "parameterizedinthiswaybackintotheconventionalmodel,bychoosingthebias b= ws. −\n",
            "Uptonowwe’vebeenfocusingontheoutputfromjustthetophiddenneuron.\n",
            "Let’stake\n",
            "alookatthebehavioroftheentirenetwork. Inparticular,we’llsupposethehiddenneurons\n",
            "arecomputingstepfunctionsparameterizedbysteppointss (topneuron)ands (bottom\n",
            "1 2\n",
            "neuron). Andthey’llhaverespectiveoutputweightsw andw . Here’sthenetwork:\n",
            "1 2\n",
            "Weightedoutputfromhiddenlayer\n",
            "w 1+w\n",
            "2\n",
            "1\n",
            "w\n",
            "1\n",
            "x\n",
            "s 1 s 2 1\n",
            "What’sbeingplottedontherightistheweightedoutputw\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a\n",
            "2\n",
            "fromthehiddenlayer. \n",
            "(cid:12)\n",
            "134 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Here, a and a aretheoutputsfromthetopandbottomhiddenneurons, respectively5. 1 2\n",
            "Theseoutputsaredenotedwithasbecausethey’reoftenknownastheneurons’activations. Tryincreasinganddecreasingthesteppoints ofthetophiddenneuron. Getafeel\n",
            "1\n",
            "forhowthischangestheweightedoutputfromthehiddenlayer. It’sparticularlyworth\n",
            "understandingwhathappenswhens goespasts . You’llseethatthegraphchangesshape\n",
            "1 2\n",
            "whenthishappens,sincewehavemovedfromasituationwherethetophiddenneuron\n",
            "isthefirsttobeactivatedtoasituationwherethebottomhiddenneuronisthefirsttobe\n",
            "activated. Similarly,trymanipulatingthesteppoints ofthebottomhiddenneuron,andgetafeel\n",
            "2\n",
            "forhowthischangesthecombinedoutputfromthehiddenneurons. 4\n",
            "Tryincreasinganddecreasingeachoftheoutputweights. Noticehowthisrescalesthe\n",
            "contributionfromtherespectivehiddenneurons. Whathappenswhenoneoftheweightsis\n",
            "zero?\n",
            "Finally,trysettingw tobe0.8andw tobe-0.8. Yougeta“bump”function,which\n",
            "1 2\n",
            "startsatpoints ,endsatpoints ,andhasheight0.8. Forinstance,theweightedoutput\n",
            "1 2\n",
            "mightlooklikethis:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "w 1= w 2\n",
            "−\n",
            "x\n",
            "s 1 s 2 1\n",
            "Ofcourse,wecanrescalethebumptohaveanyheightatall. Let’suseasingleparameter,h,\n",
            "todenotetheheight. ToreduceclutterI’llalsoremovethe“s 1=...”and“w 1=...”notations. Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "x\n",
            "s 1 s 2 1\n",
            "1\n",
            "−\n",
            "Trychangingthevalueofhupanddown,toseehowtheheightofthebumpchanges. Try\n",
            "changingtheheightsoit’snegative,andobservewhathappens.\n",
            "Andtrychangingthestep\n",
            "pointstoseehowthatchangestheshapeofthebump. 5Note,bytheway,thattheoutputfromthewholenetworkisσ (w\n",
            "1\n",
            "a 1+w\n",
            "2\n",
            "a 2+b),wherebisthe\n",
            "biasontheoutputneuron.Obviously,thisisn’tthesameastheweightedoutputfromthehiddenlayer,\n",
            "whichiswhatwe’replottinghere.We’regoingtofocusontheweightedoutputfromthehiddenlayer\n",
            "rightnow,andonlylaterwillwethinkabouthowthatrelatestotheoutputfromthewholenetwork. \n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 135\n",
            "(cid:12)\n",
            "You’llnotice,bytheway,thatwe’reusingourneuronsinawaythatcanbethought\n",
            "ofnotjustingraphicalterms,butinmoreconventionalprogrammingterms,asakindof\n",
            "if-then-elsestatement,e.g.:\n",
            "if input >= step point:\n",
            "add 1 to the weighted output\n",
            "else:\n",
            "add 0 to the weighted output\n",
            "4\n",
            "ForthemostpartI’mgoingtostickwiththegraphicalpointofview. Butinwhatfollowsyou\n",
            "maysometimesfindithelpfultoswitchpointsofview,andthinkaboutthingsintermsof\n",
            "if-then-else. Wecanuseourbump-makingtricktogettwobumps,bygluingtwopairsofhidden\n",
            "neuronstogetherintothesamenetwork:\n",
            "Weightedoutputfromhiddenlayer\n",
            "1\n",
            "h\n",
            "2\n",
            "0\n",
            "1\n",
            "−h\n",
            "1\n",
            "0 s1 s1s2 s21\n",
            "1 2 1 2\n",
            "I’vesuppressedtheweightshere,simplywritingthehvaluesforeachpairofhiddenneurons. Tryincreasinganddecreasingbothhvalues,andobservehowitchangesthegraph.\n",
            "Move\n",
            "thebumpsaroundbychangingthesteppoints. Moregenerally,wecanusethisideatogetasmanypeaksaswewant,ofanyheight. In\n",
            "particular,wecandividetheinterval[0,1]upintoalargenumber,N,ofsubintervals,and\n",
            "useN pairsofhiddenneuronstosetuppeaksofanydesiredheight. Let’sseehowthisworks\n",
            "forN=5. That’squiteafewneurons,soI’mgoingtopackthingsinabit. Apologiesforthe\n",
            "complexityofthediagram: Icouldhidethecomplexitybyabstractingawayfurther,butI\n",
            "thinkit’sworthputtingupwithalittlecomplexity,forthesakeofgettingamoreconcrete\n",
            "feelforhowthesenetworkswork. \n",
            "(cid:12)\n",
            "136 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Weightedoutput\n",
            "1\n",
            "4\n",
            "0\n",
            "1\n",
            "1\n",
            "−\n",
            "Youcanseethattherearefivepairsofhiddenneurons. Thesteppointsfortherespective\n",
            "pairsofneuronsare0,1/5,then1/5,2/5,andsoon,outto4/5,5/5. Thesevaluesarefixed\n",
            "–theymakeitsowegetfiveevenlyspacedbumpsonthegraph.\n",
            "Eachpairofneuronshasavalueofhassociatedtoit. Remember,theconnectionsoutput\n",
            "fromtheneuronshaveweightshand h(notmarked). Clickononeofthehvalues,and\n",
            "dragthemousetotherightorlefttoc−hangethevalue. Asyoudoso,watchthefunction\n",
            "change. Bychangingtheoutputweightswe’reactuallydesigningthefunction! Contrariwise,tryclickingonthegraph,anddraggingupordowntochangetheheight\n",
            "ofanyofthebumpfunctions. Asyouchangetheheights,youcanseethecorresponding\n",
            "changeinhvalues. And,althoughit’snotshown,thereisalsoachangeinthecorresponding\n",
            "outputweights,whichare+hand h. −\n",
            "Inotherwords,wecandirectlymanipulatethefunctionappearinginthegraphonthe\n",
            "right,andseethatreflectedinthehvaluesontheleft. Afunthingtodoistoholdthemouse\n",
            "buttondownanddragthemousefromonesideofthegraphtotheother.\n",
            "Asyoudothisyou\n",
            "drawoutafunction,andgettowatchtheparametersintheneuralnetworkadapt. Timeforachallenge. Let’sthinkbacktothefunctionIplottedatthebeginningofthechapter:\n",
            "\n",
            "(cid:12)\n",
            "4.2. Universalitywithoneinputandoneoutput (cid:12) 137\n",
            "(cid:12)\n",
            "f(x)\n",
            "4\n",
            "Ididn’tsayitatthetime,butwhatIplottedisactuallythefunction\n",
            "f(x)=0.2+0.4x2 +0.3xsin(15x)+0.05cos(50x), (4.1)\n",
            "plottedover x from0to1,andwiththe y axistakingvaluesfrom0to1. That’sobviouslynotatrivialfunction.\n",
            "You’regoingtofigureouthowtocomputeitusinganeuralnetwork. (cid:80)\n",
            "Inournetworksabovewe’vebeenanalyzingtheweightedcombination w a output\n",
            "j j j\n",
            "fromthehiddenneurons. Wenowknowhowtogetalotofcontroloverthisquantity. But,\n",
            "asInotedearlier,thisquantityisnotwhat’soutputfromthenetwork. What’soutputfrom\n",
            "thenetworkisσ ( (cid:80) j w j a j+b)where bisthebiasontheoutputneuron. Istheresomeway\n",
            "wecanachievecontrolovertheactualoutputfromthenetwork? Thesolutionistodesignaneuralnetworkwhosehiddenlayerhasaweightedoutput\n",
            "givenbyσ\n",
            "−\n",
            "1 f(x),whereσ\n",
            "−\n",
            "1isjusttheinverseoftheσfunction. Thatis,wewantthe\n",
            "weightedoutp◦utfromthehiddenlayertobe:\n",
            "2\n",
            "σ\n",
            "−\n",
            "1 f(x)\n",
            "◦\n",
            "1\n",
            "1\n",
            "1\n",
            "−\n",
            "2\n",
            "−\n",
            "Ifwecandothis,thentheoutputfromthenetworkasawholewillbeagoodapproximation\n",
            "to f(x) 6. Yourchallenge,then,istodesignaneuralnetworktoapproximatethegoalfunction\n",
            "shownjustabove. Tolearnasmuchaspossible,Iwantyoutosolvetheproblemtwice. The\n",
            "firsttime,pleaseclickonthegraph,directlyadjustingtheheightsofthedifferentbump\n",
            "6NotethatIhavesetthebiasontheoutputneuronto0.\n",
            "\n",
            "(cid:12)\n",
            "138 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "functions. Youshouldfinditfairlyeasytogetagoodmatchtothegoalfunction. How\n",
            "wellyou’redoingismeasuredbytheaveragedeviationbetweenthegoalfunctionandthe\n",
            "functionthenetworkisactuallycomputing. Yourchallengeistodrivetheaveragedeviation\n",
            "aslowaspossible. Youcompletethechallengewhenyoudrivetheaveragedeviationto0.40\n",
            "orbelow7\n",
            "4\n",
            "2\n",
            "Weightedoutput\n",
            "σ =0.38\n",
            "1\n",
            "1\n",
            "1\n",
            "−\n",
            "2\n",
            "−\n",
            "You’venowfiguredoutalltheelementsnecessaryforthenetworktoapproximatelycompute\n",
            "thefunction f(x)!\n",
            "It’sonlyacoarseapproximation,butwecouldeasilydomuchbetter,\n",
            "merelybyincreasingthenumberofpairsofhiddenneurons,allowingmorebumps. In particular, it’s easy to convert all the data we have found back into the standard\n",
            "parametrizationusedforneuralnetworks.\n",
            "Letmejustrecapquicklyhowthatworks. Thefirstlayerofweightsallhavesomelarge,constantvalue,sayw=1000. Thebiasesonthehiddenneuronsarejust b= ws. So,forinstance,forthesecond\n",
            "hiddenneurons=0.2becomes b= 1000 0.2= −200. Thefinallayerofweightsarede−termin×edbyth−ehvalues. So,forinstance,thevalue\n",
            "you’vechosenaboveforthefirsth,h= 0.6,meansthattheoutputweightsfromthetop\n",
            "twohiddenneuronsare 0.6and0.6,re−spectively. Andsoon,fortheentirelayerofoutput\n",
            "weights. −\n",
            "Finally,thebiasontheoutputneuronis0. That’severything: wenowhaveacompletedescriptionofaneuralnetworkwhichdoes\n",
            "aprettygoodjobcomputingouroriginalgoalfunction. Andweunderstandhowtoimprove\n",
            "thequalityoftheapproximationbyimprovingthenumberofhiddenneurons. What’smore,therewasnothingspecialaboutouroriginalgoalfunction, f(x)=0.2+\n",
            "0.4x2 +0.3sin(15x)+0.05cos(50x). Wecouldhaveusedthisprocedureforanycontinuous\n",
            "functionfrom[0,1]to[0,1]. Inessence,we’reusingoursingle-layerneuralnetworksto\n",
            "7Thisparagraphreferstointeractiveelement,availableonline.Thegraphshowsthefinalresultof\n",
            "manualminimizationofaveragedeviation. \n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 139\n",
            "(cid:12)\n",
            "buildalookuptableforthefunction. Andwe’llbeabletobuildonthisideatoprovidea\n",
            "generalproofofuniversality. 4.3 Many input variables\n",
            "Let’sextendourresultstothecaseofmanyinputvariables. Thissoundscomplicated,but\n",
            "alltheideasweneedcanbeunderstoodinthecaseofjusttwoinputs.\n",
            "Solet’saddressthe\n",
            "two-inputcase. We’llstartbyconsideringwhathappenswhenwehavetwoinputstoaneuron:\n",
            "Here,wehaveinputs x and y,withcorrespondingweightsw andw ,andabias bonthe\n",
            "1 2\n",
            "neuron. Let’ssettheweightw to0,andthenplayaroundwiththefirstweight,w ,andthe\n",
            "2 1\n",
            "bias, b,toseehowtheyaffecttheoutputfromtheneuron:\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 8\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 2\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=11,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "w 1=8,b= 5\n",
            "−\n",
            "1\n",
            "1\n",
            "y 00\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w 1=5,b= 5\n",
            "−\n",
            "\n",
            "(cid:12)\n",
            "140 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "Asyoucansee,withw 2=0theinput y makesnodifferencetotheoutputfromtheneuron. It’sasthough x istheonlyinput. Giventhis,whatdoyouthinkhappenswhenweincreasetheweightw\n",
            "1\n",
            "tow 1=100,\n",
            "withw remaining0? Ifyoudon’timmediatelyseetheanswer,ponderthequestionfora\n",
            "2\n",
            "bit,andseeifyoucanfigureoutwhathappens.\n",
            "Thentryitoutandseeifyou’reright. I’ve\n",
            "shownwhathappensinthefollowingmovie:\n",
            "Justasinourearlierdiscussion,astheinputweightgetslargertheoutputapproachesa\n",
            "stepfunction. Thedifferenceisthatnowthestepfunctionisinthreedimensions. Alsoas\n",
            "before,wecanmovethelocationofthesteppointaroundbymodifyingthebias.\n",
            "Theactual\n",
            "locationofthesteppointiss b/w . x 1\n",
            "Let’sredotheaboveusing≡the−positionofthestepastheparameter:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s x=0.7\n",
            "Here,weassumetheweightonthe x inputhassomelargevalue–I’veusedw 1=1000–\n",
            "andtheweightw 2=0. Thenumberontheneuronisthesteppoint,andthelittle x above\n",
            "thenumberremindsusthatthestepisinthe x direction. Ofcourse,it’salsopossibleto\n",
            "getastepfunctioninthe y direction,bymakingtheweightonthe y inputverylarge(say,\n",
            "w 2=1000),andtheweightonthe x equalto0,i.e.,w 1=0:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.25\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s y=0.5\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s y=0.7\n",
            "Thenumberontheneuronisagainthesteppoint,andinthiscasethelittle y abovethe\n",
            "numberremindsusthatthestepisinthe y direction. Icouldhaveexplicitlymarkedthe\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 141\n",
            "(cid:12)\n",
            "weightsonthe x and y inputs,butdecidednotto,sinceitwouldmakethediagramrather\n",
            "cluttered. Butdokeepinmindthatthelittle y markerimplicitlytellsusthatthe y weightis\n",
            "large,andthe x weightis0. Wecanusethestepfunctionswe’vejustconstructedtocomputeathree-dimensional\n",
            "bumpfunction. Todothis,weusetwoneurons,eachcomputingastepfunctioninthe x\n",
            "direction. Thenwecombinethosestepfunctionswithweighthand h,respectively,where\n",
            "histhedesiredheightofthebump. It’sallillustratedinthefollowin−gdiagram:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.3,s2 x=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.7,h=1\n",
            "Trychangingthevalueoftheheight,h. Observehowitrelatestotheweightsinthenetwork.\n",
            "Andseehowitchangestheheightofthebumpfunctionontheright. Also,trychangingthesteppoint0.30associatedtothetophiddenneuron. Witnesshow\n",
            "itchangestheshapeofthebump. Whathappenswhenyoumoveitpastthesteppoint0.70\n",
            "associatedtothebottomhiddenneuron? We’vefiguredouthowtomakeabumpfunctioninthe x direction. Ofcourse,wecan\n",
            "easilymakeabumpfunctioninthe ydirection,byusingtwostepfunctionsinthe ydirection.\n",
            "Recallthatwedothisbymakingtheweightlargeonthe y input,andtheweight0onthe x\n",
            "input. Here’stheresult:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.4,s2 y=0.6,h=0.8\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 y=0.3,s2 y=0.7,h=0.75\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "Thislooksnearlyidenticaltotheearliernetwork! Theonlythingexplicitlyshownaschanging\n",
            "isthatthere’snowlittle y markersonourhiddenneurons. Thatremindsusthatthey’re\n",
            "\n",
            "(cid:12)\n",
            "142 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "producing y stepfunctions,not x stepfunctions,andsotheweightisverylargeonthe y\n",
            "input,andzeroonthex input,notviceversa. Asbefore,Idecidednottoshowthisexplicitly,\n",
            "inordertoavoidclutter. Let’sconsiderwhathappenswhenweadduptwobumpfunctions,\n",
            "oneinthe x direction,theotherinthe y direction,bothofheighth:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=1\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "s1 x=0.4,s2 x=0.6\n",
            "s1 y=0.3,s2 y=0.7,h=0.6\n",
            "TosimplifythediagramI’vedroppedtheconnectionswithzeroweight. Fornow,I’veleftin\n",
            "thelittlexand ymarkersonthehiddenneurons,toremindyouinwhatdirectionsthebump\n",
            "functionsarebeingcomputed.\n",
            "We’lldropeventhosemarkerslater,sincethey’reimplied\n",
            "bytheinputvariable. Tryvaryingtheparameterh. Asyoucansee,thiscausestheoutput\n",
            "weightstochange,andalsotheheightsofboththe x and y bumpfunctions. Whatwe’ve\n",
            "builtlooksalittlelikeatowerfunction:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "Towerfunction\n",
            "Ifwecouldbuildsuchtowerfunctions,thenwecouldusethemtoapproximatearbitrary\n",
            "functions,justbyaddingupmanytowersofdifferentheights,andindifferentlocations:\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 143\n",
            "(cid:12)\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Ofcourse,wehaven’tyetfiguredouthowtobuildatowerfunction.Whatwehaveconstructed\n",
            "lookslikeacentraltower,ofheight2h,withasurroundingplateau,ofheighth. Butwecanmakeatowerfunction. Rememberthatearlierwesawneuronscanbeused\n",
            "toimplementatypeof’inlineif-then-elsestatement:\n",
            "if input >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "Thatwasforaneuronwithjustasingleinput. Whatwewantistoapplyasimilarideato\n",
            "thecombinedoutputfromthehiddenneurons:\n",
            "if combined output from hidden neurons >= threshold:\n",
            "output 1\n",
            "else:\n",
            "output 0\n",
            "If we choose the threshold appropriately — say, a value of 3h/2, which is sandwiched\n",
            "betweentheheightoftheplateauandtheheightofthecentraltower–wecouldsquashthe\n",
            "plateaudowntozero,andleavejustthetowerstanding. Canyouseehowtodothis?\n",
            "Tryexperimentingwiththefollowingnetworktofigureit\n",
            "out. Notethatwe’renowplottingtheoutputfromtheentirenetwork,notjusttheweighted\n",
            "outputfromthehiddenlayer. Thismeansweaddabiastermtotheweightedoutputfromthe\n",
            "hiddenlayer,andapplythesigmafunction. Canyoufindvaluesforhand bwhichproduce\n",
            "atower? Thisisabittricky,soifyouthinkaboutthisforawhileandremainstuck,here’s\n",
            "twohints: (1)Togettheoutputneurontoshowtherightkindofif-then-elsebehaviour,\n",
            "weneedtheinputweights(allhor h)tobelarge;and(2)thevalueof bdeterminesthe\n",
            "scaleoftheif-then-elsethreshold−. \n",
            "(cid:12)\n",
            "144 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=0.3,b= 0.5\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=1.0,b= 2.0\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=3.0,b= 5.0\n",
            "−\n",
            "Withourinitialparameters,theoutputlookslikeaflattenedversionoftheearlierdiagram,\n",
            "withitstowerandplateau. Togetthedesiredbehaviour,weincreasetheparameterhuntil\n",
            "itbecomeslarge.\n",
            "Thatgivestheif-then-elsethresholdingbehaviour. Second, togetthe\n",
            "thresholdright,we’llchoose b 3h/2. Tryit,andseehowitworks! Here’swhatitlookslike,w≈he−nweuseh=10:\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 5\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 7\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 12\n",
            "−\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "h=10,b= 15\n",
            "−\n",
            "Evenforthisrelativelymodestvalueofh,wegetaprettygoodtowerfunction. And,of\n",
            "course,wecanmakeitasgoodaswewantbyincreasinghstillfurther,andkeepingthebias\n",
            "as b= 3h/2. Let−’strygluingtwosuchnetworkstogether,inordertocomputetwodifferenttower\n",
            "functions. To make the respective roles of the two sub-networks clear I’ve put them in\n",
            "separateboxes,below: eachboxcomputesatowerfunction,usingthetechniquedescribed\n",
            "above. Thegraphontherightshowstheweightedoutputfromthesecondhiddenlayer,that\n",
            "is,it’saweightedcombinationoftowerfunctions. 1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.8,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "w1=0.2,w2=0.5\n",
            "1\n",
            "y 00 1\n",
            "x\n",
            "tuptuO\n",
            "4\n",
            "w1=0.2,w2=0.9\n",
            "\n",
            "(cid:12)\n",
            "4.3. Manyinputvariables (cid:12) 145\n",
            "(cid:12)\n",
            "Inparticular,youcanseethatbymodifyingtheweightsinthefinallayeryoucanchangethe\n",
            "heightoftheoutputtowers. Thesameideacanbeusedtocomputeasmanytowersaswelike.\n",
            "Wecanalsomake\n",
            "themasthinaswelike,andwhateverheightwelike. Asaresult,wecanensurethatthe\n",
            "weightedoutputfromthesecondhiddenlayerapproximatesanydesiredfunctionoftwo\n",
            "variables:\n",
            "1\n",
            "1\n",
            "y 0 0\n",
            "x\n",
            "tuptuO\n",
            "Manytowers\n",
            "4\n",
            "Inparticular,bymakingtheweightedoutputfromthesecondhiddenlayeragoodapproxi-\n",
            "mationtoσ 1 f,weensuretheoutputfromournetworkwillbeagoodapproximationto\n",
            "−\n",
            "anydesiredfun◦ction, f. Whataboutfunctionsofmorethantwovariables? Let’strythreevariables x ,x ,x . Thefollowingnetworkcanbeusedtocomputea\n",
            "1 2 3\n",
            "towerfunctioninfourdimensions:\n",
            "Here,the x ,x ,x denoteinputstothenetwork. Thes ,t andsoonaresteppointsfor\n",
            "1 2 3 1 1\n",
            "neurons–thatis,alltheweightsinthefirstlayerarelarge,andthebiasesaresettogivethe\n",
            "steppointss\n",
            "1\n",
            ",t\n",
            "1\n",
            ",s\n",
            "2\n",
            ",.... Theweightsinthesecondlayeralternate+h, h,wherehissome\n",
            "verylargenumber.\n",
            "Andtheoutputbiasis 5h/2. −\n",
            "Thisnetworkcomputesafunctionwh−ichis1providedthreeconditionsaremet: x is\n",
            "1\n",
            "betweens andt ; x isbetweens andt ;and x isbetweens andt . Thenetworkis0\n",
            "1 1 2 2 2 3 3 3\n",
            "\n",
            "(cid:12)\n",
            "146 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "everywhereelse. Thatis,it’sakindoftowerwhichis1inalittleregionofinputspace,and\n",
            "0everywhereelse. Bygluingtogethermanysuchnetworkswecangetasmanytowersaswewant,and\n",
            "soapproximateanarbitraryfunctionofthreevariables. Exactlythesameideaworksinm\n",
            "dimensions. Theonlychangeneededistomaketheoutputbias( m+1/2)h,inorderto\n",
            "gettherightkindofsandwichingbehaviortoleveltheplateau. −\n",
            "Okay,sowenowknowhowtouseneuralnetworkstoapproximateareal-valuedfunction\n",
            "ofmanyvariables. Whataboutvector-valuedfunctions f(x 1 ,...,x m) Rn? Ofcourse,sucha\n",
            "functioncanberegardedasjustnseparatereal-valuedfunctions,f1 (x\n",
            "1\n",
            "∈,...,x m),f2 (x\n",
            "1\n",
            ",...,x m),\n",
            "andsoon. Sowecreateanetworkapproximating f1,anothernetworkfor f2,andsoon.\n",
            "4\n",
            "Andthenwesimplyglueallthenetworkstogether.\n",
            "Sothat’salsoeasytocopewith. Problem\n",
            "We’veseenhowtousenetworkswithtwohiddenlayerstoapproximateanarbitrary\n",
            "• function. Canyoufindaproofshowingthatit’spossiblewithjustasinglehidden\n",
            "layer? Asahint,tryworkinginthecaseofjusttwoinputvariables,andshowingthat:\n",
            "(a)it’spossibletogetstepfunctionsnotjustinthexor ydirections,butinanarbitrary\n",
            "direction;(b)byaddingupmanyoftheconstructionsfrompart(a)it’spossibleto\n",
            "approximateatowerfunctionwhichiscircularinshape, ratherthanrectangular;\n",
            "(c)usingthesecirculartowers,it’spossibletoapproximateanarbitraryfunction. To\n",
            "dopart(c)itmayhelptouseideasfromabitlaterinthischapter. 4.4 Extension beyond sigmoid neurons\n",
            "We’veprovedthatnetworksmadeupofsigmoidneuronscancomputeanyfunction. Recall\n",
            "thatinasigmoidneurontheinputsx\n",
            "1\n",
            ",x\n",
            "2\n",
            ",...resultintheoutputσ( (cid:80)\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b),wherew\n",
            "j\n",
            "aretheweights, bisthebias,andσisthesigmoidfunction:\n",
            "Whatifweconsideradifferenttypeofneuron,oneusingsomeotheractivationfunction,\n",
            "s(z):\n",
            "\n",
            "(cid:12)\n",
            "4.4. Extensionbeyondsigmoidneurons (cid:12) 147\n",
            "(cid:12)\n",
            "Thatis,we’llassumethatifourneuronshaveinputs x ,x ,...,weightsw ,w ,...andbias\n",
            "1 2 1 2\n",
            "(cid:80)\n",
            "b,thentheoutputiss(\n",
            "j\n",
            "w\n",
            "j\n",
            "x j+b). Wecanusethisactivationfunctiontogetastepfunction,justaswedidwiththesigmoid. Tryrampinguptheweightinthefollowing,saytow=100:\n",
            "Outputfromneuron\n",
            "1 w=100,b= 3\n",
            "w=15,b= −3\n",
            "w=8,b= −3\n",
            "4\n",
            "w=6,b= −3\n",
            "w=4,b= −3\n",
            "−\n",
            "0 1\n",
            "1 w=6,b= 5\n",
            "w=6,b= −3\n",
            "w=6,b= −1\n",
            "−\n",
            "0 1\n",
            "Justaswiththesigmoid,thiscausestheactivationfunctiontocontract,andultimatelyit\n",
            "becomesaverygoodapproximationtoastepfunction. Trychangingthebias,andyou’llsee\n",
            "thatwecansetthepositionofthesteptobewhereverwechoose.\n",
            "Andsowecanuseallthe\n",
            "sametricksasbeforetocomputeanydesiredfunction. Whatpropertiesdoess(z)needtosatisfyinorderforthistowork? Wedoneedtoassume\n",
            "thats(z)iswell-definedasz andz . Thesetwolimitsarethetwovaluestaken\n",
            "onbyourstepfunction. We→als∞oneedto→as∞sumethattheselimitsaredifferentfromone\n",
            "another.\n",
            "Iftheyweren’t,there’dbenostep,simplyaflatgraph! Butprovidedtheactivation\n",
            "functions(z)satisfiestheseproperties,neuronsbasedonsuchanactivationfunctionare\n",
            "universalforcomputation. Problems\n",
            "Earlierinthebookwemetanothertypeofneuronknownasarectifiedlinearunit. • Explainwhysuchneuronsdon’tsatisfytheconditionsjustgivenforuniversality. Finda\n",
            "proofofuniversalityshowingthatrectifiedlinearunitsareuniversalforcomputation. Supposeweconsiderlinearneurons,i.e.,neuronswiththeactivationfunctions(z)=z. • Explainwhylinearneuronsdon’tsatisfytheconditionsjustgivenforuniversality.Show\n",
            "thatsuchneuronscan’tbeusedtodouniversalcomputation. \n",
            "(cid:12)\n",
            "148 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "4.5 Fixing up the step functions\n",
            "Uptonow,we’vebeenassumingthatourneuronscanproducestepfunctionsexactly. That’s\n",
            "aprettygoodapproximation,butitisonlyanapproximation.\n",
            "Infact,therewillbeanarrow\n",
            "windowoffailure,illustratedinthefollowinggraph,inwhichthefunctionbehavesvery\n",
            "differentlyfromastepfunction:\n",
            "4\n",
            "InthesewindowsoffailuretheexplanationI’vegivenforuniversalitywillfail. Now,it’snotaterriblefailure. Bymakingtheweightsinputtotheneuronsbigenough\n",
            "we can make these windows of failure as small as we like. Certainly, we can make the\n",
            "windowmuchnarrowerthanI’veshownabove–narrower,indeed,thanoureyecouldsee. Soperhapswemightnotworrytoomuchaboutthisproblem.\n",
            "Nonetheless,it’dbenicetohavesomewayofaddressingtheproblem. Infact,theproblemturnsouttobeeasytofix. Let’slookatthefixforneuralnetworks\n",
            "computingfunctionswithjustoneinputandoneoutput. Thesameideasworkalsotoaddress\n",
            "theproblemwhentherearemoreinputsandoutputs. Inparticular,supposewewantournetworktocomputesomefunction, f. Asbefore,we\n",
            "dothisbytryingtodesignournetworksothattheweightedoutputfromourhiddenlayerof\n",
            "neuronsisσ\n",
            "−\n",
            "1 f(x):\n",
            "◦\n",
            "\n",
            "(cid:12)\n",
            "4.5. Fixingupthestepfunctions (cid:12) 149\n",
            "(cid:12)\n",
            "Ifweweretodothisusingthetechniquedescribedearlier,we’dusethehiddenneuronsto\n",
            "produceasequenceofbumpfunctions:\n",
            "4\n",
            "Again,I’veexaggeratedthesizeofthewindowsoffailure,inordertomakethemeasierto\n",
            "see. Itshouldbeprettyclearthatifweaddallthesebumpfunctionsupwe’llendupwitha\n",
            "reasonableapproximationtoσ\n",
            "−\n",
            "1 f(x),exceptwithinthewindowsoffailure. Supposethatinsteadofusing◦theapproximationjustdescribed,weuseasetofhidden\n",
            "neuronstocomputeanapproximationtohalfouroriginalgoalfunction,i.e.,toσ\n",
            "−\n",
            "1 f(x) /2. Ofcourse,thislooksjustlikeascaleddownversionofthelastgraph: ◦\n",
            "Andsupposeweuseanothersetofhiddenneuronstocomputeanapproximationtoσ 1\n",
            "−\n",
            "f(x) /2,butwiththebasesofthebumpsshiftedbyhalfthewidthofabump: ◦\n",
            "Nowwehavetwodifferentapproximationstoσ\n",
            "−\n",
            "1 f(x) /2. Ifweaddupthetwoapproxi-\n",
            "mationswe’llgetanoverallapproximationtoσ\n",
            "−\n",
            "1◦f(x).\n",
            "Thatoverallapproximationwill\n",
            "◦\n",
            "\n",
            "(cid:12)\n",
            "150 (cid:12) Avisualproofthatneuralnetscancomputeanyfunction\n",
            "(cid:12)\n",
            "stillhavefailuresinsmallwindows. Buttheproblemwillbemuchlessthanbefore.\n",
            "The\n",
            "reasonisthatpointsinafailurewindowforoneapproximationwon’tbeinafailurewindow\n",
            "fortheother. Andsotheapproximationwillbeafactorroughly2betterinthosewindows. Wecoulddoevenbetterbyaddingupalargenumber,M,ofoverlappingapproximations\n",
            "tothefunctionσ\n",
            "−\n",
            "1 f(x) /M. Providedthewindowsoffailurearenarrowenough,apoint\n",
            "willonlyeverbeino◦newindowoffailure.\n",
            "Andprovidedwe’reusingalargeenoughnumber\n",
            "M ofoverlappingapproximations,theresultwillbeanexcellentoverallapproximation. Conclusion\n",
            "4\n",
            "Theexplanationforuniversalitywe’vediscussediscertainlynotapracticalprescriptionfor\n",
            "howtocomputeusingneuralnetworks! Inthis,it’smuchlikeproofsofuniversalityforNAND\n",
            "gatesandthelike. Forthisreason,I’vefocusedmostlyontryingtomaketheconstruction\n",
            "clearandeasytofollow,andnotonoptimizingthedetailsoftheconstruction. However,you\n",
            "mayfinditafunandinstructiveexercisetoseeifyoucanimprovetheconstruction. Althoughtheresultisn’tdirectlyusefulinconstructingnetworks,it’simportantbecause\n",
            "ittakesoffthetablethequestionofwhetheranyparticularfunctioniscomputableusinga\n",
            "neuralnetwork. Theanswertothatquestionisalways“yes”. Sotherightquestiontoaskis\n",
            "notwhetheranyparticularfunctioniscomputable,butratherwhat’sagoodwaytocompute\n",
            "thefunction. Theuniversalityconstructionwe’vedevelopedusesjusttwohiddenlayerstocomputean\n",
            "arbitraryfunction. Furthermore,aswe’vediscussed,it’spossibletogetthesameresultwith\n",
            "justasinglehiddenlayer. Giventhis,youmightwonderwhywewouldeverbeinterested\n",
            "indeepnetworks,i.e.,networkswithmanyhiddenlayers. Can’twesimplyreplacethose\n",
            "networkswithshallow,singlehiddenlayernetworks? Whileinprinciplethat’spossible,therearegoodpracticalreasonstousedeepnetworks.\n",
            "AsarguedinChapter1,deepnetworkshaveahierarchicalstructurewhichmakesthem\n",
            "particularlywelladaptedtolearnthehierarchiesofknowledgethatseemtobeusefulin\n",
            "solvingreal-worldproblems. Putmoreconcretely,whenattackingproblemssuchasimage\n",
            "recognition,ithelpstouseasystemthatunderstandsnotjustindividualpixels,butalso\n",
            "increasinglymorecomplexconcepts: fromedgestosimplegeometricshapes,alltheway\n",
            "upthroughcomplex,multi-objectscenes. Inlaterchapters,we’llseeevidencesuggesting\n",
            "thatdeepnetworksdoabetterjobthanshallownetworksatlearningsuchhierarchiesof\n",
            "knowledge. Tosumup: universalitytellsusthatneuralnetworkscancomputeanyfunction;\n",
            "andempiricalevidencesuggeststhatdeepnetworksarethenetworksbestadaptedtolearn\n",
            "thefunctionsusefulinsolvingmanyreal-worldproblems. 7Chapteracknowledgments:ThankstoJenDoddandChrisOlahformanydiscussionsaboutuniver-\n",
            "salityinneuralnetworks.Mythanks,inparticular,toChrisforsuggestingtheuseofalookuptableto\n",
            "proveuniversality.Theinteractivevisualformofthechapterisinspiredbytheworkofpeoplesuchas\n",
            "MikeBostock,AmitPatel,BretVictor,andStevenWittens. \n",
            "(cid:12)\n",
            "(cid:12) 151\n",
            "(cid:12)\n",
            "5555\n",
            "Why are deep neural networks\n",
            "hard to train? 5\n",
            "Imagineyou’reanengineerwhohasbeenaskedtodesignacomputerfromscratch.\n",
            "Oneday\n",
            "you’reworkingawayinyouroffice,designinglogicalcircuits,settingoutANDgates,ORgates,\n",
            "andsoon,whenyourbosswalksinwithbadnews. Thecustomerhasjustaddedasurprising\n",
            "designrequirement: thecircuitfortheentirecomputermustbejusttwolayersdeep:\n",
            "You’redumbfounded,andtellyourboss: “Thecustomeriscrazy!”\n",
            "Yourbossreplies: “Ithinkthey’recrazy,too. Butwhatthecustomerwants,theyget.”\n",
            "Infact,there’salimitedsenseinwhichthecustomerisn’tcrazy. Supposeyou’reallowed\n",
            "touseaspeciallogicalgatewhichletsyouANDtogetherasmanyinputsasyouwant. And\n",
            "you’realsoallowedamany-inputNANDgate,thatis,agatewhichcanANDmultipleinputs\n",
            "andthennegatetheoutput. Withthesespecialgatesitturnsouttobepossibletocompute\n",
            "anyfunctionatallusingacircuitthat’sjusttwolayersdeep. Butjustbecausesomethingispossibledoesn’tmakeitagoodidea. Inpractice,when\n",
            "solvingcircuitdesignproblems(ormostanykindofalgorithmicproblem),weusuallystart\n",
            "\n",
            "(cid:12)\n",
            "152 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "byfiguringouthowtosolvesub-problems,andthengraduallyintegratethesolutions. In\n",
            "otherwords,webuilduptoasolutionthroughmultiplelayersofabstraction. Forinstance,supposewe’redesigningalogicalcircuittomultiplytwonumbers. Chances\n",
            "arewewanttobuilditupoutofsub-circuitsdoingoperationslikeaddingtwonumbers. Thesub-circuitsforaddingtwonumberswill,inturn,bebuiltupoutofsub-sub-circuitsfor\n",
            "addingtwobits. Veryroughlyspeakingourcircuitwilllooklike:\n",
            "5\n",
            "Thatis,ourfinalcircuitcontainsatleastthreelayersofcircuitelements. Infact,it’llprobably\n",
            "containmorethanthreelayers,aswebreakthesub-tasksdownintosmallerunitsthanI’ve\n",
            "described. Butyougetthegeneralidea.\n",
            "Sodeepcircuitsmaketheprocessofdesigneasier.\n",
            "Butthey’renotjusthelpfulfordesign. Thereare,infact,mathematicalproofsshowingthatforsomefunctionsveryshallowcircuits\n",
            "requireexponentiallymorecircuitelementstocomputethandodeepcircuits. Forinstance,\n",
            "afamousseriesofpapersintheearly1980s1showedthatcomputingtheparityofasetof\n",
            "bitsrequiresexponentiallymanygates,ifdonewithashallowcircuit. Ontheotherhand,if\n",
            "youusedeepercircuitsit’seasytocomputetheparityusingasmallcircuit: youjustcompute\n",
            "theparityofpairsofbits,thenusethoseresultstocomputetheparityofpairsofpairsofbits,\n",
            "andsoon,buildingupquicklytotheoverallparity. Deepcircuitsthuscanbeintrinsically\n",
            "muchmorepowerfulthanshallowcircuits. Uptonow,thisbookhasapproachedneuralnetworkslikethecrazycustomer. Almostall\n",
            "thenetworkswe’veworkedwithhavejustasinglehiddenlayerofneurons(plustheinput\n",
            "andoutputlayers):\n",
            "1Thehistoryissomewhatcomplex,soIwon’tgivedetailedreferences.SeeJohanHåstad’s2012paper\n",
            "Onthecorrelationofparityandsmall-depthcircuitsforanaccountoftheearlyhistoryandreferences. \n",
            "(cid:12)\n",
            "(cid:12) 153\n",
            "(cid:12)\n",
            "5\n",
            "Thesesimplenetworkshavebeenremarkablyuseful: inearlierchaptersweusednetworks\n",
            "likethistoclassifyhandwrittendigitswithbetterthan98percentaccuracy! Nonetheless,\n",
            "intuitivelywe’dexpectnetworkswithmanymorehiddenlayerstobemorepowerful:\n",
            "Suchnetworkscouldusetheintermediatelayerstobuildupmultiplelayersofabstraction,\n",
            "justaswedoinBooleancircuits. Forinstance,ifwe’redoingvisualpatternrecognition,\n",
            "thentheneuronsinthefirstlayermightlearntorecognizeedges,theneuronsinthesecond\n",
            "layercouldlearntorecognizemorecomplexshapes,saytriangleorrectangles,builtupfrom\n",
            "edges. Thethirdlayerwouldthenrecognizestillmorecomplexshapes.\n",
            "Andsoon. These\n",
            "multiplelayersofabstractionseemlikelytogivedeepnetworksacompellingadvantagein\n",
            "learningtosolvecomplexpatternrecognitionproblems. Moreover,justasinthecaseof\n",
            "circuits,therearetheoreticalresultssuggestingthatdeepnetworksareintrinsicallymore\n",
            "powerfulthanshallownetworks2. 2ForcertainproblemsandnetworkarchitecturesthisisprovedinOnthenumberofresponseregionsof\n",
            "deepfeedforwardnetworkswithpiece-wiselinearactivations,byRazvanPascanu,GuidoMontúfar,and\n",
            "YoshuaBengio(2014).Seealsothemoreinformaldiscussioninsection2ofLearningdeeparchitectures\n",
            "forAI,byYoshuaBengio(2009). \n",
            "(cid:12)\n",
            "154 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "Howcanwetrainsuchdeepnetworks? Inthischapter,we’lltrytrainingdeepnetworks\n",
            "usingourworkhorselearningalgorithm–stochasticgradientdescentbybackpropagation. Butwe’llrunintotrouble,withourdeepnetworksnotperformingmuch(ifatall)better\n",
            "thanshallownetworks. Thatfailureseemssurprisinginthelightofthediscussionabove. Ratherthangiveupon\n",
            "deepnetworks,we’lldigdownandtrytounderstandwhat’smakingourdeepnetworkshard\n",
            "totrain. Whenwelookclosely,we’lldiscoverthatthedifferentlayersinourdeepnetwork\n",
            "arelearningatvastlydifferentspeeds. Inparticular,whenlaterlayersinthenetworkare\n",
            "learningwell,earlylayersoftengetstuckduringtraining,learningalmostnothingatall. This\n",
            "stucknessisn’tsimplyduetobadluck. Rather,we’lldiscovertherearefundamentalreasons\n",
            "thelearningslowdownoccurs,connectedtoouruseofgradient-basedlearningtechniques. Aswedelveintotheproblemmoredeeply,we’lllearnthattheoppositephenomenon\n",
            "canalsooccur: theearlylayersmaybelearningwell,butlaterlayerscanbecomestuck. In\n",
            "5\n",
            "fact,we’llfindthatthere’sanintrinsicinstabilityassociatedtolearningbygradientdescent\n",
            "indeep,many-layerneuralnetworks. Thisinstabilitytendstoresultineithertheearlyor\n",
            "thelaterlayersgettingstuckduringtraining. Thisallsoundslikebadnews. Butbydelvingintothesedifficulties,wecanbegintogain\n",
            "insightintowhat’srequiredtotraindeepnetworkseffectively. Andsotheseinvestigations\n",
            "aregoodpreparationforthenextchapter,wherewe’llusedeeplearningtoattackimage\n",
            "recognitionproblems. 5.1 The vanishing gradient problem\n",
            "So,whatgoeswrongwhenwetrytotrainadeepnetwork? Toanswerthatquestion,let’sfirstrevisitthecaseofanetworkwithjustasinglehidden\n",
            "layer. Asperusual,we’llusetheMNISTdigitclassificationproblemasourplaygroundfor\n",
            "learningandexperimentation3. Ifyouwish,youcanfollowalongbytrainingnetworksonyourcomputer.\n",
            "Itisalso,of\n",
            "course,finetojustreadalong. Ifyoudowishtofollowlive,thenyou’llneedPython2.7,\n",
            "Numpy,andacopyofthecode,whichyoucangetbycloningtherelevantrepositoryfrom\n",
            "thecommandline:\n",
            "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
            "Ifyoudon’tusegitthenyoucandownloadthedataandcodehere. You’llneedtochange\n",
            "intothesrcsubdirectory.\n",
            "Then,fromaPythonshellweloadtheMNISTdata:\n",
            ">>> import mnist_loader\n",
            ">>> training_data, validation_data, test_data = \\\n",
            "... mnist_loader.load_data_wrapper()\n",
            "Wesetupournetwork:\n",
            ">>> import network2\n",
            ">>> net = network2.Network([784, 30, 10])\n",
            "3IintroducedtheMNISTproblemanddatahere1.5andhere1.6. \n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 155\n",
            "(cid:12)\n",
            "Thisnetworkhas784neuronsintheinputlayer,correspondingtothe28 28=784pixels\n",
            "intheinputimage. Weuse30hiddenneurons,aswellas10outputneuro×ns,corresponding\n",
            "tothe10possibleclassificationsfortheMNISTdigits(‘0’,‘1’,‘2’,...,‘9’). Let’strytrainingournetworkfor30completeepochs,usingmini-batchesof10training\n",
            "examplesatatime,alearningrateη =0.1,andregularizationparameterλ =5.0. Aswe\n",
            "trainwe’llmonitortheclassificationaccuracyonthevalidation_data4:\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Wegetaclassificationaccuracyof96.48percent(orthereabouts–it’llvaryabitfromrunto\n",
            "run),comparabletoourearlierresultswithasimilarconfiguration. Now,let’saddanotherhiddenlayer,alsowith30neuronsinit,andtrytrainingwiththe\n",
            "samehyper-parameters:\n",
            "5\n",
            ">>> net = network2.Network([784, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thisgivesanimprovedclassificationaccuracy,96.90percent. That’sencouraging: alittle\n",
            "moredepthishelping. Let’saddanother30-neuronhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Thatdoesn’thelpatall. Infact,theresultdropsbackdownto96.57percent,closetoour\n",
            "originalshallownetwork. Andsupposeweinsertonefurtherhiddenlayer:\n",
            ">>> net = network2.Network([784, 30, 30, 30, 30, 10])\n",
            ">>> net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,\n",
            "... evaluation_data=validation_data, monitor_evaluation_accuracy=True)\n",
            "Theclassificationaccuracydropsagain,to96.53percent. That’sprobablynotastatistically\n",
            "significantdrop,butit’snotencouraging,either.\n",
            "Thisbehaviourseemsstrange. Intuitively,extrahiddenlayersoughttomakethenetwork\n",
            "able to learn more complex classification functions, and thus do a better job classifying. Certainly,thingsshouldn’tgetworse,sincetheextralayerscan,intheworstcase,simplydo\n",
            "nothing5. Butthat’snotwhat’sgoingon.\n",
            "So what is going on? Let’s assume that the extra hidden layers really could help in\n",
            "principle,andtheproblemisthatourlearningalgorithmisn’tfindingtherightweightsand\n",
            "biases. We’dliketofigureoutwhat’sgoingwronginourlearningalgorithm,andhowtodo\n",
            "better. Togetsomeinsightintowhat’sgoingwrong, let’svisualizehowthenetworklearns. Below,I’veplottedpartofa[784,30,30,10]network,i.e.,anetworkwithtwohiddenlayers,\n",
            "each containing 30 hidden neurons. Each neuron in the diagram has a little bar on it,\n",
            "4Notethatthenetworksislikelytotakesomeminutestotrain,dependingonthespeedofyour\n",
            "machine.Soifyou’rerunningthecodeyoumaywishtocontinuereadingandreturnlater,notwaitfor\n",
            "thecodetofinishexecuting. 5Seethislaterproblem5.2tounderstandhowtobuildahiddenlayerthatdoesnothing.\n",
            "\n",
            "(cid:12)\n",
            "156 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "representinghowquicklythatneuronischangingasthenetworklearns.\n",
            "Abigbarmeans\n",
            "theneuron’sweightsandbiasarechangingrapidly,whileasmallbarmeanstheweights\n",
            "andbiasarechangingslowly. Moreprecisely,thebarsdenotethegradient∂C/∂bforeach\n",
            "neuron,i.e.,therateofchangeofthecostwithrespecttotheneuron’sbias. BackinChapter\n",
            "2wesawthatthisgradientquantitycontrollednotjusthowrapidlythebiaschangesduring\n",
            "learning,butalsohowrapidlytheweightsinputtotheneuronchange,too. Don’tworryif\n",
            "youdon’trecallthedetails: thethingtokeepinmindissimplythatthesebarsshowhow\n",
            "quicklyeachneuron’sweightsandbiasarechangingasthenetworklearns. Tokeepthediagramsimple,I’veshownjustthetopsixneuronsinthetwohiddenlayers. I’veomittedtheinputneurons,sincethey’vegotnoweightsorbiasestolearn. I’vealso\n",
            "omittedtheoutputneurons,sincewe’redoinglayer-wisecomparisons,anditmakesmost\n",
            "sensetocomparelayerswiththesamenumberofneurons. Theresultsareplottedatthe\n",
            "verybeginningoftraining,i.e.,immediatelyafterthenetworkisinitialized. Heretheyare6:\n",
            "5\n",
            "Thenetworkwasinitializedrandomly,andsoit’snotsurprisingthatthere’salotofvariation\n",
            "inhowrapidlytheneuronslearn. Still,onethingthatjumpsoutisthatthebarsinthesecond\n",
            "hiddenlayeraremostlymuchlargerthanthebarsinthefirsthiddenlayer. Asaresult,the\n",
            "neuronsinthesecondhiddenlayerwilllearnquiteabitfasterthantheneuronsinthefirst\n",
            "hiddenlayer. Isthismerelyacoincidence,oraretheneuronsinthesecondhiddenlayer\n",
            "likelytolearnfasterthanneuronsinthefirsthiddenlayeringeneral? Todeterminewhetherthisisthecase,ithelpstohaveaglobalwayofcomparingthe\n",
            "speedoflearninginthefirstandsecondhiddenlayers. Todothis,let’sdenotethegradient\n",
            "asδl j= ∂C/∂bl j ,i.e.,thegradientforthe j-thneuroninthel-thlayer7Wecanthinkofthe\n",
            "gradientδ1asavectorwhoseentriesdeterminehowquicklythefirsthiddenlayerlearns,\n",
            "andδ2 asavectorwhoseentriesdeterminehowquicklythesecondhiddenlayerlearns. 6Thedataplottedisgeneratedusingtheprogramgenerate_gradient.py.Thesameprogramis\n",
            "alsousedtogeneratetheresultsquotedlaterinthissection. 7BackinChapter2wereferredtothisastheerror,butherewe’lladopttheinformalterm“gradient”. Isay“informal”becauseofcoursethisdoesn’texplicitlyincludethepartialderivativesofthecostwith\n",
            "respecttotheweights,∂C/∂w.\n",
            "\n",
            "(cid:12)\n",
            "5.1. Thevanishinggradientproblem (cid:12) 157\n",
            "(cid:12)\n",
            "We’llthenusethelengthsofthesevectorsas(rough!) globalmeasuresofthespeedatwhich\n",
            "thelayersarelearning. So,forinstance,thelength δ1 measuresthespeedatwhichthe\n",
            "firsthiddenlayerislearning,whilethelength δ2 m(cid:107)eas(cid:107)uresthespeedatwhichthesecond\n",
            "hiddenlayerislearning.\n",
            "(cid:107) (cid:107)\n",
            "Withthesedefinitions, andinthesameconfigurationaswasplottedabove, wefind\n",
            "δ1 =0.07...and δ2 =0.31....\n",
            "Sothisconfirmsourearliersuspicion: theneuronsin\n",
            "(cid:107)the(cid:107)secondhiddenla(cid:107)yer(cid:107)reallyarelearningmuchfasterthantheneuronsinthefirsthidden\n",
            "layer. What happens if we add more hidden layers? If we have three hidden layers, in a\n",
            "[784,30,30,30,10]network, thentherespectivespeedsoflearningturnouttobe0.012,\n",
            "0.060,and0.283. Again,earlierhiddenlayersarelearningmuchslowerthanlaterhidden\n",
            "layers. Supposeweaddyetanotherlayerwith30hiddenneurons. Inthatcase,therespective\n",
            "speedsoflearningare0.003,0.017,0.070,and0.285. Thepatternholds: earlylayerslearn\n",
            "5\n",
            "slowerthanlaterlayers. We’vebeenlookingatthespeedoflearningatthestartoftraining,thatis,justafterthe\n",
            "networksareinitialized.\n",
            "Howdoesthespeedoflearningchangeaswetrainournetworks? Let’sreturntolookatthenetworkwithjusttwohiddenlayers. Thespeedoflearningchanges\n",
            "asfollows:\n",
            "Togeneratetheseresults, Iusedbatchgradientdescentwithjust1,000trainingimages,\n",
            "trainedover500epochs. Thisisabitdifferentthanthewayweusuallytrain–I’veusedno\n",
            "mini-batches,andjust1,000trainingimages,ratherthanthefull50,000imagetrainingset. I’mnottryingtodoanythingsneaky,orpullthewooloveryoureyes,butitturnsoutthat\n",
            "usingmini-batchstochasticgradientdescentgivesmuchnoisier(albeitverysimilar,when\n",
            "youaverageawaythenoise)results. UsingtheparametersI’vechosenisaneasywayof\n",
            "smoothingtheresultsout,sowecanseewhat’sgoingon. Inanycase,asyoucanseethetwolayersstartoutlearningatverydifferentspeeds(as\n",
            "wealreadyknow). Thespeedinbothlayersthendropsveryquickly,beforerebounding. But\n",
            "throughitall,thefirsthiddenlayerlearnsmuchmoreslowlythanthesecondhiddenlayer. Whataboutmorecomplexnetworks? Here’stheresultsofasimilarexperiment,butthis\n",
            "timewiththreehiddenlayers(a[784,30,30,30,10]network):\n",
            "\n",
            "(cid:12)\n",
            "158 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Finally,let’sadd\n",
            "afourthhiddenlayer(a[784,30,30,30,30,10]network),andseewhathappenswhenwe\n",
            "train:\n",
            "Again,earlyhiddenlayerslearnmuchmoreslowlythanlaterhiddenlayers. Inthiscase,\n",
            "thefirsthiddenlayerislearningroughly100timesslowerthanthefinalhiddenlayer.\n",
            "No\n",
            "wonderwewerehavingtroubletrainingthesenetworksearlier! Wehavehereanimportantobservation: inatleastsomedeepneuralnetworks,the\n",
            "gradienttendstogetsmalleraswemovebackwardthroughthehiddenlayers. Thismeans\n",
            "thatneuronsintheearlierlayerslearnmuchmoreslowlythanneuronsinlaterlayers. And\n",
            "while we’ve seen this in just a single network, there are fundamental reasons why this\n",
            "happensinmanyneuralnetworks. Thephenomenonisknownasthevanishinggradient\n",
            "\n",
            "(cid:12)\n",
            "5.2. What’scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 159\n",
            "(cid:12)\n",
            "problem8. Whydoesthevanishinggradientproblemoccur? Aretherewayswecanavoidit?\n",
            "And\n",
            "howshouldwedealwithitintrainingdeepneuralnetworks? Infact,we’lllearnshortly\n",
            "thatit’snotinevitable,althoughthealternativeisnotveryattractive,either: sometimesthe\n",
            "gradientgetsmuchlargerinearlierlayers! Thisistheexplodinggradientproblem,andit’s\n",
            "notmuchbetternewsthanthevanishinggradientproblem. Moregenerally,itturnsout\n",
            "thatthegradientindeepneuralnetworksisunstable,tendingtoeitherexplodeorvanish\n",
            "inearlierlayers. Thisinstabilityisafundamentalproblemforgradient-basedlearningin\n",
            "deepneuralnetworks. It’ssomethingweneedtounderstand,and,ifpossible,takestepsto\n",
            "address. Oneresponsetovanishing(orunstable)gradientsistowonderifthey’rereallysuch\n",
            "aproblem. Momentarilysteppingawayfromneuralnets,imagineweweretryingtonu-\n",
            "mericallyminimizeafunction f(x)ofasinglevariable. Wouldn’titbegoodnewsifthe\n",
            "5\n",
            "derivative f (cid:48)(x)wassmall? Wouldn’tthatmeanwewerealreadynearanextremum? Ina\n",
            "similarway,mightthesmallgradientinearlylayersofadeepnetworkmeanthatwedon’t\n",
            "needtodomuchadjustmentoftheweightsandbiases? Ofcourse,thisisn’tthecase. Recallthatwerandomlyinitializedtheweightandbiases\n",
            "inthenetwork. Itisextremelyunlikelyourinitialweightsandbiaseswilldoagoodjobat\n",
            "whateveritiswewantournetworktodo. Tobeconcrete,considerthefirstlayerofweights\n",
            "ina[784,30,30,30,10]networkfortheMNISTproblem. Therandominitializationmeans\n",
            "thefirstlayerthrowsawaymostinformationabouttheinputimage. Eveniflaterlayershave\n",
            "beenextensivelytrained,theywillstillfinditextremelydifficulttoidentifytheinputimage,\n",
            "simplybecausetheydon’thaveenoughinformation. Andsoitcan’tpossiblybethecasethat\n",
            "notmuchlearningneedstobedoneinthefirstlayer.\n",
            "Ifwe’regoingtotraindeepnetworks,\n",
            "weneedtofigureouthowtoaddressthevanishinggradientproblem. 5.2 What’s causing the vanishing gradient problem? Unstable\n",
            "gradients in deep neural nets\n",
            "Togetinsightintowhythevanishinggradientproblemoccurs,let’sconsiderthesimplest\n",
            "deepneuralnetwork: onewithjustasingleneuronineachlayer. Here’sanetworkwith\n",
            "threehiddenlayers:\n",
            "Here,w ,w ,...aretheweights, b ,b ,...arethebiases,andC issomecostfunction. Just\n",
            "1 2 1 2\n",
            "toremindyouhowthisworks,theoutputa\n",
            "j\n",
            "fromthe j-thneuronisσ (z j),whereσisthe\n",
            "usualsigmoidactivationfunction,andz j=w\n",
            "j\n",
            "a\n",
            "j\n",
            "1+b\n",
            "j\n",
            "istheweightedinputtotheneuron. I’vedrawnthecostC attheendtoemphasizet−hatthecostisafunctionofthenetwork’s\n",
            "output,a : iftheactualoutputfromthenetworkisclosetothedesiredoutput,thenthecost\n",
            "4\n",
            "willbelow,whileifit’sfaraway,thecostwillbehigh. 8SeeGradientflowinrecurrentnets: thedifficultyoflearninglong-termdependencies,bySepp\n",
            "Hochreiter,YoshuaBengio,PaoloFrasconi,andJürgenSchmidhuber(2001).Thispaperstudiedrecurrent\n",
            "neuralnets,buttheessentialphenomenonisthesameasinthefeedforwardnetworkswearestudying. SeealsoSeppHochreiter’searlierDiplomaThesis,UntersuchungenzudynamischenneuronalenNetzen\n",
            "(1991,inGerman).\n",
            "\n",
            "(cid:12)\n",
            "160 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "We’regoingtostudythegradient∂C/∂b associatedtothefirsthiddenneuron. We’ll\n",
            "1\n",
            "figureoutanexpressionfor∂C/∂b ,andbystudyingthatexpressionwe’llunderstandwhy\n",
            "1\n",
            "thevanishinggradientproblemoccurs. I’llstartbysimplyshowingyoutheexpressionfor∂C/∂b .\n",
            "Itlooksforbidding, but\n",
            "1\n",
            "it’sactuallygotasimplestructure,whichI’lldescribeinamoment. Here’stheexpression\n",
            "(ignorethenetwork,fornow,andnotethatσ isjustthederivativeoftheσfunction):\n",
            "(cid:48)\n",
            "Thestructureintheexpressionisasfollows: thereisaσ (cid:48)(z j)termintheproductforeach\n",
            "5 neuroninthenetwork;aweightw termforeachweightinthenetwork;andafinal∂C/∂a\n",
            "j 4\n",
            "term,correspondingtothecostfunctionattheend. NoticethatI’veplacedeachterminthe\n",
            "expressionabovethecorrespondingpartofthenetwork.\n",
            "Sothenetworkitselfisamnemonic\n",
            "fortheexpression. You’rewelcometotakethisexpressionforgranted,andskiptothediscussionofhowit\n",
            "relatestothevanishinggradientproblem. There’snoharmindoingthis,sincetheexpression\n",
            "is a special case of our earlier discussion of backpropagation. But there’s also a simple\n",
            "explanationofwhytheexpressionistrue,andsoit’sfun(andperhapsenlightening)totake\n",
            "alookatthatexplanation. Imaginewemakeasmallchange∆b inthebias b .\n",
            "Thatwillsetoffacascadingseries\n",
            "1 1\n",
            "ofchangesintherestofthenetwork. First,itcausesachange∆a intheoutputfromthe\n",
            "1\n",
            "firsthiddenneuron. That,inturn,willcauseachange∆z intheweightedinputtothe\n",
            "2\n",
            "secondhiddenneuron. Thenachange∆a intheoutputfromthesecondhiddenneuron. 2\n",
            "Andsoon,allthewaythroughtoachange∆C inthecostattheoutput. Wehave\n",
            "∂C ∆C\n",
            ". (5.1)\n",
            "∂b\n",
            "1 ≈\n",
            "∆b\n",
            "1\n",
            "Thissuggeststhatwecanfigureoutanexpressionforthegradient∂C/∂b bycarefully\n",
            "1\n",
            "trackingtheeffectofeachstepinthiscascade. Todothis,let’sthinkabouthow∆b causestheoutputa fromthefirsthiddenneuron\n",
            "1 1\n",
            "tochange. Wehavea 1= σ (z 1)= σ (w 1 a 0+b 1),so\n",
            "∆a 1 ≈ ∂σ (w ∂ 1 a b 0 1 +b 1)∆b 1= σ (cid:48)(z 1) ∆b 1 . (5.2)\n",
            "Thatσ (cid:48)(z 1)termshouldlookfamiliar: it’sthefirstterminourclaimedexpressionforthe\n",
            "gradient∂C/∂b . Intuitively,thistermconvertsachange∆b inthebiasintoachange∆a\n",
            "1 1 1\n",
            "intheoutputactivation. Thatchange∆a inturncausesachangeintheweightedinput\n",
            "1\n",
            "z 2=w\n",
            "2\n",
            "a 1+b\n",
            "2\n",
            "tothesecondhiddenneuron:\n",
            "∂z\n",
            "∆z\n",
            "2\n",
            "≈\n",
            "∂a\n",
            "2\n",
            "1\n",
            "∆a 1=w\n",
            "2\n",
            "∆a\n",
            "1\n",
            ". (5.3)\n",
            "Combiningourexpressionsfor∆z and∆a ,weseehowthechangeinthebiasb propagates\n",
            "2 1 1\n",
            "\n",
            "(cid:12)\n",
            "5.2. What’scausingthevanishinggradientproblem? Unstablegradientsindeepneuralnets (cid:12) 161\n",
            "(cid:12)\n",
            "alongthenetworktoaffectz :\n",
            "2\n",
            "∆z\n",
            "2\n",
            "σ (cid:48)(z 1)w\n",
            "2\n",
            "∆b\n",
            "1\n",
            ". (5.4)\n",
            "≈\n",
            "Again,thatshouldlookfamiliar: we’venowgotthefirsttwotermsinourclaimedexpression\n",
            "forthegradient∂C/∂b .\n",
            "1\n",
            "Wecankeepgoinginthisfashion,trackingthewaychangespropagatethroughtherest\n",
            "ofthenetwork. Ateachneuronwepickupaσ (cid:48)(z j)term,andthrougheachweightwepick\n",
            "upaw term. Theendresultisanexpressionrelatingthefinalchange∆C incosttothe\n",
            "j\n",
            "initialchange∆b inthebias:\n",
            "1\n",
            "∂C\n",
            "∆C\n",
            "≈\n",
            "σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)...σ (cid:48)(z 4)∂a\n",
            "4\n",
            "∆b 1 . (5.5)\n",
            "5\n",
            "Dividingby∆b wedoindeedgetthedesiredexpressionforthegradient:\n",
            "1\n",
            "∂C ∂C\n",
            "∂b = σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)...σ (cid:48)(z 4)∂a . (5.6)\n",
            "1 4\n",
            "Whythevanishinggradientproblemoccurs: Tounderstandwhythevanishinggradi-\n",
            "entproblemoccurs,let’sexplicitlywriteouttheentireexpressionforthegradient:\n",
            "∂C ∂C\n",
            "∂b = σ (cid:48)(z 1)w 2 σ (cid:48)(z 2)w 3 σ (cid:48)(z 3)w 4 σ (cid:48)(z 4)∂a . (5.7)\n",
            "1 4\n",
            "Exceptingtheverylastterm,thisexpressionisaproductoftermsoftheformw\n",
            "j\n",
            "σ (cid:48)(z j). To\n",
            "understandhoweachofthosetermsbehave,let’slookataplotofthefunctionσ:\n",
            "(cid:48)\n",
            "Derivativeofsigmoidfunction\n",
            "0.2\n",
            "0.1\n",
            "0\n",
            "4 2 0 2 4\n",
            "− −\n",
            "Thederivativereachesamaximumatσ (cid:48)(0)=1/4. Now,ifweuseourstandardapproachto\n",
            "initializingtheweightsinthenetwork,thenwe’llchoosetheweightsusingaGaussianwith\n",
            "mean0andstandarddeviation1. Sotheweightswillusuallysatisfy w <1. Puttingthese\n",
            "j\n",
            "observationstogether,weseethatthetermsw j σ (cid:48)(z j)willusuallysa|tis|fy w j σ (cid:48)(z j) <1/4. Andwhenwetakeaproductofmanysuchterms,theproductwilltend|toexpone|ntially\n",
            "decrease: themoreterms,thesmallertheproductwillbe.\n",
            "Thisisstartingtosmelllikea\n",
            "possibleexplanationforthevanishinggradientproblem. To make this all a bit more explicit, let’s compare the expression for ∂C/∂b to an\n",
            "1\n",
            "expressionforthegradientwithrespecttoalaterbias,say∂C/∂b . Ofcourse,wehaven’t\n",
            "3\n",
            "\n",
            "(cid:12)\n",
            "162 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "explicitlyworkedoutanexpressionfor∂C/∂b ,butitfollowsthesamepatterndescribed\n",
            "3\n",
            "abovefor∂C/∂b . Here’sthecomparisonofthetwoexpressions:\n",
            "1\n",
            "5\n",
            "Thetwoexpressionssharemanyterms. Butthegradient∂C/∂b includestwoextraterms\n",
            "1\n",
            "eachoftheformw\n",
            "j\n",
            "σ (cid:48)(z j). Aswe’veseen,suchtermsaretypicallylessthan1/4inmagnitude. Andsothegradient∂C/∂b willusuallybeafactorof16(ormore)smallerthan∂C/∂b . 1 3\n",
            "Thisistheessentialoriginofthevanishinggradientproblem.\n",
            "Ofcourse,thisisaninformalargument,notarigorousproofthatthevanishinggradient\n",
            "problemwilloccur. Thereareseveralpossibleescapeclauses.\n",
            "Inparticular,wemightwonder\n",
            "whethertheweightsw j couldgrowduringtraining. Iftheydo,it’spossiblethetermsw j σ (cid:48)(z j)\n",
            "intheproductwillnolongersatisfy w j σ (cid:48)(z j) <1/4. Indeed,ifthetermsgetlargeenough\n",
            "–greaterthan1–thenwewillnolo|ngerhav|eavanishinggradientproblem. Instead,the\n",
            "gradientwillactuallygrowexponentiallyaswemovebackwardthroughthelayers. Instead\n",
            "ofavanishinggradientproblem,we’llhaveanexplodinggradientproblem. Theexplodinggradientproblem: Let’slookatanexplicitexamplewhereexploding\n",
            "gradientsoccur. Theexampleissomewhatcontrived: I’mgoingtofixparametersinthe\n",
            "networkinjusttherightwaytoensurewegetanexplodinggradient. Buteventhoughthe\n",
            "exampleiscontrived,ithasthevirtueoffirmlyestablishingthatexplodinggradientsaren’t\n",
            "merelyahypotheticalpossibility,theyreallycanhappen. Therearetwostepstogettinganexplodinggradient. First,wechoosealltheweights\n",
            "inthenetworktobelarge,sayw 1=w 2=w 3=w 4=100. Second,we’llchoosethebiases\n",
            "sothattheσ (cid:48)(z j)termsarenottoosmall. That’sactuallyprettyeasytodo: allweneed\n",
            "doischoosethebiasestoensurethattheweightedinputtoeachneuronisz j=0(andso\n",
            "σ (cid:48)(z j)=1/4). So,forinstance,wewantz 1=w\n",
            "1\n",
            "a 0+b 1=0. Wecanachievethisbysetting\n",
            "b 1= 100 a 0 . Wecanusethesameideatoselecttheotherbiases. Whenwedothis,we\n",
            "seeth−atall×thetermsw\n",
            "j\n",
            "σ (cid:48)(z j)areequalto100 1/4=25.\n",
            "Withthesechoiceswegetan\n",
            "explodinggradient. ×\n",
            "The unstable gradient problem: The fundamental problem here isn’t so much the\n",
            "vanishinggradientproblemortheexplodinggradientproblem. It’sthatthegradientin\n",
            "earlylayersistheproductoftermsfromallthelaterlayers.\n",
            "Whentherearemanylayers,\n",
            "that’sanintrinsicallyunstablesituation. Theonlywayalllayerscanlearnatclosetothe\n",
            "samespeedisifallthoseproductsoftermscomeclosetobalancingout. Withoutsome\n",
            "mechanismorunderlyingreasonforthatbalancingtooccur,it’shighlyunlikelytohappen\n",
            "simplybychance. Inshort,therealproblemhereisthatneuralnetworkssufferfroman\n",
            "unstablegradientproblem. Asaresult,ifweusestandardgradient-basedlearningtechniques,\n",
            "differentlayersinthenetworkwilltendtolearnatwildlydifferentspeeds. Exercise\n",
            "\n",
            "(cid:12)\n",
            "5.3. Unstablegradientsinmorecomplexnetworks (cid:12) 163\n",
            "(cid:12)\n",
            "Inourdiscussionofthevanishinggradientproblem,wemadeuseofthefactthat\n",
            "• σ (cid:48)(z) <1/4. Supposeweusedadifferentactivationfunction,onewhosederivative\n",
            "|could|bemuchlarger. Wouldthathelpusavoidtheunstablegradientproblem? The prevalence of the vanishing gradient problem: We’ve seen that the gradient can\n",
            "eithervanishorexplodeintheearlylayersofadeepnetwork. Infact,whenusingsigmoid\n",
            "neuronsthegradientwillusuallyvanish. Toseewhy,consideragaintheexpression wσ (cid:48)(z). Toavoidthevanishinggradientproblemweneed wσ (cid:48)(z) 1. Youmightthinkth|iscould|\n",
            "happeneasilyif wisverylarge. However,it’smo|rediffic|u≥ltthanitlooks. Thereasonis\n",
            "thattheσ (cid:48)(z)termalsodependsonw: σ (cid:48)(z)= σ (cid:48)(wa+b),whereaistheinputactivation. Sowhenwemake wlarge,weneedtobecarefulthatwe’renotsimultaneouslymaking\n",
            "σ (cid:48)(wa+b)small. Thatturnsouttobeaconsiderableconstraint.\n",
            "Thereasonisthatwhen\n",
            "wemakewlargewetendtomakewa+bverylarge. Lookingatthegraphofσ\n",
            "(cid:48)\n",
            "youcansee\n",
            "thatthisputsusoffinthe“wings”oftheσ function,whereittakesverysmallvalues. The 5\n",
            "(cid:48)\n",
            "onlywaytoavoidthisisiftheinputactivationfallswithinafairlynarrowrangeofvalues\n",
            "(thisqualitativeexplanationismadequantitativeinthefirstproblembelow). Sometimes\n",
            "thatwillchancetohappen.\n",
            "Moreoften,though,itdoesnothappen.\n",
            "Andsointhegeneric\n",
            "casewehavevanishinggradients. Problems\n",
            "Considertheproduct wσ (cid:48)(wa+b). Suppose wσ (cid:48)(wa+b) 1. (1)Arguethat\n",
            "• thiscanonlyeveroccu|rif w 4. |(2)Supposi|ngthat w |4≥,considerthesetof\n",
            "inputactivationsaforwhich| w|≥σ (cid:48)(wa+b) 1. Showtha|t|th≥esetofasatisfyingthat\n",
            "constraintcanrangeoveran|intervalnogr|e≥aterinwidththan\n",
            "2 (cid:18) w(1+ (cid:112) 1 4/w) (cid:19)\n",
            "ln | | − | | 1 . (5.8)\n",
            "w 2 −\n",
            "| |\n",
            "(3)Shownumericallythattheaboveexpressionboundingthewidthoftherangeis\n",
            "greatestat w 6.9,whereittakesavalue 0.45. Andsoevengiventhateverything\n",
            "linesupjus|tp|e≈rfectly,westillhaveafairly≈narrowrangeofinputactivationswhich\n",
            "canavoidthevanishinggradientproblem. Identityneuron: Consideraneuronwithasingleinput, x,acorrespondingweight,\n",
            "• w ,abias b,andaweightw ontheoutput. Showthatbychoosingtheweightsand\n",
            "1 2\n",
            "biasappropriately,wecanensurew\n",
            "2\n",
            "σ (w\n",
            "1\n",
            "x+b) xforx [0,1]. Suchaneuroncan\n",
            "thusbeusedasakindofidentityneuron,thatis≈,aneuron∈whoseoutputisthesame\n",
            "(uptorescalingbyaweightfactor)asitsinput. Hint: Ithelpstorewrite x=1/2+ ∆,\n",
            "toassumew issmall,andtouseaTaylorseriesexpansioninw ∆. 1 1\n",
            "5.3 Unstable gradients in more complex networks\n",
            "We’vebeenstudyingtoynetworks,withjustoneneuronineachhiddenlayer. Whatabout\n",
            "morecomplexdeepnetworks,withmanyneuronsineachhiddenlayer?\n",
            "\n",
            "(cid:12)\n",
            "164 (cid:12) Whyaredeepneuralnetworkshardtotrain?\n",
            "(cid:12)\n",
            "5\n",
            "Infact,muchthesamebehaviouroccursinsuchnetworks. Intheearlierchapteronback-\n",
            "propagationwesawthatthegradientinthel-thlayerofanLlayernetworkisgivenby:\n",
            "δl = Σ (cid:48)(zl )(wl+1 ) TΣ (cid:48)(zl+1 )(wl+2 ) T...Σ (cid:48)(zL ) a C (5.9)\n",
            "∇\n",
            "Here,Σ (cid:48)(zl )isadiagonalmatrixwhoseentriesaretheσ (cid:48)(z)valuesfortheweightedinputs\n",
            "tothel-thlayer. Thewl aretheweightmatricesforthedifferentlayers. And C isthe\n",
            "a\n",
            "vectorofpartialderivativesofC withrespecttotheoutputactivations. ∇\n",
            "Thisisamuchmorecomplicatedexpressionthaninthesingle-neuroncase. Still, if\n",
            "youlookclosely,theessentialformisverysimilar,withlotsofpairsoftheform(wj ) TΣ (cid:48)(zj ). What’smore,thematricesΣ (cid:48)(zj )havesmallentriesonthediagonal,nonelargerthan1/4. Providedtheweightmatriceswj aren’ttoolarge,eachadditionalterm(wj ) TΣ (cid:48)(zl )tendsto\n",
            "makethegradientvectorsmaller,leadingtoavanishinggradient. Moregenerally,thelarge\n",
            "numberoftermsintheproducttendstoleadtoanunstablegradient,justasinourearlier\n",
            "example. Inpractice,empiricallyitistypicallyfoundinsigmoidnetworksthatgradients\n",
            "vanishexponentiallyquicklyinearlierlayers. Asaresult,learningslowsdowninthoselayers. Thisslowdownisn’tmerelyanaccidentoraninconvenience: it’safundamentalconsequence\n",
            "oftheapproachwe’retakingtolearning. 5.4 Other obstacles to deep learning\n",
            "Inthischapterwe’vefocusedonvanishinggradients–and,moregenerally,unstablegra-\n",
            "dients–asanobstacletodeeplearning. Infact,unstablegradientsarejustoneobstacle\n",
            "todeeplearning,albeitanimportantfundamentalobstacle.\n",
            "Muchongoingresearchaims\n",
            "tobetterunderstandthechallengesthatcanoccurwhentrainingdeepnetworks. Iwon’t\n",
            "comprehensivelysummarizethatworkhere,butjustwanttobrieflymentionacoupleof\n",
            "papers,togiveyoutheflavorofsomeofthequestionspeopleareasking. Asafirstexample,in2010GlorotandBengio9foundevidencesuggestingthattheuseof\n",
            "9Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks,byXavierGlorotand\n",
            "YoshuaBengio(2010).SeealsotheearlierdiscussionoftheuseofsigmoidsinEfficientBackProp,by\n",
            "YannLeCun,LéonBottou,GenevieveOrrandKlaus-RobertMüller(1998). \n",
            "(cid:12)\n",
            "5.4. Otherobstaclestodeeplearning (cid:12) 165\n",
            "(cid:12)\n",
            "sigmoidactivationfunctionscancauseproblemstrainingdeepnetworks. Inparticular,they\n",
            "foundevidencethattheuseofsigmoidswillcausetheactivationsinthefinalhiddenlayerto\n",
            "saturatenear0earlyintraining,substantiallyslowingdownlearning. Theysuggestedsome\n",
            "alternativeactivationfunctions,whichappearnottosufferasmuchfromthissaturation\n",
            "problem. Asasecondexample,in2013Sutskever,Martens,DahlandHinton10studiedtheimpact\n",
            "ondeeplearningofboththerandomweightinitializationandthemomentumschedulein\n",
            "momentum-basedstochasticgradientdescent. Inbothcases,makinggoodchoicesmadea\n",
            "substantialdifferenceintheabilitytotraindeepnetworks.\n",
            "Theseexamplessuggestthat“Whatmakesdeepnetworkshardtotrain?” isacomplex\n",
            "question. Inthischapter,we’vefocusedontheinstabilitiesassociatedtogradient-based\n",
            "learning in deep networks. The results in the last two paragraphs suggest that there is\n",
            "alsoaroleplayedbythechoiceofactivationfunction,thewayweightsareinitialized,and\n",
            "5\n",
            "evendetailsofhowlearningbygradientdescentisimplemented. And,ofcourse,choiceof\n",
            "networkarchitectureandotherhyper-parametersisalsoimportant. Thus,manyfactorscan\n",
            "playaroleinmakingdeepnetworkshardtotrain,andunderstandingallthosefactorsis\n",
            "stillasubjectofongoingresearch. Thisallseemsratherdownbeatandpessimism-inducing. Butthegoodnewsisthatinthenextchapterwe’llturnthataround,anddevelopseveral\n",
            "approachestodeeplearningthattosomeextentmanagetoovercomeorroutearoundall\n",
            "thesechallenges. 10Ontheimportanceofinitializationandmomentumindeeplearning,byIlyaSutskever,James\n",
            "Martens,GeorgeDahlandGeoffreyHinton(2013).\n",
            "\n",
            "(cid:12)\n",
            "166 (cid:12) Whyaredeepneuralnetworkshardtotrain? (cid:12)\n",
            "5\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 167\n",
            "(cid:12)\n",
            "6666\n",
            "Deep learning\n",
            "6\n",
            "Inthelastchapterwelearnedthatdeepneuralnetworksareoftenmuchhardertotrain\n",
            "thanshallowneuralnetworks. That’sunfortunate,sincewehavegoodreasontobelieve\n",
            "thatifwecouldtraindeepnetsthey’dbemuchmorepowerfulthanshallownets. Butwhile\n",
            "thenewsfromthelastchapterisdiscouraging,wewon’tletitstopus. Inthischapter,we’ll\n",
            "developtechniqueswhichcanbeusedtotraindeepnetworks,andapplytheminpractice. We’llalsolookatthebroaderpicture,brieflyreviewingrecentprogressonusingdeepnets\n",
            "forimagerecognition,speechrecognition,andotherapplications. Andwe’lltakeabrief,\n",
            "speculativelookatwhatthefuturemayholdforneuralnets,andforartificialintelligence.\n",
            "Thechapterisalongone.\n",
            "Tohelpyounavigate,let’stakeatour. Thesectionsareonly\n",
            "looselycoupled,soprovidedyouhavesomebasicfamiliaritywithneuralnets,youcanjump\n",
            "towhatevermostinterestsyou. Themainpartofthechapterisanintroductiontooneofthemostwidelyusedtypesof\n",
            "deepnetwork: deepconvolutionalnetworks. We’llworkthroughadetailedexample–code\n",
            "andall–ofusingconvolutionalnetstosolvetheproblemofclassifyinghandwrittendigits\n",
            "fromtheMNISTdataset:\n",
            "We’llstartouraccountofconvolutionalnetworkswiththeshallownetworksusedtoattackthis\n",
            "problemearlierinthebook. Throughmanyiterationswe’llbuildupmoreandmorepowerful\n",
            "networks. Aswegowe’llexploremanypowerfultechniques: convolutions,pooling,theuse\n",
            "ofGPUstodofarmoretrainingthanwedidwithourshallownetworks,thealgorithmic\n",
            "expansionofourtrainingdata(toreduceoverfitting),theuseofthedropouttechnique(also\n",
            "toreduceoverfitting),theuseofensemblesofnetworks,andothers. Theresultwillbea\n",
            "\n",
            "(cid:12)\n",
            "168 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "systemthatoffersnear-humanperformance. Ofthe10,000MNISTtestimages–images\n",
            "notseenduringtraining! –oursystemwillclassify9,967correctly.\n",
            "Here’sapeekatthe33\n",
            "imageswhicharemisclassified. Notethatthecorrectclassificationisinthetopright;our\n",
            "program’sclassificationisinthebottomright:\n",
            "6\n",
            "Manyofthesearetoughevenforahumantoclassify. Consider,forexample,thethirdimage\n",
            "inthetoprow. Tomeitlooksmorelikea“9”thanan“8”,whichistheofficialclassification. Ournetworkalsothinksit’sa“9”.\n",
            "Thiskindof“error”isattheveryleastunderstandable,\n",
            "andperhapsevencommendable. Weconcludeourdiscussionofimagerecognitionwitha\n",
            "surveyofsomeofthespectacularrecentprogressusingnetworks(particularlyconvolutional\n",
            "nets)todoimagerecognition. Theremainderofthechapterdiscussesdeeplearningfromabroaderandlessdetailed\n",
            "perspective. We’llbrieflysurveyothermodelsofneuralnetworks,suchasrecurrentneural\n",
            "netsandlongshort-termmemoryunits,andhowsuchmodelscanbeappliedtoproblemsin\n",
            "speechrecognition,naturallanguageprocessing,andotherareas. Andwe’llspeculateabout\n",
            "thefutureofneuralnetworksanddeeplearning,rangingfromideaslikeintention-driven\n",
            "userinterfaces,totheroleofdeeplearninginartificialintelligence. Thechapterbuildsontheearlierchaptersinthebook,makinguseofandintegrating\n",
            "ideassuchasbackpropagation,regularization,thesoftmaxfunction,andsoon. However,to\n",
            "readthechapteryoudon’tneedtohaveworkedindetailthroughalltheearlierchapters. It\n",
            "will,however,helptohavereadChapter1,onthebasicsofneuralnetworks. WhenIuse\n",
            "conceptsfromChapters2to5,Iprovidelinkssoyoucanfamiliarizeyourself,ifnecessary. It’sworthnotingwhatthechapterisnot. It’snotatutorialonthelatestandgreatest\n",
            "neuralnetworkslibraries. Norarewegoingtobetrainingdeepnetworkswithdozensof\n",
            "layerstosolveproblemsattheveryleadingedge. Rather,thefocusisonunderstanding\n",
            "someofthecoreprinciplesbehinddeepneuralnetworks,andapplyingtheminthesimple,\n",
            "easy-to-understandcontextoftheMNISTproblem. Putanotherway: thechapterisnot\n",
            "goingtobringyourightuptothefrontier. Rather,theintentofthisandearlierchaptersisto\n",
            "focusonfundamentals,andsotoprepareyoutounderstandawiderangeofcurrentwork.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 169\n",
            "(cid:12)\n",
            "6.1 Introducing convolutional networks\n",
            "Inearlierchapters,wetaughtourneuralnetworkstodoaprettygoodjobrecognizingimages\n",
            "ofhandwrittendigits:\n",
            "Wedidthisusingnetworksinwhichadjacentnetworklayersarefullyconnectedtoone\n",
            "another. Thatis, everyneuroninthenetworkisconnectedtoeveryneuroninadjacent\n",
            "layers:\n",
            "6\n",
            "Inparticular,foreachpixelintheinputimage,weencodedthepixel’sintensityasthevalue\n",
            "foracorrespondingneuronintheinputlayer. Forthe28 28pixelimageswe’vebeenusing,\n",
            "thismeansournetworkhas784(=28 28)inputneuro×ns.\n",
            "Wethentrainedthenetwork’s\n",
            "weightsandbiasessothatthenetwork×’soutputwould–wehope! –correctlyidentifythe\n",
            "inputimage: ‘0’,‘1’,‘2’,...,‘8’,or‘9’.\n",
            "Ourearliernetworksworkprettywell: we’veobtainedaclassificationaccuracybetter\n",
            "than98percent,usingtrainingandtestdatafromtheMNISThandwrittendigitdataset. But\n",
            "uponreflection,it’sstrangetousenetworkswithfully-connectedlayerstoclassifyimages. Thereasonisthatsuchanetworkarchitecturedoesnottakeintoaccountthespatialstructure\n",
            "oftheimages. Forinstance,ittreatsinputpixelswhicharefarapartandclosetogether\n",
            "onexactlythesamefooting. Suchconceptsofspatialstructuremustinsteadbeinferred\n",
            "fromthetrainingdata. Butwhatif,insteadofstartingwithanetworkarchitecturewhichis\n",
            "tabularasa,weusedanarchitecturewhichtriestotakeadvantageofthespatialstructure? \n",
            "(cid:12)\n",
            "170 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "In this section I describe convolutional neural networks1. These networks use a special\n",
            "architecturewhichisparticularlywell-adaptedtoclassifyimages. Usingthisarchitecture\n",
            "makesconvolutionalnetworksfasttotrain. This,inturn,helpsustraindeep,many-layer\n",
            "networks,whichareverygoodatclassifyingimages. Today,deepconvolutionalnetworksor\n",
            "someclosevariantareusedinmostneuralnetworksforimagerecognition. Convolutionalneuralnetworksusethreebasicideas: localreceptivefields,sharedweights,\n",
            "andpooling.\n",
            "Let’slookateachoftheseideasinturn. Local receptive fields: In the fully-connected layers shown earlier, the inputs were\n",
            "depictedasaverticallineofneurons. Inaconvolutionalnet,it’llhelptothinkinsteadof\n",
            "theinputsasa28 28squareofneurons,whosevaluescorrespondtothe28 28pixel\n",
            "intensitieswe’reus×ingasinputs: ×\n",
            "6\n",
            "As per usual, we’ll connect the input pixels to a layer of hidden neurons. But we won’t\n",
            "connecteveryinputpixeltoeveryhiddenneuron. Instead,weonlymakeconnectionsin\n",
            "small,localizedregionsoftheinputimage. Tobemoreprecise,eachneuroninthefirsthiddenlayerwillbeconnectedtoasmall\n",
            "regionoftheinputneurons,say,forexample,a5 5region,correspondingto25input\n",
            "pixels. So,foraparticularhiddenneuron,wemight×haveconnectionsthatlooklikethis:\n",
            "Thatregionintheinputimageiscalledthelocalreceptivefieldforthehiddenneuron. It’sa\n",
            "littlewindowontheinputpixels.\n",
            "Eachconnectionlearnsaweight. Andthehiddenneuron\n",
            "1Theoriginsofconvolutionalneuralnetworksgobacktothe1970s.Buttheseminalpaperestablishing\n",
            "themodernsubjectofconvolutionalnetworkswasa1998paper,Gradient-basedlearningappliedto\n",
            "documentrecognition,byYannLeCun,LéonBottou,YoshuaBengio,andPatrickHaffner.LeCunhas\n",
            "sincemadeaninterestingremarkontheterminologyforconvolutionalnets:“The[biological]neural\n",
            "inspirationinmodelslikeconvolutionalnetsisverytenuous. That’swhyIcallthem‘convolutional\n",
            "nets’not‘convolutionalneuralnets’,andwhywecallthenodes‘units’andnot‘neurons’”.Despitethis\n",
            "remark,convolutionalnetsusemanyofthesameideasastheneuralnetworkswe’vestudieduptonow:\n",
            "ideassuchasbackpropagation,gradientdescent,regularization,non-linearactivationfunctions,andso\n",
            "on.Andsowewillfollowcommonpractice,andconsiderthematypeofneuralnetwork.Iwillusethe\n",
            "terms“convolutionalneuralnetwork”and“convolutionalnet(work)”interchangeably.Iwillalsouse\n",
            "theterms“[artificial]neuron”and“unit”interchangeably. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 171\n",
            "(cid:12)\n",
            "learnsanoverallbiasaswell. Youcanthinkofthatparticularhiddenneuronaslearningto\n",
            "analyzeitsparticularlocalreceptivefield. We then slide the local receptive field across the entire input image. For each local\n",
            "receptivefield,thereisadifferenthiddenneuroninthefirsthiddenlayer. Toillustratethis\n",
            "concretely,let’sstartwithalocalreceptivefieldinthetop-leftcorner:\n",
            "Thenweslidethelocalreceptivefieldoverbyonepixeltotheright(i.e.,byoneneuron),to\n",
            "connecttoasecondhiddenneuron: 6\n",
            "Andsoon,buildingupthefirsthiddenlayer. Notethatifwehavea28 28inputimage,and\n",
            "5 5localreceptivefields,thentherewillbe24 24neuronsinthe×hiddenlayer. Thisis\n",
            "be×causewecanonlymovethelocalreceptivefield×23neuronsacross(or23neuronsdown),\n",
            "beforecollidingwiththeright-handside(orbottom)oftheinputimage. I’veshownthelocalreceptivefieldbeingmovedbyonepixelatatime. Infact,sometimes\n",
            "adifferentstridelengthisused. Forinstance,wemightmovethelocalreceptivefield2\n",
            "pixelstotheright(ordown),inwhichcasewe’dsayastridelengthof2isused. Inthis\n",
            "chapterwe’llmostlystickwithstridelength1,butit’sworthknowingthatpeoplesometimes\n",
            "experimentwithdifferentstridelengths2. Sharedweightsandbiases: I’vesaidthateachhiddenneuronhasabiasand5 5\n",
            "weightsconnectedtoitslocalreceptivefield. WhatIdidnotyetmentionisthatwe’rego×ing\n",
            "tousethesameweightsandbiasforeachofthe24 24hiddenneurons.\n",
            "Inotherwords,\n",
            "forthe j,k-thhiddenneuron,theoutputis: ×\n",
            "(cid:130) 4 4 (cid:140)\n",
            "(cid:88)(cid:88)\n",
            "σ b+ w\n",
            "l,m\n",
            "a\n",
            "j+l,k+m\n",
            ". (6.1)\n",
            "l=0m=0\n",
            "2Aswasdoneinearlierchapters,ifwe’reinterestedintryingdifferentstridelengthsthenwecanuse\n",
            "validationdatatopickoutthestridelengthwhichgivesthebestperformance.Formoredetails,seethe\n",
            "earlierdiscussionofhowtochoosehyper-parametersinaneuralnetwork.Thesameapproachmayalso\n",
            "beusedtochoosethesizeofthelocalreceptivefield–thereis,ofcourse,nothingspecialaboutusinga\n",
            "5 5localreceptivefield. Ingeneral,largerlocalreceptivefieldstendtobehelpfulwhentheinput\n",
            "im×agesaresignificantlylargerthanthe28 28pixelMNISTimages.\n",
            "×\n",
            "\n",
            "(cid:12)\n",
            "172 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Here,σistheneuralactivationfunction–perhapsthesigmoidfunctionweusedinearlier\n",
            "chapters. bisthesharedvalueforthebias.\n",
            "w isa5 5arrayofsharedweights. And,\n",
            "l,m\n",
            "finally,weusea todenotetheinputactivationatposit×ion x,y. x,y\n",
            "Thismeansthatalltheneuronsinthefirsthiddenlayerdetectexactlythesamefeature3,\n",
            "justatdifferentlocationsintheinputimage. Toseewhythismakessense,supposethe\n",
            "weightsandbiasaresuchthatthehiddenneuroncanpickout, say, averticaledgeina\n",
            "particularlocalreceptivefield. Thatabilityisalsolikelytobeusefulatotherplacesinthe\n",
            "image. Andsoitisusefultoapplythesamefeaturedetectoreverywhereintheimage. Toput\n",
            "itinslightlymoreabstractterms,convolutionalnetworksarewelladaptedtothetranslation\n",
            "invarianceofimages: moveapictureofacat(say)alittleways,andit’sstillanimageofa\n",
            "cat4. Forthisreason,wesometimescallthemapfromtheinputlayertothehiddenlayera\n",
            "featuremap. Wecalltheweightsdefiningthefeaturemapthesharedweights.\n",
            "Andwecall\n",
            "thebiasdefiningthefeaturemapinthiswaythesharedbias. Thesharedweightsandbias\n",
            "areoftensaidtodefineakernelorfilter. Intheliterature,peoplesometimesusetheseterms\n",
            "inslightlydifferentways,andforthatreasonI’mnotgoingtobemoreprecise;rather,ina\n",
            "6\n",
            "moment,we’lllookatsomeconcreteexamples. ThenetworkstructureI’vedescribedsofarcandetectjustasinglekindoflocalized\n",
            "feature. Todoimagerecognitionwe’llneedmorethanonefeaturemap. Andsoacomplete\n",
            "convolutionallayerconsistsofseveraldifferentfeaturemaps:\n",
            "Intheexampleshown,thereare3featuremaps. Eachfeaturemapisdefinedbyasetof\n",
            "5 5sharedweights,andasinglesharedbias. Theresultisthatthenetworkcandetect\n",
            "3×differentkindsoffeatures, witheachfeaturebeingdetectableacrosstheentireimage. I’veshownjust3featuremaps,tokeepthediagramabovesimple. However,inpractice\n",
            "convolutionalnetworksmayusemore(andperhapsmanymore)featuremaps. Oneofthe\n",
            "earlyconvolutionalnetworks,LeNet-5,used6featuremaps,eachassociatedtoa5 5local\n",
            "receptivefield,torecognizeMNISTdigits. Sotheexampleillustratedaboveisactual×lypretty\n",
            "closetoLeNet-5. Intheexampleswedeveloplaterinthechapterwe’lluseconvolutional\n",
            "layerswith20and40featuremaps. Let’stakeaquickpeekatsomeofthefeatureswhich\n",
            "arelearned. 3Ihaven’tpreciselydefinedthenotionofafeature. Informally,thinkofthefeaturedetectedbya\n",
            "hiddenneuronasthekindofinputpatternthatwillcausetheneurontoactivate:itmightbeanedgein\n",
            "theimage,forinstance,ormaybesomeothertypeofshape. 4Infact,fortheMNISTdigitclassificationproblemwe’vebeenstudying,theimagesarecenteredand\n",
            "size-normalized.SoMNISThaslesstranslationinvariancethanimagesfound“inthewild”,sotospeak. Still,featureslikeedgesandcornersarelikelytobeusefulacrossmuchoftheinputspace.\n",
            "\n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 173\n",
            "(cid:12)\n",
            "The20imagescorrespondto20differentfeaturemaps(orfilters,orkernels). Eachmapis\n",
            "representedasa5 5blockimage,correspondingtothe5 5weightsinthelocalreceptive\n",
            "field. Whiterblock×smeanasmaller(typically,morenega×tive)weight,sothefeaturemap 6\n",
            "respondslesstocorrespondinginputpixels. Darkerblocksmeanalargerweight,sothe\n",
            "featuremaprespondsmoretothecorrespondinginputpixels. Veryroughlyspeaking,the\n",
            "imagesaboveshowthetypeoffeaturestheconvolutionallayerrespondsto. Sowhatcanweconcludefromthesefeaturemaps? It’sclearthereisspatialstructure\n",
            "here beyond what we’d expect at random: many of the features have clear sub-regions\n",
            "oflightanddark. Thatshowsournetworkreallyislearningthingsrelatedtothespatial\n",
            "structure.\n",
            "However,beyondthat,it’sdifficulttoseewhatthesefeaturedetectorsarelearning. Certainly,we’renotlearning(say)theGaborfilterswhichhavebeenusedinmanytraditional\n",
            "approachestoimagerecognition. Infact,there’snowalotofworkonbetterunderstanding\n",
            "the features learnt by convolutional networks. If you’re interested in following up on\n",
            "thatwork,IsuggeststartingwiththepaperVisualizingandUnderstandingConvolutional\n",
            "NetworksbyMatthewZeilerandRobFergus(2013). Abigadvantageofsharingweightsandbiasesisthatitgreatlyreducesthenumberof\n",
            "parametersinvolvedinaconvolutionalnetwork. Foreachfeaturemapweneed25=5 5\n",
            "sharedweights,plusasinglesharedbias. Soeachfeaturemaprequires26parameters. If×we\n",
            "have20featuremapsthat’satotalof20 26=520parametersdefiningtheconvolutional\n",
            "layer. Bycomparison,supposewehadafu×llyconnectedfirstlayer,with784=28 28input\n",
            "neurons,andarelativelymodest30hiddenneurons,asweusedinmanyofthe×examples\n",
            "earlierinthebook. That’satotalof784 30weights,plusanextra30biases,foratotal\n",
            "of23,550parameters.\n",
            "Inotherwords,the×fully-connectedlayerwouldhavemorethan40\n",
            "timesasmanyparametersastheconvolutionallayer.\n",
            "Ofcourse,wecan’treallydoadirectcomparisonbetweenthenumberofparameters,\n",
            "sincethetwomodelsaredifferentinessentialways. But,intuitively,itseemslikelythatthe\n",
            "useoftranslationinvariancebytheconvolutionallayerwillreducethenumberofparameters\n",
            "it needs to get the same performance as the fully-connected model. That, in turn, will\n",
            "resultinfastertrainingfortheconvolutionalmodel,and,ultimately,willhelpusbuilddeep\n",
            "networksusingconvolutionallayers. Incidentally,thenameconvolutionalcomesfromthefactthattheoperationinEquation\n",
            "(125)issometimesknownasaconvolution. Alittlemoreprecisely,peoplesometimeswrite\n",
            "thatequationasa1 = σ (b+w a0 ),wherea1 denotesthesetofoutputactivationsfrom\n",
            "onefeaturemap,a0istheseto∗finputactivations,and iscalledaconvolutionoperation. ∗\n",
            "\n",
            "(cid:12)\n",
            "174 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "We’renotgoingtomakeanydeepuseofthemathematicsofconvolutions,soyoudon’tneed\n",
            "toworrytoomuchaboutthisconnection. Butit’sworthatleastknowingwherethename\n",
            "comesfrom. Poolinglayers: Inadditiontotheconvolutionallayersjustdescribed,convolutional\n",
            "neuralnetworksalsocontainpoolinglayers. Poolinglayersareusuallyusedimmediately\n",
            "afterconvolutionallayers. Whatthepoolinglayersdoissimplifytheinformationinthe\n",
            "outputfromtheconvolutionallayer. Indetail,apoolinglayertakeseachfeaturemap5outputfromtheconvolutionallayer\n",
            "andpreparesacondensedfeaturemap. Forinstance,eachunitinthepoolinglayermay\n",
            "summarizearegionof(say)2 2neuronsinthepreviouslayer. Asaconcreteexample,\n",
            "onecommonprocedureforpoo×lingisknownasmax-pooling. Inmax-pooling,apooling\n",
            "unitsimplyoutputsthemaximumactivationinthe2 2inputregion,asillustratedinthe\n",
            "followingdiagram: ×\n",
            "6\n",
            "Notethatsincewehave24 24neuronsoutputfromtheconvolutionallayer,afterpooling\n",
            "wehave12 12neurons. ×\n",
            "Asment×ionedabove,theconvolutionallayerusuallyinvolvesmorethanasinglefeature\n",
            "map. Weapplymax-poolingtoeachfeaturemapseparately. Soiftherewerethreefeature\n",
            "maps,thecombinedconvolutionalandmax-poolinglayerswouldlooklike:\n",
            "Wecanthinkofmax-poolingasawayforthenetworktoaskwhetheragivenfeatureisfound\n",
            "anywhereinaregionoftheimage. Itthenthrowsawaytheexactpositionalinformation.\n",
            "Theintuitionisthatonceafeaturehasbeenfound,itsexactlocationisn’tasimportantasits\n",
            "roughlocationrelativetootherfeatures. Abigbenefitisthattherearemanyfewerpooled\n",
            "features,andsothishelpsreducethenumberofparametersneededinlaterlayers.\n",
            "Max-poolingisn’ttheonlytechniqueusedforpooling. Anothercommonapproachis\n",
            "knownasL2pooling. Here,insteadoftakingthemaximumactivationofa2 2regionof\n",
            "×\n",
            "5Thenomenclatureisbeingusedlooselyhere.Inparticular,I’musing“featuremap”tomeannotthe\n",
            "functioncomputedbytheconvolutionallayer,butrathertheactivationofthehiddenneuronsoutput\n",
            "fromthelayer.Thiskindofmildabuseofnomenclatureisprettycommonintheresearchliterature. \n",
            "(cid:12)\n",
            "6.1. Introducingconvolutionalnetworks (cid:12) 175\n",
            "(cid:12)\n",
            "neurons,wetakethesquarerootofthesumofthesquaresoftheactivationsinthe2 2\n",
            "region. Whilethedetailsaredifferent,theintuitionissimilartomax-pooling: L2pooling×is\n",
            "awayofcondensinginformationfromtheconvolutionallayer. Inpractice,bothtechniques\n",
            "havebeenwidelyused. Andsometimespeopleuseothertypesofpoolingoperation. If\n",
            "you’rereallytryingtooptimizeperformance,youmayusevalidationdatatocompareseveral\n",
            "differentapproachestopooling,andchoosetheapproachwhichworksbest. Butwe’renot\n",
            "goingtoworryaboutthatkindofdetailedoptimization.\n",
            "Puttingitalltogether: Wecannowputalltheseideastogethertoformacomplete\n",
            "convolutionalneuralnetwork. It’ssimilartothearchitecturewewerejustlookingat,but\n",
            "hastheadditionofalayerof10outputneurons,correspondingtothe10possiblevaluesfor\n",
            "MNISTdigits(‘0’,‘1’,‘2’,etc):\n",
            "6\n",
            "Thenetworkbeginswith28 28inputneurons,whichareusedtoencodethepixelintensities\n",
            "fortheMNISTimage. This×isthenfollowedbyaconvolutionallayerusinga5 5local\n",
            "receptivefieldand3featuremaps. Theresultisalayerof3 24 24hiddenfeature×neurons. Thenextstepisamax-poolinglayer,appliedto2 2regio×ns,×acrosseachofthe3feature\n",
            "maps. Theresultisalayerof3 12 12hiddenfe×atureneurons. × ×\n",
            "Thefinallayerofconnectionsinthenetworkisafully-connectedlayer. Thatis,thislayer\n",
            "connectseveryneuronfromthemax-pooledlayertoeveryoneofthe10outputneurons. Thisfully-connectedarchitectureisthesameasweusedinearlierchapters. Note,however,\n",
            "thatinthediagramabove,I’veusedasinglearrow,forsimplicity,ratherthanshowingall\n",
            "theconnections. Ofcourse,youcaneasilyimaginetheconnections. This convolutional architecture is quite different to the architectures used in earlier\n",
            "chapters. Buttheoverallpictureissimilar: anetworkmadeofmanysimpleunits,whose\n",
            "behaviorsaredeterminedbytheirweightsandbiases.\n",
            "Andtheoverallgoalisstillthesame:\n",
            "tousetrainingdatatotrainthenetwork’sweightsandbiasessothatthenetworkdoesa\n",
            "goodjobclassifyinginputdigits. In particular, just as earlier in the book, we will train our network using stochastic\n",
            "gradientdescentandbackpropagation. Thismostlyproceedsinexactlythesamewayasin\n",
            "earlierchapters.\n",
            "However,wedoneedtomakeafewmodificationstothebackpropagation\n",
            "procedure. Thereasonisthatourearlierderivationofbackpropagationwasfornetworks\n",
            "withfully-connectedlayers. Fortunately,it’sstraightforwardtomodifythederivationfor\n",
            "convolutionalandmax-poolinglayers. Ifyou’dliketounderstandthedetails,thenIinvite\n",
            "youtoworkthroughthefollowingproblem. Bewarnedthattheproblemwilltakesometime\n",
            "toworkthrough,unlessyou’vereallyinternalizedtheearlierderivationofbackpropagation\n",
            "(inwhichcaseit’seasy). Problem\n",
            "\n",
            "(cid:12)\n",
            "176 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "BackpropagationinaconvolutionalnetworkThecoreequationsofbackpropagationin\n",
            "• anetworkwithfully-connectedlayersare(BP1)–(BP4). Supposewehaveanetwork\n",
            "containingaconvolutionallayer,amax-poolinglayer,andafully-connectedoutput\n",
            "layer,asinthenetworkdiscussedabove. Howaretheequationsofbackpropagation\n",
            "modified? 6.2 Convolutional neural networks in practice\n",
            "We’venowseenthecoreideasbehindconvolutionalneuralnetworks. Let’slookathowthey\n",
            "workinpractice,byimplementingsomeconvolutionalnetworks,andapplyingthemtothe\n",
            "MNISTdigitclassificationproblem. Theprogramwe’llusetodothisiscallednetwork3.py,\n",
            "andit’sanimprovedversionoftheprogramsnetwork.pyandnetwork2.pydevelopedin\n",
            "earlierchapters6. Ifyouwishtofollowalong,thecodeisavailableonGitHub. Notethat\n",
            "we’llworkthroughthecodefornetwork3.pyitselfinthenextsection.\n",
            "Inthissection,we’ll\n",
            "usenetwork3.pyasalibrarytobuildconvolutionalnetworks. 6 The programs network.py and network2.py were implemented using Python and\n",
            "the matrix library Numpy. Those programs worked from first principles, and got right\n",
            "downintothedetailsofbackpropagation,stochasticgradientdescent,andsoon. Butnow\n",
            "thatweunderstandthosedetails,fornetwork3.pywe’regoingtouseamachinelearning\n",
            "libraryknownasTheano7. UsingTheanomakesiteasytoimplementbackpropagationfor\n",
            "convolutionalneuralnetworks,sinceitautomaticallycomputesallthemappingsinvolved. Theano is also quite a bit faster than our earlier code (which was written to be easy to\n",
            "understand, not fast), and this makes it practical to train more complex networks. In\n",
            "particular,onegreatfeatureofTheanoisthatitcanruncodeoneitheraCPUor,ifavailable,\n",
            "aGPU.RunningonaGPUprovidesasubstantialspeedupand,again,helpsmakeitpractical\n",
            "totrainmorecomplexnetworks.\n",
            "Ifyouwishtofollowalong, thenyou’llneedtogetTheanorunningonyoursystem. ToinstallTheano,followtheinstructionsattheproject’shomepage. Theexampleswhich\n",
            "followwererunusingTheano0.68. SomewererununderMacOSXYosemite,withnoGPU. SomewererunonUbuntu14.04,withanNVIDIAGPU.Andsomeoftheexperimentswere\n",
            "rununderboth. Togetnetwork3.pyrunningyou’llneedtosettheGPUflagtoeitherTrue\n",
            "orFalse(asappropriate)inthenetwork3.pysource.\n",
            "Beyondthat,togetTheanoupand\n",
            "runningonaGPUyoumayfindtheinstructionsherehelpful. Therearealsotutorialsonthe\n",
            "web,easilyfoundusingGoogle,whichcanhelpyougetthingsworking. Ifyoudon’thavea\n",
            "GPUavailablelocally,thenyoumaywishtolookintoAmazonWebServicesEC2G2spot\n",
            "instances. NotethatevenwithaGPUthecodewilltakesometimetoexecute.\n",
            "Manyofthe\n",
            "experimentstakefromminutestohourstorun. OnaCPUitmaytakedaystorunthemost\n",
            "complexoftheexperiments. Asinearlierchapters,Isuggestsettingthingsrunning,and\n",
            "continuingtoread,occasionallycomingbacktochecktheoutputfromthecode. Ifyou’re\n",
            "6Notealsothatnetwork3.pyincorporatesideasfromtheTheanolibrary’sdocumentationonconvo-\n",
            "lutionalneuralnets(notablytheimplementationofLeNet-5),fromMishaDenil’simplementationof\n",
            "dropout,andfromChrisOlah. 7SeeTheano: ACPUandGPUMathExpressionCompilerinPython,byJamesBergstra,Olivier\n",
            "Breuleux,FredericBastien,PascalLamblin,RavzanPascanu,GuillaumeDesjardins,JosephTurian,David\n",
            "Warde-Farley,andYoshuaBengio(2010).TheanoisalsothebasisforthepopularPylearn2andKeras\n",
            "neuralnetworkslibraries.OtherpopularneuralnetslibrariesatthetimeofthiswritingincludeCaffe\n",
            "andTorch. 8AsIreleasethischapter,thecurrentversionofTheanohaschangedtoversion0.7. I’veactually\n",
            "reruntheexamplesunderTheano0.7andgetextremelysimilarresultstothosereportedinthetext. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 177\n",
            "(cid:12)\n",
            "usingaCPU,youmaywishtoreducethenumberoftrainingepochsforthemorecomplex\n",
            "experiments,orperhapsomitthementirely. Togetabaseline,we’llstartwithashallowarchitectureusingjustasinglehiddenlayer,\n",
            "containing100hiddenneurons.\n",
            "We’lltrainfor60epochs,usingalearningrateofη =0.1,a\n",
            "mini-batchsizeof10,andnoregularization. Herewego9:\n",
            ">>> import network3\n",
            ">>> from network3 import Network\n",
            ">>> from network3 import ConvPoolLayer, FullyConnectedLayer, SoftmaxLayer\n",
            ">>> training_data, validation_data, test_data = network3.load_data_shared()\n",
            ">>> mini_batch_size = 10\n",
            ">>> net = Network([FullyConnectedLayer(n_in=784, n_out=100),SoftmaxLayer(n_in\n",
            "=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Iobtainedabestclassificationaccuracyof97.80percent. Thisistheclassificationaccuracy\n",
            "on the test_data, evaluated at the training epoch where we get the best classification\n",
            "accuracyonthevalidation_data.\n",
            "Usingthevalidationdatatodecidewhentoevaluate\n",
            "6\n",
            "thetestaccuracyhelpsavoidoverfittingtothetestdata(seethisearlierdiscussionoftheuse\n",
            "ofvalidationdata). Wewillfollowthispracticebelow. Yourresultsmayvaryslightly,since\n",
            "thenetwork’sweightsandbiasesarerandomlyinitialized10. This97.80percentaccuracyisclosetothe98.04percentaccuracyobtainedbackin\n",
            "Chapter3,usingasimilarnetworkarchitectureandlearninghyper-parameters. Inparticular,\n",
            "bothexamplesusedashallownetwork,withasinglehiddenlayercontaining100hidden\n",
            "neurons. Bothalsotrainedfor60epochs,usedamini-batchsizeof10,andalearningrate\n",
            "ofη =0.1. Therewere,however,twodifferencesintheearliernetwork. First,weregularizedthe\n",
            "earliernetwork,tohelpreducetheeffectsofoverfitting. Regularizingthecurrentnetwork\n",
            "doesimprovetheaccuracies,butthegainisonlysmall,andsowe’llholdoffworryingabout\n",
            "regularizationuntillater. Second,whilethefinallayerintheearliernetworkusedsigmoid\n",
            "activationsandthecross-entropycostfunction,thecurrentnetworkusesasoftmaxfinal\n",
            "layer,andthelog-likelihoodcostfunction. AsexplainedinChapter3thisisn’tabigchange. Ihaven’tmadethisswitchforanyparticularlydeepreason–mostly,I’vedoneitbecause\n",
            "softmaxpluslog-likelihoodcostismorecommoninmodernimageclassificationnetworks. Canwedobetterthantheseresultsusingadeepernetworkarchitecture? Let’sbeginbyinsertingaconvolutionallayer,rightatthebeginningofthenetwork. We’ll\n",
            "use5by5localreceptivefields,astridelengthof1,and20featuremaps. We’llalsoinsert\n",
            "amax-poolinglayer,whichcombinesthefeaturesusing2by2poolingwindows.\n",
            "Sothe\n",
            "overallnetworkarchitecturelooksmuchlikethearchitecturediscussedinthelastsection,\n",
            "butwithanextrafully-connectedlayer:\n",
            "9Codefortheexperimentsinthissectionmaybefoundinthisscript.Notethatthecodeinthescript\n",
            "simplyduplicatesandparallelsthediscussioninthissection. NotealsothatthroughoutthesectionI’veexplicitlyspecifiedthenumberoftrainingepochs.I’vedone\n",
            "thisforclarityabouthowwe’retraining.Inpractice,it’sworthusingearlystopping,thatis,tracking\n",
            "accuracyonthevalidationset,andstoppingtrainingwhenweareconfidentthevalidationaccuracyhas\n",
            "stoppedimproving. 10Infact,inthisexperimentIactuallydidthreeseparaterunstraininganetworkwiththisarchitecture. Ithenreportedthetestaccuracywhichcorrespondedtothebestvalidationaccuracyfromanyofthe\n",
            "threeruns.Usingmultiplerunshelpsreducevariationinresults,whichisusefulwhencomparingmany\n",
            "architectures,aswearedoing.I’vefollowedthisprocedurebelow,exceptwherenoted.Inpractice,it\n",
            "madelittledifferencetotheresultsobtained. \n",
            "(cid:12)\n",
            "178 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Inthisarchitecture,wecanthinkoftheconvolutionalandpoolinglayersaslearningabout\n",
            "localspatialstructureintheinputtrainingimage,whilethelater,fully-connectedlayerlearns\n",
            "atamoreabstractlevel,integratingglobalinformationfromacrosstheentireimage. Thisis\n",
            "acommonpatterninconvolutionalneuralnetworks. Let’strainsuchanetwork,andseehowitperforms11:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "6\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=20*12*12, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Thatgetsusto98.78percentaccuracy,whichisaconsiderableimprovementoveranyof\n",
            "ourpreviousresults. Indeed,we’vereducedourerrorratebybetterthanathird,whichisa\n",
            "greatimprovement.\n",
            "Inspecifyingthenetworkstructure,I’vetreatedtheconvolutionalandpoolinglayersas\n",
            "asinglelayer. Whetherthey’reregardedasseparatelayersorasasinglelayeristosome\n",
            "extentamatteroftaste. network3.pytreatsthemasasinglelayerbecauseitmakesthe\n",
            "codefornetwork3.pyalittlemorecompact. However,itiseasytomodifynetwork3.pyso\n",
            "thelayerscanbespecifiedseparately,ifdesired. Exercise\n",
            "Whatclassificationaccuracydoyougetifyouomitthefully-connectedlayer,and\n",
            "• justusetheconvolutional-poolinglayerandsoftmaxlayer? Doestheinclusionofthe\n",
            "fully-connectedlayerhelp? Canweimproveonthe98.78percentclassificationaccuracy? Let’stryinsertingasecondconvolutional-poolinglayer. We’llmaketheinsertionbetween\n",
            "theexistingconvolutional-poolinglayerandthefully-connectedhiddenlayer.\n",
            "Again,we’ll\n",
            "usea5 5localreceptivefield,andpoolover2 2regions. Let’sseewhathappenswhen\n",
            "wetrain×usingsimilarhyper-parameterstobefore×:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "ConvPoolLayer(\n",
            "11I’vecontinuedtouseamini-batchsizeof10here.Infact,aswediscussedearlieritmaybepossible\n",
            "tospeeduptrainingusinglargermini-batches.I’vecontinuedtousethesamemini-batchsizemostlyfor\n",
            "consistencywiththeexperimentsinearlierchapters. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 179\n",
            "(cid:12)\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2)),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.1, validation_data, test_data)\n",
            "Onceagain,wegetanimprovement: we’renowat99.06percentclassificationaccuracy! There’stwonaturalquestionstoaskatthispoint. Thefirstquestionis: whatdoesiteven\n",
            "meantoapplyasecondconvolutional-poolinglayer? Infact,youcanthinkofthesecond\n",
            "convolutional-poolinglayerashavingasinput12 12“images”,whose“pixels”represent\n",
            "thepresence(orabsence)ofparticularlocalizedfea×turesintheoriginalinputimage. Soyou\n",
            "canthinkofthislayerashavingasinputaversionoftheoriginalinputimage. Thatversion\n",
            "isabstractedandcondensed,butstillhasalotofspatialstructure,andsoitmakessenseto\n",
            "useasecondconvolutional-poolinglayer. That’sasatisfyingpointofview,butgivesrisetoasecondquestion. Theoutputfrom\n",
            "thepreviouslayerinvolves20separatefeaturemaps,andsothereare20 12 12inputs\n",
            "to the second convolutional-pooling layer. It’s as though we’ve got 20×separ×ate images 6\n",
            "inputtotheconvolutional-poolinglayer,notasingleimage,aswasthecaseforthefirst\n",
            "convolutional-poolinglayer. Howshouldneuronsinthesecondconvolutional-poolinglayer\n",
            "respondtothesemultipleinputimages? Infact,we’llalloweachneuroninthislayertolearn\n",
            "fromall20 5 5inputneuronsinitslocalreceptivefield. Moreinformally: thefeature\n",
            "detectorsin×the×secondconvolutional-poolinglayerhaveaccesstoallthefeaturesfromthe\n",
            "previouslayer,butonlywithintheirparticularlocalreceptivefield12. Problem\n",
            "UsingthetanhactivationfunctionSeveraltimesearlierinthebookI’vementioned\n",
            "• argumentsthatthetanhfunctionmaybeabetteractivationfunctionthanthesigmoid\n",
            "function. We’veneveractedonthosesuggestions, sincewewerealreadymaking\n",
            "plentyofprogresswiththesigmoid.\n",
            "Butnowlet’strysomeexperimentswithtanh\n",
            "as our activation function. Try training the network with tanh activations in the\n",
            "convolutionalandfully-connectedlayers13. Beginwiththesamehyper-parametersas\n",
            "forthesigmoidnetwork,buttrainfor20epochsinsteadof60. Howwelldoesyour\n",
            "networkperform? Whatifyoucontinueoutto60epochs? Tryplottingtheper-epoch\n",
            "validationaccuraciesforbothtanh-andsigmoid-basednetworks,allthewayoutto60\n",
            "epochs. Ifyourresultsaresimilartomine,you’llfindthetanhnetworkstrainalittle\n",
            "faster,butthefinalaccuraciesareverysimilar. Canyouexplainwhythetanhnetwork\n",
            "mighttrainfaster? Canyougetasimilartrainingspeedwiththesigmoid,perhaps\n",
            "bychangingthelearningrate,ordoingsomerescaling14? Tryahalf-dozeniterations\n",
            "onthelearninghyper-parametersornetworkarchitecture,searchingforwaysthat\n",
            "tanhmaybesuperiortothesigmoid. Note: Thisisanopen-endedproblem.\n",
            "Personally,\n",
            "Ididnotfindmuchadvantageinswitchingtotanh,althoughIhaven’texperimented\n",
            "exhaustively,andperhapsyoumayfindaway. Inanycase,inamomentwewillfindan\n",
            "advantageinswitchingtotherectifiedlinearactivationfunction,andsowewon’tgoany\n",
            "12Thisissuewouldhaveariseninthefirstlayeriftheinputimageswereincolor.Inthatcasewe’d\n",
            "have3inputfeaturesforeachpixel,correspondingtored,greenandbluechannelsintheinputimage. Sowe’dallowthefeaturedetectorstohaveaccesstoallcolorinformation,butonlywithinagivenlocal\n",
            "receptivefield. 13Note that you can pass activation_fn=tanh as a parameter to the ConvPoolLayer and\n",
            "FullyConnectedLayerclasses. 14Youmayperhapsfindinspirationinrecallingthatσ (z)=(1+tanh(z/2)) /2.\n",
            "\n",
            "(cid:12)\n",
            "180 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "deeperintotheuseoftanh. Usingrectifiedlinearunits: Thenetworkwe’vedevelopedatthispointisactuallyavariant\n",
            "ofoneofthenetworksusedintheseminal1998paper15introducingtheMNISTproblem,\n",
            "anetworkknownasLeNet-5. It’sagoodfoundationforfurtherexperimentation,andfor\n",
            "buildingupunderstandingandintuition. Inparticular,therearemanywayswecanvarythe\n",
            "networkinanattempttoimproveourresults. Asabeginning,let’schangeourneuronssothatinsteadofusingasigmoidactivation\n",
            "function, we use rectified linear units. That is, we’ll use the activation function f(z)\n",
            "max(0,z).\n",
            "We’lltrainfor60epochs,withalearningrateofη =0.03. Ialsofoundthat≡it\n",
            "helpsalittletousesomel2regularization,withregularizationparameterλ =0.1:\n",
            ">>> from network3 import ReLU\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5), poolsize=(2, 2), activation_fn=ReLU),\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12), filter_shape=(40, 20, 5, 5),\n",
            "6\n",
            "poolsize=(2, 2), activation_fn=ReLU),FullyConnectedLayer(n_in=40*4*4, n_out\n",
            "=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(training_data, 60, mini_batch_size, 0.03, validation_data, test_data,\n",
            "lmbda=0.1)\n",
            "Iobtainedaclassificationaccuracyof99.23percent. It’samodestimprovementoverthe\n",
            "sigmoidresults(99.06).\n",
            "However,acrossallmyexperimentsIfoundthatnetworksbased\n",
            "onrectifiedlinearunitsconsistentlyoutperformednetworksbasedonsigmoidactivation\n",
            "functions. Thereappearstobearealgaininmovingtorectifiedlinearunitsforthisproblem. What makes the rectified linear activation function better than the sigmoid or tanh\n",
            "functions? Atpresent,wehaveapoorunderstandingoftheanswertothisquestion.\n",
            "Indeed,\n",
            "rectifiedlinearunitshaveonlybeguntobewidelyusedinthepastfewyears. Thereason\n",
            "forthatrecentadoptionisempirical: afewpeopletriedrectifiedlinearunits,oftenonthe\n",
            "basisofhunchesorheuristicarguments16. Theygotgoodresultsclassifyingbenchmarkdata\n",
            "sets,andthepracticehasspread. Inanidealworldwe’dhaveatheorytellinguswhich\n",
            "activationfunctiontopickforwhichapplication. Butatpresentwe’realongwayfromsuch\n",
            "aworld. Ishouldnotbeatallsurprisediffurthermajorimprovementscanbeobtainedby\n",
            "anevenbetterchoiceofactivationfunction. AndIalsoexpectthatincomingdecadesa\n",
            "powerfultheoryofactivationfunctionswillbedeveloped. Today,westillhavetorelyon\n",
            "poorlyunderstoodrulesofthumbandexperience. Expandingthetrainingdata: Anotherwaywemayhopetoimproveourresultsisby\n",
            "algorithmicallyexpandingthetrainingdata. Asimplewayofexpandingthetrainingdatais\n",
            "todisplaceeachtrainingimagebyasinglepixel,eitheruponepixel,downonepixel,left\n",
            "onepixel,orrightonepixel. Wecandothisbyrunningtheprogramexpand_mnist.py\n",
            "fromtheshellprompt17:\n",
            "15Gradient-basedlearningappliedtodocumentrecognition,byYannLeCun,LéonBottou,Yoshua\n",
            "Bengio,andPatrickHaffner(1998). Therearemanydifferencesofdetail,butbroadlyspeakingour\n",
            "networkisquitesimilartothenetworksdescribedinthepaper. 16Acommonjustificationisthatmax(0,z)doesn’tsaturateinthelimitoflargez,unlikesigmoid\n",
            "neurons,andthishelpsrectifiedlinearunitscontinuelearning.Theargumentisfine,asfaritgoes,but\n",
            "it’shardlyadetailedjustification,moreofajust-sostory. Notethatwediscussedtheproblemswith\n",
            "saturationbackinChapter2. 17Thecodeforexpand_mnist.pyisavailablehere. \n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 181\n",
            "(cid:12)\n",
            "$ python expand_mnist.py\n",
            "Runningthisprogramtakesthe50,000MNISTtrainingimages,andpreparesanexpanded\n",
            "trainingset,with250,000trainingimages. Wecanthenusethosetrainingimagestotrain\n",
            "ournetwork. We’llusethesamenetworkasabove,withrectifiedlinearunits. Inmyinitial\n",
            "experimentsIreducedthenumberoftrainingepochs–thismadesense,sincewe’retraining\n",
            "with5timesasmuchdata. But, infact, expandingthedataturnedouttoconsiderably\n",
            "reducetheeffectofoverfitting.\n",
            "Andso,aftersomeexperimentation,Ieventuallywentback\n",
            "totrainingfor60epochs. Inanycase,let’strain:\n",
            ">>> expanded_training_data, _, _ = network3.load_data_shared(\"../data/\n",
            "mnist_expanded.pkl.gz\")\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU), 6\n",
            "ConvPoolLayer(\n",
            "image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "UsingtheexpandedtrainingdataIobtaineda99.37percenttrainingaccuracy. Sothisalmost\n",
            "trivial change gives a substantial improvement in classification accuracy.\n",
            "Indeed, as we\n",
            "discussedearlierthisideaofalgorithmicallyexpandingthedatacanbetakenfurther. Justto\n",
            "remindyouoftheflavourofsomeoftheresultsinthatearlierdiscussion: in2003Simard,\n",
            "SteinkrausandPlatt18improvedtheirMNISTperformanceto99.6percentusinganeural\n",
            "networkotherwiseverysimilartoours,usingtwoconvolutional-poolinglayers,followedby\n",
            "ahiddenfully-connectedlayerwith100neurons. Therewereafewdifferencesofdetailin\n",
            "theirarchitecture–theydidn’thavetheadvantageofusingrectifiedlinearunits,forinstance–\n",
            "butthekeytotheirimprovedperformancewasexpandingthetrainingdata.\n",
            "Theydidthisby\n",
            "rotating,translating,andskewingtheMNISTtrainingimages. Theyalsodevelopedaprocess\n",
            "of“elasticdistortion”,awayofemulatingtherandomoscillationshandmusclesundergo\n",
            "whenapersoniswriting. Bycombiningalltheseprocessestheysubstantiallyincreasedthe\n",
            "effectivesizeoftheirtrainingdata,andthat’showtheyachieved99.6percentaccuracy. Problem\n",
            "Theideaofconvolutionallayersistobehaveinaninvariantwayacrossimages. It\n",
            "• mayseemsurprising,then,thatournetworkcanlearnmorewhenallwe’vedoneis\n",
            "translatetheinputdata. Canyouexplainwhythisisactuallyquitereasonable? Insertinganextrafully-connectedlayer: Canwedoevenbetter? Onepossibilityistouse\n",
            "exactlythesameprocedureasabove,buttoexpandthesizeofthefully-connectedlayer. I\n",
            "triedwith300and1,000neurons,obtainingresultsof99.46and99.43percent,respectively. That’sinteresting,butnotreallyaconvincingwinovertheearlierresult(99.37percent)\n",
            "18BestPracticesforConvolutionalNeuralNetworksAppliedtoVisualDocumentAnalysis,byPatrice\n",
            "Simard,DaveSteinkraus,andJohnPlatt(2003). \n",
            "(cid:12)\n",
            "182 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Whataboutaddinganextrafully-connectedlayer? Let’stryinsertinganextrafully-\n",
            "connectedlayer,sothatwehavetwo100-hiddenneuronfully-connectedlayers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=40*4*4, n_out=100, activation_fn=ReLU),\n",
            "FullyConnectedLayer(n_in=100, n_out=100, activation_fn=ReLU),\n",
            "SoftmaxLayer(n_in=100, n_out=10)], mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 60, mini_batch_size, 0.03, validation_data,\n",
            "test_data, lmbda=0.1)\n",
            "Doingthis,Iobtainedatestaccuracyof99.43percent. Again,theexpandednetisn’thelping\n",
            "6\n",
            "somuch. Runningsimilarexperimentswithfully-connectedlayerscontaining300and1,000\n",
            "neuronsyieldsresultsof99.48and99.47percent. That’sencouraging,butstillfallsshortof\n",
            "areallydecisivewin. What’sgoingonhere? Isitthattheexpandedorextrafully-connectedlayersreally\n",
            "don’thelpwithMNIST?Ormightitbethatournetworkhasthecapacitytodobetter,but\n",
            "we’regoingaboutlearningthewrongway? Forinstance, maybewecouldusestronger\n",
            "regularizationtechniquestoreducethetendencytooverfit. Onepossibilityisthedropout\n",
            "techniqueintroducedbackinChapter3. Recallthatthebasicideaofdropoutistoremove\n",
            "individualactivationsatrandomwhiletrainingthenetwork.\n",
            "Thismakesthemodelmore\n",
            "robusttothelossofindividualpiecesofevidence,andthuslesslikelytorelyonparticular\n",
            "idiosyncraciesofthetrainingdata. Let’stryapplyingdropouttothefinalfully-connected\n",
            "layers:\n",
            ">>> net = Network([\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 1, 28, 28),\n",
            "filter_shape=(20, 1, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "ConvPoolLayer(image_shape=(mini_batch_size, 20, 12, 12),\n",
            "filter_shape=(40, 20, 5, 5),\n",
            "poolsize=(2, 2),\n",
            "activation_fn=ReLU),\n",
            "FullyConnectedLayer(\n",
            "n_in=40*4*4, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "FullyConnectedLayer(\n",
            "n_in=1000, n_out=1000, activation_fn=ReLU, p_dropout=0.5),\n",
            "SoftmaxLayer(n_in=1000, n_out=10, p_dropout=0.5)],\n",
            "mini_batch_size)\n",
            ">>> net.SGD(expanded_training_data, 40, mini_batch_size, 0.03,\n",
            "validation_data, test_data)\n",
            "Usingthis,weobtainanaccuracyof99.60percent,whichisasubstantialimprovementover\n",
            "ourearlierresults,especiallyourmainbenchmark,thenetworkwith100hiddenneurons,\n",
            "whereweachieved99.37percent. Therearetwochangesworthnoting.\n",
            "\n",
            "(cid:12)\n",
            "6.2. Convolutionalneuralnetworksinpractice (cid:12) 183\n",
            "(cid:12)\n",
            "First,Ireducedthenumberoftrainingepochsto40: dropoutreducedoverfitting,and\n",
            "sowelearnedfaster. Second,thefully-connectedhiddenlayershave1,000neurons,notthe100usedearlier. Ofcourse,dropouteffectivelyomitsmanyoftheneuronswhiletraining,sosomeexpansion\n",
            "istobeexpected. Infact,Itriedexperimentswithboth300and1,000hiddenneurons,and\n",
            "obtained(veryslightly)bettervalidationperformancewith1,000hiddenneurons. Usinganensembleofnetworks: Aneasywaytoimproveperformancestillfurtheristo\n",
            "createseveralneuralnetworks,andthengetthemtovotetodeterminethebestclassification. Suppose,forexample,thatwetrained5differentneuralnetworksusingtheprescription\n",
            "above, with each achieving accuracies near to 99.6 percent. Even though the networks\n",
            "wouldallhavesimilaraccuracies,theymightwellmakedifferenterrors,duetothedifferent\n",
            "randominitializations. It’splausiblethattakingavoteamongstour5networksmightyield\n",
            "aclassificationbetterthananyindividualnetwork. Thissoundstoogoodtobetrue,butthiskindofensemblingisacommontrickwithboth\n",
            "neuralnetworksandothermachinelearningtechniques. Anditdoesinfactyieldfurther\n",
            "improvements: weendupwith99.67percentaccuracy. Inotherwords,ourensembleof\n",
            "networksclassifiesallbut33ofthe10,000testimagescorrectly. 6\n",
            "Theremainingerrorsinthetestsetareshownbelow. Thelabelinthetoprightisthe\n",
            "correctclassification,accordingtotheMNISTdata,whileinthebottomrightisthelabel\n",
            "outputbyourensembleofnets:\n",
            "It’sworthlookingthroughtheseindetail. Thefirsttwodigits,a6anda5,aregenuineerrors\n",
            "byourensemble.\n",
            "However, they’realsounderstandableerrors, thekindahumancould\n",
            "plausiblymake. That6reallydoeslookalotlikea0,andthe5looksalotlikea3. Thethird\n",
            "image,supposedlyan8,actuallylookstomemorelikea9. SoI’msidingwiththenetwork\n",
            "ensemblehere: Ithinkit’sdoneabetterjobthanwhoeveroriginallydrewthedigit. Onthe\n",
            "otherhand,thefourthimage,the6,reallydoesseemtobeclassifiedbadlybyournetworks.\n",
            "Andsoon. Inmostcasesournetworks’choicesseematleastplausible,andinsomecases\n",
            "they’vedoneabetterjobclassifyingthantheoriginalpersondidwritingthedigit. Overall,\n",
            "ournetworksofferexceptionalperformance,especiallywhenyouconsiderthattheycorrectly\n",
            "classified9,967imageswhicharen’tshown.\n",
            "Inthatcontext,thefewclearerrorshereseem\n",
            "quiteunderstandable. Evenacarefulhumanmakestheoccasionalmistake. AndsoIexpect\n",
            "\n",
            "(cid:12)\n",
            "184 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "thatonlyanextremelycarefulandmethodicalhumanwoulddomuchbetter. Ournetwork\n",
            "isgettingneartohumanperformance. Whyweonlyapplieddropouttothefully-connectedlayers: Ifyoulookcarefullyat\n",
            "thecodeabove,you’llnoticethatweapplieddropoutonlytothefully-connectedsectionof\n",
            "thenetwork,nottotheconvolutionallayers. Inprinciplewecouldapplyasimilarprocedure\n",
            "totheconvolutionallayers. But, infact, there’snoneed: theconvolutionallayershave\n",
            "considerableinbuiltresistancetooverfitting. Thereasonisthatthesharedweightsmean\n",
            "thatconvolutionalfiltersareforcedtolearnfromacrosstheentireimage. Thismakesthem\n",
            "lesslikelytopickuponlocalidiosyncraciesinthetrainingdata. Andsothereislessneedto\n",
            "applyotherregularizers,suchasdropout. Goingfurther: It’spossibletoimproveperformanceonMNISTstillfurther. Rodrigo\n",
            "Benensonhascompiledaninformativesummarypage,showingprogressovertheyears,with\n",
            "linkstopapers. Manyofthesepapersusedeepconvolutionalnetworksalonglinessimilarto\n",
            "thenetworkswe’vebeenusing. Ifyoudigthroughthepapersyou’llfindmanyinteresting\n",
            "techniques,andyoumayenjoyimplementingsomeofthem. Ifyoudosoit’swisetostart\n",
            "implementationwithasimplenetworkthatcanbetrainedquickly,whichwillhelpyoumore\n",
            "6\n",
            "rapidlyunderstandwhatisgoingon. Forthemostpart,Iwon’ttrytosurveythisrecentwork. ButIcan’tresistmakingone\n",
            "exception. It’sa2010paperbyCire¸san,Meier,Gambardella,andSchmidhuber19.\n",
            "WhatI\n",
            "likeaboutthispaperishowsimpleitis. Thenetworkisamany-layerneuralnetwork,using\n",
            "onlyfully-connectedlayers(noconvolutions). Theirmostsuccessfulnetworkhadhidden\n",
            "layerscontaining2,500,2,000,1,500,1,000,and500neurons,respectively. Theyusedideas\n",
            "similartoSimardetaltoexpandtheirtrainingdata. Butapartfromthat,theyusedfew\n",
            "othertricks,includingnoconvolutionallayers: itwasaplain,vanillanetwork,ofthekind\n",
            "that,withenoughpatience,couldhavebeentrainedinthe1980s(iftheMNISTdatasethad\n",
            "existed),givenenoughcomputingpower. Theyachievedaclassificationaccuracyof99.65\n",
            "percent,moreorlessthesameasours. Thekeywastouseaverylarge,verydeepnetwork,\n",
            "andtouseaGPUtospeeduptraining. Thisletthemtrainformanyepochs. Theyalsotook\n",
            "advantageoftheirlongtrainingtimestograduallydecreasethelearningratefrom10 3to\n",
            "−\n",
            "10 6. It’safunexercisetotrytomatchtheseresultsusinganarchitectureliketheirs. −\n",
            "Why are we able to train? We saw in the last chapter that there are fundamental\n",
            "obstructionstotrainingindeep,many-layerneuralnetworks. Inparticular,wesawthatthe\n",
            "gradienttendstobequiteunstable: aswemovefromtheoutputlayertoearlierlayersthe\n",
            "gradienttendstoeithervanish(thevanishinggradientproblem)orexplode(theexploding\n",
            "gradientproblem). Sincethegradientisthesignalweusetotrain,thiscausesproblems.\n",
            "Howhaveweavoidedthoseresults? Ofcourse, theansweristhatwehaven’tavoidedtheseresults.\n",
            "Instead, we’vedone\n",
            "afewthingsthathelpusproceedanyway. Inparticular: (1)Usingconvolutionallayers\n",
            "greatlyreducesthenumberofparametersinthoselayers, makingthelearningproblem\n",
            "much easier; (2) Using more powerful regularization techniques (notably dropout and\n",
            "convolutionallayers)toreduceoverfitting,whichisotherwisemoreofaprobleminmore\n",
            "complexnetworks;(3)Usingrectifiedlinearunitsinsteadofsigmoidneurons,tospeedup\n",
            "training–empirically,oftenbyafactorof3–5;(4)UsingGPUsandbeingwillingtotrainfor\n",
            "alongperiodoftime. Inparticular,inourfinalexperimentswetrainedfor40epochsusing\n",
            "adataset5timeslargerthantherawMNISTtrainingdata. Earlierinthebookwemostly\n",
            "19Deep,Big,SimpleNeuralNetsExcelonHandwrittenDigitRecognition,byDanClaudiuCire¸san,\n",
            "UeliMeier,LucaMariaGambardella,andJürgenSchmidhuber(2010).\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 185\n",
            "(cid:12)\n",
            "trainedfor30epochsusingjusttherawtrainingdata. Combiningfactors(3)and(4)it’sas\n",
            "thoughwe’vetrainedafactorperhaps30timeslongerthanbefore. Yourresponsemaybe“Isthatit?\n",
            "Isthatallwehadtodototraindeepnetworks? What’s\n",
            "allthefussabout?”\n",
            "Ofcourse,we’veusedotherideas,too: makinguseofsufficientlylargedatasets(to\n",
            "helpavoidoverfitting);usingtherightcostfunction(toavoidalearningslowdown);using\n",
            "goodweightinitializations(alsotoavoidalearningslowdown,duetoneuronsaturation);\n",
            "algorithmicallyexpandingthetrainingdata. Wediscussedtheseandotherideasinearlier\n",
            "chapters,andhaveforthemostpartbeenabletoreusetheseideaswithlittlecommentin\n",
            "thischapter. Withthatsaid,thisreallyisarathersimplesetofideas. Simple,butpowerful,when\n",
            "usedinconcert. Gettingstartedwithdeeplearninghasturnedouttobeprettyeasy!\n",
            "Howdeeparethesenetworks,anyway? Countingtheconvolutional-poolinglayers\n",
            "as single layers, our final architecture has 4 hidden layers. Does such a network really\n",
            "deserve to be called a deep network? Of course, 4 hidden layers is many more than in\n",
            "6\n",
            "theshallownetworkswestudiedearlier. Mostofthosenetworksonlyhadasinglehidden\n",
            "layer,oroccasionally2hiddenlayers. Ontheotherhand,asof2015state-of-the-artdeep\n",
            "networkssometimeshavedozensofhiddenlayers. I’veoccasionallyheardpeopleadopta\n",
            "deeper-than-thouattitude,holdingthatifyou’renotkeeping-up-with-the-Jonesesinterms\n",
            "ofnumberofhiddenlayers,thenyou’renotreallydoingdeeplearning. I’mnotsympathetic\n",
            "tothisattitude, inpartbecauseitmakesthedefinitionofdeeplearningintosomething\n",
            "whichdependsupontheresult-of-the-moment. Therealbreakthroughindeeplearningwas\n",
            "torealizethatit’spracticaltogobeyondtheshallow1-and2-hiddenlayernetworksthat\n",
            "dominatedworkuntilthemid-2000s. Thatreallywasasignificantbreakthrough,opening\n",
            "uptheexplorationofmuchmoreexpressivemodels. Butbeyondthat,thenumberoflayers\n",
            "isnotofprimaryfundamentalinterest. Rather,theuseofdeepernetworksisatooltouseto\n",
            "helpachieveothergoals–likebetterclassificationaccuracies. Awordonprocedure: Inthissection,we’vesmoothlymovedfromsinglehidden-layer\n",
            "shallownetworkstomany-layerconvolutionalnetworks. Itallseemedsoeasy!\n",
            "Wemakea\n",
            "changeand,forthemostpart,wegetanimprovement. Ifyoustartexperimenting,Ican\n",
            "guaranteethingswon’talwaysbesosmooth. ThereasonisthatI’vepresentedacleaned-up\n",
            "narrative,omittingmanyexperiments–includingmanyfailedexperiments. Thiscleaned-up\n",
            "narrativewillhopefullyhelpyougetclearonthebasicideas. Butitalsorunstheriskof\n",
            "conveyinganincompleteimpression. Gettingagood,workingnetworkcaninvolvealotof\n",
            "trialanderror,andoccasionalfrustration. Inpractice,youshouldexpecttoengageinquite\n",
            "abitofexperimentation.\n",
            "TospeedthatprocessupyoumayfindithelpfultorevisitChapter\n",
            "3’sdiscussionofhowtochooseaneuralnetwork’shyper-parameters,andperhapsalsoto\n",
            "lookatsomeofthefurtherreadingsuggestedinthatsection. 6.3 The code for our convolutional networks\n",
            "Alright,let’stakealookatthecodeforourprogram,network3.py. Structurally,it’ssimilar\n",
            "tonetwork2.py,theprogramwedevelopedinChapter3,althoughthedetailsdiffer,due\n",
            "totheuseofTheano. We’llstartbylookingattheFullyConnectedLayerclass,whichis\n",
            "\n",
            "(cid:12)\n",
            "186 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "similartothelayersstudiedearlierinthebook. Here’sthecode(discussionbelow)20:\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "6 self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "Muchofthe__init__methodisself-explanatory,butafewremarksmayhelpclarifythe\n",
            "code. Asperusual,werandomlyinitializetheweightsandbiasesasnormalrandomvariables\n",
            "withsuitablestandarddeviations.\n",
            "Thelinesdoingthislookalittleforbidding. However,\n",
            "mostofthecomplicationisjustloadingtheweightsandbiasesintowhatTheanocallsshared\n",
            "variables. ThisensuresthatthesevariablescanbeprocessedontheGPU,ifoneisavailable. Wewon’tgettoomuchintothedetailsofthis. Ifyou’reinterested,youcandigintothe\n",
            "Theanodocumentation. Notealsothatthisweightandbiasinitializationisdesignedforthe\n",
            "sigmoidactivationfunction(asdiscussedearlier). Ideally,we’dinitializetheweightsand\n",
            "biasessomewhatdifferentlyforactivationfunctionssuchasthetanhandrectifiedlinear\n",
            "function. Thisisdiscussedfurtherinproblemsbelow.\n",
            "The__init__methodfinisheswith\n",
            "self.params = [self.w, self.b]. Thisisahandywaytobundleupallthelearnable\n",
            "parametersassociatedtothelayer. Lateron, theNetwork.SGDmethodwilluseparams\n",
            "attributestofigureoutwhatvariablesinaNetworkinstancecanlearn. Theset_inptmethodisusedtosettheinputtothelayer,andtocomputethecorre-\n",
            "spondingoutput. Iusethenameinptratherthaninputbecauseinputisabuilt-infunction\n",
            "20NoteaddedNovember2016:severalreadershavenotedthatinthelineinitializingself.w,Iset\n",
            "scale=np.sqrt(1.0/n_out),whentheargumentsofChapter3suggestabetterinitializationmaybe\n",
            "scale=np.sqrt(1.0/n_in).Thiswassimplyamistakeonmypart.InanidealworldI’drerunallthe\n",
            "examplesinthischapterwiththecorrectcode.Still,I’vemovedontootherprojects,soamgoingtolet\n",
            "theerrorgo. \n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 187\n",
            "(cid:12)\n",
            "inPython,andmessingwithbuilt-instendstocauseunpredictablebehavioranddifficult-to-\n",
            "diagnosebugs. Notethatweactuallysettheinputintwoseparateways: asself.inptand\n",
            "self.inpt_dropout. Thisisdonebecauseduringtrainingwemaywanttousedropout. If\n",
            "that’sthecasethenwewanttoremoveafractionself.p_dropoutoftheneurons. That’s\n",
            "whatthefunctiondropout_layerinthesecond-lastlineoftheset_inptmethodisdo-\n",
            "ing. Soself.inpt_dropoutandself.output_dropoutareusedduringtraining,while\n",
            "self.inptandself.outputareusedforallotherpurposes,e.g.,evaluatingaccuracyonthe\n",
            "validationandtestdata. TheConvPoolLayerandSoftmaxLayerclassdefinitionsaresimilartoFullyConnectedLayer\n",
            ". Indeed,they’resoclosethatIwon’texcerptthecodehere.\n",
            "Ifyou’reinterestedyoucanlook\n",
            "atthefulllistingfornetwork3.py,laterinthissection. However,acoupleofminordifferencesofdetailareworthmentioning. Mostobviously,\n",
            "inbothConvPoolLayerandSoftmaxLayerwecomputetheoutputactivationsintheway\n",
            "appropriate to that layer type. Fortunately, Theano makes that easy, providing built-in\n",
            "operationstocomputeconvolutions,max-pooling,andthesoftmaxfunction. Less obviously, when we introduced the softmax layer, we never discussed how to\n",
            "initializetheweightsandbiases. Elsewherewe’vearguedthatforsigmoidlayersweshould 6\n",
            "initialize the weights using suitably parameterized normal random variables. But that\n",
            "heuristicargumentwasspecifictosigmoidneurons(and,withsomeamendment,totanh\n",
            "neurons). However,there’snoparticularreasontheargumentshouldapplytosoftmaxlayers. Sothere’snoapriorireasontoapplythatinitializationagain. Ratherthandothat,Ishall\n",
            "initializealltheweightsandbiasestobe0. Thisisaratheradhocprocedure,butworks\n",
            "wellenoughinpractice. Okay,we’velookedatallthelayerclasses.\n",
            "WhatabouttheNetworkclass? Let’sstartby\n",
            "lookingatthe__init__method:\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of ‘layers‘, describing the network architecture, and\n",
            "a value for the ‘mini_batch_size‘ to be used during training\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "Mostofthisisself-explanatory,ornearlyso. Thelineself.params = [param for layer\n",
            "in ...]bundlesuptheparametersforeachlayerintoasinglelist.\n",
            "Asanticipatedabove,\n",
            "theNetwork.SGDmethodwilluseself.paramstofigureoutwhatvariablesintheNetwork\n",
            "canlearn. Thelinesself.x = T.matrix(\"x\")andself.y = T.ivector(\"y\")define\n",
            "Theanosymbolicvariablesnamedxandy. Thesewillbeusedtorepresenttheinputand\n",
            "\n",
            "(cid:12)\n",
            "188 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "desiredoutputfromthenetwork. Now,thisisn’taTheanotutorial,andsowewon’tgettoodeeplyintowhatitmeans\n",
            "thatthesearesymbolicvariables21. Buttheroughideaisthattheserepresentmathematical\n",
            "variables,notexplicitvalues. Wecandoalltheusualthingsonewoulddowithsuchvariables:\n",
            "add, subtract, andmultiplythem, applyfunctions, andsoon. Indeed, Theanoprovides\n",
            "manywaysofmanipulatingsuchsymbolicvariables,doingthingslikeconvolutions,max-\n",
            "pooling,andsoon. Butthebigwinistheabilitytodofastsymbolicdifferentiation,usinga\n",
            "verygeneralformofthebackpropagationalgorithm. Thisisextremelyusefulforapplying\n",
            "stochasticgradientdescenttoawidevarietyofnetworkarchitectures. Inparticular,thenext\n",
            "fewlinesofcodedefinesymbolicoutputsfromthenetwork. Westartbysettingtheinputto\n",
            "theinitiallayer,withtheline\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "Notethattheinputsaresetonemini-batchatatime,whichiswhythemini-batchsizeis\n",
            "there. Notealsothatwepasstheinputself.xintwice: thisisbecausewemayusethe\n",
            "networkintwodifferentways(withorwithoutdropout). Theforloopthenpropagatesthe\n",
            "6\n",
            "symbolicvariableself.xforwardthroughthelayersoftheNetwork. Thisallowsustodefine\n",
            "thefinaloutputandoutput_dropoutattributes,whichsymbolicallyrepresenttheoutput\n",
            "fromtheNetwork. Nowthatwe’veunderstoodhowaNetworkisinitialized,let’slookathowitistrained,\n",
            "usingtheSGDmethod. Thecodelookslengthy,butitsstructureisactuallyrathersimple.\n",
            "Explanatorycommentsafterthecode. def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches. i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "21TheTheanodocumentationprovidesagoodintroductiontoTheano.Andifyougetstuck,youmay\n",
            "findithelpfultolookatoneoftheothertutorialsavailableonline. Forinstance,thistutorialcovers\n",
            "manybasics.\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 189\n",
            "(cid:12)\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out, 6\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format(\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(’The corresponding test accuracy is {0:.2%}’.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "Thefirstfewlinesarestraightforward,separatingthedatasetsinto x and y components,\n",
            "andcomputingthenumberofmini-batchesusedineachdataset. Thenextfewlinesare\n",
            "moreinteresting,andshowsomeofwhatmakesTheanofuntoworkwith.\n",
            "Let’sexplicitly\n",
            "excerptthelineshere:\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "\n",
            "(cid:12)\n",
            "190 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad) for param, grad in zip(self.params, grads)]\n",
            "Intheselineswesymbolicallysetuptheregularizedlog-likelihoodcostfunction,computethe\n",
            "correspondingderivativesinthegradientfunction,aswellasthecorrespondingparameter\n",
            "updates. Theanoletsusachieveallofthisinjustthesefewlines.\n",
            "Theonlythinghiddenis\n",
            "thatcomputingthecostinvolvesacalltothecostmethodfortheoutputlayer;thatcodeis\n",
            "elsewhereinnetwork3.py. Butthatcodeisshortandsimple,anyway. Withallthesethings\n",
            "defined,thestageissettodefinethetrain_mbfunction,aTheanosymbolicfunctionwhich\n",
            "usestheupdatestoupdatetheNetworkparameters,givenamini-batchindex. Similarly,\n",
            "validate_mb_accuracyandtest_mb_accuracycomputetheaccuracyoftheNetworkon\n",
            "anygivenmini-batchofvalidationortestdata. Byaveragingoverthesefunctions,wewill\n",
            "beabletocomputeaccuraciesontheentirevalidationandtestdatasets. The remainder of the SGD method is self-explanatory – we simply iterate over the\n",
            "epochs,repeatedlytrainingthenetworkonmini-batchesoftrainingdata,andcomputing\n",
            "thevalidationandtestaccuracies. 6\n",
            "Okay,we’venowunderstoodthemostimportantpiecesofcodeinnetwork3.py.\n",
            "Let’s\n",
            "takeabrieflookattheentireprogram. Youdon’tneedtoreadthroughthisindetail,butyou\n",
            "mayenjoyglancingoverit,andperhapsdivingdownintoanypiecesthatstrikeyourfancy. Thebestwaytoreallyunderstanditis,ofcourse,bymodifyingit,addingextrafeatures,or\n",
            "refactoringanythingyouthinkcouldbedonemoreelegantly. Afterthecode,therearesome\n",
            "problemswhichcontainafewstartersuggestionsforthingstodo. Here’sthecode22:\n",
            "\"\"\"network3.py\n",
            "~~~~~~~~~~~~~~\n",
            "A Theano-based program for training and running simple neural\n",
            "networks. Supports several layer types (fully connected, convolutional, max\n",
            "pooling, softmax), and activation functions (sigmoid, tanh, and\n",
            "rectified linear units, with more easily added). When run on a CPU, this program is much faster than network.py and\n",
            "network2.py. However, unlike network.py and network2.py it can also\n",
            "be run on a GPU, which makes it faster still. Because the code is based on Theano, the code is different in many\n",
            "ways from network.py and network2.py. However, where possible I have\n",
            "tried to maintain consistency with the earlier programs. In\n",
            "particular, the API is similar to network2.py. Note that I have\n",
            "focused on making the code simple, easily readable, and easily\n",
            "modifiable.\n",
            "It is not optimized, and omits many desirable features. This program incorporates ideas from the Theano documentation on\n",
            "convolutional neural nets (notably,\n",
            "http://deeplearning.net/tutorial/lenet.html ), from Misha Denil’s\n",
            "implementation of dropout (https://github.com/mdenil/dropout ), and\n",
            "from Chris Olah (http://colah.github.io ). 22UsingTheanoonaGPUcanbealittletricky.Inparticular,it’seasytomakethemistakeofpulling\n",
            "dataofftheGPU,whichcanslowthingsdownalot.I’vetriedtoavoidthis.Withthatsaid,thiscode\n",
            "cancertainlybespedupquiteabitfurtherwithcarefuloptimizationofTheano’sconfiguration.Seethe\n",
            "Theanodocumentationformoredetails. \n",
            "(cid:12)\n",
            "6.3.\n",
            "Thecodeforourconvolutionalnetworks (cid:12) 191\n",
            "(cid:12)\n",
            "Written for Theano 0.6 and 0.7, needs some changes for more recent\n",
            "versions of Theano. \"\"\"\n",
            "#### Libraries\n",
            "# Standard library\n",
            "import cPickle\n",
            "import gzip\n",
            "# Third-party libraries\n",
            "import numpy as np\n",
            "import theano\n",
            "import theano.tensor as T\n",
            "from theano.tensor.nnet import conv\n",
            "from theano.tensor.nnet import softmax\n",
            "from theano.tensor import shared_randomstreams\n",
            "from theano.tensor.signal import downsample\n",
            "# Activation functions for neurons\n",
            "def linear(z): return z 6\n",
            "def ReLU(z): return T.maximum(0.0, z)\n",
            "from theano.tensor.nnet import sigmoid\n",
            "from theano.tensor import tanh\n",
            "#### Constants\n",
            "GPU = True\n",
            "if GPU:\n",
            "print \"Trying to run under a GPU. If this is not desired, then modify \"+\\\n",
            "\"network3.py\\nto set the GPU flag to False.\"\n",
            "try: theano.config.device = ’gpu’\n",
            "except: pass # it’s already set\n",
            "theano.config.floatX = ’float32’\n",
            "else:\n",
            "print \"Running with a CPU. If this is not desired, then the modify \"+\\\n",
            "\"network3.py to set\\nthe GPU flag to True.\"\n",
            "#### Load the MNIST data\n",
            "def load_data_shared(filename=\"../data/mnist.pkl.gz\"):\n",
            "f = gzip.open(filename, ’rb’)\n",
            "training_data, validation_data, test_data = cPickle.load(f)\n",
            "f.close()\n",
            "def shared(data):\n",
            "\"\"\"Place the data into shared variables. This allows Theano to copy\n",
            "the data to the GPU, if one is available. \"\"\"\n",
            "shared_x = theano.shared(\n",
            "np.asarray(data[0], dtype=theano.config.floatX), borrow=True)\n",
            "shared_y = theano.shared(\n",
            "np.asarray(data[1], dtype=theano.config.floatX), borrow=True)\n",
            "return shared_x, T.cast(shared_y, \"int32\")\n",
            "return [shared(training_data), shared(validation_data), shared(test_data)]\n",
            "#### Main class used to construct and train networks\n",
            "class Network(object):\n",
            "def __init__(self, layers, mini_batch_size):\n",
            "\"\"\"Takes a list of ‘layers‘, describing the network architecture, and\n",
            "a value for the ‘mini_batch_size‘ to be used during training\n",
            "\n",
            "(cid:12)\n",
            "192 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "by stochastic gradient descent. \"\"\"\n",
            "self.layers = layers\n",
            "self.mini_batch_size = mini_batch_size\n",
            "self.params = [param for layer in self.layers for param in layer.params]\n",
            "self.x = T.matrix(\"x\")\n",
            "self.y = T.ivector(\"y\")\n",
            "init_layer = self.layers[0]\n",
            "init_layer.set_inpt(self.x, self.x, self.mini_batch_size)\n",
            "for j in xrange(1, len(self.layers)):\n",
            "prev_layer, layer = self.layers[j-1], self.layers[j]\n",
            "layer.set_inpt(\n",
            "prev_layer.output, prev_layer.output_dropout, self.mini_batch_size)\n",
            "self.output = self.layers[-1].output\n",
            "self.output_dropout = self.layers[-1].output_dropout\n",
            "def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
            "validation_data, test_data, lmbda=0.0):\n",
            "\"\"\"Train the network using mini-batch stochastic gradient descent.\"\"\"\n",
            "6 training_x, training_y = training_data\n",
            "validation_x, validation_y = validation_data\n",
            "test_x, test_y = test_data\n",
            "# compute number of minibatches for training, validation and testing\n",
            "num_training_batches = size(training_data)/mini_batch_size\n",
            "num_validation_batches = size(validation_data)/mini_batch_size\n",
            "num_test_batches = size(test_data)/mini_batch_size\n",
            "# define the (regularized) cost function, symbolic gradients, and updates\n",
            "l2_norm_squared = sum([(layer.w**2).sum() for layer in self.layers])\n",
            "cost = self.layers[-1].cost(self)+\\\n",
            "0.5*lmbda*l2_norm_squared/num_training_batches\n",
            "grads = T.grad(cost, self.params)\n",
            "updates = [(param, param-eta*grad)\n",
            "for param, grad in zip(self.params, grads)]\n",
            "# define functions to train a mini-batch, and to compute the\n",
            "# accuracy in validation and test mini-batches.\n",
            "i = T.lscalar() # mini-batch index\n",
            "train_mb = theano.function(\n",
            "[i], cost, updates=updates,\n",
            "givens={\n",
            "self.x:\n",
            "training_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "training_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "validate_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "validation_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "self.y:\n",
            "validation_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "test_mb_accuracy = theano.function(\n",
            "[i], self.layers[-1].accuracy(self.y),\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size],\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 193\n",
            "(cid:12)\n",
            "self.y:\n",
            "test_y[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "self.test_mb_predictions = theano.function(\n",
            "[i], self.layers[-1].y_out,\n",
            "givens={\n",
            "self.x:\n",
            "test_x[i*self.mini_batch_size: (i+1)*self.mini_batch_size]\n",
            "})\n",
            "# Do the actual training\n",
            "best_validation_accuracy = 0.0\n",
            "for epoch in xrange(epochs):\n",
            "for minibatch_index in xrange(num_training_batches):\n",
            "iteration = num_training_batches*epoch+minibatch_index\n",
            "if iteration % 1000 == 0:\n",
            "print(\"Training mini-batch number {0}\".format(iteration))\n",
            "cost_ij = train_mb(minibatch_index)\n",
            "if (iteration+1) % num_training_batches == 0:\n",
            "validation_accuracy = np.mean(\n",
            "[validate_mb_accuracy(j) for j in xrange(num_validation_batches)])\n",
            "print(\"Epoch {0}: validation accuracy {1:.2%}\".format( 6\n",
            "epoch, validation_accuracy))\n",
            "if validation_accuracy >= best_validation_accuracy:\n",
            "print(\"This is the best validation accuracy to date.\")\n",
            "best_validation_accuracy = validation_accuracy\n",
            "best_iteration = iteration\n",
            "if test_data:\n",
            "test_accuracy = np.mean(\n",
            "[test_mb_accuracy(j) for j in xrange(num_test_batches)])\n",
            "print(’The corresponding test accuracy is {0:.2%}’.format(\n",
            "test_accuracy))\n",
            "print(\"Finished training network.\")\n",
            "print(\"Best validation accuracy of {0:.2%} obtained at iteration {1}\".format(\n",
            "best_validation_accuracy, best_iteration))\n",
            "print(\"Corresponding test accuracy of {0:.2%}\".format(test_accuracy))\n",
            "#### Define layer types\n",
            "class ConvPoolLayer(object):\n",
            "\"\"\"Used to create a combination of a convolutional and a max-pooling\n",
            "layer. A more sophisticated implementation would separate the\n",
            "two, but for our purposes we’ll always use them together, and it\n",
            "simplifies the code, so it makes sense to combine them.\n",
            "\"\"\"\n",
            "def __init__(self, filter_shape, image_shape, poolsize=(2, 2),\n",
            "activation_fn=sigmoid):\n",
            "\"\"\"‘filter_shape‘ is a tuple of length 4, whose entries are the number\n",
            "of filters, the number of input feature maps, the filter height, and the\n",
            "filter width. ‘image_shape‘ is a tuple of length 4, whose entries are the\n",
            "mini-batch size, the number of input feature maps, the image\n",
            "height, and the image width. ‘poolsize‘ is a tuple of length 2, whose entries are the y and\n",
            "x pooling sizes. \"\"\"\n",
            "self.filter_shape = filter_shape\n",
            "\n",
            "(cid:12)\n",
            "194 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "self.image_shape = image_shape\n",
            "self.poolsize = poolsize\n",
            "self.activation_fn=activation_fn\n",
            "# initialize weights and biases\n",
            "n_out = (filter_shape[0]*np.prod(filter_shape[2:])/np.prod(poolsize))\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=np.sqrt(1.0/n_out), size=filter_shape),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(loc=0, scale=1.0, size=(filter_shape[0],)),\n",
            "dtype=theano.config.floatX),\n",
            "borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape(self.image_shape)\n",
            "conv_out = conv.conv2d(\n",
            "6 input=self.inpt, filters=self.w, filter_shape=self.filter_shape,\n",
            "image_shape=self.image_shape)\n",
            "pooled_out = downsample.max_pool_2d(\n",
            "input=conv_out, ds=self.poolsize, ignore_border=True)\n",
            "self.output = self.activation_fn(\n",
            "pooled_out + self.b.dimshuffle(’x’, 0, ’x’, ’x’))\n",
            "self.output_dropout = self.output # no dropout in the convolutional layers\n",
            "class FullyConnectedLayer(object):\n",
            "def __init__(self, n_in, n_out, activation_fn=sigmoid, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.activation_fn = activation_fn\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.asarray(\n",
            "np.random.normal(\n",
            "loc=0.0, scale=np.sqrt(1.0/n_out), size=(n_in, n_out)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.asarray(np.random.normal(loc=0.0, scale=1.0, size=(n_out,)),\n",
            "dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = self.activation_fn(\n",
            "(1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer(\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = self.activation_fn(\n",
            "T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "\n",
            "(cid:12)\n",
            "6.3. Thecodeforourconvolutionalnetworks (cid:12) 195\n",
            "(cid:12)\n",
            "class SoftmaxLayer(object):\n",
            "def __init__(self, n_in, n_out, p_dropout=0.0):\n",
            "self.n_in = n_in\n",
            "self.n_out = n_out\n",
            "self.p_dropout = p_dropout\n",
            "# Initialize weights and biases\n",
            "self.w = theano.shared(\n",
            "np.zeros((n_in, n_out), dtype=theano.config.floatX),\n",
            "name=’w’, borrow=True)\n",
            "self.b = theano.shared(\n",
            "np.zeros((n_out,), dtype=theano.config.floatX),\n",
            "name=’b’, borrow=True)\n",
            "self.params = [self.w, self.b]\n",
            "def set_inpt(self, inpt, inpt_dropout, mini_batch_size):\n",
            "self.inpt = inpt.reshape((mini_batch_size, self.n_in))\n",
            "self.output = softmax((1-self.p_dropout)*T.dot(self.inpt, self.w) + self.b)\n",
            "self.y_out = T.argmax(self.output, axis=1)\n",
            "self.inpt_dropout = dropout_layer( 6\n",
            "inpt_dropout.reshape((mini_batch_size, self.n_in)), self.p_dropout)\n",
            "self.output_dropout = softmax(T.dot(self.inpt_dropout, self.w) + self.b)\n",
            "def cost(self, net):\n",
            "\"Return the log-likelihood cost.\"\n",
            "return -T.mean(T.log(self.output_dropout)[T.arange(net.y.shape[0]), net.y])\n",
            "def accuracy(self, y):\n",
            "\"Return the accuracy for the mini-batch.\"\n",
            "return T.mean(T.eq(y, self.y_out))\n",
            "#### Miscellanea\n",
            "def size(data):\n",
            "\"Return the size of the dataset ‘data‘.\"\n",
            "return data[0].get_value(borrow=True).shape[0]\n",
            "def dropout_layer(layer, p_dropout):\n",
            "srng = shared_randomstreams.RandomStreams(\n",
            "np.random.RandomState(0).randint(999999))\n",
            "mask = srng.binomial(n=1, p=1-p_dropout, size=layer.shape)\n",
            "return layer*T.cast(mask, theano.config.floatX)\n",
            "Problems\n",
            "At present, the SGD method requires the user to manually choose the number of\n",
            "• epochstotrainfor. Earlierinthebookwediscussedanautomatedwayofselecting\n",
            "thenumberofepochstotrainfor,knownasearlystopping.\n",
            "Modifynetwork3.pyto\n",
            "implementearlystopping. AddaNetworkmethodtoreturntheaccuracyonanarbitrarydataset. • ModifytheSGDmethodtoallowthelearningrateηtobeafunctionoftheepoch\n",
            "• number. Hint: Afterworkingonthisproblemforawhile,youmayfinditusefultosee\n",
            "thediscussionatthislink. EarlierinthechapterIdescribedatechniqueforexpandingthetrainingdatabyapply-\n",
            "• ing(small)rotations,skewing,andtranslation. Modifynetwork3.pytoincorporate\n",
            "allthesetechniques. Note: Unlessyouhaveatremendousamountofmemory, itis\n",
            "notpracticaltoexplicitlygeneratetheentireexpandeddataset. Soyoushouldconsider\n",
            "\n",
            "(cid:12)\n",
            "196 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "alternateapproaches. Addtheabilitytoloadandsavenetworkstonetwork3.py. • Ashortcomingofthecurrentcodeisthatitprovidesfewdiagnostictools. Canyou\n",
            "• thinkofanydiagnosticstoaddthatwouldmakeiteasiertounderstandtowhatextent\n",
            "anetworkisoverfitting? Addthem. We’veusedthesameinitializationprocedureforrectifiedlinearunitsasforsigmoid\n",
            "• (andtanh)neurons. Ourargumentforthatinitializationwasspecifictothesigmoid\n",
            "function.Consideranetworkmadeentirelyofrectifiedlinearunits(includingoutputs). Showthatrescalingalltheweightsinthenetworkbyaconstantfactorc>0simply\n",
            "rescalestheoutputsbyafactor cL 1, where L isthenumberoflayers. Howdoes\n",
            "−\n",
            "thischangeifthefinallayerisasoftmax? Whatdoyouthinkofusingthesigmoid\n",
            "initialization procedure for the rectified linear units? Can you think of a better\n",
            "initialization procedure?\n",
            "Note: This is a very open-ended problem, not something\n",
            "withasimpleself-containedanswer.\n",
            "Still,consideringtheproblemwillhelpyoubetter\n",
            "understandnetworkscontainingrectifiedlinearunits. Ouranalysisoftheunstablegradientproblemwasforsigmoidneurons. Howdoes\n",
            "6 • theanalysischangefornetworksmadeupofrectifiedlinearunits? Canyouthinkofa\n",
            "goodwayofmodifyingsuchanetworksoitdoesn’tsufferfromtheunstablegradient\n",
            "problem? Note: Thewordgoodinthesecondpartofthismakestheproblemaresearch\n",
            "problem. It’sactuallyeasytothinkofwaysofmakingsuchmodifications.\n",
            "ButIhaven’t\n",
            "investigatedinenoughdepthtoknowofareallygoodtechnique. 6.4 Recent progress in image recognition\n",
            "In1998,theyearMNISTwasintroduced,ittookweekstotrainastate-of-the-artworkstation\n",
            "toachieveaccuraciessubstantiallyworsethanthosewecanachieveusingaGPUandless\n",
            "thananhouroftraining. Thus, MNISTisnolongeraproblemthatpushesthelimitsof\n",
            "availabletechnique;rather,thespeedoftrainingmeansthatitisaproblemgoodforteaching\n",
            "andlearningpurposes. Meanwhile,thefocusofresearchhasmovedon,andmodernwork\n",
            "involvesmuchmorechallengingimagerecognitionproblems.Inthissection,Ibrieflydescribe\n",
            "somerecentworkonimagerecognitionusingneuralnetworks. Thesectionisdifferenttomostofthebook. ThroughthebookI’vefocusedonideaslikely\n",
            "tobeoflastinginterest–ideassuchasbackpropagation,regularization,andconvolutional\n",
            "networks. I’vetriedtoavoidresultswhicharefashionableasIwrite,butwhoselong-term\n",
            "valueisunknown. Inscience,suchresultsaremoreoftenthannotephemerawhichfadeand\n",
            "havelittlelastingimpact. Giventhis,askepticmightsay: “well,surelytherecentprogress\n",
            "inimagerecognitionisanexampleofsuchephemera? Inanothertwoorthreeyears,things\n",
            "willhavemovedon. Sosurelytheseresultsareonlyofinteresttoafewspecialistswhowant\n",
            "tocompeteattheabsolutefrontier? Whybotherdiscussingit?”\n",
            "Such a skeptic is right that some of the finer details of recent papers will gradually\n",
            "diminishinperceivedimportance. Withthatsaid,thepastfewyearshaveseenextraordinary\n",
            "improvementsusingdeepnetstoattackextremelydifficultimagerecognitiontasks.\n",
            "Imagine\n",
            "ahistorianofsciencewritingaboutcomputervisionintheyear2100. Theywillidentifythe\n",
            "years2011to2015(andprobablyafewyearsbeyond)asatimeofhugebreakthroughs,\n",
            "drivenbydeepconvolutionalnets. Thatdoesn’tmeandeepconvolutionalnetswillstillbe\n",
            "usedin2100,muchlessdetailedideassuchasdropout,rectifiedlinearunits,andsoon. But\n",
            "itdoesmeanthatanimportanttransitionistakingplace,rightnow,inthehistoryofideas. \n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 197\n",
            "(cid:12)\n",
            "It’sabitlikewatchingthediscoveryoftheatom,ortheinventionofantibiotics: invention\n",
            "anddiscoveryonahistoricscale. Andsowhilewewon’tdigdowndeepintodetails,it’s\n",
            "worthgettingsomeideaoftheexcitingdiscoveriescurrentlybeingmade.\n",
            "The2012LRMDpaper: Letmestartwitha2012paper23fromagroupofresearchers\n",
            "from Stanford and Google. I’ll refer to this paper as LRMD, after the last names of the\n",
            "firstfourauthors. LRMDusedaneuralnetworktoclassifyimagesfromImageNet,avery\n",
            "challengingimagerecognitionproblem. The2011ImageNetdatathattheyusedincluded\n",
            "16millionfullcolorimages,in20thousandcategories. Theimageswerecrawledfromthe\n",
            "opennet,andclassifiedbyworkersfromAmazon’sMechanicalTurkservice. Here’safew\n",
            "ImageNetimages24:\n",
            "6\n",
            "Theseare,respectively,inthecategoriesforbeadingplane,brownrootrotfungus,scalded\n",
            "milk,andthecommonroundworm. Ifyou’relookingforachallenge,Iencourageyouto\n",
            "visitImageNet’slistofhandtools,whichdistinguishesbetweenbeadingplanes,blockplanes,\n",
            "chamferplanes,andaboutadozenothertypesofplane,amongstothercategories. Idon’t\n",
            "knowaboutyou,butIcannotconfidentlydistinguishbetweenallthesetooltypes. Thisis\n",
            "obviouslyamuchmorechallengingimagerecognitiontaskthanMNIST!LRMD’snetwork\n",
            "obtainedarespectable15.8percentaccuracyforcorrectlyclassifyingImageNetimages. That\n",
            "maynotsoundimpressive,butitwasahugeimprovementoverthepreviousbestresult\n",
            "of9.3percentaccuracy. Thatjumpsuggestedthatneuralnetworksmightofferapowerful\n",
            "approachtoverychallengingimagerecognitiontasks,suchasImageNet. The2012KSHpaper: TheworkofLRMDwasfollowedbya2012paperofKrizhevsky,\n",
            "SutskeverandHinton(KSH)25.KSHtrainedandtestedadeepconvolutionalneuralnetwork\n",
            "usingarestrictedsubsetoftheImageNetdata. Thesubsettheyusedcamefromapopular\n",
            "machinelearningcompetition–theImageNetLarge-ScaleVisualRecognitionChallenge\n",
            "(ILSVRC).Usingacompetitiondatasetgavethemagoodwayofcomparingtheirapproach\n",
            "tootherleadingtechniques. TheILSVRC-2012trainingsetcontainedabout1.2million\n",
            "ImageNet images, drawn from 1,000 categories. The validation and test sets contained\n",
            "50,000and150,000images,respectively,drawnfromthesame1,000categories. OnedifficultyinrunningtheILSVRCcompetitionisthatmanyImageNetimagescontain\n",
            "multipleobjects. Supposeanimageshowsalabradorretrieverchasingasoccerball. The\n",
            "so-called“correct”ImageNetclassificationoftheimagemightbeasalabradorretriever. 23Buildinghigh-levelfeaturesusinglargescaleunsupervisedlearning,byQuocLe,Marc’Aurelio\n",
            "Ranzato,RajatMonga,MatthieuDevin,KaiChen,GregCorrado,JeffDean,andAndrewNg(2012).\n",
            "Notethatthedetailedarchitectureofthenetworkusedinthepaperdifferedinmanydetailsfromthe\n",
            "deepconvolutionalnetworkswe’vebeenstudying.Broadlyspeaking,however,LRMDisbasedonmany\n",
            "similarideas. 24Thesearefromthe2014dataset,whichissomewhatchangedfrom2011.Qualitatively,however,\n",
            "thedatasetisextremelysimilar.DetailsaboutImageNetareavailableintheoriginalImageNetpaper,\n",
            "ImageNet:alarge-scalehierarchicalimagedatabase,byJiaDeng,WeiDong,RichardSocher,Li-JiaLi,\n",
            "KaiLi,andLiFei-Fei(2009). 25ImageNetclassificationwithdeepconvolutionalneuralnetworks,byAlexKrizhevsky,IlyaSutskever,\n",
            "andGeoffreyE.Hinton(2012).\n",
            "\n",
            "(cid:12)\n",
            "198 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "Shouldanalgorithmbepenalizedifitlabelstheimageasasoccerball? Becauseofthis\n",
            "ambiguity,analgorithmwasconsideredcorrectiftheactualImageNetclassificationwas\n",
            "amongthe5classificationsthealgorithmconsideredmostlikely. Bythistop-5criterion,\n",
            "KSH’sdeepconvolutionalnetworkachievedanaccuracyof84.7percent,vastlybetterthan\n",
            "thenext-bestcontestentry,whichachievedanaccuracyof73.8percent. Usingthemore\n",
            "restrictivemetricofgettingthelabelexactlyright,KSH’snetworkachievedanaccuracyof\n",
            "63.3percent. It’sworthbrieflydescribingKSH’snetwork,sinceithasinspiredmuchsubsequentwork. It’salso,asweshallsee,closelyrelatedtothenetworkswetrainedearlierinthischapter,\n",
            "albeitmoreelaborate. KSHusedadeepconvolutionalneuralnetwork,trainedontwoGPUs. TheyusedtwoGPUsbecausetheparticulartypeofGPUtheywereusing(anNVIDIAGeForce\n",
            "GTX580)didn’thaveenoughon-chipmemorytostoretheirentirenetwork. Sotheysplit\n",
            "thenetworkintotwoparts,partitionedacrossthetwoGPUs.\n",
            "TheKSHnetworkhas7layersofhiddenneurons. Thefirst5hiddenlayersareconvolu-\n",
            "tionallayers(somewithmax-pooling),whilethenext2layersarefully-connectedlayers. Theoutputlayerisa1,000-unitsoftmaxlayer,correspondingtothe1,000imageclasses. 6 Here’sasketchofthenetwork,takenfromtheKSHpaper26.\n",
            "Thedetailsareexplainedbelow. Notethatmanylayersaresplitinto2parts,correspondingtothe2GPUs. Theinputlayercontains3 224 224neurons,representingtheRGBvaluesfora224 224\n",
            "image. Recallthat,asmen×tione×dearlier,ImageNetcontainsimagesofvaryingresol×ution. Thisposesaproblem,sinceaneuralnetwork’sinputlayerisusuallyofafixedsize. KSHdealt\n",
            "withthisbyrescalingeachimagesotheshortersidehadlength256. Theythencropped\n",
            "outa256 256areainthecenteroftherescaledimage. Finally,KSHextractedrandom\n",
            "224 224×subimages(andhorizontalreflections)fromthe256 256images. Theydidthis\n",
            "rand×omcroppingasawayofexpandingthetrainingdata,an×dthusreducingoverfitting. ThisisparticularlyhelpfulinalargenetworksuchasKSH’s. Itwasthese224 224images\n",
            "whichwereusedasinputstothenetwork. Inmostcasesthecroppedimagestil×lcontainsthe\n",
            "mainobjectfromtheuncroppedimage. MovingontothehiddenlayersinKSH’snetwork,thefirsthiddenlayerisaconvolutional\n",
            "layer,withamax-poolingstep. Ituseslocalreceptivefieldsofsize11 11,andastride\n",
            "lengthof4pixels. Thereareatotalof96featuremaps. Thefeaturemaps×aresplitintotwo\n",
            "groupsof48each,withthefirst48featuremapsresidingononeGPU,andthesecond48\n",
            "featuremapsresidingontheotherGPU.Themax-poolinginthisandlaterlayersisdonein\n",
            "3 3regions,butthepoolingregionsareallowedtooverlap,andarejust2pixelsapart. ×\n",
            "26ThankstoIlyaSutskever.\n",
            "\n",
            "(cid:12)\n",
            "6.4. Recentprogressinimagerecognition (cid:12) 199\n",
            "(cid:12)\n",
            "Thesecondhiddenlayerisalsoaconvolutionallayer,withamax-poolingstep. Ituses\n",
            "5 5localreceptivefields,andthere’satotalof256featuremaps,splitinto128oneach\n",
            "GP×U.Notethatthefeaturemapsonlyuse48inputchannels,notthefull96outputfromthe\n",
            "previouslayer(aswouldusuallybethecase). Thisisbecauseanysinglefeaturemaponly\n",
            "usesinputsfromthesameGPU.Inthissensethenetworkdepartsfromtheconvolutional\n",
            "architecturewedescribedearlierinthechapter,thoughobviouslythebasicideaisstillthe\n",
            "same. Thethird,fourthandfifthhiddenlayersareconvolutionallayers,butunliketheprevious\n",
            "layers,theydonotinvolvemax-pooling. Theirrespectivesparametersare: (3)384feature\n",
            "maps,with3 3localreceptivefields,and256inputchannels;(4)384featuremaps,with\n",
            "3 3localrece×ptivefields,and192inputchannels;and(5)256featuremaps,with3 3local\n",
            "re×ceptivefields,and192inputchannels. Notethatthethirdlayerinvolvessomein×ter-GPU\n",
            "communication(asdepictedinthefigure)inorderthatthefeaturemapsuseall256input\n",
            "channels. Thesixthandseventhhiddenlayersarefully-connectedlayers,with4,096neuronsin\n",
            "eachlayer. Theoutputlayerisa1,000-unitsoftmaxlayer. 6\n",
            "TheKSHnetworktakesadvantageofmanytechniques. Insteadofusingthesigmoidor\n",
            "tanhactivationfunctions,KSHuserectifiedlinearunits,whichspeduptrainingsignificantly. KSH’snetworkhadroughly60millionlearnedparameters,andwasthus,evenwiththelarge\n",
            "trainingset,susceptibletooverfitting. Toovercomethis,theyexpandedthetrainingsetusing\n",
            "therandomcroppingstrategywediscussedabove. Theyalsofurtheraddressedoverfitting\n",
            "byusingavariantofl2regularization,anddropout. Thenetworkitselfwastrainedusing\n",
            "momentum-basedmini-batchstochasticgradientdescent. That’s an overview of many of the core ideas in the KSH paper.\n",
            "I’ve omitted some\n",
            "details, for which you should look at the paper.\n",
            "You can also look at Alex Krizhevsky’s\n",
            "cuda-convnet(andsuccessors),whichcontainscodeimplementingmanyoftheideas. A\n",
            "Theano-basedimplementationhasalsobeendeveloped27,withthecodeavailablehere. The\n",
            "codeisrecognizablyalongsimilarlinestothatdevelopedinthischapter,althoughtheuseof\n",
            "multipleGPUscomplicatesthingssomewhat. TheCaffeneuralnetsframeworkalsoincludes\n",
            "aversionoftheKSHnetwork,seetheirModelZoofordetails. The 2014 ILSVRC competition: Since 2012, rapid progress continues to be made. Considerthe2014ILSVRCcompetition. Asin2012,itinvolvedatrainingsetof1.2million\n",
            "images, in 1,000 categories, and the figure of merit was whether the top 5 predictions\n",
            "includedthecorrectcategory. Thewinningteam,basedprimarilyatGoogle28,usedadeep\n",
            "convolutionalnetworkwith22layersofneurons. TheycalledtheirnetworkGoogLeNet,\n",
            "asahomagetoLeNet-5.\n",
            "GoogLeNetachievedatop-5accuracyof93.33percent,agiant\n",
            "improvementoverthe2013winner(Clarifai,with88.3percent),andthe2012winner(KSH,\n",
            "with84.7percent). JusthowgoodisGoogLeNet’s93.33percentaccuracy?\n",
            "In2014ateamofresearchers\n",
            "wroteasurveypaperabouttheILSVRCcompetition29. Oneofthequestionstheyaddressis\n",
            "howwellhumansperformonILSVRC.Todothis,theybuiltasystemwhichletshumans\n",
            "27Theano-basedlarge-scalevisualrecognitionwithmultipleGPUs,byWeiguangDing,RuoyanWang,\n",
            "FeiMao,andGrahamTaylor(2014). 28Goingdeeperwithconvolutions,byChristianSzegedy,WeiLiu,YangqingJia,PierreSermanet,Scott\n",
            "Reed,DragomirAnguelov,DumitruErhan,VincentVanhoucke,andAndrewRabinovich(2014). 29ImageNetlargescalevisualrecognitionchallenge,byOlgaRussakovsky,JiaDeng,HaoSu,Jonathan\n",
            "Krause,SanjeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,\n",
            "AlexanderC.Berg,andLiFei-Fei(2014). \n",
            "(cid:12)\n",
            "200 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "classifyILSVRCimages. Asoneoftheauthors,AndrejKarpathy,explainsinaninformative\n",
            "blogpost,itwasalotoftroubletogetthehumansuptoGoogLeNet’sperformance:\n",
            "...thetaskoflabelingimageswith5outof1000categoriesquicklyturned\n",
            "outtobeextremelychallenging,evenforsomefriendsinthelabwhohave\n",
            "beenworkingonILSVRCanditsclassesforawhile. Firstwethought\n",
            "we would put it up on [Amazon Mechanical Turk]. Then we thought\n",
            "wecouldrecruitpaidundergrads. ThenIorganizedalabelingpartyof\n",
            "intenselabelingeffortonlyamongthe(expertlabelers)inourlab. ThenI\n",
            "developedamodifiedinterfacethatusedGoogLeNetpredictionstoprune\n",
            "thenumberofcategoriesfrom1000toonlyabout100. Itwasstilltoo\n",
            "hard–peoplekeptmissingcategoriesandgettinguptorangesof13–15%\n",
            "errorrates. IntheendIrealizedthattogetanywherecompetitivelyclose\n",
            "toGoogLeNet,itwasmostefficientifIsatdownandwentthroughthe\n",
            "painfullylongtrainingprocessandthesubsequentcarefulannotation\n",
            "processmyself... Thelabelinghappenedatarateofabout1perminute,\n",
            "butthisdecreasedovertime... Someimagesareeasilyrecognized,while\n",
            "6\n",
            "some images (such as those of fine-grained breeds of dogs, birds, or\n",
            "monkeys)canrequiremultipleminutesofconcentratedeffort. Ibecame\n",
            "verygoodatidentifyingbreedsofdogs...\n",
            "Basedonthesampleofimages\n",
            "Iworkedon,theGoogLeNetclassificationerrorturnedouttobe6.8%... My own error in the end turned out to be 5.1%, approximately 1.7%\n",
            "better. In other words, an expert human, working painstakingly, was with great effort able to\n",
            "narrowlybeatthedeepneuralnetwork. Infact, Karpathyreportsthatasecondhuman\n",
            "expert,trainedonasmallersampleofimages,wasonlyabletoattaina12.0percenttop-5\n",
            "errorrate,significantlybelowGoogLeNet’sperformance. Abouthalftheerrorsweredueto\n",
            "theexpert“failingtospotandconsiderthegroundtruthlabelasanoption”.\n",
            "These are astonishing results. Indeed, since this work, several teams have reported\n",
            "systems whose top-5 error rate is actually better than 5.1%. This has sometimes been\n",
            "reportedinthemediaasthesystemshavingbetter-than-humanvision.\n",
            "Whiletheresultsare\n",
            "genuinelyexciting,therearemanycaveatsthatmakeitmisleadingtothinkofthesystems\n",
            "ashavingbetter-than-humanvision. TheILSVRCchallengeisinmanywaysaratherlimited\n",
            "problem – a crawl of the open web is not necessarily representative of images found in\n",
            "applications! And,ofcourse,thetop-5criterionisquiteartificial. Wearestillalongway\n",
            "fromsolvingtheproblemofimagerecognitionor,morebroadly,computervision. Still,it’s\n",
            "extremelyencouragingtoseesomuchprogressmadeonsuchachallengingproblem,over\n",
            "justafewyears. Otheractivity: I’vefocusedonImageNet,butthere’saconsiderableamountofother\n",
            "activityusingneuralnetstodoimagerecognition. Letmebrieflydescribeafewinteresting\n",
            "recentresults,justtogivetheflavourofsomecurrentwork. OneencouragingpracticalsetofresultscomesfromateamatGoogle,whoapplieddeep\n",
            "convolutionalnetworkstotheproblemofrecognizingstreetnumbersinGoogle’sStreetView\n",
            "imagery30. Intheirpaper,theyreportdetectingandautomaticallytranscribingnearly100\n",
            "millionstreetnumbersatanaccuracysimilartothatofahumanoperator. Thesystemisfast:\n",
            "theirsystemtranscribedallofStreetView’simagesofstreetnumbersinFranceinlessthan\n",
            "30Multi-digitNumberRecognitionfromStreetViewImageryusingDeepConvolutionalNeuralNet-\n",
            "works,byIanJ.Goodfellow,YaroslavBulatov,JulianIbarz,SachaArnoud,andVinayShet(2013). \n",
            "(cid:12)\n",
            "6.4.\n",
            "Recentprogressinimagerecognition (cid:12) 201\n",
            "(cid:12)\n",
            "anhour! Theysay: “Havingthisnewdatasetsignificantlyincreasedthegeocodingqualityof\n",
            "GoogleMapsinseveralcountriesespeciallytheonesthatdidnotalreadyhaveothersources\n",
            "ofgoodgeocoding.” Andtheygoontomakethebroaderclaim: “Webelievewiththismodel\n",
            "wehavesolved[opticalcharacterrecognition]forshortsequences[ofcharacters]formany\n",
            "applications.”\n",
            "I’veperhapsgiventheimpressionthatit’sallaparadeofencouragingresults. Ofcourse,\n",
            "someofthemostinterestingworkreportsonfundamentalthingswedon’tyetunderstand. Forinstance,a2013paper31showedthatdeepnetworksmaysufferfromwhatareeffectively\n",
            "blindspots. Considerthelinesofimagesbelow. OntheleftisanImageNetimageclassified\n",
            "correctlybytheirnetwork. Ontherightisaslightlyperturbedimage(theperturbationisin\n",
            "themiddle)whichisclassifiedincorrectlybythenetwork. Theauthorsfoundthatthereare\n",
            "such“adversarial”imagesforeverysampleimage,notjustafewspecialones. 6\n",
            "Thisisadisturbingresult.ThepaperusedanetworkbasedonthesamecodeasKSH’snetwork\n",
            "–thatis,justthetypeofnetworkthatisbeingincreasinglywidelyused. Whilesuchneural\n",
            "networkscomputefunctionswhichare,inprinciple,continuous,resultslikethissuggestthat\n",
            "inpracticethey’relikelytocomputefunctionswhichareverynearlydiscontinuous. Worse,\n",
            "they’llbediscontinuousinwaysthatviolateourintuitionaboutwhatisreasonablebehavior. That’sconcerning. Furthermore,it’snotyetwellunderstoodwhat’scausingthediscontinuity:\n",
            "isitsomethingaboutthelossfunction? Theactivationfunctionsused? Thearchitectureof\n",
            "thenetwork?\n",
            "Somethingelse? Wedon’tyetknow.\n",
            "Now,theseresultsarenotquiteasbadastheysound. Althoughsuchadversarialimages\n",
            "arecommon,they’realsounlikelyinpractice. Asthepapernotes:\n",
            "Theexistenceoftheadversarialnegativesappearstobeincontradiction\n",
            "withthenetwork’sabilitytoachievehighgeneralizationperformance. Indeed,ifthenetworkcangeneralizewell,howcanitbeconfusedby\n",
            "theseadversarialnegatives,whichareindistinguishablefromtheregular\n",
            "examples? Theexplanationisthatthesetofadversarialnegativesisof\n",
            "extremelylowprobability,andthusisnever(orrarely)observedinthe\n",
            "31Intriguingpropertiesofneuralnetworks,byChristianSzegedy,WojciechZaremba,IlyaSutskever,\n",
            "JoanBruna,DumitruErhan,IanGoodfellow,andRobFergus(2013)\n",
            "\n",
            "(cid:12)\n",
            "202 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "testset,yetitisdense(muchliketherationalnumbers),andsoitisfound\n",
            "nearvirtuallyeverytestcase. Nonetheless, it is distressing that we understand neural nets so poorly that this kind of\n",
            "resultshouldbearecentdiscovery. Ofcourse,amajorbenefitoftheresultsisthatthey\n",
            "havestimulatedmuchfollowupwork. Forexample,onerecentpaper32 showsthatgiven\n",
            "atrainednetworkit’spossibletogenerateimageswhichlooktoahumanlikewhitenoise,\n",
            "butwhichthenetworkclassifiesasbeinginaknowncategorywithaveryhighdegreeof\n",
            "confidence. Thisisanotherdemonstrationthatwehavealongwaytogoinunderstanding\n",
            "neuralnetworksandtheiruseinimagerecognition. Despiteresultslikethis,theoverallpictureisencouraging.\n",
            "We’reseeingrapidprogress\n",
            "onextremelydifficultbenchmarks,likeImageNet. We’realsoseeingrapidprogressinthe\n",
            "solutionofreal-worldproblems,likerecognizingstreetnumbersinStreetView. Butwhilethis\n",
            "isencouragingit’snotenoughjusttoseeimprovementsonbenchmarks,orevenreal-world\n",
            "applications. Therearefundamentalphenomenawhichwestillunderstandpoorly,such\n",
            "astheexistenceofadversarialimages. Whensuchfundamentalproblemsarestillbeing\n",
            "discovered(nevermindsolved),itisprematuretosaythatwe’renearsolvingtheproblemof\n",
            "6\n",
            "imagerecognition. Atthesametimesuchproblemsareanexcitingstimulustofurtherwork. 6.5 Other approaches to deep neural nets\n",
            "Throughthisbook,we’veconcentratedonasingleproblem: classifyingtheMNISTdigits. It’sajuicyproblemwhichforcedustounderstandmanypowerfulideas: stochasticgradient\n",
            "descent,backpropagation,convolutionalnets,regularization,andmore.Butit’salsoanarrow\n",
            "problem. Ifyoureadtheneuralnetworksliterature,you’llrunintomanyideaswehaven’t\n",
            "discussed: recurrentneuralnetworks,Boltzmannmachines,generativemodels,transfer\n",
            "learning,reinforcementlearning,andsoon,onandonâA˘˛eandon! Neuralnetworksisa\n",
            "vastfield. However,manyimportantideasarevariationsonideaswe’vealreadydiscussed,\n",
            "andcanbeunderstoodwithalittleeffort.\n",
            "InthissectionIprovideaglimpseoftheseasyet\n",
            "unseenvistas. Thediscussionisn’tdetailed,norcomprehensive–thatwouldgreatlyexpand\n",
            "thebook. Rather,it’simpressionistic,anattempttoevoketheconceptualrichnessofthe\n",
            "field,andtorelatesomeofthoserichestowhatwe’vealreadyseen. Throughthesection,I’ll\n",
            "provideafewlinkstoothersources,asentreestolearnmore. Ofcourse,manyoftheselinks\n",
            "willsoonbesuperseded,andyoumaywishtosearchoutmorerecentliterature.\n",
            "Thatpoint\n",
            "notwithstanding,Iexpectmanyoftheunderlyingideastobeoflastinginterest. Recurrentneuralnetworks(RNNs): Inthefeedforwardnetswe’vebeenusingthere\n",
            "isasingleinputwhichcompletelydeterminestheactivationsofalltheneuronsthrough\n",
            "theremaininglayers. It’saverystaticpicture: everythinginthenetworkisfixed,witha\n",
            "frozen,crystallinequalitytoit. Butsupposeweallowtheelementsinthenetworktokeep\n",
            "changinginadynamicway. Forinstance,thebehaviourofhiddenneuronsmightnotjustbe\n",
            "determinedbytheactivationsinprevioushiddenlayers,butalsobytheactivationsatearlier\n",
            "times. Indeed,aneuron’sactivationmightbedeterminedinpartbyitsownactivationatan\n",
            "earliertime. That’scertainlynotwhathappensinafeedforwardnetwork. Orperhapsthe\n",
            "activationsofhiddenandoutputneuronswon’tbedeterminedjustbythecurrentinputto\n",
            "thenetwork,butalsobyearlierinputs. 32DeepNeuralNetworksareEasilyFooled:HighConfidencePredictionsforUnrecognizableImages,\n",
            "byAnhNguyen,JasonYosinski,andJeffClune(2014). \n",
            "(cid:12)\n",
            "6.5. Otherapproachestodeepneuralnets (cid:12) 203\n",
            "(cid:12)\n",
            "Neuralnetworkswiththiskindoftime-varyingbehaviourareknownasrecurrentneural\n",
            "networksorRNNs. Therearemanydifferentwaysofmathematicallyformalizingtheinformal\n",
            "descriptionofrecurrentnetsgiveninthelastparagraph.\n",
            "Youcangettheflavourofsomeof\n",
            "thesemathematicalmodelsbyglancingattheWikipediaarticleonRNNs. AsIwrite,that\n",
            "pagelistsnofewerthan13differentmodels. Butmathematicaldetailsaside,thebroadidea\n",
            "isthatRNNsareneuralnetworksinwhichthereissomenotionofdynamicchangeovertime. And,notsurprisingly,they’reparticularlyusefulinanalysingdataorprocessesthatchange\n",
            "overtime. Suchdataandprocessesarisenaturallyinproblemssuchasspeechornatural\n",
            "language,forexample. OnewayRNNsarecurrentlybeingusedistoconnectneuralnetworksmorecloselyto\n",
            "traditionalwaysofthinkingaboutalgorithms, waysofthinkingbasedonconceptssuch\n",
            "asTuringmachinesand(conventional)programminglanguages. A2014paperdeveloped\n",
            "an RNN which could take as input a character-by-character description of a (very, very\n",
            "simple!) Pythonprogram,andusethatdescriptiontopredicttheoutput. Informally,the\n",
            "networkislearningto“understand”certainPythonprograms. Asecondpaper,alsofrom\n",
            "2014,usedRNNsasastartingpointtodevelopwhattheycalledaneuralTuringmachine\n",
            "6\n",
            "(NTM).Thisisauniversalcomputerwhoseentirestructurecanbetrainedusinggradient\n",
            "descent. TheytrainedtheirNTMtoinferalgorithmsforseveralsimpleproblems,suchas\n",
            "sortingandcopying. Asitstands,theseareextremelysimpletoymodels. LearningtoexecutethePythonpro-\n",
            "gramprint(398345+42598)doesn’tmakeanetworkintoafull-fledgedPythoninterpreter! It’snotclearhowmuchfurtheritwillbepossibletopushtheideas.\n",
            "Still,theresultsare\n",
            "intriguing. Historically,neuralnetworkshavedonewellatpatternrecognitionproblems\n",
            "whereconventionalalgorithmicapproacheshavetrouble. Viceversa,conventionalalgorith-\n",
            "micapproachesaregoodatsolvingproblemsthatneuralnetsaren’tsogoodat. No-one\n",
            "todayimplementsawebserveroradatabaseprogramusinganeuralnetwork! It’dbegreat\n",
            "todevelopunifiedmodelsthatintegratethestrengthsofbothneuralnetworksandmore\n",
            "traditionalapproachestoalgorithms. RNNsandideasinspiredbyRNNsmayhelpusdothat. RNNshavealsobeenusedinrecentyearstoattackmanyotherproblems. They’vebeen\n",
            "particularlyusefulinspeechrecognition. ApproachesbasedonRNNshave,forexample,\n",
            "setrecordsfortheaccuracyofphonemerecognition. They’vealsobeenusedtodevelop\n",
            "improvedmodelsofthelanguagepeopleusewhilespeaking. Betterlanguagemodelshelp\n",
            "disambiguateutterancesthatotherwisesoundalike.Agoodlanguagemodelwill,forexample,\n",
            "tellusthat“toinfinityandbeyond”ismuchmorelikelythan“twoinfinityandbeyond”,\n",
            "despitethefactthatthephrasessoundidentical. RNNshavebeenusedtosetnewrecords\n",
            "forcertainlanguagebenchmarks. Thisworkis,incidentally,partofabroaderuseofdeepneuralnetsofalltypes,notjust\n",
            "RNNs,inspeechrecognition.\n",
            "Forexample,anapproachbasedondeepnetshasachieved\n",
            "outstandingresultsonlargevocabularycontinuousspeechrecognition. Andanothersystem\n",
            "basedondeepnetshasbeendeployedinGoogle’sAndroidoperatingsystem(forrelated\n",
            "technicalwork,seeVincentVanhoucke’s2012–2015papers). I’vesaidalittleaboutwhatRNNscando,butnotsomuchabouthowtheywork.\n",
            "It\n",
            "perhapswon’tsurpriseyoutolearnthatmanyoftheideasusedinfeedforwardnetworkscan\n",
            "alsobeusedinRNNs. Inparticular,wecantrainRNNsusingstraightforwardmodificationsto\n",
            "gradientdescentandbackpropagation. Manyotherideasusedinfeedforwardnets,ranging\n",
            "fromregularizationtechniquestoconvolutionstotheactivationandcostfunctionsused,are\n",
            "alsousefulinrecurrentnets. Andsomanyofthetechniqueswe’vedevelopedinthebook\n",
            "\n",
            "(cid:12)\n",
            "204 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "canbeadaptedforusewithRNNs. Longshort-termmemoryunits(LSTMs): OnechallengeaffectingRNNsisthatearly\n",
            "modelsturnedouttobeverydifficulttotrain,hardereventhandeepfeedforwardnetworks. ThereasonistheunstablegradientproblemdiscussedinChapter5. Recallthattheusual\n",
            "manifestationofthisproblemisthatthegradientgetssmallerandsmallerasitispropagated\n",
            "back through layers. This makes learning in early layers extremely slow. The problem\n",
            "actuallygetsworseinRNNs,sincegradientsaren’tjustpropagatedbackwardthroughlayers,\n",
            "they’repropagatedbackwardthroughtime. Ifthenetworkrunsforalongtimethatcan\n",
            "makethegradientextremelyunstableandhardtolearnfrom.\n",
            "Fortunately,it’spossibleto\n",
            "incorporateanideaknownaslongshort-termmemoryunits(LSTMs)intoRNNs. Theunits\n",
            "wereintroducedbyHochreiterandSchmidhuberin1997withtheexplicitpurposeofhelping\n",
            "addresstheunstablegradientproblem.\n",
            "LSTMsmakeitmucheasiertogetgoodresultswhen\n",
            "trainingRNNs,andmanyrecentpapers(includingmanythatIlinkedabove)makeuseof\n",
            "LSTMsorrelatedideas. Deepbeliefnets,generativemodels,andBoltzmannmachines: Moderninterestin\n",
            "deeplearningbeganin2006,withpapersexplaininghowtotrainatypeofneuralnetwork\n",
            "6 knownasadeepbeliefnetwork(DBN)33.DBNswereinfluentialforseveralyears,buthave\n",
            "sincelessenedinpopularity,whilemodelssuchasfeedforwardnetworksandrecurrentneural\n",
            "netshavebecomefashionable. Despitethis,DBNshaveseveralpropertiesthatmakethem\n",
            "interesting. OnereasonDBNsareinterestingisthatthey’reanexampleofwhat’scalledagenerative\n",
            "model. Inafeedforwardnetwork,wespecifytheinputactivations,andtheydeterminethe\n",
            "activationsofthefeatureneuronslaterinthenetwork. AgenerativemodellikeaDBNcan\n",
            "beusedinasimilarway,butit’salsopossibletospecifythevaluesofsomeofthefeature\n",
            "neuronsandthen“runthenetworkbackward”,generatingvaluesfortheinputactivations. Moreconcretely,aDBNtrainedonimagesofhandwrittendigitscan(potentially,andwith\n",
            "somecare)alsobeusedtogenerateimagesthatlooklikehandwrittendigits. Inotherwords,\n",
            "theDBNwouldinsomesensebelearningtowrite. Inthis,agenerativemodelismuchlike\n",
            "thehumanbrain: notonlycanitreaddigits,itcanalsowritethem. InGeoffreyHinton’s\n",
            "memorablephrase,torecognizeshapes,firstlearntogenerateimages. A second reason DBNs are interesting is that they can do unsupervised and semi-\n",
            "supervisedlearning. Forinstance,whentrainedwithimagedata,DBNscanlearnuseful\n",
            "featuresforunderstandingotherimages,evenifthetrainingimagesareunlabelled. Andthe\n",
            "abilitytodounsupervisedlearningisextremelyinterestingbothforfundamentalscientific\n",
            "reasons,and–ifitcanbemadetoworkwellenough–forpracticalapplications. Giventheseattractivefeatures,whyhaveDBNslessenedinpopularityasmodelsfor\n",
            "deeplearning? Partofthereasonisthatmodelssuchasfeedforwardandrecurrentnets\n",
            "haveachievedmanyspectacularresults,suchastheirbreakthroughsonimageandspeech\n",
            "recognitionbenchmarks. It’snotsurprisingandquiterightthatthere’snowlotsofattention\n",
            "beingpaidtothesemodels. There’sanunfortunatecorollary,however. Themarketplace\n",
            "ofideasoftenfunctionsinawinner-take-allfashion,withnearlyallattentiongoingtothe\n",
            "currentfashion-of-the-momentinanygivenarea. Itcanbecomeextremelydifficultforpeople\n",
            "toworkonmomentarilyunfashionableideas,evenwhenthoseideasareobviouslyofreal\n",
            "long-terminterest. MypersonalopinionisthatDBNsandothergenerativemodelslikely\n",
            "deservemoreattentionthantheyarecurrentlyreceiving. AndIwon’tbesurprisedifDBNs\n",
            "33SeeAfastlearningalgorithmfordeepbeliefnets,byGeoffreyHinton,SimonOsindero,andYee-Whye\n",
            "Teh(2006),aswellastherelatedworkinReducingthedimensionalityofdatawithneuralnetworks,by\n",
            "GeoffreyHintonandRuslanSalakhutdinov(2006).\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 205\n",
            "(cid:12)\n",
            "orarelatedmodelonedaysurpassthecurrentlyfashionablemodels. Foranintroductionto\n",
            "DBNs,seethisoverview. I’vealsofoundthisarticlehelpful. Itisn’tprimarilyaboutdeep\n",
            "beliefnets,perse,butdoescontainmuchusefulinformationaboutrestrictedBoltzmann\n",
            "machines,whichareakeycomponentofDBNs. Otherideas: Whatelseisgoingoninneuralnetworksanddeeplearning?\n",
            "Well,there’s\n",
            "ahugeamountofotherfascinatingwork. Activeareasofresearchincludeusingneural\n",
            "networkstodonaturallanguageprocessing(seealsothisinformativereviewpaper),machine\n",
            "translation,aswellasperhapsmoresurprisingapplicationssuchasmusicinformatics. There\n",
            "are,ofcourse,manyotherareastoo. Inmanycases,havingreadthisbookyoushouldbeable\n",
            "tobeginfollowingrecentwork,although(ofcourse)you’llneedtofillingapsinpresumed\n",
            "backgroundknowledge.\n",
            "Let me finish this section by mentioning a particularly fun paper. It combines deep\n",
            "convolutionalnetworkswithatechniqueknownasreinforcementlearninginordertolearn\n",
            "to play video games well (see also this followup). The idea is to use the convolutional\n",
            "networktosimplifythepixeldatafromthegamescreen,turningitintoasimplersetof\n",
            "features,whichcanbeusedtodecidewhichactiontotake: “goleft”,“godown”,“fire”,and\n",
            "6\n",
            "soon. Whatisparticularlyinterestingisthatasinglenetworklearnedtoplaysevendifferent\n",
            "classicvideogamesprettywell,outperforminghumanexpertsonthreeofthegames. Now,\n",
            "thisallsoundslikeastunt,andthere’snodoubtthepaperwaswellmarketed,withthe\n",
            "title“PlayingAtariwithreinforcementlearning”.\n",
            "Butlookingpastthesurfacegloss,consider\n",
            "thatthissystemistakingrawpixeldata–itdoesn’tevenknowthegamerules! –andfrom\n",
            "that data learning to do high-quality decision-making in several very different and very\n",
            "adversarialenvironments,eachwithitsowncomplexsetofrules. That’sprettyneat. 6.6 On the future of neural networks\n",
            "Intention-drivenuserinterfaces: There’sanoldjokeinwhichanimpatientprofessortellsa\n",
            "confusedstudent: “don’tlistentowhatIsay;listentowhatImean”. Historically,computers\n",
            "haveoftenbeen,liketheconfusedstudent,inthedarkaboutwhattheirusersmean. Butthis\n",
            "ischanging.IstillremembermysurprisethefirsttimeImisspelledaGooglesearchquery,only\n",
            "tohaveGooglesay“Didyoumean[correctedquery]?” andtoofferthecorrespondingsearch\n",
            "results. GoogleCEOLarryPageoncedescribedtheperfectsearchengineasunderstanding\n",
            "exactlywhat[yourqueries]meanandgivingyoubackexactlywhatyouwant. Thisisavisionofanintention-drivenuserinterface. Inthisvision,insteadofresponding\n",
            "tousers’literalqueries,searchwillusemachinelearningtotakevagueuserinput,discern\n",
            "preciselywhatwasmeant,andtakeactiononthebasisofthoseinsights. Theideaofintention-driveninterfacescanbeappliedfarmorebroadlythansearch. Overthenextfewdecades,thousandsofcompanieswillbuildproductswhichusemachine\n",
            "learningtomakeuserinterfacesthatcantolerateimprecision,whilediscerningandactingon\n",
            "theuser’strueintent. We’realreadyseeingearlyexamplesofsuchintention-driveninterfaces:\n",
            "Apple’sSiri;WolframAlpha;IBM’sWatson;systemswhichcanannotatephotosandvideos;\n",
            "andmuchmore. Mostoftheseproductswillfail. Inspireduserinterfacedesignishard, andIexpect\n",
            "manycompanieswilltakepowerfulmachinelearningtechnologyanduseittobuildinsipid\n",
            "userinterfaces. Thebestmachinelearningintheworldwon’thelpifyouruserinterface\n",
            "conceptstinks. Buttherewillbearesidueofproductswhichsucceed.\n",
            "Overtimethatwill\n",
            "causeaprofoundchangeinhowwerelatetocomputers. Notsolongago–let’ssay,2005\n",
            "\n",
            "(cid:12)\n",
            "206 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "–userstookitforgrantedthattheyneededprecisioninmostinteractionswithcomputers. Indeed,computerliteracytoagreatextentmeantinternalizingtheideathatcomputersare\n",
            "extremelyliteral;asinglemisplacedsemi-colonmaycompletelychangethenatureofan\n",
            "interactionwithacomputer. ButoverthenextfewdecadesIexpectwe’lldevelopmany\n",
            "successfulintention-drivenuserinterfaces,andthatwilldramaticallychangewhatweexpect\n",
            "wheninteractingwithcomputers. Machine learning, data science, and the virtuous circle of innovation: Of course,\n",
            "machinelearningisn’tjustbeingusedtobuildintention-driveninterfaces. Anothernotable\n",
            "applicationisindatascience,wheremachinelearningisusedtofindthe“knownunknowns”\n",
            "hiddenindata. Thisisalreadyafashionablearea,andmuchhasbeenwrittenaboutit,so\n",
            "Iwon’tsaymuch. ButIdowanttomentiononeconsequenceofthisfashionthatisnot\n",
            "so often remarked: over the long run it’s possible the biggest breakthrough in machine\n",
            "learningwon’tbeanysingleconceptualbreakthrough. Rather,thebiggestbreakthroughwill\n",
            "bethatmachinelearningresearchbecomesprofitable,throughapplicationstodatascience\n",
            "andotherareas. Ifacompanycaninvest1dollarinmachinelearningresearchandget1\n",
            "dollarand10centsbackreasonablyrapidly,thenalotofmoneywillendupinmachine\n",
            "6\n",
            "learningresearch. Putanotherway,machinelearningisanenginedrivingthecreationof\n",
            "severalmajornewmarketsandareasofgrowthintechnology. Theresultwillbelargeteams\n",
            "ofpeoplewithdeepsubjectexpertise, andwithaccesstoextraordinaryresources.\n",
            "That\n",
            "willpropelmachinelearningfurtherforward,creatingmoremarketsandopportunities,a\n",
            "virtuouscircleofinnovation. The role of neural networks and deep learning: I’ve been talking broadly about\n",
            "machinelearningasacreatorofnewopportunitiesfortechnology. Whatwillbethespecific\n",
            "roleofneuralnetworksanddeeplearninginallthis?\n",
            "Toanswerthequestion,ithelpstolookathistory. Backinthe1980stherewasagreat\n",
            "dealofexcitementandoptimismaboutneuralnetworks,especiallyafterbackpropagation\n",
            "becamewidelyknown. Thatexcitementfaded,andinthe1990sthemachinelearningbaton\n",
            "passedtoothertechniques,suchassupportvectormachines. Today,neuralnetworksare\n",
            "againridinghigh,settingallsortsofrecords,defeatingallcomersonmanyproblems. But\n",
            "whoistosaythattomorrowsomenewapproachwon’tbedevelopedthatsweepsneural\n",
            "networksawayagain? Orperhapsprogresswithneuralnetworkswillstagnate,andnothing\n",
            "willimmediatelyarisetotaketheirplace? Forthisreason,it’smucheasiertothinkbroadlyaboutthefutureofmachinelearning\n",
            "thanaboutneuralnetworksspecifically. Partoftheproblemisthatweunderstandneural\n",
            "networkssopoorly. Whyisitthatneuralnetworkscangeneralizesowell? Howisitthat\n",
            "theyavoidoverfittingaswellastheydo,giventheverylargenumberofparametersthey\n",
            "learn? Whyisitthatstochasticgradientdescentworksaswellasitdoes? Howwellwill\n",
            "neuralnetworksperformasdatasetsarescaled? Forinstance,ifImageNetwasexpanded\n",
            "byafactorof10,wouldneuralnetworks’performanceimprovemoreorlessthanother\n",
            "machinelearningtechniques? Theseareallsimple,fundamentalquestions.\n",
            "And,atpresent,\n",
            "weunderstandtheanswerstothesequestionsverypoorly. Whilethat’sthecase,it’sdifficult\n",
            "tosaywhatroleneuralnetworkswillplayinthefutureofmachinelearning. Iwillmakeoneprediction: Ibelievedeeplearningisheretostay.\n",
            "Theabilitytolearn\n",
            "hierarchiesofconcepts,buildingupmultiplelayersofabstraction,seemstobefundamental\n",
            "tomakingsenseoftheworld. Thisdoesn’tmeantomorrow’sdeeplearnerswon’tberadically\n",
            "differentthantoday’s. Wecouldseemajorchangesintheconstituentunitsused,inthe\n",
            "architectures,orinthelearningalgorithms. Thosechangesmaybedramaticenoughthatwe\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 207\n",
            "(cid:12)\n",
            "nolongerthinkoftheresultingsystemsasneuralnetworks. Butthey’dstillbedoingdeep\n",
            "learning. Willneuralnetworksanddeeplearningsoonleadtoartificialintelligence? Inthis\n",
            "bookwe’vefocusedonusingneuralnetstodospecifictasks, suchasclassifyingimages. Let’sbroadenourambitions, andask: whataboutgeneral-purposethinkingcomputers? Canneuralnetworksanddeeplearninghelpussolvetheproblemof(general)artificial\n",
            "intelligence(AI)?And,ifso,giventherapidrecentprogressofdeeplearning,canweexpect\n",
            "generalAIanytimesoon? Addressingthesequestionscomprehensivelywouldtakeaseparatebook.\n",
            "Instead,let\n",
            "meofferoneobservation. It’sbasedonanideaknownasConway’slaw:\n",
            "Anyorganizationthatdesignsasystem... willinevitablyproduceadesign\n",
            "whosestructureisacopyoftheorganization’scommunicationstructure. So,forexample,Conway’slawsuggeststhatthedesignofaBoeing747aircraftwillmirror\n",
            "theextendedorganizationalstructureofBoeinganditscontractorsatthetimethe747was\n",
            "designed. Orforasimple,specificexample,consideracompanybuildingacomplexsoftware\n",
            "application. Iftheapplication’sdashboardissupposedtobeintegratedwithsomemachine 6\n",
            "learningalgorithm,thepersonbuildingthedashboardbetterbetalkingtothecompany’s\n",
            "machinelearningexpert. Conway’slawismerelythatobservation,writlarge. UponfirsthearingConway’slaw, manypeoplerespondeither“Well, isn’tthatbanal\n",
            "andobvious?” or“Isn’tthatwrong?” Letmestartwiththeobjectionthatit’swrong. Asan\n",
            "instanceofthisobjection,considerthequestion: wheredoesBoeing’saccountingdepartment\n",
            "showupinthedesignofthe747?\n",
            "Whatabouttheirjanitorialdepartment?\n",
            "Theirinternal\n",
            "catering? Andtheansweristhatthesepartsoftheorganizationprobablydon’tshowup\n",
            "explicitlyanywhereinthe747. SoweshouldunderstandConway’slawasreferringonlyto\n",
            "thosepartsofanorganizationconcernedexplicitlywithdesignandengineering. Whatabouttheotherobjection,thatConway’slawisbanalandobvious? Thismayper-\n",
            "hapsbetrue,butIdon’tthinkso,fororganizationstoooftenactwithdisregardforConway’s\n",
            "law. Teamsbuildingnewproductsareoftenbloatedwithlegacyhiresor,contrariwise,lacka\n",
            "personwithsomecrucialexpertise. Thinkofalltheproductswhichhaveuselesscomplicating\n",
            "features. Orthinkofalltheproductswhichhaveobviousmajordeficiencies–e.g.,aterrible\n",
            "userinterface. Problemsinbothclassesareoftencausedbyamismatchbetweentheteam\n",
            "that was needed to produce a good product, and the team that was actually assembled. Conway’slawmaybeobvious,butthatdoesn’tmeanpeopledon’troutinelyignoreit. Conway’slawappliestothedesignandengineeringofsystemswherewestartoutwith\n",
            "aprettygoodunderstandingofthelikelyconstituentparts,andhowtobuildthem. Itcan’t\n",
            "beapplieddirectlytothedevelopmentofartificialintelligence,becauseAIisn’t(yet)sucha\n",
            "problem: wedon’tknowwhattheconstituentpartsare. Indeed,we’renotevensurewhat\n",
            "basicquestionstobeasking. Inotherswords,atthispointAIismoreaproblemofscience\n",
            "thanofengineering. Imaginebeginningthedesignofthe747withoutknowingaboutjet\n",
            "enginesortheprinciplesofaerodynamics. Youwouldn’tknowwhatkindsofexpertstohire\n",
            "intoyourorganization. AsWernhervonBraunputit,“basicresearchiswhatI’mdoingwhen\n",
            "Idon’tknowwhatI’mdoing”. IsthereaversionofConway’slawthatappliestoproblems\n",
            "whicharemoresciencethanengineering? Togaininsightintothisquestion,considerthehistoryofmedicine. Intheearlydays,\n",
            "medicine was the domain of practitioners like Galen and Hippocrates, who studied the\n",
            "entirebody. Butasourknowledgegrew,peoplewereforcedtospecialize.\n",
            "Wediscovered\n",
            "\n",
            "(cid:12)\n",
            "208 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "manydeepnewideas34: thinkofthingslikethegermtheoryofdisease,forinstance,orthe\n",
            "understandingofhowantibodieswork,ortheunderstandingthattheheart,lungs,veins\n",
            "andarteriesformacompletecardiovascularsystem. Suchdeepinsightsformedthebasisfor\n",
            "subfieldssuchasepidemiology,immunology,andtheclusterofinter-linkedfieldsaroundthe\n",
            "cardiovascularsystem. Andsothestructureofourknowledgehasshapedthesocialstructure\n",
            "ofmedicine. Thisisparticularlystrikinginthecaseofimmunology: realizingtheimmune\n",
            "systemexistsandisasystemworthyofstudyisanextremelynon-trivialinsight. Sowehave\n",
            "anentirefieldofmedicine–withspecialists,conferences,evenprizes,andsoon–organized\n",
            "aroundsomethingwhichisnotjustinvisible,it’sarguablynotadistinctthingatall. Thisisacommonpatternthathasbeenrepeatedinmanywell-establishedsciences:\n",
            "not just medicine, but physics, mathematics, chemistry, and others. The fields start out\n",
            "monolithic,withjustafewdeepideas.\n",
            "Earlyexpertscanmasterallthoseideas.\n",
            "Butastime\n",
            "passesthatmonolithiccharacterchanges. Wediscovermanydeepnewideas,toomanyfor\n",
            "anyonepersontoreallymaster. Asaresult,thesocialstructureofthefieldre-organizes\n",
            "anddividesaroundthoseideas. Insteadofamonolith,wehavefieldswithinfieldswithin\n",
            "fields,acomplex,recursive,self-referentialsocialstructure,whoseorganizationmirrorsthe\n",
            "6\n",
            "connectionsbetweenourdeepestinsights. Andsothestructureofourknowledgeshapesthe\n",
            "socialorganizationofscience.\n",
            "Butthatsocialshapeinturnconstrainsandhelpsdetermine\n",
            "whatwecandiscover. ThisisthescientificanalogueofConway’slaw. Sowhat’sthisgottodowithdeeplearningorAI? Well,sincetheearlydaysofAItherehavebeenargumentsaboutitthatgo,ononeside,\n",
            "“Hey,it’snotgoingtobesohard,we’vegot[super-specialweapon]onourside”,counteredby\n",
            "“[super-specialweapon]won’tbeenough”. Deeplearningisthelatestsuper-specialweapon\n",
            "I’veheardusedinsucharguments35;earlierversionsoftheargumentusedlogic,orProlog,\n",
            "orexpertsystems,orwhateverthemostpowerfultechniqueofthedaywas. Theproblem\n",
            "withsuchargumentsisthattheydon’tgiveyouanygoodwayofsayingjusthowpowerful\n",
            "anygivencandidatesuper-specialweaponis. Ofcourse,we’vejustspentachapterreviewing\n",
            "evidencethatdeeplearningcansolveextremelychallengingproblems. Itcertainlylooksvery\n",
            "excitingandpromising. ButthatwasalsotrueofsystemslikePrologorEuriskoorexpert\n",
            "systemsintheirday. Andsothemerefactthatasetofideaslooksverypromisingdoesn’t\n",
            "meanmuch. Howcanwetellifdeeplearningistrulydifferentfromtheseearlierideas? Is\n",
            "theresomewayofmeasuringhowpowerfulandpromisingasetofideasis? Conway’slaw\n",
            "suggeststhatasaroughandheuristicproxymetricwecanevaluatethecomplexityofthe\n",
            "socialstructureassociatedtothoseideas. So,therearetwoquestionstoask. First,howpowerfulasetofideasareassociatedto\n",
            "deeplearning,accordingtothismetricofsocialcomplexity? Second,howpowerfulatheory\n",
            "willweneed,inordertobeabletobuildageneralartificialintelligence? Astothefirstquestion: whenwelookatdeeplearningtoday,it’sanexcitingandfast-\n",
            "paced but also relatively monolithic field. There are a few deep ideas, and a few main\n",
            "conferences,withsubstantialoverlapbetweenseveraloftheconferences. Andthereispaper\n",
            "afterpaperleveragingthesamebasicsetofideas: usingstochasticgradientdescent(ora\n",
            "closevariation)tooptimizeacostfunction. It’sfantasticthoseideasaresosuccessful. But\n",
            "34Myapologiesforoverloading“deep”.Iwon’tdefine“deepideas”precisely,butlooselyImeanthe\n",
            "kindofideawhichisthebasisforarichfieldofenquiry.Thebackpropagationalgorithmandthegerm\n",
            "theoryofdiseasearebothgoodexamples. 35Interestingly,oftennotbyleadingexpertsindeeplearning,whohavebeenquiterestrained.See,\n",
            "forexample,thisthoughtfulpostbyYannLeCun.Thisisadifferencefrommanyearlierincarnationsof\n",
            "theargument.\n",
            "\n",
            "(cid:12)\n",
            "6.6. Onthefutureofneuralnetworks (cid:12) 209\n",
            "(cid:12)\n",
            "whatwedon’tyetseeislotsofwell-developedsubfields,eachexploringtheirownsetsof\n",
            "deepideas,pushingdeeplearninginmanydirections. Andso,accordingtothemetricof\n",
            "socialcomplexity,deeplearningis,ifyou’llforgivetheplayonwords,stillarathershallow\n",
            "field. It’sstillpossibleforonepersontomastermostofthedeepestideasinthefield. On the second question: how complex and powerful a set of ideas will be needed\n",
            "to obtain AI? Of course, the answer to this question is: no-one knows for sure.\n",
            "But in\n",
            "theappendixIexaminesomeoftheexistingevidenceonthisquestion. Iconcludethat,\n",
            "evenratheroptimistically,it’sgoingtotakemany,manydeepideastobuildanAI.Andso\n",
            "Conway’slawsuggeststhattogettosuchapointwewillnecessarilyseetheemergence\n",
            "ofmanyinterrelatingdisciplines, withacomplexandsurprisingstructuremirroringthe\n",
            "structureinourdeepestinsights. Wedon’tyetseethisrichsocialstructureintheuseof\n",
            "neuralnetworksanddeeplearning. Andso,Ibelievethatweareseveraldecades(atleast)\n",
            "fromusingdeeplearningtodevelopgeneralAI. I’vegonetoalotoftroubletoconstructanargumentwhichistentative,perhapsseems\n",
            "ratherobvious,andwhichhasanindefiniteconclusion. Thiswillnodoubtfrustratepeople\n",
            "who crave certainty. Reading around online, I see many people who loudly assert very\n",
            "definite,verystronglyheldopinionsaboutAI,oftenonthebasisofflimsyreasoningand\n",
            "non-existentevidence. Myfrankopinionisthis: it’stooearlytosay. Astheoldjokegoes,if\n",
            "youaskascientisthowfarawaysomediscoveryisandtheysay“10years”(ormore),what\n",
            "theymeanis“I’vegotnoidea”. AI,likecontrolledfusionandafewothertechnologies,has\n",
            "been10yearsawayfor60plusyears. Ontheflipside,whatwedefinitelydohaveindeep\n",
            "learningisapowerfultechniquewhoselimitshavenotyetbeenfound,andmanywide-open\n",
            "fundamentalproblems. That’sanexcitingcreativeopportunity.\n",
            "\n",
            "(cid:12)\n",
            "210 (cid:12) Deeplearning\n",
            "(cid:12)\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 211\n",
            "(cid:12)\n",
            "AAAA\n",
            "Is there a simple algorithm for\n",
            "intelligence? A\n",
            "Inthisbook,we’vefocusedonthenutsandboltsofneuralnetworks: howtheywork,and\n",
            "howtheycanbeusedtosolvepatternrecognitionproblems.\n",
            "Thisismaterialwithmany\n",
            "immediatepracticalapplications. But,ofcourse,onereasonforinterestinneuralnetsisthe\n",
            "hopethatonedaytheywillgofarbeyondsuchbasicpatternrecognitionproblems. Perhaps\n",
            "they,orsomeotherapproachbasedondigitalcomputers,willeventuallybeusedtobuild\n",
            "thinkingmachines,machinesthatmatchorsurpasshumanintelligence? Thisnotionfar\n",
            "exceedsthematerialdiscussedinthebook–orwhatanyoneintheworldknowshowtodo. Butit’sfuntospeculate. Therehasbeenmuchdebateaboutwhetherit’sevenpossibleforcomputerstomatch\n",
            "humanintelligence. I’mnotgoingtoengagewiththatquestion. Despiteongoingdispute,I\n",
            "believeit’snotinseriousdoubtthatanintelligentcomputerispossible–althoughitmaybe\n",
            "extremelycomplicated,andperhapsfarbeyondcurrenttechnology–andcurrentnaysayers\n",
            "willonedayseemmuchlikethevitalists. Rather,thequestionIexplorehereiswhetherthereisasimplesetofprincipleswhich\n",
            "canbeusedtoexplainintelligence? Inparticular,andmoreconcretely,isthereasimple\n",
            "algorithmforintelligence? Theideathatthereisatrulysimplealgorithmforintelligenceisaboldidea.\n",
            "Itperhaps\n",
            "soundstoooptimistictobetrue. Manypeoplehaveastrongintuitivesensethatintelligence\n",
            "hasconsiderableirreduciblecomplexity. They’resoimpressedbytheamazingvarietyand\n",
            "flexibilityofhumanthoughtthattheyconcludethatasimplealgorithmforintelligencemust\n",
            "beimpossible. Despitethisintuition,Idon’tthinkit’swisetorushtojudgement. Thehistory\n",
            "ofscienceisfilledwithinstanceswhereaphenomenoninitiallyappearedextremelycomplex,\n",
            "butwaslaterexplainedbysomesimplebutpowerfulsetofideas. Consider,forexample,theearlydaysofastronomy. Humanshaveknownsinceancient\n",
            "timesthatthereisamenagerieofobjectsinthesky: thesun,themoon,theplanets,the\n",
            "comets,andthestars. Theseobjectsbehaveinverydifferentways–starsmoveinastately,\n",
            "\n",
            "(cid:12)\n",
            "212 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A regularwayacrossthesky,forexample,whilecometsappearasifoutofnowhere,streak\n",
            "acrossthesky,andthendisappear.\n",
            "Inthe16thcenturyonlyafoolishoptimistcouldhave\n",
            "imaginedthatalltheseobjects’motionscouldbeexplainedbyasimplesetofprinciples. Butinthe17thcenturyNewtonformulatedhistheoryofuniversalgravitation,whichnot\n",
            "onlyexplainedallthesemotions,butalsoexplainedterrestrialphenomenasuchasthetides\n",
            "andthebehaviourofEarth-boundprojecticles. The16thcentury’sfoolishoptimistseemsin\n",
            "retrospectlikeapessimist,askingfortoolittle.\n",
            "Ofcourse,sciencecontainsmanymoresuchexamples. Considerthemyriadchemical\n",
            "substancesmakingupourworld,sobeautifullyexplainedbyMendeleev’speriodictable,\n",
            "whichis,inturn,explainedbyafewsimpleruleswhichmaybeobtainedfromquantum\n",
            "mechanics. Orthepuzzleofhowthereissomuchcomplexityanddiversityinthebiological\n",
            "world,whoseoriginturnsouttolieintheprincipleofevolutionbynaturalselection. These\n",
            "andmanyotherexamplessuggestthatitwouldnotbewisetoruleoutasimpleexplanation\n",
            "ofintelligencemerelyonthegroundsthatwhatourbrains–currentlythebestexamplesof\n",
            "intelligence–aredoingappearstobeverycomplicated1. Contrariwise, anddespitetheseoptimisticexamples, itisalsologicallypossiblethat\n",
            "intelligencecanonlybeexplainedbyalargenumberoffundamentallydistinctmechanisms. Inthecaseofourbrains,thosemanymechanismsmayperhapshaveevolvedinresponse\n",
            "tomanydifferentselectionpressuresinourspecies’evolutionaryhistory. Ifthispointof\n",
            "viewiscorrect,thenintelligenceinvolvesconsiderableirreduciblecomplexity,andnosimple\n",
            "algorithmforintelligenceispossible. Whichofthesetwopointsofviewiscorrect? Togetinsightintothisquestion,let’saskacloselyrelatedquestion,whichiswhether\n",
            "there’sasimpleexplanationofhowhumanbrainswork. Inparticular,let’slookatsomeways\n",
            "ofquantifyingthecomplexityofthebrain. Ourfirstapproachistheviewofthebrainfrom\n",
            "connectomics. Thisisallabouttherawwiring: howmanyneuronsthereareinthebrain,\n",
            "howmanyglialcells,andhowmanyconnectionstherearebetweentheneurons. You’ve\n",
            "probablyheardthenumbersbefore–thebraincontainsontheorderof100billionneurons,\n",
            "100billionglialcells,and100trillionconnectionsbetweenneurons. Thosenumbersare\n",
            "staggering. They’re also intimidating. If we need to understand the details of all those\n",
            "connections(nottomentiontheneuronsandglialcells)inordertounderstandhowthe\n",
            "brainworks,thenwe’recertainlynotgoingtoendupwithasimplealgorithmforintelligence. There’sasecond,moreoptimisticpointofview,theviewofthebrainfrommolecular\n",
            "biology. Theideaistoaskhowmuchgeneticinformationisneededtodescribethebrain’s\n",
            "architecture.Togetahandleonthisquestion,we’llstartbyconsideringthegeneticdifferences\n",
            "betweenhumansandchimpanzees. You’veprobablyheardthesoundbitethat“humanbeings\n",
            "are98percentchimpanzee”. Thissayingissometimesvaried–popularvariationsalsogive\n",
            "thenumberas95or99percent. Thevariationsoccurbecausethenumberswereoriginally\n",
            "estimatedbycomparingsamplesofthehumanandchimpgenomes,nottheentiregenomes. However,in2007theentirechimpanzeegenomewassequenced(seealsohere),andwe\n",
            "nowknowthathumanandchimpDNAdifferatroughly125millionDNAbasepairs. That’s\n",
            "outofatotalofroughly3billionDNAbasepairsineachgenome.\n",
            "Soit’snotrighttosay\n",
            "humanbeingsare98percentchimpanzee–we’remorelike96percentchimpanzee. 1ThroughthisappendixIassumethatforacomputertobeconsideredintelligentitscapabilitiesmust\n",
            "matchorexceedhumanthinkingability.AndsoI’llregardthequestion“Isthereasimplealgorithmfor\n",
            "intelligence?”asequivalentto“Isthereasimplealgorithmwhichcan‘think’alongessentiallythesame\n",
            "linesasthehumanbrain?”It’sworthnoting,however,thattheremaywellbeformsofintelligencethat\n",
            "don’tsubsumehumanthought,butnonethelessgobeyonditininterestingways. \n",
            "(cid:12)\n",
            "(cid:12) 213\n",
            "(cid:12)\n",
            "Howmuchinformationisinthat125millionbasepairs? Eachbasepaircanbelabelled A\n",
            "byoneoffourpossibilities–the“letters”ofthegeneticcode,thebasesadenine,cytosine,\n",
            "guanine,andthymine. Soeachbasepaircanbedescribedusingtwobitsofinformation\n",
            "–justenoughinformationtospecifyoneofthefourlabels.\n",
            "So125millionbasepairsis\n",
            "equivalentto250millionbitsofinformation. That’sthegeneticdifferencebetweenhumans\n",
            "andchimps! Ofcourse,that250millionbitsaccountsforallthegeneticdifferencesbetweenhumans\n",
            "andchimps. We’reonlyinterestedinthedifferenceassociatedtothebrain. Unfortunately,\n",
            "no-oneknowswhatfractionofthetotalgeneticdifferenceisneededtoexplainthedifference\n",
            "between the brains. But let’s assume for the sake of argument that about half that 250\n",
            "millionbitsaccountsforthebraindifferences. That’satotalof125millionbits.\n",
            "125millionbitsisanimpressivelylargenumber. Let’sgetasenseforhowlargeitis\n",
            "bytranslatingitintomorehumanterms. Inparticular,howmuchwouldbeanequivalent\n",
            "amountofEnglishtext? ItturnsoutthattheinformationcontentofEnglishtextisabout\n",
            "1 bit per letter. That sounds low – after all, the alphabet has 26 letters – but there is a\n",
            "tremendousamountofredundancyinEnglishtext. Ofcourse,youmightarguethatour\n",
            "genomesareredundant,too,sotwobitsperbasepairisanoverestimate. Butwe’llignore\n",
            "that,sinceatworstitmeansthatwe’reoverestimatingourbrain’sgeneticcomplexity. With\n",
            "theseassumptions,weseethatthegeneticdifferencebetweenourbrainsandchimpbrains\n",
            "isequivalenttoabout125millionletters,orabout25millionEnglishwords. That’sabout\n",
            "30timesasmuchastheKingJamesBible. That’salotofinformation. Butit’snotincomprehensiblylarge.\n",
            "It’sonahumanscale. Maybenosinglehumancouldeverunderstandallthat’swritteninthatcode,butagroup\n",
            "ofpeoplecouldperhapsunderstanditcollectively,throughappropriatespecialization. And\n",
            "althoughit’salotofinformation,it’sminusculewhencomparedtotheinformationrequired\n",
            "todescribethe100billionneurons,100billionglialcells,and100trillionconnectionsin\n",
            "ourbrains. Evenifweuseasimple,coarsedescription–say,10floatingpointnumbersto\n",
            "characterizeeachconnection–thatwouldrequireabout70quadrillionbits. Thatmeansthe\n",
            "geneticdescriptionisafactorofabouthalfabillionlesscomplexthanthefullconnectome\n",
            "forthehumanbrain. Whatwelearnfromthisisthatourgenomecannotpossiblycontainadetaileddescription\n",
            "ofallourneuralconnections. Rather,itmustspecifyjustthebroadarchitectureandbasic\n",
            "principlesunderlyingthebrain. Butthatarchitectureandthoseprinciplesseemtobeenough\n",
            "toguaranteethatwehumanswillgrowuptobeintelligent. Ofcourse,therearecaveats\n",
            "–growingchildrenneedahealthy,stimulatingenvironmentandgoodnutritiontoachieve\n",
            "theirintellectualpotential. Butprovidedwegrowupinareasonableenvironment,ahealthy\n",
            "human will have remarkable intelligence. In some sense, the information in our genes\n",
            "containstheessenceofhowwethink. Andfurthermore,theprinciplescontainedinthat\n",
            "geneticinformationseemlikelytobewithinourabilitytocollectivelygrasp. Allthenumbersaboveareveryroughestimates. It’spossiblethat125millionbitsis\n",
            "atremendousoverestimate,thatthereissomemuchmorecompactsetofcoreprinciples\n",
            "underlyinghumanthought.Maybemostofthat125millionbitsisjustfine-tuningofrelatively\n",
            "minordetails. Ormaybewewereoverlyconservativeinhowwecomputedthenumbers.\n",
            "Obviously,that’dbegreatifitweretrue! Forourcurrentpurposes,thekeypointisthis: the\n",
            "architectureofthebrainiscomplicated,butit’snotnearlyascomplicatedasyoumightthink\n",
            "basedonthenumberofconnectionsinthebrain. Theviewofthebrainfrommolecular\n",
            "biologysuggestswehumansoughttoonedaybeabletounderstandthebasicprinciples\n",
            "\n",
            "(cid:12)\n",
            "214 (cid:12) Isthereasimplealgorithmforintelligence?\n",
            "(cid:12)\n",
            "A behindthebrain’sarchitecture. InthelastfewparagraphsI’veignoredthefactthat125millionbitsmerelyquantifies\n",
            "the genetic difference between human and chimp brains. Not all our brain function is\n",
            "duetothose125millionbits. Chimpsareremarkablethinkersintheirownright. Maybe\n",
            "the key to intelligence lies mostly in the mental abilities (and genetic information) that\n",
            "chimpsandhumanshaveincommon. Ifthisiscorrect,thenhumanbrainsmightbejusta\n",
            "minorupgradetochimpanzeebrains,atleastintermsofthecomplexityoftheunderlying\n",
            "principles.\n",
            "Despitetheconventionalhumanchauvinismaboutouruniquecapabilities,this\n",
            "isn’tinconceivable: thechimpanzeeandhumangeneticlinesdivergedjust5millionyears\n",
            "ago, a blink in evolutionary timescales. However, in the absence of a more compelling\n",
            "argument,I’msympathetictotheconventionalhumanchauvinism: myguessisthatthe\n",
            "mostinterestingprinciplesunderlyinghumanthoughtlieinthat125millionbits,notinthe\n",
            "partofthegenomewesharewithchimpanzees. Adoptingtheviewofthebrainfrommolecularbiologygaveusareductionofroughly\n",
            "nineordersofmagnitudeinthecomplexityofourdescription. Whileencouraging,itdoesn’t\n",
            "telluswhetherornotatrulysimplealgorithmforintelligenceispossible.\n",
            "Canwegetany\n",
            "furtherreductionsincomplexity? And, moretothepoint, canwesettlethequestionof\n",
            "whetherasimplealgorithmforintelligenceispossible? Unfortunately,thereisn’tyetanyevidencestrongenoughtodecisivelysettlethisques-\n",
            "tion. Letmedescribesomeoftheavailableevidence, withthecaveatthatthisisavery\n",
            "briefandincompleteoverview,meanttoconveytheflavourofsomerecentwork,notto\n",
            "comprehensivelysurveywhatisknown. Amongtheevidencesuggestingthattheremaybeasimplealgorithmforintelligence\n",
            "isanexperimentreportedinApril2000inthejournalNature. Ateamofscientistsledby\n",
            "MrigankaSur“rewired”thebrainsofnewbornferrets. Usually,thesignalfromaferret’s\n",
            "eyesistransmittedtoapartofthebrainknownasthevisualcortex. Butfortheseferrets\n",
            "thescientiststookthesignalfromtheeyesandrerouteditsoitinsteadwenttotheauditory\n",
            "cortex,i.e,thebrainregionthat’susuallyusedforhearing. Tounderstandwhathappenedwhentheydidthis,weneedtoknowabitaboutthevisual\n",
            "cortex. Thevisualcortexcontainsmanyorientationcolumns. Thesearelittleslabsofneurons,\n",
            "eachofwhichrespondstovisualstimulifromsomeparticulardirection. Youcanthinkofthe\n",
            "orientationcolumnsastinydirectionalsensors: whensomeoneshinesabrightlightfrom\n",
            "someparticulardirection,acorrespondingorientationcolumnisactivated. Ifthelightis\n",
            "moved,adifferentorientationcolumnisactivated. Oneofthemostimportanthigh-level\n",
            "structures in the visual cortex is the orientation map, which charts how the orientation\n",
            "columnsarelaidout. Whatthescientistsfoundisthatwhenthevisualsignalfromtheferrets’eyeswasrerouted\n",
            "totheauditorycortex,theauditorycortexchanged. Orientationcolumnsandanorientation\n",
            "mapbegantoemergeintheauditorycortex. Itwasmoredisorderlythantheorientationmap\n",
            "usuallyfoundinthevisualcortex,butunmistakablysimilar. Furthermore,thescientistsdid\n",
            "somesimpletestsofhowtheferretsrespondedtovisualstimuli,trainingthemtorespond\n",
            "differently when lights flashed from different directions. These tests suggested that the\n",
            "ferretscouldstilllearnto“see”,atleastinarudimentaryfashion,usingtheauditorycortex. Thisisanastonishingresult.\n",
            "Itsuggeststhattherearecommonprinciplesunderlying\n",
            "howdifferentpartsofthebrainlearntorespondtosensorydata. Thatcommonalitypro-\n",
            "videsatleastsomesupportfortheideathatthereisasetofsimpleprinciplesunderlying\n",
            "intelligence. However,weshouldn’tkidourselvesabouthowgoodtheferrets’visionwasin\n",
            "\n",
            "(cid:12)\n",
            "(cid:12) 215\n",
            "(cid:12)\n",
            "theseexperiments. Thebehaviouralteststestedonlyverygrossaspectsofvision. And,of A\n",
            "course,wecan’tasktheferretsifthey’ve“learnedtosee”. Sotheexperimentsdon’tprove\n",
            "thattherewiredauditorycortexwasgivingtheferretsahigh-fidelityvisualexperience. And\n",
            "sotheyprovideonlylimitedevidenceinfavouroftheideathatcommonprinciplesunderlie\n",
            "howdifferentpartsofthebrainlearn. Whatevidenceisthereagainsttheideaofasimplealgorithmforintelligence? Some\n",
            "evidencecomesfromthefieldsofevolutionarypsychologyandneuroanatomy. Sincethe\n",
            "1960sevolutionarypsychologistshavediscoveredawiderangeofhumanuniversals,complex\n",
            "behaviourscommontoallhumans,acrossculturesandupbringing. Thesehumanuniversals\n",
            "include the incest taboo between mother and son, the use of music and dance, as well\n",
            "asmuchcomplexlinguisticstructure,suchastheuseofswearwords(i.e.,taboowords),\n",
            "pronouns,andevenstructuresasbasicastheverb. Complementingtheseresults,agreat\n",
            "dealofevidencefromneuroanatomyshowsthatmanyhumanbehavioursarecontrolled\n",
            "byparticularlocalizedareasofthebrain,andthoseareasseemtobesimilarinallpeople.\n",
            "Takentogether,thesefindingssuggestthatmanyveryspecializedbehavioursarehardwired\n",
            "intoparticularpartsofourbrains. Somepeopleconcludefromtheseresultsthatseparateexplanationsmustberequiredfor\n",
            "thesemanybrainfunctions,andthatasaconsequencethereisanirreduciblecomplexityto\n",
            "thebrain’sfunction,acomplexitythatmakesasimpleexplanationforthebrain’soperation\n",
            "(and, perhaps, a simple algorithm for intelligence) impossible. For example, one well-\n",
            "knownartificialintelligenceresearcherwiththispointofviewisMarvinMinsky. Inthe\n",
            "1970sand1980sMinskydevelopedhis“SocietyofMind”theory,basedontheideathat\n",
            "humanintelligenceistheresultofalargesocietyofindividuallysimple(butverydifferent)\n",
            "computationalprocesseswhichMinskycallsagents. Inhisbookdescribingthetheory,Minsky\n",
            "sumsupwhatheseesasthepowerofthispointofview:\n",
            "Whatmagicaltrickmakesusintelligent? Thetrickisthatthereisnotrick.\n",
            "Thepower\n",
            "ofintelligencestemsfromourvastdiversity,notfromanysingle,perfectprinciple. Ina\n",
            "response 2 to reviews of his book, Minsky elaborated on the motivation for the Society\n",
            "of Mind, giving an argument similar to that stated above, based on neuroanatomy and\n",
            "evolutionarypsychology:\n",
            "Wenowknowthatthebrainitselfiscomposedofhundredsofdifferent\n",
            "regionsandnuclei,eachwithsignificantlydifferentarchitecturalelements\n",
            "andarrangements,andthatmanyofthemareinvolvedwithdemonstrably\n",
            "differentaspectsofourmentalactivities. Thismodernmassofknowledge\n",
            "showsthatmanyphenomenatraditionallydescribedbycommonsense\n",
            "terms like “intelligence” or “understanding” actually involve complex\n",
            "assembliesofmachinery. Minsky is, of course, not the only person to hold a point of view along these lines; I’m\n",
            "merelygivinghimasanexampleofasupporterofthislineofargument. Ifindtheargument\n",
            "interesting, but don’t believe the evidence is compelling. While it’s true that the brain\n",
            "iscomposedofalargenumberofdifferentregions, withdifferentfunctions, itdoesnot\n",
            "thereforefollowthatasimpleexplanationforthebrain’sfunctionisimpossible. Perhaps\n",
            "those architectural differences arise out of common underlying principles, much as the\n",
            "motionofcomets,theplanets,thesunandthestarsallarisefromasinglegravitationalforce. NeitherMinskynoranyoneelsehasarguedconvincinglyagainstsuchunderlyingprinciples.\n",
            "2InContemplatingMinds:AForumforArtificialIntelligence,editedbyWilliamJ.Clancey,Stephen\n",
            "W.Smoliar,andMarkStefik(MITPress,1994). \n",
            "(cid:12)\n",
            "216 (cid:12) Isthereasimplealgorithmforintelligence? (cid:12)\n",
            "A Myownprejudiceisinfavouroftherebeingasimplealgorithmforintelligence. And\n",
            "themainreasonIliketheidea,aboveandbeyondthe(inconclusive)argumentsabove,is\n",
            "thatit’sanoptimisticidea. Whenitcomestoresearch,anunjustifiedoptimismisoftenmore\n",
            "productivethanaseeminglybetterjustifiedpessimism,foranoptimisthasthecourageto\n",
            "setoutandtrynewthings. That’sthepathtodiscovery,evenifwhatisdiscoveredisperhaps\n",
            "notwhatwasoriginallyhoped. Apessimistmaybemore“correct”insomenarrowsense,\n",
            "butwilldiscoverlessthantheoptimist. Thispointofviewisinstarkcontrasttothewayweusuallyjudgeideas: byattempting\n",
            "tofigureoutwhethertheyarerightorwrong. That’sasensiblestrategyfordealingwith\n",
            "theroutineminutiaeofday-to-dayresearch. Butitcanbethewrongwayofjudgingabig,\n",
            "boldidea,thesortofideathatdefinesanentireresearchprogram. Sometimes,wehaveonly\n",
            "weakevidenceaboutwhethersuchanideaiscorrectornot. Wecanmeeklyrefusetofollow\n",
            "theidea,insteadspendingallourtimesquintingattheavailableevidence,tryingtodiscern\n",
            "what’strue. Orwecanacceptthatno-oneyetknows,andinsteadworkhardondeveloping\n",
            "thebig,boldidea,intheunderstandingthatwhilewehavenoguaranteeofsuccess,itis\n",
            "onlythusthatourunderstandingadvances. Withallthatsaid,initsmostoptimisticform,Idon’tbelievewe’lleverfindasimple\n",
            "algorithmforintelligence. Tobemoreconcrete,Idon’tbelievewe’lleverfindareallyshort\n",
            "Python(orCorLisp,orwhatever)program–let’ssay,anywhereuptoathousandlines\n",
            "ofcode–whichimplementsartificialintelligence. NordoIthinkwe’lleverfindareally\n",
            "easily-describedneuralnetworkthatcanimplementartificialintelligence.\n",
            "ButIdobelieve\n",
            "it’sworthactingasthoughwecouldfindsuchaprogramornetwork. That’sthepathto\n",
            "insight,andbypursuingthatpathwemayonedayunderstandenoughtowritealonger\n",
            "programorbuildamoresophisticatednetworkwhichdoesexhibitintelligence. Andsoit’s\n",
            "worthactingasthoughanextremelysimplealgorithmforintelligenceexists. Inthe1980s,theeminentmathematicianandcomputerscientistJackSchwartzwas\n",
            "invited to a debate between artificial intelligence proponents and artificial intelligence\n",
            "skeptics. Thedebatebecameunruly,withtheproponentsmakingover-the-topclaimsabout\n",
            "theamazingthingsjustroundthecorner,andtheskepticsdoublingdownontheirpessimism,\n",
            "claimingartificialintelligencewasoutrightimpossible.\n",
            "Schwartzwasanoutsidertothe\n",
            "debate,andremainedsilentasthediscussionheatedup. Duringalull,hewasaskedto\n",
            "speakupandstatehisthoughtsontheissuesunderdiscussion. Hesaid: “Well,someof\n",
            "thesedevelopmentsmaylieonehundredNobelprizesaway”(ref,page22). Itseemstome\n",
            "aperfectresponse.\n",
            "Thekeytoartificialintelligenceissimple,powerfulideas,andwecan\n",
            "andshouldsearchoptimisticallyforthoseideas. Butwe’regoingtoneedmanysuchideas,\n",
            "andwe’vestillgotalongwaytogo! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a chatbot interface\n",
        "print(\"Chat with the PDF content (type 'exit', 'quit', or 'bye' to end):\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYou: \")\n",
        "    if query.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    response = qa.invoke({\"query\": query})\n",
        "    print(\"\\nAssistant:\", response['result'])"
      ],
      "metadata": {
        "id": "oc9D164m8zcQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}